{
  "query": "内核态切换优化",
  "timestamp": "2025-12-26 02:10:23",
  "retrieved_files": [
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.6031193733215332,
      "chunks": [
        {
          "chunk_id": 24,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4444,
          "end_line": 4559,
          "content": [
            "static void __init",
            "rcu_boot_init_percpu_data(int cpu)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\trdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);",
            "\tINIT_WORK(&rdp->strict_work, strict_work_handler);",
            "\tWARN_ON_ONCE(ct->dynticks_nesting != 1);",
            "\tWARN_ON_ONCE(rcu_dynticks_in_eqs(rcu_dynticks_snap(cpu)));",
            "\trdp->barrier_seq_snap = rcu_state.barrier_sequence;",
            "\trdp->rcu_ofl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_ofl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->rcu_onl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_onl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->last_sched_clock = jiffies;",
            "\trdp->cpu = cpu;",
            "\trcu_boot_init_nocb_percpu_data(rdp);",
            "}",
            "int rcutree_prepare_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct context_tracking *ct = per_cpu_ptr(&context_tracking, cpu);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trdp->qlen_last_fqs_check = 0;",
            "\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\trdp->blimit = blimit;",
            "\tct->dynticks_nesting = 1;\t/* CPU not up, no tearing. */",
            "\traw_spin_unlock_rcu_node(rnp);\t\t/* irqs remain disabled. */",
            "",
            "\t/*",
            "\t * Only non-NOCB CPUs that didn't have early-boot callbacks need to be",
            "\t * (re-)initialized.",
            "\t */",
            "\tif (!rcu_segcblist_is_enabled(&rdp->cblist))",
            "\t\trcu_segcblist_init(&rdp->cblist);  /* Re-enable callbacks. */",
            "",
            "\t/*",
            "\t * Add CPU to leaf rcu_node pending-online bitmask.  Any needed",
            "\t * propagation up the rcu_node tree will happen at the beginning",
            "\t * of the next grace period.",
            "\t */",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp);\t\t/* irqs already disabled. */",
            "\trdp->gp_seq = READ_ONCE(rnp->gp_seq);",
            "\trdp->gp_seq_needed = rdp->gp_seq;",
            "\trdp->cpu_no_qs.b.norm = true;",
            "\trdp->core_needs_qs = false;",
            "\trdp->rcu_iw_pending = false;",
            "\trdp->rcu_iw = IRQ_WORK_INIT_HARD(rcu_iw_handler);",
            "\trdp->rcu_iw_gp_seq = rdp->gp_seq - 1;",
            "\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuonl\"));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcu_preempt_deferred_qs_init(rdp);",
            "\trcu_spawn_one_boost_kthread(rnp);",
            "\trcu_spawn_cpu_nocb_kthread(cpu);",
            "\tWRITE_ONCE(rcu_state.n_online_cpus, rcu_state.n_online_cpus + 1);",
            "",
            "\treturn 0;",
            "}",
            "static void rcutree_affinity_setting(unsigned int cpu, int outgoing)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\trcu_boost_kthread_setaffinity(rdp->mynode, outgoing);",
            "}",
            "bool rcu_cpu_beenfullyonline(int cpu)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\treturn smp_load_acquire(&rdp->beenonline);",
            "}",
            "int rcutree_online_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask |= rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_INACTIVE)",
            "\t\treturn 0; /* Too early in boot for scheduler work. */",
            "\tsync_sched_exp_online_cleanup(cpu);",
            "\trcutree_affinity_setting(cpu, -1);",
            "",
            "\t// Stop-machine done, so allow nohz_full to disable tick.",
            "\ttick_dep_clear(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}",
            "int rcutree_offline_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask &= ~rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcutree_affinity_setting(cpu, cpu);",
            "",
            "\t// nohz_full CPUs need the tick for stop-machine to work quickly",
            "\ttick_dep_set(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rcu_boot_init_percpu_data, rcutree_prepare_cpu, rcutree_affinity_setting, rcu_cpu_beenfullyonline, rcutree_online_cpu, rcutree_offline_cpu",
          "description": "初始化每个CPU的RCU私有数据结构，处理CPU上线/下线时的RCU状态同步，配置中断亲和性，更新全局在线CPU计数器",
          "similarity": 0.5947232246398926
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1354,
          "end_line": 1556,
          "content": [
            "static void rcu_strict_gp_boundary(void *unused)",
            "{",
            "\tinvoke_rcu_core();",
            "}",
            "static void rcu_poll_gp_seq_start(unsigned long *snap)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t// If RCU was idle, note beginning of GP.",
            "\tif (!rcu_seq_state(rcu_state.gp_seq_polled))",
            "\t\trcu_seq_start(&rcu_state.gp_seq_polled);",
            "",
            "\t// Either way, record current state.",
            "\t*snap = rcu_state.gp_seq_polled;",
            "}",
            "static void rcu_poll_gp_seq_end(unsigned long *snap)",
            "{",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t// If the previously noted GP is still in effect, record the",
            "\t// end of that GP.  Either way, zero counter to avoid counter-wrap",
            "\t// problems.",
            "\tif (*snap && *snap == rcu_state.gp_seq_polled) {",
            "\t\trcu_seq_end(&rcu_state.gp_seq_polled);",
            "\t\trcu_state.gp_seq_polled_snap = 0;",
            "\t\trcu_state.gp_seq_polled_exp_snap = 0;",
            "\t} else {",
            "\t\t*snap = 0;",
            "\t}",
            "}",
            "static void rcu_poll_gp_seq_start_unlocked(unsigned long *snap)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_init_invoked()) {",
            "\t\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t}",
            "\trcu_poll_gp_seq_start(snap);",
            "\tif (rcu_init_invoked())",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "}",
            "static void rcu_poll_gp_seq_end_unlocked(unsigned long *snap)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tif (rcu_init_invoked()) {",
            "\t\tif (rcu_scheduler_active != RCU_SCHEDULER_INACTIVE)",
            "\t\t\tlockdep_assert_irqs_enabled();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t}",
            "\trcu_poll_gp_seq_end(snap);",
            "\tif (rcu_init_invoked())",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "}",
            "static noinline_for_stack bool rcu_gp_init(void)",
            "{",
            "\tunsigned long flags;",
            "\tunsigned long oldmask;",
            "\tunsigned long mask;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\traw_spin_lock_irq_rcu_node(rnp);",
            "\tif (!READ_ONCE(rcu_state.gp_flags)) {",
            "\t\t/* Spurious wakeup, tell caller to go back to sleep.  */",
            "\t\traw_spin_unlock_irq_rcu_node(rnp);",
            "\t\treturn false;",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags, 0); /* Clear all flags: New GP. */",
            "",
            "\tif (WARN_ON_ONCE(rcu_gp_in_progress())) {",
            "\t\t/*",
            "\t\t * Grace period already in progress, don't start another.",
            "\t\t * Not supposed to be able to happen.",
            "\t\t */",
            "\t\traw_spin_unlock_irq_rcu_node(rnp);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/* Advance to a new grace period and initialize state. */",
            "\trecord_gp_stall_check_time();",
            "\t/* Record GP times before starting GP, hence rcu_seq_start(). */",
            "\trcu_seq_start(&rcu_state.gp_seq);",
            "\tASSERT_EXCLUSIVE_WRITER(rcu_state.gp_seq);",
            "\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq, TPS(\"start\"));",
            "\trcu_poll_gp_seq_start(&rcu_state.gp_seq_polled_snap);",
            "\traw_spin_unlock_irq_rcu_node(rnp);",
            "",
            "\t/*",
            "\t * Apply per-leaf buffered online and offline operations to",
            "\t * the rcu_node tree. Note that this new grace period need not",
            "\t * wait for subsequent online CPUs, and that RCU hooks in the CPU",
            "\t * offlining path, when combined with checks in this function,",
            "\t * will handle CPUs that are currently going offline or that will",
            "\t * go offline later.  Please also refer to \"Hotplug CPU\" section",
            "\t * of RCU's Requirements documentation.",
            "\t */",
            "\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_ONOFF);",
            "\t/* Exclude CPU hotplug operations. */",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tlocal_irq_save(flags);",
            "\t\tarch_spin_lock(&rcu_state.ofl_lock);",
            "\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (rnp->qsmaskinit == rnp->qsmaskinitnext &&",
            "\t\t    !rnp->wait_blkd_tasks) {",
            "\t\t\t/* Nothing to do on this leaf rcu_node structure. */",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\t\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\t\t\tlocal_irq_restore(flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/* Record old state, apply changes to ->qsmaskinit field. */",
            "\t\toldmask = rnp->qsmaskinit;",
            "\t\trnp->qsmaskinit = rnp->qsmaskinitnext;",
            "",
            "\t\t/* If zero-ness of ->qsmaskinit changed, propagate up tree. */",
            "\t\tif (!oldmask != !rnp->qsmaskinit) {",
            "\t\t\tif (!oldmask) { /* First online CPU for rcu_node. */",
            "\t\t\t\tif (!rnp->wait_blkd_tasks) /* Ever offline? */",
            "\t\t\t\t\trcu_init_new_rnp(rnp);",
            "\t\t\t} else if (rcu_preempt_has_tasks(rnp)) {",
            "\t\t\t\trnp->wait_blkd_tasks = true; /* blocked tasks */",
            "\t\t\t} else { /* Last offline CPU and can propagate. */",
            "\t\t\t\trcu_cleanup_dead_rnp(rnp);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If all waited-on tasks from prior grace period are",
            "\t\t * done, and if all this rcu_node structure's CPUs are",
            "\t\t * still offline, propagate up the rcu_node tree and",
            "\t\t * clear ->wait_blkd_tasks.  Otherwise, if one of this",
            "\t\t * rcu_node structure's CPUs has since come back online,",
            "\t\t * simply clear ->wait_blkd_tasks.",
            "\t\t */",
            "\t\tif (rnp->wait_blkd_tasks &&",
            "\t\t    (!rcu_preempt_has_tasks(rnp) || rnp->qsmaskinit)) {",
            "\t\t\trnp->wait_blkd_tasks = false;",
            "\t\t\tif (!rnp->qsmaskinit)",
            "\t\t\t\trcu_cleanup_dead_rnp(rnp);",
            "\t\t}",
            "",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tarch_spin_unlock(&rcu_state.ofl_lock);",
            "\t\tlocal_irq_restore(flags);",
            "\t}",
            "\trcu_gp_slow(gp_preinit_delay); /* Races with CPU hotplug. */",
            "",
            "\t/*",
            "\t * Set the quiescent-state-needed bits in all the rcu_node",
            "\t * structures for all currently online CPUs in breadth-first",
            "\t * order, starting from the root rcu_node structure, relying on the",
            "\t * layout of the tree within the rcu_state.node[] array.  Note that",
            "\t * other CPUs will access only the leaves of the hierarchy, thus",
            "\t * seeing that no grace period is in progress, at least until the",
            "\t * corresponding leaf node has been initialized.",
            "\t *",
            "\t * The grace period cannot complete until the initialization",
            "\t * process finishes, because this kthread handles both.",
            "\t */",
            "\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_INIT);",
            "\trcu_for_each_node_breadth_first(rnp) {",
            "\t\trcu_gp_slow(gp_init_delay);",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trdp = this_cpu_ptr(&rcu_data);",
            "\t\trcu_preempt_check_blocked_tasks(rnp);",
            "\t\trnp->qsmask = rnp->qsmaskinit;",
            "\t\tWRITE_ONCE(rnp->gp_seq, rcu_state.gp_seq);",
            "\t\tif (rnp == rdp->mynode)",
            "\t\t\t(void)__note_gp_changes(rnp, rdp);",
            "\t\trcu_preempt_boost_start_gp(rnp);",
            "\t\ttrace_rcu_grace_period_init(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t    rnp->level, rnp->grplo,",
            "\t\t\t\t\t    rnp->grphi, rnp->qsmask);",
            "\t\t/* Quiescent states for tasks on any now-offline CPUs. */",
            "\t\tmask = rnp->qsmask & ~rnp->qsmaskinitnext;",
            "\t\trnp->rcu_gp_init_mask = mask;",
            "\t\tif ((mask || rnp->wait_blkd_tasks) && rcu_is_leaf_node(rnp))",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\telse",
            "\t\t\traw_spin_unlock_irq_rcu_node(rnp);",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t}",
            "",
            "\t// If strict, make all CPUs aware of new grace period.",
            "\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))",
            "\t\ton_each_cpu(rcu_strict_gp_boundary, NULL, 0);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "rcu_strict_gp_boundary, rcu_poll_gp_seq_start, rcu_poll_gp_seq_end, rcu_poll_gp_seq_start_unlocked, rcu_poll_gp_seq_end_unlocked, rcu_gp_init",
          "description": "初始化新grace period并处理CPU热插拔，通过遍历rcu_node树更新qsmask字段，设置严格模式下的全局通知，并协调初始化过程与后续处理。",
          "similarity": 0.5936099886894226
        },
        {
          "chunk_id": 26,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4752,
          "end_line": 4863,
          "content": [
            "static int rcu_pm_notify(struct notifier_block *self,",
            "\t\t\t unsigned long action, void *hcpu)",
            "{",
            "\tswitch (action) {",
            "\tcase PM_HIBERNATION_PREPARE:",
            "\tcase PM_SUSPEND_PREPARE:",
            "\t\trcu_async_hurry();",
            "\t\trcu_expedite_gp();",
            "\t\tbreak;",
            "\tcase PM_POST_HIBERNATION:",
            "\tcase PM_POST_SUSPEND:",
            "\t\trcu_unexpedite_gp();",
            "\t\trcu_async_relax();",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "\treturn NOTIFY_OK;",
            "}",
            "static void __init rcu_start_exp_gp_kworkers(void)",
            "{",
            "\tconst char *par_gp_kworker_name = \"rcu_exp_par_gp_kthread_worker\";",
            "\tconst char *gp_kworker_name = \"rcu_exp_gp_kthread_worker\";",
            "\tstruct sched_param param = { .sched_priority = kthread_prio };",
            "",
            "\trcu_exp_gp_kworker = kthread_create_worker(0, gp_kworker_name);",
            "\tif (IS_ERR_OR_NULL(rcu_exp_gp_kworker)) {",
            "\t\tpr_err(\"Failed to create %s!\\n\", gp_kworker_name);",
            "\t\trcu_exp_gp_kworker = NULL;",
            "\t\treturn;",
            "\t}",
            "",
            "\trcu_exp_par_gp_kworker = kthread_create_worker(0, par_gp_kworker_name);",
            "\tif (IS_ERR_OR_NULL(rcu_exp_par_gp_kworker)) {",
            "\t\tpr_err(\"Failed to create %s!\\n\", par_gp_kworker_name);",
            "\t\trcu_exp_par_gp_kworker = NULL;",
            "\t\tkthread_destroy_worker(rcu_exp_gp_kworker);",
            "\t\trcu_exp_gp_kworker = NULL;",
            "\t\treturn;",
            "\t}",
            "",
            "\tsched_setscheduler_nocheck(rcu_exp_gp_kworker->task, SCHED_FIFO, &param);",
            "\tsched_setscheduler_nocheck(rcu_exp_par_gp_kworker->task, SCHED_FIFO,",
            "\t\t\t\t   &param);",
            "}",
            "static inline void rcu_alloc_par_gp_wq(void)",
            "{",
            "}",
            "static void __init rcu_start_exp_gp_kworkers(void)",
            "{",
            "}",
            "static inline void rcu_alloc_par_gp_wq(void)",
            "{",
            "\trcu_par_gp_wq = alloc_workqueue(\"rcu_par_gp\", WQ_MEM_RECLAIM, 0);",
            "\tWARN_ON(!rcu_par_gp_wq);",
            "}",
            "static int __init rcu_spawn_gp_kthread(void)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "\tstruct sched_param sp;",
            "\tstruct task_struct *t;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\trcu_scheduler_fully_active = 1;",
            "\tt = kthread_create(rcu_gp_kthread, NULL, \"%s\", rcu_state.name);",
            "\tif (WARN_ONCE(IS_ERR(t), \"%s: Could not start grace-period kthread, OOM is now expected behavior\\n\", __func__))",
            "\t\treturn 0;",
            "\tif (kthread_prio) {",
            "\t\tsp.sched_priority = kthread_prio;",
            "\t\tsched_setscheduler_nocheck(t, SCHED_FIFO, &sp);",
            "\t}",
            "\trnp = rcu_get_root();",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\t// Reset .gp_activity and .gp_req_activity before setting .gp_kthread.",
            "\tsmp_store_release(&rcu_state.gp_kthread, t);  /* ^^^ */",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\twake_up_process(t);",
            "\t/* This is a pre-SMP initcall, we expect a single CPU */",
            "\tWARN_ON(num_online_cpus() > 1);",
            "\t/*",
            "\t * Those kthreads couldn't be created on rcu_init() -> rcutree_prepare_cpu()",
            "\t * due to rcu_scheduler_fully_active.",
            "\t */",
            "\trcu_spawn_cpu_nocb_kthread(smp_processor_id());",
            "\trcu_spawn_one_boost_kthread(rdp->mynode);",
            "\trcu_spawn_core_kthreads();",
            "\t/* Create kthread worker for expedited GPs */",
            "\trcu_start_exp_gp_kworkers();",
            "\treturn 0;",
            "}",
            "void rcu_scheduler_starting(void)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tWARN_ON(num_online_cpus() != 1);",
            "\tWARN_ON(nr_context_switches() > 0);",
            "\trcu_test_sync_prims();",
            "",
            "\t// Fix up the ->gp_seq counters.",
            "\tlocal_irq_save(flags);",
            "\trcu_for_each_node_breadth_first(rnp)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq = rcu_state.gp_seq;",
            "\tlocal_irq_restore(flags);",
            "",
            "\t// Switch out of early boot mode.",
            "\trcu_scheduler_active = RCU_SCHEDULER_INIT;",
            "\trcu_test_sync_prims();",
            "}"
          ],
          "function_name": "rcu_pm_notify, rcu_start_exp_gp_kworkers, rcu_alloc_par_gp_wq, rcu_start_exp_gp_kworkers, rcu_alloc_par_gp_wq, rcu_spawn_gp_kthread, rcu_scheduler_starting",
          "description": "管理系统休眠唤醒时的RCU急迫性切换，创建并配置用于处理急迫GRACE周期的内核线程工作者，同时初始化相关资源和调度参数。",
          "similarity": 0.5931375026702881
        },
        {
          "chunk_id": 27,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4895,
          "end_line": 5082,
          "content": [
            "static void __init rcu_init_one(void)",
            "{",
            "\tstatic const char * const buf[] = RCU_NODE_NAME_INIT;",
            "\tstatic const char * const fqs[] = RCU_FQS_NAME_INIT;",
            "\tstatic struct lock_class_key rcu_node_class[RCU_NUM_LVLS];",
            "\tstatic struct lock_class_key rcu_fqs_class[RCU_NUM_LVLS];",
            "",
            "\tint levelspread[RCU_NUM_LVLS];\t\t/* kids/node in each level. */",
            "\tint cpustride = 1;",
            "\tint i;",
            "\tint j;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tBUILD_BUG_ON(RCU_NUM_LVLS > ARRAY_SIZE(buf));  /* Fix buf[] init! */",
            "",
            "\t/* Silence gcc 4.8 false positive about array index out of range. */",
            "\tif (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)",
            "\t\tpanic(\"rcu_init_one: rcu_num_lvls out of range\");",
            "",
            "\t/* Initialize the level-tracking arrays. */",
            "",
            "\tfor (i = 1; i < rcu_num_lvls; i++)",
            "\t\trcu_state.level[i] =",
            "\t\t\trcu_state.level[i - 1] + num_rcu_lvl[i - 1];",
            "\trcu_init_levelspread(levelspread, num_rcu_lvl);",
            "",
            "\t/* Initialize the elements themselves, starting from the leaves. */",
            "",
            "\tfor (i = rcu_num_lvls - 1; i >= 0; i--) {",
            "\t\tcpustride *= levelspread[i];",
            "\t\trnp = rcu_state.level[i];",
            "\t\tfor (j = 0; j < num_rcu_lvl[i]; j++, rnp++) {",
            "\t\t\traw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));",
            "\t\t\tlockdep_set_class_and_name(&ACCESS_PRIVATE(rnp, lock),",
            "\t\t\t\t\t\t   &rcu_node_class[i], buf[i]);",
            "\t\t\traw_spin_lock_init(&rnp->fqslock);",
            "\t\t\tlockdep_set_class_and_name(&rnp->fqslock,",
            "\t\t\t\t\t\t   &rcu_fqs_class[i], fqs[i]);",
            "\t\t\trnp->gp_seq = rcu_state.gp_seq;",
            "\t\t\trnp->gp_seq_needed = rcu_state.gp_seq;",
            "\t\t\trnp->completedqs = rcu_state.gp_seq;",
            "\t\t\trnp->qsmask = 0;",
            "\t\t\trnp->qsmaskinit = 0;",
            "\t\t\trnp->grplo = j * cpustride;",
            "\t\t\trnp->grphi = (j + 1) * cpustride - 1;",
            "\t\t\tif (rnp->grphi >= nr_cpu_ids)",
            "\t\t\t\trnp->grphi = nr_cpu_ids - 1;",
            "\t\t\tif (i == 0) {",
            "\t\t\t\trnp->grpnum = 0;",
            "\t\t\t\trnp->grpmask = 0;",
            "\t\t\t\trnp->parent = NULL;",
            "\t\t\t} else {",
            "\t\t\t\trnp->grpnum = j % levelspread[i - 1];",
            "\t\t\t\trnp->grpmask = BIT(rnp->grpnum);",
            "\t\t\t\trnp->parent = rcu_state.level[i - 1] +",
            "\t\t\t\t\t      j / levelspread[i - 1];",
            "\t\t\t}",
            "\t\t\trnp->level = i;",
            "\t\t\tINIT_LIST_HEAD(&rnp->blkd_tasks);",
            "\t\t\trcu_init_one_nocb(rnp);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[0]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[1]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[2]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[3]);",
            "\t\t\tspin_lock_init(&rnp->exp_lock);",
            "\t\t\tmutex_init(&rnp->boost_kthread_mutex);",
            "\t\t\traw_spin_lock_init(&rnp->exp_poll_lock);",
            "\t\t\trnp->exp_seq_poll_rq = RCU_GET_STATE_COMPLETED;",
            "\t\t\tINIT_WORK(&rnp->exp_poll_wq, sync_rcu_do_polled_gp);",
            "\t\t}",
            "\t}",
            "",
            "\tinit_swait_queue_head(&rcu_state.gp_wq);",
            "\tinit_swait_queue_head(&rcu_state.expedited_wq);",
            "\trnp = rcu_first_leaf_node();",
            "\tfor_each_possible_cpu(i) {",
            "\t\twhile (i > rnp->grphi)",
            "\t\t\trnp++;",
            "\t\tper_cpu_ptr(&rcu_data, i)->mynode = rnp;",
            "\t\trcu_boot_init_percpu_data(i);",
            "\t}",
            "}",
            "static void __init sanitize_kthread_prio(void)",
            "{",
            "\tint kthread_prio_in = kthread_prio;",
            "",
            "\tif (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 2",
            "\t    && IS_BUILTIN(CONFIG_RCU_TORTURE_TEST))",
            "\t\tkthread_prio = 2;",
            "\telse if (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 1)",
            "\t\tkthread_prio = 1;",
            "\telse if (kthread_prio < 0)",
            "\t\tkthread_prio = 0;",
            "\telse if (kthread_prio > 99)",
            "\t\tkthread_prio = 99;",
            "",
            "\tif (kthread_prio != kthread_prio_in)",
            "\t\tpr_alert(\"%s: Limited prio to %d from %d\\n\",",
            "\t\t\t __func__, kthread_prio, kthread_prio_in);",
            "}",
            "void rcu_init_geometry(void)",
            "{",
            "\tulong d;",
            "\tint i;",
            "\tstatic unsigned long old_nr_cpu_ids;",
            "\tint rcu_capacity[RCU_NUM_LVLS];",
            "\tstatic bool initialized;",
            "",
            "\tif (initialized) {",
            "\t\t/*",
            "\t\t * Warn if setup_nr_cpu_ids() had not yet been invoked,",
            "\t\t * unless nr_cpus_ids == NR_CPUS, in which case who cares?",
            "\t\t */",
            "\t\tWARN_ON_ONCE(old_nr_cpu_ids != nr_cpu_ids);",
            "\t\treturn;",
            "\t}",
            "",
            "\told_nr_cpu_ids = nr_cpu_ids;",
            "\tinitialized = true;",
            "",
            "\t/*",
            "\t * Initialize any unspecified boot parameters.",
            "\t * The default values of jiffies_till_first_fqs and",
            "\t * jiffies_till_next_fqs are set to the RCU_JIFFIES_TILL_FORCE_QS",
            "\t * value, which is a function of HZ, then adding one for each",
            "\t * RCU_JIFFIES_FQS_DIV CPUs that might be on the system.",
            "\t */",
            "\td = RCU_JIFFIES_TILL_FORCE_QS + nr_cpu_ids / RCU_JIFFIES_FQS_DIV;",
            "\tif (jiffies_till_first_fqs == ULONG_MAX)",
            "\t\tjiffies_till_first_fqs = d;",
            "\tif (jiffies_till_next_fqs == ULONG_MAX)",
            "\t\tjiffies_till_next_fqs = d;",
            "\tadjust_jiffies_till_sched_qs();",
            "",
            "\t/* If the compile-time values are accurate, just leave. */",
            "\tif (rcu_fanout_leaf == RCU_FANOUT_LEAF &&",
            "\t    nr_cpu_ids == NR_CPUS)",
            "\t\treturn;",
            "\tpr_info(\"Adjusting geometry for rcu_fanout_leaf=%d, nr_cpu_ids=%u\\n\",",
            "\t\trcu_fanout_leaf, nr_cpu_ids);",
            "",
            "\t/*",
            "\t * The boot-time rcu_fanout_leaf parameter must be at least two",
            "\t * and cannot exceed the number of bits in the rcu_node masks.",
            "\t * Complain and fall back to the compile-time values if this",
            "\t * limit is exceeded.",
            "\t */",
            "\tif (rcu_fanout_leaf < 2 ||",
            "\t    rcu_fanout_leaf > sizeof(unsigned long) * 8) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Compute number of nodes that can be handled an rcu_node tree",
            "\t * with the given number of levels.",
            "\t */",
            "\trcu_capacity[0] = rcu_fanout_leaf;",
            "\tfor (i = 1; i < RCU_NUM_LVLS; i++)",
            "\t\trcu_capacity[i] = rcu_capacity[i - 1] * RCU_FANOUT;",
            "",
            "\t/*",
            "\t * The tree must be able to accommodate the configured number of CPUs.",
            "\t * If this limit is exceeded, fall back to the compile-time values.",
            "\t */",
            "\tif (nr_cpu_ids > rcu_capacity[RCU_NUM_LVLS - 1]) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Calculate the number of levels in the tree. */",
            "\tfor (i = 0; nr_cpu_ids > rcu_capacity[i]; i++) {",
            "\t}",
            "\trcu_num_lvls = i + 1;",
            "",
            "\t/* Calculate the number of rcu_nodes at each level of the tree. */",
            "\tfor (i = 0; i < rcu_num_lvls; i++) {",
            "\t\tint cap = rcu_capacity[(rcu_num_lvls - 1) - i];",
            "\t\tnum_rcu_lvl[i] = DIV_ROUND_UP(nr_cpu_ids, cap);",
            "\t}",
            "",
            "\t/* Calculate the total number of rcu_node structures. */",
            "\trcu_num_nodes = 0;",
            "\tfor (i = 0; i < rcu_num_lvls; i++)",
            "\t\trcu_num_nodes += num_rcu_lvl[i];",
            "}"
          ],
          "function_name": "rcu_init_one, sanitize_kthread_prio, rcu_init_geometry",
          "description": "构建多级RCU节点树结构，初始化各层级的锁类和节点属性，动态调整RCU树的几何形态以适配当前CPU数量和层级分布需求。",
          "similarity": 0.5820034146308899
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1221,
          "end_line": 1330,
          "content": [
            "static bool __note_gp_changes(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\tbool ret = false;",
            "\tbool need_qs;",
            "\tconst bool offloaded = rcu_rdp_is_offloaded(rdp);",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\tif (rdp->gp_seq == rnp->gp_seq)",
            "\t\treturn false; /* Nothing to do. */",
            "",
            "\t/* Handle the ends of any preceding grace periods first. */",
            "\tif (rcu_seq_completed_gp(rdp->gp_seq, rnp->gp_seq) ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) {",
            "\t\tif (!offloaded)",
            "\t\t\tret = rcu_advance_cbs(rnp, rdp); /* Advance CBs. */",
            "\t\trdp->core_needs_qs = false;",
            "\t\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuend\"));",
            "\t} else {",
            "\t\tif (!offloaded)",
            "\t\t\tret = rcu_accelerate_cbs(rnp, rdp); /* Recent CBs. */",
            "\t\tif (rdp->core_needs_qs)",
            "\t\t\trdp->core_needs_qs = !!(rnp->qsmask & rdp->grpmask);",
            "\t}",
            "",
            "\t/* Now handle the beginnings of any new-to-this-CPU grace periods. */",
            "\tif (rcu_seq_new_gp(rdp->gp_seq, rnp->gp_seq) ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) {",
            "\t\t/*",
            "\t\t * If the current grace period is waiting for this CPU,",
            "\t\t * set up to detect a quiescent state, otherwise don't",
            "\t\t * go looking for one.",
            "\t\t */",
            "\t\ttrace_rcu_grace_period(rcu_state.name, rnp->gp_seq, TPS(\"cpustart\"));",
            "\t\tneed_qs = !!(rnp->qsmask & rdp->grpmask);",
            "\t\trdp->cpu_no_qs.b.norm = need_qs;",
            "\t\trdp->core_needs_qs = need_qs;",
            "\t\tzero_cpu_stall_ticks(rdp);",
            "\t}",
            "\trdp->gp_seq = rnp->gp_seq;  /* Remember new grace-period state. */",
            "\tif (ULONG_CMP_LT(rdp->gp_seq_needed, rnp->gp_seq_needed) || rdp->gpwrap)",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU) && READ_ONCE(rdp->gpwrap))",
            "\t\tWRITE_ONCE(rdp->last_sched_clock, jiffies);",
            "\tWRITE_ONCE(rdp->gpwrap, false);",
            "\trcu_gpnum_ovf(rnp, rdp);",
            "\treturn ret;",
            "}",
            "static void note_gp_changes(struct rcu_data *rdp)",
            "{",
            "\tunsigned long flags;",
            "\tbool needwake;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tlocal_irq_save(flags);",
            "\trnp = rdp->mynode;",
            "\tif ((rdp->gp_seq == rcu_seq_current(&rnp->gp_seq) &&",
            "\t     !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */",
            "\t    !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */",
            "\t\tlocal_irq_restore(flags);",
            "\t\treturn;",
            "\t}",
            "\tneedwake = __note_gp_changes(rnp, rdp);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\trcu_strict_gp_check_qs();",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "void rcu_gp_slow_register(atomic_t *rgssp)",
            "{",
            "\tWARN_ON_ONCE(rcu_gp_slow_suppress);",
            "",
            "\tWRITE_ONCE(rcu_gp_slow_suppress, rgssp);",
            "}",
            "void rcu_gp_slow_unregister(atomic_t *rgssp)",
            "{",
            "\tWARN_ON_ONCE(rgssp && rgssp != rcu_gp_slow_suppress && rcu_gp_slow_suppress != NULL);",
            "",
            "\tWRITE_ONCE(rcu_gp_slow_suppress, NULL);",
            "}",
            "static bool rcu_gp_slow_is_suppressed(void)",
            "{",
            "\tatomic_t *rgssp = READ_ONCE(rcu_gp_slow_suppress);",
            "",
            "\treturn rgssp && atomic_read(rgssp);",
            "}",
            "static void rcu_gp_slow(int delay)",
            "{",
            "\tif (!rcu_gp_slow_is_suppressed() && delay > 0 &&",
            "\t    !(rcu_seq_ctr(rcu_state.gp_seq) % (rcu_num_nodes * PER_RCU_NODE_PERIOD * delay)))",
            "\t\tschedule_timeout_idle(delay);",
            "}",
            "void rcu_gp_set_torture_wait(int duration)",
            "{",
            "\tif (IS_ENABLED(CONFIG_RCU_TORTURE_TEST) && duration > 0)",
            "\t\tWRITE_ONCE(sleep_duration, duration);",
            "}",
            "static void rcu_gp_torture_wait(void)",
            "{",
            "\tunsigned long duration;",
            "",
            "\tif (!IS_ENABLED(CONFIG_RCU_TORTURE_TEST))",
            "\t\treturn;",
            "\tduration = xchg(&sleep_duration, 0UL);",
            "\tif (duration > 0) {",
            "\t\tpr_alert(\"%s: Waiting %lu jiffies\\n\", __func__, duration);",
            "\t\tschedule_timeout_idle(duration);",
            "\t\tpr_alert(\"%s: Wait complete\\n\", __func__);",
            "\t}",
            "}"
          ],
          "function_name": "__note_gp_changes, note_gp_changes, rcu_gp_slow_register, rcu_gp_slow_unregister, rcu_gp_slow_is_suppressed, rcu_gp_slow, rcu_gp_set_torture_wait, rcu_gp_torture_wait",
          "description": "提供慢速模式下的grace period注册/注销接口，通过原子变量控制抑制状态，并实现基于延迟的调度控制逻辑，支持严格模式下的边界检查。",
          "similarity": 0.5759063959121704
        }
      ]
    },
    {
      "source_file": "kernel/sched/cpufreq_schedutil.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:03:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpufreq_schedutil.c`\n\n---\n\n# `sched/cpufreq_schedutil.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpufreq_schedutil.c` 实现了 Linux 内核中基于调度器提供的 CPU 利用率数据的 **schedutil CPUFreq 调速器（governor）**。该调速器通过实时获取调度器计算的 CPU 利用率（包括 CFS、RT、DL 任务以及 I/O 等待状态），动态调整 CPU 频率，以在性能与能效之间取得平衡。其核心优势在于直接利用调度器的 `util` 信息，避免传统调速器依赖采样机制带来的延迟和不准确性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct sugov_tunables`**  \n  调速器可调参数，包含：\n  - `rate_limit_us`：频率更新的最小时间间隔（微秒），防止过于频繁的频率切换。\n\n- **`struct sugov_policy`**  \n  每个 `cpufreq_policy` 对应的 schedutil 策略实例，包含：\n  - `policy`：关联的 CPUFreq 策略。\n  - `update_lock`：保护频率更新的自旋锁。\n  - `last_freq_update_time` / `freq_update_delay_ns`：控制频率更新速率。\n  - `next_freq` / `cached_raw_freq`：目标频率与原始计算频率缓存。\n  - `irq_work` / `worker` / `thread`：用于慢速切换平台（非 fast-switch）的异步工作队列机制。\n  - `limits_changed` / `need_freq_update`：标志策略限制（如 min/max freq）是否变更。\n\n- **`struct sugov_cpu`**  \n  每个 CPU 的 schedutil 状态，包含：\n  - `update_util`：注册到调度器的回调接口（`update_util_data`）。\n  - `util` / `bw_min`：当前有效利用率及带宽最小值。\n  - `iowait_boost` / `iowait_boost_pending`：I/O 等待唤醒时的频率提升机制。\n  - `last_update`：上次更新时间戳。\n\n### 主要函数\n\n- **`sugov_should_update_freq()`**  \n  判断是否应执行频率更新，考虑硬件是否支持本 CPU 更新、策略限制变更、以及频率更新间隔限制。\n\n- **`sugov_update_next_freq()`**  \n  更新目标频率，处理策略限制变更场景，避免不必要的驱动回调。\n\n- **`get_next_freq()`**  \n  核心频率计算函数，根据 CPU 利用率、最大容量和参考频率，计算目标频率，并通过 `cpufreq_driver_resolve_freq()` 映射到驱动支持的频率。\n\n- **`sugov_get_util()`**  \n  获取当前 CPU 的综合利用率，整合 CFS/RT/DL 任务利用率、boost 值，并调用 `sugov_effective_cpu_perf()` 计算有效性能目标。\n\n- **`sugov_effective_cpu_perf()`**  \n  计算最终的有效性能目标，确保不低于最小性能要求，并限制不超过实际需求。\n\n- **`sugov_iowait_reset()` / `sugov_iowait_boost()`**  \n  实现 I/O 等待唤醒时的动态频率提升机制：短时间内连续 I/O 唤醒会逐步提升 boost 值（从 `IOWAIT_BOOST_MIN` 到最大 OPP），超过一个 tick 无 I/O 唤醒则重置。\n\n- **`get_capacity_ref_freq()`**  \n  获取用于计算 CPU 容量的参考频率，优先使用架构特定的 `arch_scale_freq_ref()`，其次为最大频率或当前频率。\n\n- **`sugov_deferred_update()`**  \n  在不支持 fast-switch 的平台上，通过 `irq_work` 触发异步频率更新。\n\n## 3. 关键实现\n\n### 频率计算算法\n- **频率不变性支持**：若系统支持频率不变调度（`arch_scale_freq_invariant()`），则直接使用调度器提供的频率不变利用率 `util`，按比例计算目标频率：  \n  `next_freq = C * max_freq * util / max`  \n  其中常数 `C = 1.25`，使在 `util/max = 0.8` 时达到 `max_freq`，提供性能余量。\n- **非频率不变性**：使用原始利用率 `util_raw` 乘以 `(curr_freq / max_freq)` 近似频率不变利用率，再计算目标频率。\n\n### I/O 等待 Boost 机制\n- 当任务因 I/O 完成而唤醒时，标记 `SCHED_CPUFREQ_IOWAIT`。\n- 若在 **一个 tick 内** 多次发生 I/O 唤醒，则 `iowait_boost` 值倍增（上限为最大 OPP 对应的利用率）。\n- 若超过一个 tick 无 I/O 唤醒，则重置 boost 值为 `IOWAIT_BOOST_MIN`（`SCHED_CAPACITY_SCALE / 8`），避免对偶发 I/O 过度响应，提升能效。\n\n### 快速切换（Fast-Switch）与异步更新\n- **Fast-Switch 平台**：支持在调度上下文中直接调用 `cpufreq_driver_fast_switch()` 更新频率，延迟最低。\n- **非 Fast-Switch 平台**：通过 `irq_work` 触发内核线程（`kthread_worker`）异步执行频率更新，避免在中断上下文或持有 rq 锁时调用可能阻塞的驱动接口。\n\n### 策略限制变更处理\n- 当用户空间修改 policy 的 min/max 频率时，`sugov_limits()` 设置 `limits_changed` 标志。\n- 下次更新时，强制重新计算频率，并通过内存屏障（`smp_mb()`）确保读取到最新的策略限制。\n\n## 4. 依赖关系\n\n- **调度器子系统**：\n  - 依赖 `update_util_data` 回调机制（通过 `cpufreq_add_update_util_hook()` 注册）。\n  - 调用 `cpu_util_cfs_boost()`、`effective_cpu_util()` 等函数获取综合利用率。\n  - 使用 `scx_cpuperf_target()`（若启用了 SCHED_CLASS_EXT）。\n- **CPUFreq 核心**：\n  - 依赖 `cpufreq_policy`、`cpufreq_driver_resolve_freq()`、`cpufreq_driver_fast_switch()` 等接口。\n  - 使用 `cpufreq_this_cpu_can_update()` 判断硬件更新能力。\n- **架构相关支持**：\n  - 依赖 `arch_scale_freq_ref()` 和 `arch_scale_freq_invariant()` 提供频率不变性信息。\n- **内核基础设施**：\n  - 使用 `irq_work`、`kthread_worker` 实现异步更新。\n  - 依赖 `TICK_NSEC` 定义 tick 时间。\n\n## 5. 使用场景\n\n- **默认高性能能效平衡场景**：现代 Linux 发行版通常将 `schedutil` 作为默认 CPUFreq 调速器，适用于大多数桌面、服务器和移动设备。\n- **实时性要求较高的系统**：由于其低延迟特性（尤其在 fast-switch 平台上），适合对响应时间敏感的应用。\n- **能效敏感设备**：通过 I/O boost 机制和精确的利用率跟踪，在保证交互性能的同时降低空闲功耗。\n- **异构多核系统（如 big.LITTLE）**：结合调度器的 CPU capacity 信息，为不同性能核提供差异化频率调整。",
      "similarity": 0.591320812702179,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 381,
          "end_line": 496,
          "content": [
            "static inline bool sugov_hold_freq(struct sugov_cpu *sg_cpu) { return false; }",
            "static inline void ignore_dl_rate_limit(struct sugov_cpu *sg_cpu)",
            "{",
            "\tif (cpu_bw_dl(cpu_rq(sg_cpu->cpu)) > sg_cpu->bw_min)",
            "\t\tWRITE_ONCE(sg_cpu->sg_policy->limits_changed, true);",
            "}",
            "static inline bool sugov_update_single_common(struct sugov_cpu *sg_cpu,",
            "\t\t\t\t\t      u64 time, unsigned long max_cap,",
            "\t\t\t\t\t      unsigned int flags)",
            "{",
            "\tunsigned long boost;",
            "",
            "\tsugov_iowait_boost(sg_cpu, time, flags);",
            "\tsg_cpu->last_update = time;",
            "",
            "\tignore_dl_rate_limit(sg_cpu);",
            "",
            "\tif (!sugov_should_update_freq(sg_cpu->sg_policy, time))",
            "\t\treturn false;",
            "",
            "\tboost = sugov_iowait_apply(sg_cpu, time, max_cap);",
            "\tsugov_get_util(sg_cpu, boost);",
            "",
            "\treturn true;",
            "}",
            "static void sugov_update_single_freq(struct update_util_data *hook, u64 time,",
            "\t\t\t\t     unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tunsigned int cached_freq = sg_policy->cached_raw_freq;",
            "\tunsigned long max_cap;",
            "\tunsigned int next_f;",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))",
            "\t\treturn;",
            "",
            "\tnext_f = get_next_freq(sg_policy, sg_cpu->util, max_cap);",
            "",
            "\tif (sugov_hold_freq(sg_cpu) && next_f < sg_policy->next_freq &&",
            "\t    !sg_policy->need_freq_update) {",
            "\t\tnext_f = sg_policy->next_freq;",
            "",
            "\t\t/* Restore cached freq as next_freq has changed */",
            "\t\tsg_policy->cached_raw_freq = cached_freq;",
            "\t}",
            "",
            "\tif (!sugov_update_next_freq(sg_policy, time, next_f))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This code runs under rq->lock for the target CPU, so it won't run",
            "\t * concurrently on two different CPUs for the same target and it is not",
            "\t * necessary to acquire the lock in the fast switch case.",
            "\t */",
            "\tif (sg_policy->policy->fast_switch_enabled) {",
            "\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);",
            "\t} else {",
            "\t\traw_spin_lock(&sg_policy->update_lock);",
            "\t\tsugov_deferred_update(sg_policy);",
            "\t\traw_spin_unlock(&sg_policy->update_lock);",
            "\t}",
            "}",
            "static void sugov_update_single_perf(struct update_util_data *hook, u64 time,",
            "\t\t\t\t     unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tunsigned long prev_util = sg_cpu->util;",
            "\tunsigned long max_cap;",
            "",
            "\t/*",
            "\t * Fall back to the \"frequency\" path if frequency invariance is not",
            "\t * supported, because the direct mapping between the utilization and",
            "\t * the performance levels depends on the frequency invariance.",
            "\t */",
            "\tif (!arch_scale_freq_invariant()) {",
            "\t\tsugov_update_single_freq(hook, time, flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tif (!sugov_update_single_common(sg_cpu, time, max_cap, flags))",
            "\t\treturn;",
            "",
            "\tif (sugov_hold_freq(sg_cpu) && sg_cpu->util < prev_util)",
            "\t\tsg_cpu->util = prev_util;",
            "",
            "\tcpufreq_driver_adjust_perf(sg_cpu->cpu, sg_cpu->bw_min,",
            "\t\t\t\t   sg_cpu->util, max_cap);",
            "",
            "\tsg_cpu->sg_policy->last_freq_update_time = time;",
            "}",
            "static unsigned int sugov_next_freq_shared(struct sugov_cpu *sg_cpu, u64 time)",
            "{",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tunsigned long util = 0, max_cap;",
            "\tunsigned int j;",
            "",
            "\tmax_cap = arch_scale_cpu_capacity(sg_cpu->cpu);",
            "",
            "\tfor_each_cpu(j, policy->cpus) {",
            "\t\tstruct sugov_cpu *j_sg_cpu = &per_cpu(sugov_cpu, j);",
            "\t\tunsigned long boost;",
            "",
            "\t\tboost = sugov_iowait_apply(j_sg_cpu, time, max_cap);",
            "\t\tsugov_get_util(j_sg_cpu, boost);",
            "",
            "\t\tutil = max(j_sg_cpu->util, util);",
            "\t}",
            "",
            "\treturn get_next_freq(sg_policy, util, max_cap);",
            "}"
          ],
          "function_name": "sugov_hold_freq, ignore_dl_rate_limit, sugov_update_single_common, sugov_update_single_freq, sugov_update_single_perf, sugov_next_freq_shared",
          "description": "实现单核/多核频率调整逻辑，sugov_update_single_freq处理单核频率更新，sugov_update_single_perf处理性能调优路径，sugov_next_freq_shared计算多核共享场景下的全局目标频率。",
          "similarity": 0.5996114015579224
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 701,
          "end_line": 809,
          "content": [
            "static void sugov_kthread_stop(struct sugov_policy *sg_policy)",
            "{",
            "\t/* kthread only required for slow path */",
            "\tif (sg_policy->policy->fast_switch_enabled)",
            "\t\treturn;",
            "",
            "\tkthread_flush_worker(&sg_policy->worker);",
            "\tkthread_stop(sg_policy->thread);",
            "\tmutex_destroy(&sg_policy->work_lock);",
            "}",
            "static void sugov_clear_global_tunables(void)",
            "{",
            "\tif (!have_governor_per_policy())",
            "\t\tglobal_tunables = NULL;",
            "}",
            "static int sugov_init(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy;",
            "\tstruct sugov_tunables *tunables;",
            "\tint ret = 0;",
            "",
            "\t/* State should be equivalent to EXIT */",
            "\tif (policy->governor_data)",
            "\t\treturn -EBUSY;",
            "",
            "\tcpufreq_enable_fast_switch(policy);",
            "",
            "\tsg_policy = sugov_policy_alloc(policy);",
            "\tif (!sg_policy) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto disable_fast_switch;",
            "\t}",
            "",
            "\tret = sugov_kthread_create(sg_policy);",
            "\tif (ret)",
            "\t\tgoto free_sg_policy;",
            "",
            "\tmutex_lock(&global_tunables_lock);",
            "",
            "\tif (global_tunables) {",
            "\t\tif (WARN_ON(have_governor_per_policy())) {",
            "\t\t\tret = -EINVAL;",
            "\t\t\tgoto stop_kthread;",
            "\t\t}",
            "\t\tpolicy->governor_data = sg_policy;",
            "\t\tsg_policy->tunables = global_tunables;",
            "",
            "\t\tgov_attr_set_get(&global_tunables->attr_set, &sg_policy->tunables_hook);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttunables = sugov_tunables_alloc(sg_policy);",
            "\tif (!tunables) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto stop_kthread;",
            "\t}",
            "",
            "\ttunables->rate_limit_us = cpufreq_policy_transition_delay_us(policy);",
            "",
            "\tpolicy->governor_data = sg_policy;",
            "\tsg_policy->tunables = tunables;",
            "",
            "\tret = kobject_init_and_add(&tunables->attr_set.kobj, &sugov_tunables_ktype,",
            "\t\t\t\t   get_governor_parent_kobj(policy), \"%s\",",
            "\t\t\t\t   schedutil_gov.name);",
            "\tif (ret)",
            "\t\tgoto fail;",
            "",
            "out:",
            "\tmutex_unlock(&global_tunables_lock);",
            "\treturn 0;",
            "",
            "fail:",
            "\tkobject_put(&tunables->attr_set.kobj);",
            "\tpolicy->governor_data = NULL;",
            "\tsugov_clear_global_tunables();",
            "",
            "stop_kthread:",
            "\tsugov_kthread_stop(sg_policy);",
            "\tmutex_unlock(&global_tunables_lock);",
            "",
            "free_sg_policy:",
            "\tsugov_policy_free(sg_policy);",
            "",
            "disable_fast_switch:",
            "\tcpufreq_disable_fast_switch(policy);",
            "",
            "\tpr_err(\"initialization failed (error %d)\\n\", ret);",
            "\treturn ret;",
            "}",
            "static void sugov_exit(struct cpufreq_policy *policy)",
            "{",
            "\tstruct sugov_policy *sg_policy = policy->governor_data;",
            "\tstruct sugov_tunables *tunables = sg_policy->tunables;",
            "\tunsigned int count;",
            "",
            "\tmutex_lock(&global_tunables_lock);",
            "",
            "\tcount = gov_attr_set_put(&tunables->attr_set, &sg_policy->tunables_hook);",
            "\tpolicy->governor_data = NULL;",
            "\tif (!count)",
            "\t\tsugov_clear_global_tunables();",
            "",
            "\tmutex_unlock(&global_tunables_lock);",
            "",
            "\tsugov_kthread_stop(sg_policy);",
            "\tsugov_policy_free(sg_policy);",
            "\tcpufreq_disable_fast_switch(policy);",
            "}"
          ],
          "function_name": "sugov_kthread_stop, sugov_clear_global_tunables, sugov_init, sugov_exit",
          "description": "sugov_kthread_stop 停止慢速路径相关内核线程并释放锁资源；sugov_clear_global_tunables 清除全局调谐参数指针；sugov_init 初始化CPU频率策略模块，分配策略结构体并创建内核线程；sugov_exit 释放策略资源，停止线程并禁用快速切换功能",
          "similarity": 0.5806645154953003
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 62,
          "end_line": 168,
          "content": [
            "static bool sugov_should_update_freq(struct sugov_policy *sg_policy, u64 time)",
            "{",
            "\ts64 delta_ns;",
            "",
            "\t/*",
            "\t * Since cpufreq_update_util() is called with rq->lock held for",
            "\t * the @target_cpu, our per-CPU data is fully serialized.",
            "\t *",
            "\t * However, drivers cannot in general deal with cross-CPU",
            "\t * requests, so while get_next_freq() will work, our",
            "\t * sugov_update_commit() call may not for the fast switching platforms.",
            "\t *",
            "\t * Hence stop here for remote requests if they aren't supported",
            "\t * by the hardware, as calculating the frequency is pointless if",
            "\t * we cannot in fact act on it.",
            "\t *",
            "\t * This is needed on the slow switching platforms too to prevent CPUs",
            "\t * going offline from leaving stale IRQ work items behind.",
            "\t */",
            "\tif (!cpufreq_this_cpu_can_update(sg_policy->policy))",
            "\t\treturn false;",
            "",
            "\tif (unlikely(READ_ONCE(sg_policy->limits_changed))) {",
            "\t\tWRITE_ONCE(sg_policy->limits_changed, false);",
            "\t\tsg_policy->need_freq_update = true;",
            "",
            "\t\t/*",
            "\t\t * The above limits_changed update must occur before the reads",
            "\t\t * of policy limits in cpufreq_driver_resolve_freq() or a policy",
            "\t\t * limits update might be missed, so use a memory barrier to",
            "\t\t * ensure it.",
            "\t\t *",
            "\t\t * This pairs with the write memory barrier in sugov_limits().",
            "\t\t */",
            "\t\tsmp_mb();",
            "",
            "\t\treturn true;",
            "\t}",
            "",
            "\tdelta_ns = time - sg_policy->last_freq_update_time;",
            "",
            "\treturn delta_ns >= sg_policy->freq_update_delay_ns;",
            "}",
            "static bool sugov_update_next_freq(struct sugov_policy *sg_policy, u64 time,",
            "\t\t\t\t   unsigned int next_freq)",
            "{",
            "\tif (sg_policy->need_freq_update) {",
            "\t\tsg_policy->need_freq_update = false;",
            "\t\t/*",
            "\t\t * The policy limits have changed, but if the return value of",
            "\t\t * cpufreq_driver_resolve_freq() after applying the new limits",
            "\t\t * is still equal to the previously selected frequency, the",
            "\t\t * driver callback need not be invoked unless the driver",
            "\t\t * specifically wants that to happen on every update of the",
            "\t\t * policy limits.",
            "\t\t */",
            "\t\tif (sg_policy->next_freq == next_freq &&",
            "\t\t    !cpufreq_driver_test_flags(CPUFREQ_NEED_UPDATE_LIMITS))",
            "\t\t\treturn false;",
            "\t} else if (sg_policy->next_freq == next_freq) {",
            "\t\treturn false;",
            "\t}",
            "",
            "\tsg_policy->next_freq = next_freq;",
            "\tsg_policy->last_freq_update_time = time;",
            "",
            "\treturn true;",
            "}",
            "static void sugov_deferred_update(struct sugov_policy *sg_policy)",
            "{",
            "\tif (!sg_policy->work_in_progress) {",
            "\t\tsg_policy->work_in_progress = true;",
            "\t\tirq_work_queue(&sg_policy->irq_work);",
            "\t}",
            "}",
            "static __always_inline",
            "unsigned long get_capacity_ref_freq(struct cpufreq_policy *policy)",
            "{",
            "\tunsigned int freq = arch_scale_freq_ref(policy->cpu);",
            "",
            "\tif (freq)",
            "\t\treturn freq;",
            "",
            "\tif (arch_scale_freq_invariant())",
            "\t\treturn policy->cpuinfo.max_freq;",
            "",
            "\t/*",
            "\t * Apply a 25% margin so that we select a higher frequency than",
            "\t * the current one before the CPU is fully busy:",
            "\t */",
            "\treturn policy->cur + (policy->cur >> 2);",
            "}",
            "static unsigned int get_next_freq(struct sugov_policy *sg_policy,",
            "\t\t\t\t  unsigned long util, unsigned long max)",
            "{",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tunsigned int freq;",
            "",
            "\tfreq = get_capacity_ref_freq(policy);",
            "\tfreq = map_util_freq(util, freq, max);",
            "",
            "\tif (freq == sg_policy->cached_raw_freq && !sg_policy->need_freq_update)",
            "\t\treturn sg_policy->next_freq;",
            "",
            "\tsg_policy->cached_raw_freq = freq;",
            "\treturn cpufreq_driver_resolve_freq(policy, freq);",
            "}"
          ],
          "function_name": "sugov_should_update_freq, sugov_update_next_freq, sugov_deferred_update, get_capacity_ref_freq, get_next_freq",
          "description": "实现了频率更新核心逻辑，sugov_should_update_freq判断是否需要更新频率，sugov_update_next_freq计算并记录目标频率，sugov_deferred_update触发异步更新，get_capacity_ref_freq获取基准频率，get_next_freq结合利用率计算最终目标频率。",
          "similarity": 0.5717169046401978
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 508,
          "end_line": 651,
          "content": [
            "static void",
            "sugov_update_shared(struct update_util_data *hook, u64 time, unsigned int flags)",
            "{",
            "\tstruct sugov_cpu *sg_cpu = container_of(hook, struct sugov_cpu, update_util);",
            "\tstruct sugov_policy *sg_policy = sg_cpu->sg_policy;",
            "\tunsigned int next_f;",
            "",
            "\traw_spin_lock(&sg_policy->update_lock);",
            "",
            "\tsugov_iowait_boost(sg_cpu, time, flags);",
            "\tsg_cpu->last_update = time;",
            "",
            "\tignore_dl_rate_limit(sg_cpu);",
            "",
            "\tif (sugov_should_update_freq(sg_policy, time)) {",
            "\t\tnext_f = sugov_next_freq_shared(sg_cpu, time);",
            "",
            "\t\tif (!sugov_update_next_freq(sg_policy, time, next_f))",
            "\t\t\tgoto unlock;",
            "",
            "\t\tif (sg_policy->policy->fast_switch_enabled)",
            "\t\t\tcpufreq_driver_fast_switch(sg_policy->policy, next_f);",
            "\t\telse",
            "\t\t\tsugov_deferred_update(sg_policy);",
            "\t}",
            "unlock:",
            "\traw_spin_unlock(&sg_policy->update_lock);",
            "}",
            "static void sugov_work(struct kthread_work *work)",
            "{",
            "\tstruct sugov_policy *sg_policy = container_of(work, struct sugov_policy, work);",
            "\tunsigned int freq;",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Hold sg_policy->update_lock shortly to handle the case where:",
            "\t * in case sg_policy->next_freq is read here, and then updated by",
            "\t * sugov_deferred_update() just before work_in_progress is set to false",
            "\t * here, we may miss queueing the new update.",
            "\t *",
            "\t * Note: If a work was queued after the update_lock is released,",
            "\t * sugov_work() will just be called again by kthread_work code; and the",
            "\t * request will be proceed before the sugov thread sleeps.",
            "\t */",
            "\traw_spin_lock_irqsave(&sg_policy->update_lock, flags);",
            "\tfreq = sg_policy->next_freq;",
            "\tsg_policy->work_in_progress = false;",
            "\traw_spin_unlock_irqrestore(&sg_policy->update_lock, flags);",
            "",
            "\tmutex_lock(&sg_policy->work_lock);",
            "\t__cpufreq_driver_target(sg_policy->policy, freq, CPUFREQ_RELATION_L);",
            "\tmutex_unlock(&sg_policy->work_lock);",
            "}",
            "static void sugov_irq_work(struct irq_work *irq_work)",
            "{",
            "\tstruct sugov_policy *sg_policy;",
            "",
            "\tsg_policy = container_of(irq_work, struct sugov_policy, irq_work);",
            "",
            "\tkthread_queue_work(&sg_policy->worker, &sg_policy->work);",
            "}",
            "static ssize_t rate_limit_us_show(struct gov_attr_set *attr_set, char *buf)",
            "{",
            "\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);",
            "",
            "\treturn sprintf(buf, \"%u\\n\", tunables->rate_limit_us);",
            "}",
            "static ssize_t",
            "rate_limit_us_store(struct gov_attr_set *attr_set, const char *buf, size_t count)",
            "{",
            "\tstruct sugov_tunables *tunables = to_sugov_tunables(attr_set);",
            "\tstruct sugov_policy *sg_policy;",
            "\tunsigned int rate_limit_us;",
            "",
            "\tif (kstrtouint(buf, 10, &rate_limit_us))",
            "\t\treturn -EINVAL;",
            "",
            "\ttunables->rate_limit_us = rate_limit_us;",
            "",
            "\tlist_for_each_entry(sg_policy, &attr_set->policy_list, tunables_hook)",
            "\t\tsg_policy->freq_update_delay_ns = rate_limit_us * NSEC_PER_USEC;",
            "",
            "\treturn count;",
            "}",
            "static void sugov_tunables_free(struct kobject *kobj)",
            "{",
            "\tstruct gov_attr_set *attr_set = to_gov_attr_set(kobj);",
            "",
            "\tkfree(to_sugov_tunables(attr_set));",
            "}",
            "static void sugov_policy_free(struct sugov_policy *sg_policy)",
            "{",
            "\tkfree(sg_policy);",
            "}",
            "static int sugov_kthread_create(struct sugov_policy *sg_policy)",
            "{",
            "\tstruct task_struct *thread;",
            "\tstruct sched_attr attr = {",
            "\t\t.size\t\t= sizeof(struct sched_attr),",
            "\t\t.sched_policy\t= SCHED_DEADLINE,",
            "\t\t.sched_flags\t= SCHED_FLAG_SUGOV,",
            "\t\t.sched_nice\t= 0,",
            "\t\t.sched_priority\t= 0,",
            "\t\t/*",
            "\t\t * Fake (unused) bandwidth; workaround to \"fix\"",
            "\t\t * priority inheritance.",
            "\t\t */",
            "\t\t.sched_runtime\t=  1000000,",
            "\t\t.sched_deadline = 10000000,",
            "\t\t.sched_period\t= 10000000,",
            "\t};",
            "\tstruct cpufreq_policy *policy = sg_policy->policy;",
            "\tint ret;",
            "",
            "\t/* kthread only required for slow path */",
            "\tif (policy->fast_switch_enabled)",
            "\t\treturn 0;",
            "",
            "\tkthread_init_work(&sg_policy->work, sugov_work);",
            "\tkthread_init_worker(&sg_policy->worker);",
            "\tthread = kthread_create(kthread_worker_fn, &sg_policy->worker,",
            "\t\t\t\t\"sugov:%d\",",
            "\t\t\t\tcpumask_first(policy->related_cpus));",
            "\tif (IS_ERR(thread)) {",
            "\t\tpr_err(\"failed to create sugov thread: %ld\\n\", PTR_ERR(thread));",
            "\t\treturn PTR_ERR(thread);",
            "\t}",
            "",
            "\tret = sched_setattr_nocheck(thread, &attr);",
            "\tif (ret) {",
            "\t\tkthread_stop(thread);",
            "\t\tpr_warn(\"%s: failed to set SCHED_DEADLINE\\n\", __func__);",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tsg_policy->thread = thread;",
            "\tkthread_bind_mask(thread, policy->related_cpus);",
            "\tinit_irq_work(&sg_policy->irq_work, sugov_irq_work);",
            "\tmutex_init(&sg_policy->work_lock);",
            "",
            "\twake_up_process(thread);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "sugov_update_shared, sugov_work, sugov_irq_work, rate_limit_us_show, rate_limit_us_store, sugov_tunables_free, sugov_policy_free, sugov_kthread_create",
          "description": "管理频率调节的工作线程和参数配置，sugov_kthread_create创建慢速切换场景的后台线程，rate_limit_us_*/提供速率限制配置接口，sugov_work/sugov_irq_work处理异步频率更新任务。",
          "similarity": 0.5571359395980835
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/cpufreq_schedutil.c",
          "start_line": 204,
          "end_line": 330,
          "content": [
            "unsigned long sugov_effective_cpu_perf(int cpu, unsigned long actual,",
            "\t\t\t\t unsigned long min,",
            "\t\t\t\t unsigned long max)",
            "{",
            "\t/* Add dvfs headroom to actual utilization */",
            "\tactual = map_util_perf(actual);",
            "\t/* Actually we don't need to target the max performance */",
            "\tif (actual < max)",
            "\t\tmax = actual;",
            "",
            "\t/*",
            "\t * Ensure at least minimum performance while providing more compute",
            "\t * capacity when possible.",
            "\t */",
            "\treturn max(min, max);",
            "}",
            "static void sugov_get_util(struct sugov_cpu *sg_cpu, unsigned long boost)",
            "{",
            "\tunsigned long min, max, util = scx_cpuperf_target(sg_cpu->cpu);",
            "",
            "\tif (!scx_switched_all())",
            "\t\tutil += cpu_util_cfs_boost(sg_cpu->cpu);",
            "\tutil = effective_cpu_util(sg_cpu->cpu, util, &min, &max);",
            "\tutil = max(util, boost);",
            "\tsg_cpu->bw_min = min;",
            "\tsg_cpu->util = sugov_effective_cpu_perf(sg_cpu->cpu, util, min, max);",
            "}",
            "static bool sugov_iowait_reset(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       bool set_iowait_boost)",
            "{",
            "\ts64 delta_ns = time - sg_cpu->last_update;",
            "",
            "\t/* Reset boost only if a tick has elapsed since last request */",
            "\tif (delta_ns <= TICK_NSEC)",
            "\t\treturn false;",
            "",
            "\tsg_cpu->iowait_boost = set_iowait_boost ? IOWAIT_BOOST_MIN : 0;",
            "\tsg_cpu->iowait_boost_pending = set_iowait_boost;",
            "",
            "\treturn true;",
            "}",
            "static void sugov_iowait_boost(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       unsigned int flags)",
            "{",
            "\tbool set_iowait_boost = flags & SCHED_CPUFREQ_IOWAIT;",
            "",
            "\t/* Reset boost if the CPU appears to have been idle enough */",
            "\tif (sg_cpu->iowait_boost &&",
            "\t    sugov_iowait_reset(sg_cpu, time, set_iowait_boost))",
            "\t\treturn;",
            "",
            "\t/* Boost only tasks waking up after IO */",
            "\tif (!set_iowait_boost)",
            "\t\treturn;",
            "",
            "\t/* Ensure boost doubles only one time at each request */",
            "\tif (sg_cpu->iowait_boost_pending)",
            "\t\treturn;",
            "\tsg_cpu->iowait_boost_pending = true;",
            "",
            "\t/* Double the boost at each request */",
            "\tif (sg_cpu->iowait_boost) {",
            "\t\tsg_cpu->iowait_boost =",
            "\t\t\tmin_t(unsigned int, sg_cpu->iowait_boost << 1, SCHED_CAPACITY_SCALE);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* First wakeup after IO: start with minimum boost */",
            "\tsg_cpu->iowait_boost = IOWAIT_BOOST_MIN;",
            "}",
            "static unsigned long sugov_iowait_apply(struct sugov_cpu *sg_cpu, u64 time,",
            "\t\t\t       unsigned long max_cap)",
            "{",
            "\t/* No boost currently required */",
            "\tif (!sg_cpu->iowait_boost)",
            "\t\treturn 0;",
            "",
            "\t/* Reset boost if the CPU appears to have been idle enough */",
            "\tif (sugov_iowait_reset(sg_cpu, time, false))",
            "\t\treturn 0;",
            "",
            "\tif (!sg_cpu->iowait_boost_pending) {",
            "\t\t/*",
            "\t\t * No boost pending; reduce the boost value.",
            "\t\t */",
            "\t\tsg_cpu->iowait_boost >>= 1;",
            "\t\tif (sg_cpu->iowait_boost < IOWAIT_BOOST_MIN) {",
            "\t\t\tsg_cpu->iowait_boost = 0;",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            "\tsg_cpu->iowait_boost_pending = false;",
            "",
            "\t/*",
            "\t * sg_cpu->util is already in capacity scale; convert iowait_boost",
            "\t * into the same scale so we can compare.",
            "\t */",
            "\treturn (sg_cpu->iowait_boost * max_cap) >> SCHED_CAPACITY_SHIFT;",
            "}",
            "static bool sugov_hold_freq(struct sugov_cpu *sg_cpu)",
            "{",
            "\tunsigned long idle_calls;",
            "\tbool ret;",
            "",
            "\t/*",
            "\t * The heuristics in this function is for the fair class. For SCX, the",
            "\t * performance target comes directly from the BPF scheduler. Let's just",
            "\t * follow it.",
            "\t */",
            "\tif (scx_switched_all())",
            "\t\treturn false;",
            "",
            "\t/* if capped by uclamp_max, always update to be in compliance */",
            "\tif (uclamp_rq_is_capped(cpu_rq(sg_cpu->cpu)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Maintain the frequency if the CPU has not been idle recently, as",
            "\t * reduction is likely to be premature.",
            "\t */",
            "\tidle_calls = tick_nohz_get_idle_calls_cpu(sg_cpu->cpu);",
            "\tret = idle_calls == sg_cpu->saved_idle_calls;",
            "",
            "\tsg_cpu->saved_idle_calls = idle_calls;",
            "\treturn ret;",
            "}"
          ],
          "function_name": "sugov_effective_cpu_perf, sugov_get_util, sugov_iowait_reset, sugov_iowait_boost, sugov_iowait_apply, sugov_hold_freq",
          "description": "处理利用率计算和I/O等待优化，sugov_effective_cpu_perf计算有效性能需求，sugov_get_util获取考虑boost后的利用率，sugov_iowait_*系列函数管理I/O等待场景下的频率提升机制。",
          "similarity": 0.5315530300140381
        }
      ]
    },
    {
      "source_file": "mm/mmu_notifier.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:53:38\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mmu_notifier.c`\n\n---\n\n# mmu_notifier.c 技术文档\n\n## 1. 文件概述\n\n`mmu_notifier.c` 是 Linux 内核中实现 **MMU Notifier（内存管理单元通知器）** 机制的核心文件。该机制允许内核子系统（如 KVM、RDMA、DAX 等）在用户虚拟地址空间发生页表变更（如页面回收、映射撤销等）时收到通知，从而同步维护其私有页表（如影子页表 SPTEs）或缓存状态。  \n本文件主要实现了基于 **区间树（interval tree）** 的高效范围监听机制，并通过一种类似 seqcount 的 **碰撞重试（collision-retry）读写同步模型**，在保证高并发性的同时避免在关键路径上使用阻塞锁。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct mmu_notifier_subscriptions`**  \n  每个 `mm_struct` 关联的订阅信息容器，包含：\n  - `list`：传统 MMU notifier 链表（用于非区间监听）\n  - `itree`：基于红黑树的区间树，存储 `mmu_interval_notifier`\n  - `invalidate_seq`：序列号，用于实现读写同步（奇数表示正在无效化）\n  - `active_invalidate_ranges`：当前活跃的无效化操作计数\n  - `deferred_list`：延迟处理的区间插入/删除队列\n  - `wq`：等待队列，用于唤醒等待无效化完成的读者\n  - `lock`：保护上述字段的自旋锁\n\n- **全局 SRCU 实例 `srcu`**  \n  用于安全地遍历和回调 MMU notifier 列表，避免在 RCU 临界区内睡眠。\n\n- **Lockdep 映射 `__mmu_notifier_invalidate_range_start_map`**  \n  用于死锁检测，标记 `invalidate_range_start` 的锁上下文。\n\n### 主要函数\n\n- **`mn_itree_inv_start_range()`**  \n  开始一个虚拟地址范围的无效化操作：增加计数、检查是否有监听者、若存在则将 `invalidate_seq` 设为奇数，并返回首个匹配的监听器。\n\n- **`mn_itree_inv_next()`**  \n  在无效化过程中迭代获取下一个匹配的区间监听器。\n\n- **`mn_itree_inv_end()`**  \n  结束无效化操作：减少计数；若为最后一个操作且处于完全排除状态，则将 `invalidate_seq` 加 1（变为偶数），并处理 `deferred_list` 中的延迟插入/删除，最后唤醒等待队列。\n\n- **`mmu_interval_read_begin()`**（片段）  \n  开始一个读端临界区：读取当前监听器的 `invalidate_seq`，用于后续与全局 `invalidate_seq` 比较以检测是否发生碰撞（即无效化操作介入）。\n\n> 注：`mmu_interval_read_retry()` 函数虽未完整给出，但其作用是比对 `interval_sub->invalidate_seq` 与读开始时保存的全局 `seq`，若不同则说明发生碰撞需重试。\n\n## 3. 关键实现\n\n### 碰撞重试同步机制（Collision-Retry Lock）\n\n- 使用 `invalidate_seq` 序列号模拟读写锁，但允许多个写者并发。\n- **写者（无效化操作）**：\n  - 进入时：`active_invalidate_ranges++`；若有监听者，则 `seq |= 1`（设为奇数）。\n  - 退出时：`active_invalidate_ranges--`；若为最后一个写者且 `seq` 为奇数，则 `seq++`（变为偶数）。\n- **读者（如获取 SPTE）**：\n  - 调用 `mmu_interval_read_begin()` 获取当前 `seq`。\n  - 在持有用户锁（如 mmap_lock）期间执行操作。\n  - 调用 `mmu_interval_read_retry()` 检查 `interval_sub->invalidate_seq` 是否等于初始 `seq`。若不等，说明无效化已发生，需重试。\n- **优势**：避免在 `invalidate_range_start` 中使用阻塞锁，提升 mm 路径性能。\n\n### 区间树与延迟更新\n\n- 所有 `mmu_interval_notifier` 按虚拟地址范围注册到 `itree` 中，支持高效范围查询。\n- 在无效化过程中（`invalidate_seq` 为奇数），禁止直接修改 `itree`。\n- 插入/删除操作被暂存到 `deferred_list`，在最后一个 `inv_end` 时批量处理，确保树结构一致性。\n\n### SRCU 用于安全回调\n\n- 全局 `srcu` 用于遍历 `mm->notifier_subscriptions->list` 并调用传统 notifier 回调。\n- 允许回调函数睡眠（相比 RCU 更灵活），同时保证在 `mm` 销毁前完成所有回调。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/mmu_notifier.h>`：核心 API 和数据结构定义\n  - `<linux/interval_tree.h>`：区间树操作\n  - `<linux/srcu.h>`、`<linux/rcupdate.h>`：同步原语\n  - `<linux/mm.h>`、`<linux/sched/mm.h>`：内存管理相关\n  - `<linux/rculist.h>`、`<linux/slab.h>`：链表和内存分配\n\n- **内核子系统依赖**：\n  - **Memory Management (MM)**：依赖 `mm_struct` 生命周期管理（`mmdrop` 释放 subscriptions）\n  - **KVM / VFIO / RDMA / DAX**：作为主要使用者，注册 `mmu_interval_notifier` 监听 VA 变更\n  - **Lockdep**：用于死锁检测（`CONFIG_LOCKDEP`）\n\n## 5. 使用场景\n\n1. **虚拟化（KVM）**  \n   当 Guest OS 的页表被 Host 回收或修改时，KVM 通过 MMU notifier 同步更新影子页表（SPTEs），避免访问已释放的物理页。\n\n2. **高性能计算（RDMA / InfiniBand）**  \n   用户态注册内存区域用于零拷贝 DMA。当该区域被 munmap 或 swap out 时，驱动需收到通知以撤销硬件映射，防止 DMA 访问非法内存。\n\n3. **持久内存（DAX）**  \n   DAX 直接映射持久内存到用户空间。当映射被撤销时，需刷新 CPU 缓存并确保数据持久化，MMU notifier 提供必要的同步点。\n\n4. **用户态页表管理**  \n   如用户态缺页处理（userfaultfd）或自定义内存管理器，需感知内核对 VA 的修改以维护一致性。\n\n> 总结：`mmu_notifier.c` 为需要与内核页表变更保持同步的子系统提供了高效、可扩展的通知框架，是现代 Linux 内核中虚拟化、高性能 I/O 和新型存储的关键基础设施。",
      "similarity": 0.5889098644256592,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 970,
          "end_line": 1066,
          "content": [
            "int mmu_interval_notifier_insert(struct mmu_interval_notifier *interval_sub,",
            "\t\t\t\t struct mm_struct *mm, unsigned long start,",
            "\t\t\t\t unsigned long length,",
            "\t\t\t\t const struct mmu_interval_notifier_ops *ops)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions;",
            "\tint ret;",
            "",
            "\tmight_lock(&mm->mmap_lock);",
            "",
            "\tsubscriptions = smp_load_acquire(&mm->notifier_subscriptions);",
            "\tif (!subscriptions || !subscriptions->has_itree) {",
            "\t\tret = mmu_notifier_register(NULL, mm);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t\tsubscriptions = mm->notifier_subscriptions;",
            "\t}",
            "\treturn __mmu_interval_notifier_insert(interval_sub, mm, subscriptions,",
            "\t\t\t\t\t      start, length, ops);",
            "}",
            "int mmu_interval_notifier_insert_locked(",
            "\tstruct mmu_interval_notifier *interval_sub, struct mm_struct *mm,",
            "\tunsigned long start, unsigned long length,",
            "\tconst struct mmu_interval_notifier_ops *ops)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\tmm->notifier_subscriptions;",
            "\tint ret;",
            "",
            "\tmmap_assert_write_locked(mm);",
            "",
            "\tif (!subscriptions || !subscriptions->has_itree) {",
            "\t\tret = __mmu_notifier_register(NULL, mm);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t\tsubscriptions = mm->notifier_subscriptions;",
            "\t}",
            "\treturn __mmu_interval_notifier_insert(interval_sub, mm, subscriptions,",
            "\t\t\t\t\t      start, length, ops);",
            "}",
            "static bool",
            "mmu_interval_seq_released(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\t  unsigned long seq)",
            "{",
            "\tbool ret;",
            "",
            "\tspin_lock(&subscriptions->lock);",
            "\tret = subscriptions->invalidate_seq != seq;",
            "\tspin_unlock(&subscriptions->lock);",
            "\treturn ret;",
            "}",
            "void mmu_interval_notifier_remove(struct mmu_interval_notifier *interval_sub)",
            "{",
            "\tstruct mm_struct *mm = interval_sub->mm;",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\tmm->notifier_subscriptions;",
            "\tunsigned long seq = 0;",
            "",
            "\tmight_sleep();",
            "",
            "\tspin_lock(&subscriptions->lock);",
            "\tif (mn_itree_is_invalidating(subscriptions)) {",
            "\t\t/*",
            "\t\t * remove is being called after insert put this on the",
            "\t\t * deferred list, but before the deferred list was processed.",
            "\t\t */",
            "\t\tif (RB_EMPTY_NODE(&interval_sub->interval_tree.rb)) {",
            "\t\t\thlist_del(&interval_sub->deferred_item);",
            "\t\t} else {",
            "\t\t\thlist_add_head(&interval_sub->deferred_item,",
            "\t\t\t\t       &subscriptions->deferred_list);",
            "\t\t\tseq = subscriptions->invalidate_seq;",
            "\t\t}",
            "\t} else {",
            "\t\tWARN_ON(RB_EMPTY_NODE(&interval_sub->interval_tree.rb));",
            "\t\tinterval_tree_remove(&interval_sub->interval_tree,",
            "\t\t\t\t     &subscriptions->itree);",
            "\t}",
            "\tspin_unlock(&subscriptions->lock);",
            "",
            "\t/*",
            "\t * The possible sleep on progress in the invalidation requires the",
            "\t * caller not hold any locks held by invalidation callbacks.",
            "\t */",
            "\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);",
            "\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);",
            "\tif (seq)",
            "\t\twait_event(subscriptions->wq,",
            "\t\t\t   mmu_interval_seq_released(subscriptions, seq));",
            "",
            "\t/* pairs with mmgrab in mmu_interval_notifier_insert() */",
            "\tmmdrop(mm);",
            "}",
            "void mmu_notifier_synchronize(void)",
            "{",
            "\tsynchronize_srcu(&srcu);",
            "}"
          ],
          "function_name": "mmu_interval_notifier_insert, mmu_interval_notifier_insert_locked, mmu_interval_seq_released, mmu_interval_notifier_remove, mmu_notifier_synchronize",
          "description": "管理基于区间树的MMU观察者插入与移除逻辑，通过间隔树跟踪虚拟地址范围，处理无效化期间的延迟删除操作，提供同步机制确保序列号变更后唤醒等待线程，最终通过mmdrop释放mm引用",
          "similarity": 0.5562543869018555
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 86,
          "end_line": 201,
          "content": [
            "static bool",
            "mn_itree_is_invalidating(struct mmu_notifier_subscriptions *subscriptions)",
            "{",
            "\tlockdep_assert_held(&subscriptions->lock);",
            "\treturn subscriptions->invalidate_seq & 1;",
            "}",
            "static void mn_itree_inv_end(struct mmu_notifier_subscriptions *subscriptions)",
            "{",
            "\tstruct mmu_interval_notifier *interval_sub;",
            "\tstruct hlist_node *next;",
            "",
            "\tspin_lock(&subscriptions->lock);",
            "\tif (--subscriptions->active_invalidate_ranges ||",
            "\t    !mn_itree_is_invalidating(subscriptions)) {",
            "\t\tspin_unlock(&subscriptions->lock);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Make invalidate_seq even */",
            "\tsubscriptions->invalidate_seq++;",
            "",
            "\t/*",
            "\t * The inv_end incorporates a deferred mechanism like rtnl_unlock().",
            "\t * Adds and removes are queued until the final inv_end happens then",
            "\t * they are progressed. This arrangement for tree updates is used to",
            "\t * avoid using a blocking lock during invalidate_range_start.",
            "\t */",
            "\thlist_for_each_entry_safe(interval_sub, next,",
            "\t\t\t\t  &subscriptions->deferred_list,",
            "\t\t\t\t  deferred_item) {",
            "\t\tif (RB_EMPTY_NODE(&interval_sub->interval_tree.rb))",
            "\t\t\tinterval_tree_insert(&interval_sub->interval_tree,",
            "\t\t\t\t\t     &subscriptions->itree);",
            "\t\telse",
            "\t\t\tinterval_tree_remove(&interval_sub->interval_tree,",
            "\t\t\t\t\t     &subscriptions->itree);",
            "\t\thlist_del(&interval_sub->deferred_item);",
            "\t}",
            "\tspin_unlock(&subscriptions->lock);",
            "",
            "\twake_up_all(&subscriptions->wq);",
            "}",
            "unsigned long",
            "mmu_interval_read_begin(struct mmu_interval_notifier *interval_sub)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\tinterval_sub->mm->notifier_subscriptions;",
            "\tunsigned long seq;",
            "\tbool is_invalidating;",
            "",
            "\t/*",
            "\t * If the subscription has a different seq value under the user_lock",
            "\t * than we started with then it has collided.",
            "\t *",
            "\t * If the subscription currently has the same seq value as the",
            "\t * subscriptions seq, then it is currently between",
            "\t * invalidate_start/end and is colliding.",
            "\t *",
            "\t * The locking looks broadly like this:",
            "\t *   mn_itree_inv_start():                 mmu_interval_read_begin():",
            "\t *                                         spin_lock",
            "\t *                                          seq = READ_ONCE(interval_sub->invalidate_seq);",
            "\t *                                          seq == subs->invalidate_seq",
            "\t *                                         spin_unlock",
            "\t *    spin_lock",
            "\t *     seq = ++subscriptions->invalidate_seq",
            "\t *    spin_unlock",
            "\t *     op->invalidate():",
            "\t *       user_lock",
            "\t *        mmu_interval_set_seq()",
            "\t *         interval_sub->invalidate_seq = seq",
            "\t *       user_unlock",
            "\t *",
            "\t *                          [Required: mmu_interval_read_retry() == true]",
            "\t *",
            "\t *   mn_itree_inv_end():",
            "\t *    spin_lock",
            "\t *     seq = ++subscriptions->invalidate_seq",
            "\t *    spin_unlock",
            "\t *",
            "\t *                                        user_lock",
            "\t *                                         mmu_interval_read_retry():",
            "\t *                                          interval_sub->invalidate_seq != seq",
            "\t *                                        user_unlock",
            "\t *",
            "\t * Barriers are not needed here as any races here are closed by an",
            "\t * eventual mmu_interval_read_retry(), which provides a barrier via the",
            "\t * user_lock.",
            "\t */",
            "\tspin_lock(&subscriptions->lock);",
            "\t/* Pairs with the WRITE_ONCE in mmu_interval_set_seq() */",
            "\tseq = READ_ONCE(interval_sub->invalidate_seq);",
            "\tis_invalidating = seq == subscriptions->invalidate_seq;",
            "\tspin_unlock(&subscriptions->lock);",
            "",
            "\t/*",
            "\t * interval_sub->invalidate_seq must always be set to an odd value via",
            "\t * mmu_interval_set_seq() using the provided cur_seq from",
            "\t * mn_itree_inv_start_range(). This ensures that if seq does wrap we",
            "\t * will always clear the below sleep in some reasonable time as",
            "\t * subscriptions->invalidate_seq is even in the idle state.",
            "\t */",
            "\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);",
            "\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);",
            "\tif (is_invalidating)",
            "\t\twait_event(subscriptions->wq,",
            "\t\t\t   READ_ONCE(subscriptions->invalidate_seq) != seq);",
            "",
            "\t/*",
            "\t * Notice that mmu_interval_read_retry() can already be true at this",
            "\t * point, avoiding loops here allows the caller to provide a global",
            "\t * time bound.",
            "\t */",
            "",
            "\treturn seq;",
            "}"
          ],
          "function_name": "mn_itree_is_invalidating, mn_itree_inv_end, mmu_interval_read_begin",
          "description": "实现了区间树无效化状态检测与结束逻辑，通过序列号比较判断是否处于无效化状态，并在无效化结束时处理延迟的树节点插入/移除操作，同时提供读取序列号的接口防止数据竞争。",
          "similarity": 0.5419905185699463
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 385,
          "end_line": 514,
          "content": [
            "int __mmu_notifier_clear_young(struct mm_struct *mm,",
            "\t\t\t       unsigned long start,",
            "\t\t\t       unsigned long end)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint young = 0, id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription,",
            "\t\t\t\t &mm->notifier_subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->clear_young)",
            "\t\t\tyoung |= subscription->ops->clear_young(subscription,",
            "\t\t\t\t\t\t\t\tmm, start, end);",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "",
            "\treturn young;",
            "}",
            "int __mmu_notifier_test_young(struct mm_struct *mm,",
            "\t\t\t      unsigned long address)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint young = 0, id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription,",
            "\t\t\t\t &mm->notifier_subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->test_young) {",
            "\t\t\tyoung = subscription->ops->test_young(subscription, mm,",
            "\t\t\t\t\t\t\t      address);",
            "\t\t\tif (young)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "",
            "\treturn young;",
            "}",
            "static int mn_itree_invalidate(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\t       const struct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_interval_notifier *interval_sub;",
            "\tunsigned long cur_seq;",
            "",
            "\tfor (interval_sub =",
            "\t\t     mn_itree_inv_start_range(subscriptions, range, &cur_seq);",
            "\t     interval_sub;",
            "\t     interval_sub = mn_itree_inv_next(interval_sub, range)) {",
            "\t\tbool ret;",
            "",
            "\t\tret = interval_sub->ops->invalidate(interval_sub, range,",
            "\t\t\t\t\t\t    cur_seq);",
            "\t\tif (!ret) {",
            "\t\t\tif (WARN_ON(mmu_notifier_range_blockable(range)))",
            "\t\t\t\tcontinue;",
            "\t\t\tgoto out_would_block;",
            "\t\t}",
            "\t}",
            "\treturn 0;",
            "",
            "out_would_block:",
            "\t/*",
            "\t * On -EAGAIN the non-blocking caller is not allowed to call",
            "\t * invalidate_range_end()",
            "\t */",
            "\tmn_itree_inv_end(subscriptions);",
            "\treturn -EAGAIN;",
            "}",
            "static int mn_hlist_invalidate_range_start(",
            "\tstruct mmu_notifier_subscriptions *subscriptions,",
            "\tstruct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint ret = 0;",
            "\tint id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tconst struct mmu_notifier_ops *ops = subscription->ops;",
            "",
            "\t\tif (ops->invalidate_range_start) {",
            "\t\t\tint _ret;",
            "",
            "\t\t\tif (!mmu_notifier_range_blockable(range))",
            "\t\t\t\tnon_block_start();",
            "\t\t\t_ret = ops->invalidate_range_start(subscription, range);",
            "\t\t\tif (!mmu_notifier_range_blockable(range))",
            "\t\t\t\tnon_block_end();",
            "\t\t\tif (_ret) {",
            "\t\t\t\tpr_info(\"%pS callback failed with %d in %sblockable context.\\n\",",
            "\t\t\t\t\tops->invalidate_range_start, _ret,",
            "\t\t\t\t\t!mmu_notifier_range_blockable(range) ?",
            "\t\t\t\t\t\t\"non-\" :",
            "\t\t\t\t\t\t\"\");",
            "\t\t\t\tWARN_ON(mmu_notifier_range_blockable(range) ||",
            "\t\t\t\t\t_ret != -EAGAIN);",
            "\t\t\t\t/*",
            "\t\t\t\t * We call all the notifiers on any EAGAIN,",
            "\t\t\t\t * there is no way for a notifier to know if",
            "\t\t\t\t * its start method failed, thus a start that",
            "\t\t\t\t * does EAGAIN can't also do end.",
            "\t\t\t\t */",
            "\t\t\t\tWARN_ON(ops->invalidate_range_end);",
            "\t\t\t\tret = _ret;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\tif (ret) {",
            "\t\t/*",
            "\t\t * Must be non-blocking to get here.  If there are multiple",
            "\t\t * notifiers and one or more failed start, any that succeeded",
            "\t\t * start are expecting their end to be called.  Do so now.",
            "\t\t */",
            "\t\thlist_for_each_entry_rcu(subscription, &subscriptions->list,",
            "\t\t\t\t\t hlist, srcu_read_lock_held(&srcu)) {",
            "\t\t\tif (!subscription->ops->invalidate_range_end)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tsubscription->ops->invalidate_range_end(subscription,",
            "\t\t\t\t\t\t\t\trange);",
            "\t\t}",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__mmu_notifier_clear_young, __mmu_notifier_test_young, mn_itree_invalidate, mn_hlist_invalidate_range_start",
          "description": "实现了年轻位清除(test_young/clear_young)和范围无效化(invalidate_range_start/end)的通用框架，通过RCU遍历订阅列表并分发到具体实现，支持非阻塞场景下的错误处理与回滚。",
          "similarity": 0.525338888168335
        },
        {
          "chunk_id": 2,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 261,
          "end_line": 363,
          "content": [
            "static void mn_itree_release(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\t     struct mm_struct *mm)",
            "{",
            "\tstruct mmu_notifier_range range = {",
            "\t\t.flags = MMU_NOTIFIER_RANGE_BLOCKABLE,",
            "\t\t.event = MMU_NOTIFY_RELEASE,",
            "\t\t.mm = mm,",
            "\t\t.start = 0,",
            "\t\t.end = ULONG_MAX,",
            "\t};",
            "\tstruct mmu_interval_notifier *interval_sub;",
            "\tunsigned long cur_seq;",
            "\tbool ret;",
            "",
            "\tfor (interval_sub =",
            "\t\t     mn_itree_inv_start_range(subscriptions, &range, &cur_seq);",
            "\t     interval_sub;",
            "\t     interval_sub = mn_itree_inv_next(interval_sub, &range)) {",
            "\t\tret = interval_sub->ops->invalidate(interval_sub, &range,",
            "\t\t\t\t\t\t    cur_seq);",
            "\t\tWARN_ON(!ret);",
            "\t}",
            "",
            "\tmn_itree_inv_end(subscriptions);",
            "}",
            "static void mn_hlist_release(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\t     struct mm_struct *mm)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint id;",
            "",
            "\t/*",
            "\t * SRCU here will block mmu_notifier_unregister until",
            "\t * ->release returns.",
            "\t */",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu))",
            "\t\t/*",
            "\t\t * If ->release runs before mmu_notifier_unregister it must be",
            "\t\t * handled, as it's the only way for the driver to flush all",
            "\t\t * existing sptes and stop the driver from establishing any more",
            "\t\t * sptes before all the pages in the mm are freed.",
            "\t\t */",
            "\t\tif (subscription->ops->release)",
            "\t\t\tsubscription->ops->release(subscription, mm);",
            "",
            "\tspin_lock(&subscriptions->lock);",
            "\twhile (unlikely(!hlist_empty(&subscriptions->list))) {",
            "\t\tsubscription = hlist_entry(subscriptions->list.first,",
            "\t\t\t\t\t   struct mmu_notifier, hlist);",
            "\t\t/*",
            "\t\t * We arrived before mmu_notifier_unregister so",
            "\t\t * mmu_notifier_unregister will do nothing other than to wait",
            "\t\t * for ->release to finish and for mmu_notifier_unregister to",
            "\t\t * return.",
            "\t\t */",
            "\t\thlist_del_init_rcu(&subscription->hlist);",
            "\t}",
            "\tspin_unlock(&subscriptions->lock);",
            "\tsrcu_read_unlock(&srcu, id);",
            "",
            "\t/*",
            "\t * synchronize_srcu here prevents mmu_notifier_release from returning to",
            "\t * exit_mmap (which would proceed with freeing all pages in the mm)",
            "\t * until the ->release method returns, if it was invoked by",
            "\t * mmu_notifier_unregister.",
            "\t *",
            "\t * The notifier_subscriptions can't go away from under us because",
            "\t * one mm_count is held by exit_mmap.",
            "\t */",
            "\tsynchronize_srcu(&srcu);",
            "}",
            "void __mmu_notifier_release(struct mm_struct *mm)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\tmm->notifier_subscriptions;",
            "",
            "\tif (subscriptions->has_itree)",
            "\t\tmn_itree_release(subscriptions, mm);",
            "",
            "\tif (!hlist_empty(&subscriptions->list))",
            "\t\tmn_hlist_release(subscriptions, mm);",
            "}",
            "int __mmu_notifier_clear_flush_young(struct mm_struct *mm,",
            "\t\t\t\t\tunsigned long start,",
            "\t\t\t\t\tunsigned long end)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint young = 0, id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription,",
            "\t\t\t\t &mm->notifier_subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->clear_flush_young)",
            "\t\t\tyoung |= subscription->ops->clear_flush_young(",
            "\t\t\t\tsubscription, mm, start, end);",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "",
            "\treturn young;",
            "}"
          ],
          "function_name": "mn_itree_release, mn_hlist_release, __mmu_notifier_release, __mmu_notifier_clear_flush_young",
          "description": "提供了订阅资源释放机制，包含基于区间树的释放流程(mn_itree_release)和普通链表的释放流程(mn_hlist_release)，通过SRCU保护遍历所有注册的MMU通知器并调用其release回调进行资源清理。",
          "similarity": 0.5038332343101501
        },
        {
          "chunk_id": 4,
          "file_path": "mm/mmu_notifier.c",
          "start_line": 519,
          "end_line": 666,
          "content": [
            "int __mmu_notifier_invalidate_range_start(struct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\trange->mm->notifier_subscriptions;",
            "\tint ret;",
            "",
            "\tif (subscriptions->has_itree) {",
            "\t\tret = mn_itree_invalidate(subscriptions, range);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "\tif (!hlist_empty(&subscriptions->list))",
            "\t\treturn mn_hlist_invalidate_range_start(subscriptions, range);",
            "\treturn 0;",
            "}",
            "static void",
            "mn_hlist_invalidate_end(struct mmu_notifier_subscriptions *subscriptions,",
            "\t\t\tstruct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription, &subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->invalidate_range_end) {",
            "\t\t\tif (!mmu_notifier_range_blockable(range))",
            "\t\t\t\tnon_block_start();",
            "\t\t\tsubscription->ops->invalidate_range_end(subscription,",
            "\t\t\t\t\t\t\t\trange);",
            "\t\t\tif (!mmu_notifier_range_blockable(range))",
            "\t\t\t\tnon_block_end();",
            "\t\t}",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "}",
            "void __mmu_notifier_invalidate_range_end(struct mmu_notifier_range *range)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions =",
            "\t\trange->mm->notifier_subscriptions;",
            "",
            "\tlock_map_acquire(&__mmu_notifier_invalidate_range_start_map);",
            "\tif (subscriptions->has_itree)",
            "\t\tmn_itree_inv_end(subscriptions);",
            "",
            "\tif (!hlist_empty(&subscriptions->list))",
            "\t\tmn_hlist_invalidate_end(subscriptions, range);",
            "\tlock_map_release(&__mmu_notifier_invalidate_range_start_map);",
            "}",
            "void __mmu_notifier_arch_invalidate_secondary_tlbs(struct mm_struct *mm,",
            "\t\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tstruct mmu_notifier *subscription;",
            "\tint id;",
            "",
            "\tid = srcu_read_lock(&srcu);",
            "\thlist_for_each_entry_rcu(subscription,",
            "\t\t\t\t &mm->notifier_subscriptions->list, hlist,",
            "\t\t\t\t srcu_read_lock_held(&srcu)) {",
            "\t\tif (subscription->ops->arch_invalidate_secondary_tlbs)",
            "\t\t\tsubscription->ops->arch_invalidate_secondary_tlbs(",
            "\t\t\t\tsubscription, mm,",
            "\t\t\t\tstart, end);",
            "\t}",
            "\tsrcu_read_unlock(&srcu, id);",
            "}",
            "int __mmu_notifier_register(struct mmu_notifier *subscription,",
            "\t\t\t    struct mm_struct *mm)",
            "{",
            "\tstruct mmu_notifier_subscriptions *subscriptions = NULL;",
            "\tint ret;",
            "",
            "\tmmap_assert_write_locked(mm);",
            "\tBUG_ON(atomic_read(&mm->mm_users) <= 0);",
            "",
            "\t/*",
            "\t * Subsystems should only register for invalidate_secondary_tlbs() or",
            "\t * invalidate_range_start()/end() callbacks, not both.",
            "\t */",
            "\tif (WARN_ON_ONCE(subscription &&",
            "\t\t\t (subscription->ops->arch_invalidate_secondary_tlbs &&",
            "\t\t\t (subscription->ops->invalidate_range_start ||",
            "\t\t\t  subscription->ops->invalidate_range_end))))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!mm->notifier_subscriptions) {",
            "\t\t/*",
            "\t\t * kmalloc cannot be called under mm_take_all_locks(), but we",
            "\t\t * know that mm->notifier_subscriptions can't change while we",
            "\t\t * hold the write side of the mmap_lock.",
            "\t\t */",
            "\t\tsubscriptions = kzalloc(",
            "\t\t\tsizeof(struct mmu_notifier_subscriptions), GFP_KERNEL);",
            "\t\tif (!subscriptions)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tINIT_HLIST_HEAD(&subscriptions->list);",
            "\t\tspin_lock_init(&subscriptions->lock);",
            "\t\tsubscriptions->invalidate_seq = 2;",
            "\t\tsubscriptions->itree = RB_ROOT_CACHED;",
            "\t\tinit_waitqueue_head(&subscriptions->wq);",
            "\t\tINIT_HLIST_HEAD(&subscriptions->deferred_list);",
            "\t}",
            "",
            "\tret = mm_take_all_locks(mm);",
            "\tif (unlikely(ret))",
            "\t\tgoto out_clean;",
            "",
            "\t/*",
            "\t * Serialize the update against mmu_notifier_unregister. A",
            "\t * side note: mmu_notifier_release can't run concurrently with",
            "\t * us because we hold the mm_users pin (either implicitly as",
            "\t * current->mm or explicitly with get_task_mm() or similar).",
            "\t * We can't race against any other mmu notifier method either",
            "\t * thanks to mm_take_all_locks().",
            "\t *",
            "\t * release semantics on the initialization of the",
            "\t * mmu_notifier_subscriptions's contents are provided for unlocked",
            "\t * readers.  acquire can only be used while holding the mmgrab or",
            "\t * mmget, and is safe because once created the",
            "\t * mmu_notifier_subscriptions is not freed until the mm is destroyed.",
            "\t * As above, users holding the mmap_lock or one of the",
            "\t * mm_take_all_locks() do not need to use acquire semantics.",
            "\t */",
            "\tif (subscriptions)",
            "\t\tsmp_store_release(&mm->notifier_subscriptions, subscriptions);",
            "",
            "\tif (subscription) {",
            "\t\t/* Pairs with the mmdrop in mmu_notifier_unregister_* */",
            "\t\tmmgrab(mm);",
            "\t\tsubscription->mm = mm;",
            "\t\tsubscription->users = 1;",
            "",
            "\t\tspin_lock(&mm->notifier_subscriptions->lock);",
            "\t\thlist_add_head_rcu(&subscription->hlist,",
            "\t\t\t\t   &mm->notifier_subscriptions->list);",
            "\t\tspin_unlock(&mm->notifier_subscriptions->lock);",
            "\t} else",
            "\t\tmm->notifier_subscriptions->has_itree = true;",
            "",
            "\tmm_drop_all_locks(mm);",
            "\tBUG_ON(atomic_read(&mm->mm_users) <= 0);",
            "\treturn 0;",
            "",
            "out_clean:",
            "\tkfree(subscriptions);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__mmu_notifier_invalidate_range_start, mn_hlist_invalidate_end, __mmu_notifier_invalidate_range_end, __mmu_notifier_arch_invalidate_secondary_tlbs, __mmu_notifier_register",
          "description": "封装了完整的MMU通知器生命周期管理，包含范围无效化启动/结束接口(__mmu_notifier_invalidate_range_start/end)、架构特定TLB刷新(__mmu_notifier_arch_invalidate_secondary_tlbs)以及注册接口(__mmu_notifier_register)。",
          "similarity": 0.5013250708580017
        }
      ]
    }
  ]
}