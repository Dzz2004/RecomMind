{
  "query": "页式管理与段式管理的内存分配策略对比",
  "timestamp": "2025-12-26 00:02:22",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.59955233335495,
      "chunks": [
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.6202256679534912
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mempolicy.c",
          "start_line": 484,
          "end_line": 639,
          "content": [
            "static void mpol_rebind_preferred(struct mempolicy *pol,",
            "\t\t\t\t\t\tconst nodemask_t *nodes)",
            "{",
            "\tpol->w.cpuset_mems_allowed = *nodes;",
            "}",
            "static void mpol_rebind_policy(struct mempolicy *pol, const nodemask_t *newmask)",
            "{",
            "\tif (!pol || pol->mode == MPOL_LOCAL)",
            "\t\treturn;",
            "\tif (!mpol_store_user_nodemask(pol) &&",
            "\t    nodes_equal(pol->w.cpuset_mems_allowed, *newmask))",
            "\t\treturn;",
            "",
            "\tmpol_ops[pol->mode].rebind(pol, newmask);",
            "}",
            "void mpol_rebind_task(struct task_struct *tsk, const nodemask_t *new)",
            "{",
            "\tmpol_rebind_policy(tsk->mempolicy, new);",
            "}",
            "void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "",
            "\tmmap_write_lock(mm);",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tvma_start_write(vma);",
            "\t\tmpol_rebind_policy(vma->vm_policy, new);",
            "\t}",
            "\tmmap_write_unlock(mm);",
            "}",
            "static bool strictly_unmovable(unsigned long flags)",
            "{",
            "\t/*",
            "\t * STRICT without MOVE flags lets do_mbind() fail immediately with -EIO",
            "\t * if any misplaced page is found.",
            "\t */",
            "\treturn (flags & (MPOL_MF_STRICT | MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ==",
            "\t\t\t MPOL_MF_STRICT;",
            "}",
            "static inline bool queue_folio_required(struct folio *folio,",
            "\t\t\t\t\tstruct queue_pages *qp)",
            "{",
            "\tint nid = folio_nid(folio);",
            "\tunsigned long flags = qp->flags;",
            "",
            "\treturn node_isset(nid, *qp->nmask) == !(flags & MPOL_MF_INVERT);",
            "}",
            "static void queue_folios_pmd(pmd_t *pmd, struct mm_walk *walk)",
            "{",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "",
            "\tif (unlikely(is_pmd_migration_entry(*pmd))) {",
            "\t\tqp->nr_failed++;",
            "\t\treturn;",
            "\t}",
            "\tfolio = pmd_folio(*pmd);",
            "\tif (is_huge_zero_folio(folio)) {",
            "\t\twalk->action = ACTION_CONTINUE;",
            "\t\treturn;",
            "\t}",
            "\tif (!queue_folio_required(folio, qp))",
            "\t\treturn;",
            "\tif (!(qp->flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t    !vma_migratable(walk->vma) ||",
            "\t    !migrate_folio_add(folio, qp->pagelist, qp->flags))",
            "\t\tqp->nr_failed++;",
            "}",
            "static int queue_folios_pte_range(pmd_t *pmd, unsigned long addr,",
            "\t\t\tunsigned long end, struct mm_walk *walk)",
            "{",
            "\tconst fpb_t fpb_flags = FPB_IGNORE_DIRTY | FPB_IGNORE_SOFT_DIRTY;",
            "\tstruct vm_area_struct *vma = walk->vma;",
            "\tstruct folio *folio;",
            "\tstruct queue_pages *qp = walk->private;",
            "\tunsigned long flags = qp->flags;",
            "\tpte_t *pte, *mapped_pte;",
            "\tpte_t ptent;",
            "\tspinlock_t *ptl;",
            "\tint max_nr, nr;",
            "",
            "\tptl = pmd_trans_huge_lock(pmd, vma);",
            "\tif (ptl) {",
            "\t\tqueue_folios_pmd(pmd, walk);",
            "\t\tspin_unlock(ptl);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tmapped_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);",
            "\tif (!pte) {",
            "\t\twalk->action = ACTION_AGAIN;",
            "\t\treturn 0;",
            "\t}",
            "\tfor (; addr != end; pte += nr, addr += nr * PAGE_SIZE) {",
            "\t\tmax_nr = (end - addr) >> PAGE_SHIFT;",
            "\t\tnr = 1;",
            "\t\tptent = ptep_get(pte);",
            "\t\tif (pte_none(ptent))",
            "\t\t\tcontinue;",
            "\t\tif (!pte_present(ptent)) {",
            "\t\t\tif (is_migration_entry(pte_to_swp_entry(ptent)))",
            "\t\t\t\tqp->nr_failed++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfolio = vm_normal_folio(vma, addr, ptent);",
            "\t\tif (!folio || folio_is_zone_device(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio) && max_nr != 1)",
            "\t\t\tnr = folio_pte_batch(folio, addr, pte, ptent,",
            "\t\t\t\t\t     max_nr, fpb_flags,",
            "\t\t\t\t\t     NULL, NULL, NULL);",
            "\t\t/*",
            "\t\t * vm_normal_folio() filters out zero pages, but there might",
            "\t\t * still be reserved folios to skip, perhaps in a VDSO.",
            "\t\t */",
            "\t\tif (folio_test_reserved(folio))",
            "\t\t\tcontinue;",
            "\t\tif (!queue_folio_required(folio, qp))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_large(folio)) {",
            "\t\t\t/*",
            "\t\t\t * A large folio can only be isolated from LRU once,",
            "\t\t\t * but may be mapped by many PTEs (and Copy-On-Write may",
            "\t\t\t * intersperse PTEs of other, order 0, folios).  This is",
            "\t\t\t * a common case, so don't mistake it for failure (but",
            "\t\t\t * there can be other cases of multi-mapped pages which",
            "\t\t\t * this quick check does not help to filter out - and a",
            "\t\t\t * search of the pagelist might grow to be prohibitive).",
            "\t\t\t *",
            "\t\t\t * migrate_pages(&pagelist) returns nr_failed folios, so",
            "\t\t\t * check \"large\" now so that queue_pages_range() returns",
            "\t\t\t * a comparable nr_failed folios.  This does imply that",
            "\t\t\t * if folio could not be isolated for some racy reason",
            "\t\t\t * at its first PTE, later PTEs will not give it another",
            "\t\t\t * chance of isolation; but keeps the accounting simple.",
            "\t\t\t */",
            "\t\t\tif (folio == qp->large)",
            "\t\t\t\tcontinue;",
            "\t\t\tqp->large = folio;",
            "\t\t}",
            "\t\tif (!(flags & (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)) ||",
            "\t\t    !vma_migratable(vma) ||",
            "\t\t    !migrate_folio_add(folio, qp->pagelist, flags)) {",
            "\t\t\tqp->nr_failed += nr;",
            "\t\t\tif (strictly_unmovable(flags))",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tpte_unmap_unlock(mapped_pte, ptl);",
            "\tcond_resched();",
            "out:",
            "\tif (qp->nr_failed && strictly_unmovable(flags))",
            "\t\treturn -EIO;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "mpol_rebind_preferred, mpol_rebind_policy, mpol_rebind_task, mpol_rebind_mm, strictly_unmovable, queue_folio_required, queue_folios_pmd, queue_folios_pte_range",
          "description": "处理页表项遍历与迁移操作，包含页帧队列检测、迁移标志验证及大页迁移逻辑，确保内存分配符合NUMA策略要求。",
          "similarity": 0.6017496585845947
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.5938807129859924
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mempolicy.c",
          "start_line": 168,
          "end_line": 268,
          "content": [
            "static u8 get_il_weight(int node)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tu8 weight = 1;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state)",
            "\t\tweight = state->iw_table[node];",
            "\trcu_read_unlock();",
            "\treturn weight;",
            "}",
            "static void reduce_interleave_weights(unsigned int *bw, u8 *new_iw)",
            "{",
            "\tu64 sum_bw = 0;",
            "\tunsigned int cast_sum_bw, scaling_factor = 1, iw_gcd = 0;",
            "\tint nid;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tsum_bw += bw[nid];",
            "",
            "\t/* Scale bandwidths to whole numbers in the range [1, weightiness] */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t/*",
            "\t\t * Try not to perform 64-bit division.",
            "\t\t * If sum_bw < scaling_factor, then sum_bw < U32_MAX.",
            "\t\t * If sum_bw > scaling_factor, then round the weight up to 1.",
            "\t\t */",
            "\t\tscaling_factor = weightiness * bw[nid];",
            "\t\tif (bw[nid] && sum_bw < scaling_factor) {",
            "\t\t\tcast_sum_bw = (unsigned int)sum_bw;",
            "\t\t\tnew_iw[nid] = scaling_factor / cast_sum_bw;",
            "\t\t} else {",
            "\t\t\tnew_iw[nid] = 1;",
            "\t\t}",
            "\t\tif (!iw_gcd)",
            "\t\t\tiw_gcd = new_iw[nid];",
            "\t\tiw_gcd = gcd(iw_gcd, new_iw[nid]);",
            "\t}",
            "",
            "\t/* 1:2 is strictly better than 16:32. Reduce by the weights' GCD. */",
            "\tfor_each_node_state(nid, N_MEMORY)",
            "\t\tnew_iw[nid] /= iw_gcd;",
            "}",
            "int mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)",
            "{",
            "\tstruct weighted_interleave_state *new_wi_state, *old_wi_state = NULL;",
            "\tunsigned int *old_bw, *new_bw;",
            "\tunsigned int bw_val;",
            "\tint i;",
            "",
            "\tbw_val = min(coords->read_bandwidth, coords->write_bandwidth);",
            "\tnew_bw = kcalloc(nr_node_ids, sizeof(unsigned int), GFP_KERNEL);",
            "\tif (!new_bw)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew_wi_state = kmalloc(struct_size(new_wi_state, iw_table, nr_node_ids),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!new_wi_state) {",
            "\t\tkfree(new_bw);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\tnew_wi_state->mode_auto = true;",
            "\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\tnew_wi_state->iw_table[i] = 1;",
            "",
            "\t/*",
            "\t * Update bandwidth info, even in manual mode. That way, when switching",
            "\t * to auto mode in the future, iw_table can be overwritten using",
            "\t * accurate bw data.",
            "\t */",
            "\tmutex_lock(&wi_state_lock);",
            "",
            "\told_bw = node_bw_table;",
            "\tif (old_bw)",
            "\t\tmemcpy(new_bw, old_bw, nr_node_ids * sizeof(*old_bw));",
            "\tnew_bw[node] = bw_val;",
            "\tnode_bw_table = new_bw;",
            "",
            "\told_wi_state = rcu_dereference_protected(wi_state,",
            "\t\t\t\t\tlockdep_is_held(&wi_state_lock));",
            "\tif (old_wi_state && !old_wi_state->mode_auto) {",
            "\t\t/* Manual mode; skip reducing weights and updating wi_state */",
            "\t\tmutex_unlock(&wi_state_lock);",
            "\t\tkfree(new_wi_state);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* NULL wi_state assumes auto=true; reduce weights and update wi_state*/",
            "\treduce_interleave_weights(new_bw, new_wi_state->iw_table);",
            "\trcu_assign_pointer(wi_state, new_wi_state);",
            "",
            "\tmutex_unlock(&wi_state_lock);",
            "\tif (old_wi_state) {",
            "\t\tsynchronize_rcu();",
            "\t\tkfree(old_wi_state);",
            "\t}",
            "out:",
            "\tkfree(old_bw);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "get_il_weight, reduce_interleave_weights, mempolicy_set_node_perf",
          "description": "实现带权交错策略的权重计算与调整逻辑，通过获取节点带宽数据动态修改权重比例，支持根据性能参数更新节点间内存分配优先级。",
          "similarity": 0.5797799825668335
        },
        {
          "chunk_id": 14,
          "file_path": "mm/mempolicy.c",
          "start_line": 2513,
          "end_line": 2629,
          "content": [
            "static unsigned long alloc_pages_bulk_array_weighted_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tstruct task_struct *me = current;",
            "\tunsigned int cpuset_mems_cookie;",
            "\tunsigned long total_allocated = 0;",
            "\tunsigned long nr_allocated = 0;",
            "\tunsigned long rounds;",
            "\tunsigned long node_pages, delta;",
            "\tu8 *weights, weight;",
            "\tunsigned int weight_total = 0;",
            "\tunsigned long rem_pages = nr_pages;",
            "\tnodemask_t nodes;",
            "\tint nnodes, node;",
            "\tint resume_node = MAX_NUMNODES - 1;",
            "\tu8 resume_weight = 0;",
            "\tint prev_node;",
            "\tint i;",
            "",
            "\tif (!nr_pages)",
            "\t\treturn 0;",
            "",
            "\t/* read the nodes onto the stack, retry if done during rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnnodes = read_once_policy_nodemask(pol, &nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\t/* if the nodemask has become invalid, we cannot do anything */",
            "\tif (!nnodes)",
            "\t\treturn 0;",
            "",
            "\t/* Continue allocating from most recent node and adjust the nr_pages */",
            "\tnode = me->il_prev;",
            "\tweight = me->il_weight;",
            "\tif (weight && node_isset(node, nodes)) {",
            "\t\tnode_pages = min(rem_pages, weight);",
            "\t\tnr_allocated = __alloc_pages_bulk(gfp, node, NULL, node_pages,",
            "\t\t\t\t\t\t  NULL, page_array);",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t\t/* if that's all the pages, no need to interleave */",
            "\t\tif (rem_pages <= weight) {",
            "\t\t\tme->il_weight -= rem_pages;",
            "\t\t\treturn total_allocated;",
            "\t\t}",
            "\t\t/* Otherwise we adjust remaining pages, continue from there */",
            "\t\trem_pages -= weight;",
            "\t}",
            "\t/* clear active weight in case of an allocation failure */",
            "\tme->il_weight = 0;",
            "\tprev_node = node;",
            "",
            "\t/* create a local copy of node weights to operate on outside rcu */",
            "\tweights = kzalloc(nr_node_ids, GFP_KERNEL);",
            "\tif (!weights)",
            "\t\treturn total_allocated;",
            "",
            "\trcu_read_lock();",
            "\tstate = rcu_dereference(wi_state);",
            "\tif (state) {",
            "\t\tmemcpy(weights, state->iw_table, nr_node_ids * sizeof(u8));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\trcu_read_unlock();",
            "\t\tfor (i = 0; i < nr_node_ids; i++)",
            "\t\t\tweights[i] = 1;",
            "\t}",
            "",
            "\t/* calculate total, detect system default usage */",
            "\tfor_each_node_mask(node, nodes)",
            "\t\tweight_total += weights[node];",
            "",
            "\t/*",
            "\t * Calculate rounds/partial rounds to minimize __alloc_pages_bulk calls.",
            "\t * Track which node weighted interleave should resume from.",
            "\t *",
            "\t * if (rounds > 0) and (delta == 0), resume_node will always be",
            "\t * the node following prev_node and its weight.",
            "\t */",
            "\trounds = rem_pages / weight_total;",
            "\tdelta = rem_pages % weight_total;",
            "\tresume_node = next_node_in(prev_node, nodes);",
            "\tresume_weight = weights[resume_node];",
            "\tfor (i = 0; i < nnodes; i++) {",
            "\t\tnode = next_node_in(prev_node, nodes);",
            "\t\tweight = weights[node];",
            "\t\tnode_pages = weight * rounds;",
            "\t\t/* If a delta exists, add this node's portion of the delta */",
            "\t\tif (delta > weight) {",
            "\t\t\tnode_pages += weight;",
            "\t\t\tdelta -= weight;",
            "\t\t} else if (delta) {",
            "\t\t\t/* when delta is depleted, resume from that node */",
            "\t\t\tnode_pages += delta;",
            "\t\t\tresume_node = node;",
            "\t\t\tresume_weight = weight - delta;",
            "\t\t\tdelta = 0;",
            "\t\t}",
            "\t\t/* node_pages can be 0 if an allocation fails and rounds == 0 */",
            "\t\tif (!node_pages)",
            "\t\t\tbreak;",
            "\t\tnr_allocated = __alloc_pages_bulk(gfp, node, NULL, node_pages,",
            "\t\t\t\t\t\t  NULL, page_array);",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t\tif (total_allocated == nr_pages)",
            "\t\t\tbreak;",
            "\t\tprev_node = node;",
            "\t}",
            "\tme->il_prev = resume_node;",
            "\tme->il_weight = resume_weight;",
            "\tkfree(weights);",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_weighted_interleave",
          "description": "alloc_pages_bulk_array_weighted_interleave 实现加权交错批量页面分配，基于策略权重计算各节点分配数量，优先使用最近使用的节点权重，处理剩余页面的分配逻辑。",
          "similarity": 0.5561785101890564
        }
      ]
    },
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.5947154760360718,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.532808780670166
        },
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.5227987766265869
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.5097063183784485
        }
      ]
    },
    {
      "source_file": "mm/page_reporting.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:05:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_reporting.c`\n\n---\n\n# page_reporting.c 技术文档\n\n## 1. 文件概述\n\n`page_reporting.c` 是 Linux 内核中实现**空闲页报告（Page Reporting）**机制的核心模块。该机制允许内核将大块连续的空闲物理内存信息异步上报给注册的设备驱动（如 virtio-balloon、内存热插拔管理器等），以便这些设备可以回收或迁移这些内存，从而提升系统整体内存利用效率。本文件负责协调从伙伴系统（buddy allocator）中提取符合要求的空闲页、构建散列表（scatterlist）、调用设备驱动的报告回调，并在报告完成后将页面安全地归还到伙伴系统。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `page_reporting_order`: 可通过内核启动参数或 sysfs 配置的全局变量，指定待报告空闲页块的最小阶数（order）。默认值为 `-1`（表示未设置），有效范围为 `[0, MAX_PAGE_ORDER]`。导出为 GPL 符号供其他驱动访问。\n- `pr_dev_info`: 指向当前注册的 `page_reporting_dev_info` 结构的 RCU 保护指针，代表提供页报告服务的设备。\n\n### 主要函数\n- `__page_reporting_notify(void)`: 通知已注册的页报告设备有新的空闲页可供报告。这是内核其他部分（如内存释放路径）触发报告流程的入口点。\n- `__page_reporting_request(struct page_reporting_dev_info *prdev)`: 内部函数，用于向指定设备请求启动页报告工作。包含状态机管理和延迟调度逻辑。\n- `page_reporting_drain(...)`: 在设备完成页报告（无论成功与否）后，将散列表中的页面重新放回伙伴系统的对应 free_area 和 migratetype 列表中，并根据报告结果设置 `PG_reported` 标志位。\n- `page_reporting_cycle(...)`: 核心处理循环，遍历指定 zone、order 和 migratetype 的 free_list，隔离未报告的页面到散列表中，达到容量后触发设备报告回调。\n- `page_reporting_process_zone(...)`: 处理单个内存区域（zone）的入口函数，负责水位线检查并按 order 从高到低调用 `page_reporting_cycle`。\n\n### 关键数据结构（外部定义）\n- `struct page_reporting_dev_info`: 由设备驱动提供，包含报告回调函数 `report()`、工作队列 `work` 和状态原子变量 `state` 等。\n\n## 3. 关键实现\n\n### 状态机与延迟调度\n- 使用原子变量 `prdev->state` 管理三种状态：`IDLE`（空闲）、`REQUESTED`（已请求）、`ACTIVE`（活跃中）。\n- 当收到报告请求时，若当前为空闲状态，则调度一个延迟为 `2 * HZ`（约 2 秒）的 `delayed_work`。此延迟旨在累积足够多的空闲页再进行批量报告，减少频繁调用设备驱动的开销。\n\n### 页面隔离与归还\n- **隔离**: 在持有 zone 自旋锁期间，使用 `__isolate_free_page()` 将符合条件的 Buddy 页面从 free_list 中移除。\n- **归还**: 报告完成后，在 `page_reporting_drain()` 中调用 `__putback_isolated_page()` 将页面放回原 free_area 和 migratetype 列表。\n- **报告标记**: 仅当页面在报告后仍保持 Buddy 状态且其阶数未变时，才设置 `PG_reported` 标志，避免对已合并的大页面重复报告。\n\n### 散列表（Scatterlist）管理\n- 使用固定大小（`PAGE_REPORTING_CAPACITY`）的散列表作为设备驱动和内核之间的传输缓冲区。\n- 采用“填满即报”的策略：当散列表填满或遍历完当前 free_list 后，立即调用设备驱动的 `report()` 回调。\n- 报告完成后重置散列表（`sg_init_table`）以供下次使用。\n\n### 遍历策略与预算控制\n- **遍历顺序**: 按内存区域（zone）、迁移类型（migratetype）、页面阶数（order，从高到低）进行嵌套遍历。\n- **预算限制**: 对每个 `(zone, order, mt)` 组合设置处理预算（`budget`），防止单次处理耗时过长影响系统响应。预算基于该 free_area 中空闲页数量动态计算。\n- **列表旋转**: 在中断遍历时，将下一个待处理页面旋转到 free_list 头部（`list_rotate_to_front`），确保下次从断点继续，避免饥饿。\n\n### 水位线保护\n- 在 `page_reporting_process_zone()` 中检查 zone 的空闲页是否高于 `low_wmark + (capacity << reporting_order)`，防止因报告操作导致内存水位过低而引发分配失败或 OOM。\n\n## 4. 依赖关系\n\n- **内部依赖**:\n  - `mm/internal.h`: 提供 `__putback_isolated_page()`、`__isolate_free_page()` 等伙伴系统内部操作函数。\n  - `page_reporting.h` (本地): 定义本地辅助函数和常量（如 `PAGE_REPORTING_CAPACITY`）。\n- **外部依赖**:\n  - `<linux/mm.h>`, `<linux/mmzone.h>`: 内存管理核心头文件，提供 `struct zone`、`free_area`、页面操作宏等。\n  - `<linux/page_reporting.h>`: 定义公共接口 `struct page_reporting_dev_info` 和注册/注销 API。\n  - `<linux/scatterlist.h>`: 提供散列表操作函数（`sg_set_page`, `sg_next` 等）。\n  - 设备驱动: 必须实现 `page_reporting_dev_info.report` 回调函数，并通过 `page_reporting_register()` 注册。\n\n## 5. 使用场景\n\n- **虚拟化环境**: Virtio-balloon 驱动利用此机制向宿主机报告客户机中大块空闲内存，宿主机可将其回收用于其他虚拟机，提高物理内存利用率。\n- **内存热插拔/卸载**: 在移除内存前，通过页报告机制确保目标内存区域尽可能空闲，减少迁移成本。\n- **透明大页（THP）优化**: 协助识别和释放可用于 THP 分配的大块连续空闲内存。\n- **通用内存回收**: 任何需要感知系统大块空闲内存布局的子系统（如 CMA、HMM）均可注册为页报告设备，实现定制化内存管理策略。",
      "similarity": 0.5701783895492554,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/page_reporting.c",
          "start_line": 259,
          "end_line": 393,
          "content": [
            "static int",
            "page_reporting_process_zone(struct page_reporting_dev_info *prdev,",
            "\t\t\t    struct scatterlist *sgl, struct zone *zone)",
            "{",
            "\tunsigned int order, mt, leftover, offset = PAGE_REPORTING_CAPACITY;",
            "\tunsigned long watermark;",
            "\tint err = 0;",
            "",
            "\t/* Generate minimum watermark to be able to guarantee progress */",
            "\twatermark = low_wmark_pages(zone) +",
            "\t\t    (PAGE_REPORTING_CAPACITY << page_reporting_order);",
            "",
            "\t/*",
            "\t * Cancel request if insufficient free memory or if we failed",
            "\t * to allocate page reporting statistics for the zone.",
            "\t */",
            "\tif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))",
            "\t\treturn err;",
            "",
            "\t/* Process each free list starting from lowest order/mt */",
            "\tfor (order = page_reporting_order; order < NR_PAGE_ORDERS; order++) {",
            "\t\tfor (mt = 0; mt < MIGRATE_TYPES; mt++) {",
            "\t\t\t/* We do not pull pages from the isolate free list */",
            "\t\t\tif (is_migrate_isolate(mt))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\terr = page_reporting_cycle(prdev, zone, order, mt,",
            "\t\t\t\t\t\t   sgl, &offset);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "\t}",
            "",
            "\t/* report the leftover pages before going idle */",
            "\tleftover = PAGE_REPORTING_CAPACITY - offset;",
            "\tif (leftover) {",
            "\t\tsgl = &sgl[offset];",
            "\t\terr = prdev->report(prdev, sgl, leftover);",
            "",
            "\t\t/* flush any remaining pages out from the last report */",
            "\t\tspin_lock_irq(&zone->lock);",
            "\t\tpage_reporting_drain(prdev, sgl, leftover, !err);",
            "\t\tspin_unlock_irq(&zone->lock);",
            "\t}",
            "",
            "\treturn err;",
            "}",
            "static void page_reporting_process(struct work_struct *work)",
            "{",
            "\tstruct delayed_work *d_work = to_delayed_work(work);",
            "\tstruct page_reporting_dev_info *prdev =",
            "\t\tcontainer_of(d_work, struct page_reporting_dev_info, work);",
            "\tint err = 0, state = PAGE_REPORTING_ACTIVE;",
            "\tstruct scatterlist *sgl;",
            "\tstruct zone *zone;",
            "",
            "\t/*",
            "\t * Change the state to \"Active\" so that we can track if there is",
            "\t * anyone requests page reporting after we complete our pass. If",
            "\t * the state is not altered by the end of the pass we will switch",
            "\t * to idle and quit scheduling reporting runs.",
            "\t */",
            "\tatomic_set(&prdev->state, state);",
            "",
            "\t/* allocate scatterlist to store pages being reported on */",
            "\tsgl = kmalloc_array(PAGE_REPORTING_CAPACITY, sizeof(*sgl), GFP_KERNEL);",
            "\tif (!sgl)",
            "\t\tgoto err_out;",
            "",
            "\tsg_init_table(sgl, PAGE_REPORTING_CAPACITY);",
            "",
            "\tfor_each_zone(zone) {",
            "\t\terr = page_reporting_process_zone(prdev, sgl, zone);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tkfree(sgl);",
            "err_out:",
            "\t/*",
            "\t * If the state has reverted back to requested then there may be",
            "\t * additional pages to be processed. We will defer for 2s to allow",
            "\t * more pages to accumulate.",
            "\t */",
            "\tstate = atomic_cmpxchg(&prdev->state, state, PAGE_REPORTING_IDLE);",
            "\tif (state == PAGE_REPORTING_REQUESTED)",
            "\t\tschedule_delayed_work(&prdev->work, PAGE_REPORTING_DELAY);",
            "}",
            "int page_reporting_register(struct page_reporting_dev_info *prdev)",
            "{",
            "\tint err = 0;",
            "",
            "\tmutex_lock(&page_reporting_mutex);",
            "",
            "\t/* nothing to do if already in use */",
            "\tif (rcu_dereference_protected(pr_dev_info,",
            "\t\t\t\tlockdep_is_held(&page_reporting_mutex))) {",
            "\t\terr = -EBUSY;",
            "\t\tgoto err_out;",
            "\t}",
            "",
            "\t/*",
            "\t * If the page_reporting_order value is not set, we check if",
            "\t * an order is provided from the driver that is performing the",
            "\t * registration. If that is not provided either, we default to",
            "\t * pageblock_order.",
            "\t */",
            "",
            "\tif (page_reporting_order == -1) {",
            "\t\tif (prdev->order > 0 && prdev->order <= MAX_PAGE_ORDER)",
            "\t\t\tpage_reporting_order = prdev->order;",
            "\t\telse",
            "\t\t\tpage_reporting_order = pageblock_order;",
            "\t}",
            "",
            "\t/* initialize state and work structures */",
            "\tatomic_set(&prdev->state, PAGE_REPORTING_IDLE);",
            "\tINIT_DELAYED_WORK(&prdev->work, &page_reporting_process);",
            "",
            "\t/* Begin initial flush of zones */",
            "\t__page_reporting_request(prdev);",
            "",
            "\t/* Assign device to allow notifications */",
            "\trcu_assign_pointer(pr_dev_info, prdev);",
            "",
            "\t/* enable page reporting notification */",
            "\tif (!static_key_enabled(&page_reporting_enabled)) {",
            "\t\tstatic_branch_enable(&page_reporting_enabled);",
            "\t\tpr_info(\"Free page reporting enabled\\n\");",
            "\t}",
            "err_out:",
            "\tmutex_unlock(&page_reporting_mutex);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "page_reporting_process_zone, page_reporting_process, page_reporting_register",
          "description": "提供内存区级页面报告处理函数，包含主处理循环、工作线程执行路径及设备注册时的初始配置与状态设置。",
          "similarity": 0.5458666086196899
        },
        {
          "chunk_id": 1,
          "file_path": "mm/page_reporting.c",
          "start_line": 17,
          "end_line": 213,
          "content": [
            "static int page_order_update_notify(const char *val, const struct kernel_param *kp)",
            "{",
            "\t/*",
            "\t * If param is set beyond this limit, order is set to default",
            "\t * pageblock_order value",
            "\t */",
            "\treturn  param_set_uint_minmax(val, kp, 0, MAX_PAGE_ORDER);",
            "}",
            "static void",
            "__page_reporting_request(struct page_reporting_dev_info *prdev)",
            "{",
            "\tunsigned int state;",
            "",
            "\t/* Check to see if we are in desired state */",
            "\tstate = atomic_read(&prdev->state);",
            "\tif (state == PAGE_REPORTING_REQUESTED)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * If reporting is already active there is nothing we need to do.",
            "\t * Test against 0 as that represents PAGE_REPORTING_IDLE.",
            "\t */",
            "\tstate = atomic_xchg(&prdev->state, PAGE_REPORTING_REQUESTED);",
            "\tif (state != PAGE_REPORTING_IDLE)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Delay the start of work to allow a sizable queue to build. For",
            "\t * now we are limiting this to running no more than once every",
            "\t * couple of seconds.",
            "\t */",
            "\tschedule_delayed_work(&prdev->work, PAGE_REPORTING_DELAY);",
            "}",
            "void __page_reporting_notify(void)",
            "{",
            "\tstruct page_reporting_dev_info *prdev;",
            "",
            "\t/*",
            "\t * We use RCU to protect the pr_dev_info pointer. In almost all",
            "\t * cases this should be present, however in the unlikely case of",
            "\t * a shutdown this will be NULL and we should exit.",
            "\t */",
            "\trcu_read_lock();",
            "\tprdev = rcu_dereference(pr_dev_info);",
            "\tif (likely(prdev))",
            "\t\t__page_reporting_request(prdev);",
            "",
            "\trcu_read_unlock();",
            "}",
            "static void",
            "page_reporting_drain(struct page_reporting_dev_info *prdev,",
            "\t\t     struct scatterlist *sgl, unsigned int nents, bool reported)",
            "{",
            "\tstruct scatterlist *sg = sgl;",
            "",
            "\t/*",
            "\t * Drain the now reported pages back into their respective",
            "\t * free lists/areas. We assume at least one page is populated.",
            "\t */",
            "\tdo {",
            "\t\tstruct page *page = sg_page(sg);",
            "\t\tint mt = get_pageblock_migratetype(page);",
            "\t\tunsigned int order = get_order(sg->length);",
            "",
            "\t\t__putback_isolated_page(page, order, mt);",
            "",
            "\t\t/* If the pages were not reported due to error skip flagging */",
            "\t\tif (!reported)",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * If page was not comingled with another page we can",
            "\t\t * consider the result to be \"reported\" since the page",
            "\t\t * hasn't been modified, otherwise we will need to",
            "\t\t * report on the new larger page when we make our way",
            "\t\t * up to that higher order.",
            "\t\t */",
            "\t\tif (PageBuddy(page) && buddy_order(page) == order)",
            "\t\t\t__SetPageReported(page);",
            "\t} while ((sg = sg_next(sg)));",
            "",
            "\t/* reinitialize scatterlist now that it is empty */",
            "\tsg_init_table(sgl, nents);",
            "}",
            "static int",
            "page_reporting_cycle(struct page_reporting_dev_info *prdev, struct zone *zone,",
            "\t\t     unsigned int order, unsigned int mt,",
            "\t\t     struct scatterlist *sgl, unsigned int *offset)",
            "{",
            "\tstruct free_area *area = &zone->free_area[order];",
            "\tstruct list_head *list = &area->free_list[mt];",
            "\tunsigned int page_len = PAGE_SIZE << order;",
            "\tstruct page *page, *next;",
            "\tlong budget;",
            "\tint err = 0;",
            "",
            "\t/*",
            "\t * Perform early check, if free area is empty there is",
            "\t * nothing to process so we can skip this free_list.",
            "\t */",
            "\tif (list_empty(list))",
            "\t\treturn err;",
            "",
            "\tspin_lock_irq(&zone->lock);",
            "",
            "\t/*",
            "\t * Limit how many calls we will be making to the page reporting",
            "\t * device for this list. By doing this we avoid processing any",
            "\t * given list for too long.",
            "\t *",
            "\t * The current value used allows us enough calls to process over a",
            "\t * sixteenth of the current list plus one additional call to handle",
            "\t * any pages that may have already been present from the previous",
            "\t * list processed. This should result in us reporting all pages on",
            "\t * an idle system in about 30 seconds.",
            "\t *",
            "\t * The division here should be cheap since PAGE_REPORTING_CAPACITY",
            "\t * should always be a power of 2.",
            "\t */",
            "\tbudget = DIV_ROUND_UP(area->nr_free, PAGE_REPORTING_CAPACITY * 16);",
            "",
            "\t/* loop through free list adding unreported pages to sg list */",
            "\tlist_for_each_entry_safe(page, next, list, lru) {",
            "\t\t/* We are going to skip over the reported pages. */",
            "\t\tif (PageReported(page))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * If we fully consumed our budget then update our",
            "\t\t * state to indicate that we are requesting additional",
            "\t\t * processing and exit this list.",
            "\t\t */",
            "\t\tif (budget < 0) {",
            "\t\t\tatomic_set(&prdev->state, PAGE_REPORTING_REQUESTED);",
            "\t\t\tnext = page;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/* Attempt to pull page from list and place in scatterlist */",
            "\t\tif (*offset) {",
            "\t\t\tif (!__isolate_free_page(page, order)) {",
            "\t\t\t\tnext = page;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "",
            "\t\t\t/* Add page to scatter list */",
            "\t\t\t--(*offset);",
            "\t\t\tsg_set_page(&sgl[*offset], page, page_len, 0);",
            "",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Make the first non-reported page in the free list",
            "\t\t * the new head of the free list before we release the",
            "\t\t * zone lock.",
            "\t\t */",
            "\t\tif (!list_is_first(&page->lru, list))",
            "\t\t\tlist_rotate_to_front(&page->lru, list);",
            "",
            "\t\t/* release lock before waiting on report processing */",
            "\t\tspin_unlock_irq(&zone->lock);",
            "",
            "\t\t/* begin processing pages in local list */",
            "\t\terr = prdev->report(prdev, sgl, PAGE_REPORTING_CAPACITY);",
            "",
            "\t\t/* reset offset since the full list was reported */",
            "\t\t*offset = PAGE_REPORTING_CAPACITY;",
            "",
            "\t\t/* update budget to reflect call to report function */",
            "\t\tbudget--;",
            "",
            "\t\t/* reacquire zone lock and resume processing */",
            "\t\tspin_lock_irq(&zone->lock);",
            "",
            "\t\t/* flush reported pages from the sg list */",
            "\t\tpage_reporting_drain(prdev, sgl, PAGE_REPORTING_CAPACITY, !err);",
            "",
            "\t\t/*",
            "\t\t * Reset next to first entry, the old next isn't valid",
            "\t\t * since we dropped the lock to report the pages",
            "\t\t */",
            "\t\tnext = list_first_entry(list, struct page, lru);",
            "",
            "\t\t/* exit on error */",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\t/* Rotate any leftover pages to the head of the freelist */",
            "\tif (!list_entry_is_head(next, list, lru) && !list_is_first(&next->lru, list))",
            "\t\tlist_rotate_to_front(&next->lru, list);",
            "",
            "\tspin_unlock_irq(&zone->lock);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "page_order_update_notify, __page_reporting_request, __page_reporting_notify, page_reporting_drain, page_reporting_cycle",
          "description": "实现页面报告参数更新、请求处理、状态通知及周期性处理逻辑，包含延迟工作队列调度、内存区域遍历和页面报告流程控制。",
          "similarity": 0.5105460286140442
        },
        {
          "chunk_id": 0,
          "file_path": "mm/page_reporting.c",
          "start_line": 1,
          "end_line": 16,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/page_reporting.h>",
            "#include <linux/gfp.h>",
            "#include <linux/export.h>",
            "#include <linux/module.h>",
            "#include <linux/delay.h>",
            "#include <linux/scatterlist.h>",
            "",
            "#include \"page_reporting.h\"",
            "#include \"internal.h\"",
            "",
            "/* Initialize to an unsupported value */",
            "unsigned int page_reporting_order = -1;",
            ""
          ],
          "function_name": null,
          "description": "声明全局变量page_reporting_order用于存储页面报告的订单值，并包含相关内核模块头文件。",
          "similarity": 0.468656450510025
        },
        {
          "chunk_id": 3,
          "file_path": "mm/page_reporting.c",
          "start_line": 401,
          "end_line": 416,
          "content": [
            "void page_reporting_unregister(struct page_reporting_dev_info *prdev)",
            "{",
            "\tmutex_lock(&page_reporting_mutex);",
            "",
            "\tif (prdev == rcu_dereference_protected(pr_dev_info,",
            "\t\t\t\tlockdep_is_held(&page_reporting_mutex))) {",
            "\t\t/* Disable page reporting notification */",
            "\t\tRCU_INIT_POINTER(pr_dev_info, NULL);",
            "\t\tsynchronize_rcu();",
            "",
            "\t\t/* Flush any existing work, and lock it out */",
            "\t\tcancel_delayed_work_sync(&prdev->work);",
            "\t}",
            "",
            "\tmutex_unlock(&page_reporting_mutex);",
            "}"
          ],
          "function_name": "page_reporting_unregister",
          "description": "实现页面报告设备注销操作，包含RCU安全指针替换、延迟任务取消及互斥锁保护的资源清理逻辑。",
          "similarity": 0.43625932931900024
        }
      ]
    }
  ]
}