{
  "query": "混合内核与微内核对比",
  "timestamp": "2025-12-26 01:56:30",
  "retrieved_files": [
    {
      "source_file": "mm/sparse.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:25:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse.c`\n\n---\n\n# sparse.c 技术文档\n\n## 1. 文件概述\n\n`sparse.c` 是 Linux 内核中实现 **SPARSEMEM（稀疏内存模型）** 的核心文件，用于管理物理内存的稀疏映射。该模型将整个物理地址空间划分为固定大小的“内存段”（memory sections），仅对实际存在的内存段分配 `mem_map`（页描述符数组），从而在支持大物理地址空间的同时节省内存开销。此文件负责内存段的初始化、节点关联、存在性标记以及与内存热插拔和 vmemmap 相关的功能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`mem_section`**: 全局内存段数组，每个元素代表一个内存段，存储该段的 `mem_map` 指针及其他元数据。\n  - 在 `CONFIG_SPARSEMEM_EXTREME` 下为二级指针（动态分配根数组）\n  - 否则为静态二维数组 `[NR_SECTION_ROOTS][SECTIONS_PER_ROOT]`\n- **`section_to_node_table`**: （仅当 `NODE_NOT_IN_PAGE_FLAGS` 时）用于通过内存段号查找所属 NUMA 节点的查找表。\n- **`__highest_present_section_nr`**: 记录当前系统中编号最大的已存在内存段，用于优化遍历。\n\n### 主要函数\n- **`memory_present()`**: 标记指定 PFN 范围内的内存段为“存在”，并关联到指定 NUMA 节点。\n- **`memblocks_present()`**: 遍历所有 memblock 内存区域，调用 `memory_present()` 标记所有系统内存。\n- **`sparse_index_init()`**: （仅 `CONFIG_SPARSEMEM_EXTREME`）为指定内存段分配其所在的根数组项。\n- **`sparse_encode_mem_map()` / `sparse_decode_mem_map()`**: 编码/解码 `mem_map` 指针，使其能通过段内偏移计算出实际 PFN。\n- **`subsection_map_init()`**: （仅 `CONFIG_SPARSEMEM_VMEMMAP`）初始化子段（subsection）位图，用于更细粒度的内存管理。\n- **`page_to_nid()`**: （仅 `NODE_NOT_IN_PAGE_FLAGS`）通过页结构获取其所属 NUMA 节点。\n- **`mminit_validate_memmodel_limits()`**: 验证传入的 PFN 范围是否超出 SPARSEMEM 模型支持的最大地址。\n\n### 辅助宏与内联函数\n- **`for_each_present_section_nr()`**: 高效遍历所有已存在的内存段。\n- **`first_present_section_nr()`**: 获取第一个存在的内存段编号。\n- **`sparse_encode_early_nid()` / `sparse_early_nid()`**: 在早期启动阶段利用 `section_mem_map` 字段临时存储 NUMA 节点 ID。\n\n## 3. 关键实现\n\n### 内存段管理\n- 物理内存被划分为 `PAGES_PER_SECTION` 大小的段（通常 128MB）。\n- `mem_section` 数组索引即为段号（section number），通过 `__nr_to_section()` 宏访问。\n- 段的存在性通过 `SECTION_MARKED_PRESENT` 位标记，并维护 `__highest_present_section_nr` 以加速遍历。\n\n### NUMA 节点关联\n- 若页结构体（`struct page`）中未直接存储节点 ID（`NODE_NOT_IN_PAGE_FLAGS`），则使用 `section_to_node_table` 查找。\n- 在 `memory_present()` 中通过 `set_section_nid()` 建立段到节点的映射。\n\n### 动态内存段分配（SPARSEMEM_EXTREME）\n- 为减少静态内存占用，`mem_section` 采用二级结构：\n  - 一级：`mem_section[]` 指向多个二级数组\n  - 二级：每个 `mem_section[root]` 指向 `SECTIONS_PER_ROOT` 个 `struct mem_section`\n- `sparse_index_init()` 在需要时动态分配二级数组（使用 `kzalloc_node` 或 `memblock_alloc_node`）。\n\n### 早期启动阶段的优化\n- 在 `mem_map` 分配前，复用 `section_mem_map` 字段的高位存储 NUMA 节点 ID（`sparse_encode_early_nid()`）。\n- 此信息在分配真实 `mem_map` 前被清除。\n\n### vmemmap 子段支持\n- 当启用 `CONFIG_SPARSEMEM_VMEMMAP` 时，每个内存段进一步划分为子段（subsections）。\n- `subsection_map_init()` 初始化位图，标记哪些子段包含有效内存，支持更灵活的内存热插拔。\n\n### 地址空间验证\n- `mminit_validate_memmodel_limits()` 确保传入的 PFN 范围不超过 `PHYSMEM_END`（SPARSEMEM 模型最大支持地址），防止越界。\n\n## 4. 依赖关系\n\n- **头文件依赖**:\n  - `<linux/mm.h>`, `<linux/mmzone.h>`: 内存管理核心定义\n  - `<linux/memblock.h>`: 早期内存分配器\n  - `<linux/vmalloc.h>`: 用于 vmemmap 映射\n  - `<asm/dma.h>`: 架构相关 DMA 定义\n  - `\"internal.h\"`: MM 子系统内部头文件\n- **配置选项依赖**:\n  - `CONFIG_SPARSEMEM`: 基础稀疏内存模型\n  - `CONFIG_SPARSEMEM_EXTREME`: 动态内存段分配\n  - `CONFIG_SPARSEMEM_VMEMMAP`: 使用虚拟映射的 mem_map\n  - `CONFIG_MEMORY_HOTPLUG`: 内存热插拔支持\n  - `NODE_NOT_IN_PAGE_FLAGS`: 页结构体不包含节点 ID\n- **与其他模块交互**:\n  - **Memory Block (memblock)**: 通过 `for_each_mem_pfn_range()` 获取初始内存布局\n  - **Page Allocator**: 提供 `struct page` 数组（mem_map）\n  - **NUMA Subsystem**: 通过节点 ID 关联内存与 CPU 拓扑\n  - **Memory Hotplug**: 依赖本文件提供的段管理接口进行内存增删\n\n## 5. 使用场景\n\n- **系统启动初始化**:\n  - `memblocks_present()` 在 `mm_init()` 阶段被调用，标记所有固件报告的内存区域为“存在”。\n- **内存热插拔**:\n  - 热添加内存时，调用 `memory_present()` 标记新段；热移除时清理对应段。\n  - `sparse_index_init()` 支持动态扩展 `mem_section` 数组。\n- **页到节点转换**:\n  - 当 `page_to_nid()` 被调用时（如页面迁移、NUMA 调度），通过段查找节点。\n- **vmemmap 优化**:\n  - 在支持 `SPARSEMEM_VMEMMAP` 的架构（如 x86_64, ARM64）上，`subsection_map_init()` 使内核能按子段粒度映射 `struct page`，减少虚拟地址空间占用。\n- **调试与验证**:\n  - `mminit_validate_memmodel_limits()` 在开发阶段捕获内存模型配置错误。",
      "similarity": 0.5255008339881897,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/sparse.c",
          "start_line": 219,
          "end_line": 339,
          "content": [
            "void __init subsection_map_init(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "}",
            "static void __init memory_present(int nid, unsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn;",
            "",
            "#ifdef CONFIG_SPARSEMEM_EXTREME",
            "\tif (unlikely(!mem_section)) {",
            "\t\tunsigned long size, align;",
            "",
            "\t\tsize = sizeof(struct mem_section *) * NR_SECTION_ROOTS;",
            "\t\talign = 1 << (INTERNODE_CACHE_SHIFT);",
            "\t\tmem_section = memblock_alloc(size, align);",
            "\t\tif (!mem_section)",
            "\t\t\tpanic(\"%s: Failed to allocate %lu bytes align=0x%lx\\n\",",
            "\t\t\t      __func__, size, align);",
            "\t}",
            "#endif",
            "",
            "\tstart &= PAGE_SECTION_MASK;",
            "\tmminit_validate_memmodel_limits(&start, &end);",
            "\tfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION) {",
            "\t\tunsigned long section = pfn_to_section_nr(pfn);",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\tsparse_index_init(section, nid);",
            "\t\tset_section_nid(section, nid);",
            "",
            "\t\tms = __nr_to_section(section);",
            "\t\tif (!ms->section_mem_map) {",
            "\t\t\tms->section_mem_map = sparse_encode_early_nid(nid) |",
            "\t\t\t\t\t\t\tSECTION_IS_ONLINE;",
            "\t\t\t__section_mark_present(ms, section);",
            "\t\t}",
            "\t}",
            "}",
            "static void __init memblocks_present(void)",
            "{",
            "\tunsigned long start, end;",
            "\tint i, nid;",
            "",
            "\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid)",
            "\t\tmemory_present(nid, start, end);",
            "}",
            "static unsigned long sparse_encode_mem_map(struct page *mem_map, unsigned long pnum)",
            "{",
            "\tunsigned long coded_mem_map =",
            "\t\t(unsigned long)(mem_map - (section_nr_to_pfn(pnum)));",
            "\tBUILD_BUG_ON(SECTION_MAP_LAST_BIT > PFN_SECTION_SHIFT);",
            "\tBUG_ON(coded_mem_map & ~SECTION_MAP_MASK);",
            "\treturn coded_mem_map;",
            "}",
            "static void __meminit sparse_init_one_section(struct mem_section *ms,",
            "\t\tunsigned long pnum, struct page *mem_map,",
            "\t\tstruct mem_section_usage *usage, unsigned long flags)",
            "{",
            "\tms->section_mem_map &= ~SECTION_MAP_MASK;",
            "\tms->section_mem_map |= sparse_encode_mem_map(mem_map, pnum)",
            "\t\t| SECTION_HAS_MEM_MAP | flags;",
            "\tms->usage = usage;",
            "}",
            "static unsigned long usemap_size(void)",
            "{",
            "\treturn BITS_TO_LONGS(SECTION_BLOCKFLAGS_BITS) * sizeof(unsigned long);",
            "}",
            "size_t mem_section_usage_size(void)",
            "{",
            "\treturn sizeof(struct mem_section_usage) + usemap_size();",
            "}",
            "static inline phys_addr_t pgdat_to_phys(struct pglist_data *pgdat)",
            "{",
            "#ifndef CONFIG_NUMA",
            "\tVM_BUG_ON(pgdat != &contig_page_data);",
            "\treturn __pa_symbol(&contig_page_data);",
            "#else",
            "\treturn __pa(pgdat);",
            "#endif",
            "}",
            "static void __init check_usemap_section_nr(int nid,",
            "\t\tstruct mem_section_usage *usage)",
            "{",
            "\tunsigned long usemap_snr, pgdat_snr;",
            "\tstatic unsigned long old_usemap_snr;",
            "\tstatic unsigned long old_pgdat_snr;",
            "\tstruct pglist_data *pgdat = NODE_DATA(nid);",
            "\tint usemap_nid;",
            "",
            "\t/* First call */",
            "\tif (!old_usemap_snr) {",
            "\t\told_usemap_snr = NR_MEM_SECTIONS;",
            "\t\told_pgdat_snr = NR_MEM_SECTIONS;",
            "\t}",
            "",
            "\tusemap_snr = pfn_to_section_nr(__pa(usage) >> PAGE_SHIFT);",
            "\tpgdat_snr = pfn_to_section_nr(pgdat_to_phys(pgdat) >> PAGE_SHIFT);",
            "\tif (usemap_snr == pgdat_snr)",
            "\t\treturn;",
            "",
            "\tif (old_usemap_snr == usemap_snr && old_pgdat_snr == pgdat_snr)",
            "\t\t/* skip redundant message */",
            "\t\treturn;",
            "",
            "\told_usemap_snr = usemap_snr;",
            "\told_pgdat_snr = pgdat_snr;",
            "",
            "\tusemap_nid = sparse_early_nid(__nr_to_section(usemap_snr));",
            "\tif (usemap_nid != nid) {",
            "\t\tpr_info(\"node %d must be removed before remove section %ld\\n\",",
            "\t\t\tnid, usemap_snr);",
            "\t\treturn;",
            "\t}",
            "\t/*",
            "\t * There is a circular dependency.",
            "\t * Some platforms allow un-removable section because they will just",
            "\t * gather other removable sections for dynamic partitioning.",
            "\t * Just notify un-removable section's number here.",
            "\t */",
            "\tpr_info(\"Section %ld and %ld (node %d) have a circular dependency on usemap and pgdat allocations\\n\",",
            "\t\tusemap_snr, pgdat_snr, nid);",
            "}"
          ],
          "function_name": "subsection_map_init, memory_present, memblocks_present, sparse_encode_mem_map, sparse_init_one_section, usemap_size, mem_section_usage_size, pgdat_to_phys, check_usemap_section_nr",
          "description": "实现memory_present标记内存区段为有效，memblocks_present遍历所有内存块执行此操作。提供sparse_encode_mem_map编码内存图，sparse_init_one_section初始化区段结构体。包含使用图大小计算、PGDAT物理地址转换及使用图与PGDAT的依赖检查功能。",
          "similarity": 0.4762996733188629
        },
        {
          "chunk_id": 4,
          "file_path": "mm/sparse.c",
          "start_line": 592,
          "end_line": 692,
          "content": [
            "void online_mem_sections(unsigned long start_pfn, unsigned long end_pfn)",
            "{",
            "\tunsigned long pfn;",
            "",
            "\tfor (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {",
            "\t\tunsigned long section_nr = pfn_to_section_nr(pfn);",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\t/* onlining code should never touch invalid ranges */",
            "\t\tif (WARN_ON(!valid_section_nr(section_nr)))",
            "\t\t\tcontinue;",
            "",
            "\t\tms = __nr_to_section(section_nr);",
            "\t\tms->section_mem_map |= SECTION_IS_ONLINE;",
            "\t}",
            "}",
            "void offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn)",
            "{",
            "\tunsigned long pfn;",
            "",
            "\tfor (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {",
            "\t\tunsigned long section_nr = pfn_to_section_nr(pfn);",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\t/*",
            "\t\t * TODO this needs some double checking. Offlining code makes",
            "\t\t * sure to check pfn_valid but those checks might be just bogus",
            "\t\t */",
            "\t\tif (WARN_ON(!valid_section_nr(section_nr)))",
            "\t\t\tcontinue;",
            "",
            "\t\tms = __nr_to_section(section_nr);",
            "\t\tms->section_mem_map &= ~SECTION_IS_ONLINE;",
            "\t}",
            "}",
            "static void depopulate_section_memmap(unsigned long pfn, unsigned long nr_pages,",
            "\t\tstruct vmem_altmap *altmap)",
            "{",
            "\tunsigned long start = (unsigned long) pfn_to_page(pfn);",
            "\tunsigned long end = start + nr_pages * sizeof(struct page);",
            "",
            "\tvmemmap_free(start, end, altmap);",
            "}",
            "static void free_map_bootmem(struct page *memmap)",
            "{",
            "\tunsigned long start = (unsigned long)memmap;",
            "\tunsigned long end = (unsigned long)(memmap + PAGES_PER_SECTION);",
            "",
            "\tvmemmap_free(start, end, NULL);",
            "}",
            "static int clear_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\tDECLARE_BITMAP(map, SUBSECTIONS_PER_SECTION) = { 0 };",
            "\tDECLARE_BITMAP(tmp, SUBSECTIONS_PER_SECTION) = { 0 };",
            "\tstruct mem_section *ms = __pfn_to_section(pfn);",
            "\tunsigned long *subsection_map = ms->usage",
            "\t\t? &ms->usage->subsection_map[0] : NULL;",
            "",
            "\tsubsection_mask_set(map, pfn, nr_pages);",
            "\tif (subsection_map)",
            "\t\tbitmap_and(tmp, map, subsection_map, SUBSECTIONS_PER_SECTION);",
            "",
            "\tif (WARN(!subsection_map || !bitmap_equal(tmp, map, SUBSECTIONS_PER_SECTION),",
            "\t\t\t\t\"section already deactivated (%#lx + %ld)\\n\",",
            "\t\t\t\tpfn, nr_pages))",
            "\t\treturn -EINVAL;",
            "",
            "\tbitmap_xor(subsection_map, map, subsection_map, SUBSECTIONS_PER_SECTION);",
            "\treturn 0;",
            "}",
            "static bool is_subsection_map_empty(struct mem_section *ms)",
            "{",
            "\treturn bitmap_empty(&ms->usage->subsection_map[0],",
            "\t\t\t    SUBSECTIONS_PER_SECTION);",
            "}",
            "static int fill_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\tstruct mem_section *ms = __pfn_to_section(pfn);",
            "\tDECLARE_BITMAP(map, SUBSECTIONS_PER_SECTION) = { 0 };",
            "\tunsigned long *subsection_map;",
            "\tint rc = 0;",
            "",
            "\tsubsection_mask_set(map, pfn, nr_pages);",
            "",
            "\tsubsection_map = &ms->usage->subsection_map[0];",
            "",
            "\tif (bitmap_empty(map, SUBSECTIONS_PER_SECTION))",
            "\t\trc = -EINVAL;",
            "\telse if (bitmap_intersects(map, subsection_map, SUBSECTIONS_PER_SECTION))",
            "\t\trc = -EEXIST;",
            "\telse",
            "\t\tbitmap_or(subsection_map, map, subsection_map,",
            "\t\t\t\tSUBSECTIONS_PER_SECTION);",
            "",
            "\treturn rc;",
            "}",
            "static void depopulate_section_memmap(unsigned long pfn, unsigned long nr_pages,",
            "\t\tstruct vmem_altmap *altmap)",
            "{",
            "\tkvfree(pfn_to_page(pfn));",
            "}"
          ],
          "function_name": "online_mem_sections, offline_mem_sections, depopulate_section_memmap, free_map_bootmem, clear_subsection_map, is_subsection_map_empty, fill_subsection_map, depopulate_section_memmap",
          "description": "提供区段在线/离线操作接口，修改区段在线状态标志。实现depopulate_section_memmap释放内存映射，free_map_bootmem释放启动内存。包含子部分掩码操作函数，用于跟踪和验证内存区域的有效性。",
          "similarity": 0.4466187059879303
        },
        {
          "chunk_id": 3,
          "file_path": "mm/sparse.c",
          "start_line": 410,
          "end_line": 527,
          "content": [
            "static void __init check_usemap_section_nr(int nid,",
            "\t\tstruct mem_section_usage *usage)",
            "{",
            "}",
            "static unsigned long __init section_map_size(void)",
            "{",
            "\treturn ALIGN(sizeof(struct page) * PAGES_PER_SECTION, PMD_SIZE);",
            "}",
            "static unsigned long __init section_map_size(void)",
            "{",
            "\treturn PAGE_ALIGN(sizeof(struct page) * PAGES_PER_SECTION);",
            "}",
            "static inline void __meminit sparse_buffer_free(unsigned long size)",
            "{",
            "\tWARN_ON(!sparsemap_buf || size == 0);",
            "\tmemblock_free(sparsemap_buf, size);",
            "}",
            "static void __init sparse_buffer_init(unsigned long size, int nid)",
            "{",
            "\tphys_addr_t addr = __pa(MAX_DMA_ADDRESS);",
            "\tWARN_ON(sparsemap_buf);\t/* forgot to call sparse_buffer_fini()? */",
            "\t/*",
            "\t * Pre-allocated buffer is mainly used by __populate_section_memmap",
            "\t * and we want it to be properly aligned to the section size - this is",
            "\t * especially the case for VMEMMAP which maps memmap to PMDs",
            "\t */",
            "\tsparsemap_buf = memmap_alloc(size, section_map_size(), addr, nid, true);",
            "\tsparsemap_buf_end = sparsemap_buf + size;",
            "}",
            "static void __init sparse_buffer_fini(void)",
            "{",
            "\tunsigned long size = sparsemap_buf_end - sparsemap_buf;",
            "",
            "\tif (sparsemap_buf && size > 0)",
            "\t\tsparse_buffer_free(size);",
            "\tsparsemap_buf = NULL;",
            "}",
            "void __weak __meminit vmemmap_populate_print_last(void)",
            "{",
            "}",
            "static void __init sparse_init_nid(int nid, unsigned long pnum_begin,",
            "\t\t\t\t   unsigned long pnum_end,",
            "\t\t\t\t   unsigned long map_count)",
            "{",
            "\tstruct mem_section_usage *usage;",
            "\tunsigned long pnum;",
            "\tstruct page *map;",
            "",
            "\tusage = sparse_early_usemaps_alloc_pgdat_section(NODE_DATA(nid),",
            "\t\t\tmem_section_usage_size() * map_count);",
            "\tif (!usage) {",
            "\t\tpr_err(\"%s: node[%d] usemap allocation failed\", __func__, nid);",
            "\t\tgoto failed;",
            "\t}",
            "\tsparse_buffer_init(map_count * section_map_size(), nid);",
            "\tfor_each_present_section_nr(pnum_begin, pnum) {",
            "\t\tunsigned long pfn = section_nr_to_pfn(pnum);",
            "",
            "\t\tif (pnum >= pnum_end)",
            "\t\t\tbreak;",
            "",
            "\t\tmap = __populate_section_memmap(pfn, PAGES_PER_SECTION,",
            "\t\t\t\tnid, NULL, NULL);",
            "\t\tif (!map) {",
            "\t\t\tpr_err(\"%s: node[%d] memory map backing failed. Some memory will not be available.\",",
            "\t\t\t       __func__, nid);",
            "\t\t\tpnum_begin = pnum;",
            "\t\t\tsparse_buffer_fini();",
            "\t\t\tgoto failed;",
            "\t\t}",
            "\t\tcheck_usemap_section_nr(nid, usage);",
            "\t\tsparse_init_one_section(__nr_to_section(pnum), pnum, map, usage,",
            "\t\t\t\tSECTION_IS_EARLY);",
            "\t\tusage = (void *) usage + mem_section_usage_size();",
            "\t}",
            "\tsparse_buffer_fini();",
            "\treturn;",
            "failed:",
            "\t/* We failed to allocate, mark all the following pnums as not present */",
            "\tfor_each_present_section_nr(pnum_begin, pnum) {",
            "\t\tstruct mem_section *ms;",
            "",
            "\t\tif (pnum >= pnum_end)",
            "\t\t\tbreak;",
            "\t\tms = __nr_to_section(pnum);",
            "\t\tms->section_mem_map = 0;",
            "\t}",
            "}",
            "void __init sparse_init(void)",
            "{",
            "\tunsigned long pnum_end, pnum_begin, map_count = 1;",
            "\tint nid_begin;",
            "",
            "\tmemblocks_present();",
            "",
            "\tpnum_begin = first_present_section_nr();",
            "\tnid_begin = sparse_early_nid(__nr_to_section(pnum_begin));",
            "",
            "\t/* Setup pageblock_order for HUGETLB_PAGE_SIZE_VARIABLE */",
            "\tset_pageblock_order();",
            "",
            "\tfor_each_present_section_nr(pnum_begin + 1, pnum_end) {",
            "\t\tint nid = sparse_early_nid(__nr_to_section(pnum_end));",
            "",
            "\t\tif (nid == nid_begin) {",
            "\t\t\tmap_count++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\t/* Init node with sections in range [pnum_begin, pnum_end) */",
            "\t\tsparse_init_nid(nid_begin, pnum_begin, pnum_end, map_count);",
            "\t\tnid_begin = nid;",
            "\t\tpnum_begin = pnum_end;",
            "\t\tmap_count = 1;",
            "\t}",
            "\t/* cover the last node */",
            "\tsparse_init_nid(nid_begin, pnum_begin, pnum_end, map_count);",
            "\tvmemmap_populate_print_last();",
            "}"
          ],
          "function_name": "check_usemap_section_nr, section_map_size, section_map_size, sparse_buffer_free, sparse_buffer_init, sparse_buffer_fini, vmemmap_populate_print_last, sparse_init_nid, sparse_init",
          "description": "定义section_map_size计算区段映射大小，管理sparse_buffer缓冲区的分配释放。实现sparse_init_nid初始化节点区段，通过__populate_section_memmap填充内存图。包含错误处理逻辑，失败时清除已分配区段标识。",
          "similarity": 0.4354898929595947
        },
        {
          "chunk_id": 1,
          "file_path": "mm/sparse.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "int page_to_nid(const struct page *page)",
            "{",
            "\treturn section_to_node_table[page_to_section(page)];",
            "}",
            "static void set_section_nid(unsigned long section_nr, int nid)",
            "{",
            "\tsection_to_node_table[section_nr] = nid;",
            "}",
            "static inline void set_section_nid(unsigned long section_nr, int nid)",
            "{",
            "}",
            "static int __meminit sparse_index_init(unsigned long section_nr, int nid)",
            "{",
            "\tunsigned long root = SECTION_NR_TO_ROOT(section_nr);",
            "\tstruct mem_section *section;",
            "",
            "\t/*",
            "\t * An existing section is possible in the sub-section hotplug",
            "\t * case. First hot-add instantiates, follow-on hot-add reuses",
            "\t * the existing section.",
            "\t *",
            "\t * The mem_hotplug_lock resolves the apparent race below.",
            "\t */",
            "\tif (mem_section[root])",
            "\t\treturn 0;",
            "",
            "\tsection = sparse_index_alloc(nid);",
            "\tif (!section)",
            "\t\treturn -ENOMEM;",
            "",
            "\tmem_section[root] = section;",
            "",
            "\treturn 0;",
            "}",
            "static inline int sparse_index_init(unsigned long section_nr, int nid)",
            "{",
            "\treturn 0;",
            "}",
            "static inline unsigned long sparse_encode_early_nid(int nid)",
            "{",
            "\treturn ((unsigned long)nid << SECTION_NID_SHIFT);",
            "}",
            "static inline int sparse_early_nid(struct mem_section *section)",
            "{",
            "\treturn (section->section_mem_map >> SECTION_NID_SHIFT);",
            "}",
            "static void __meminit mminit_validate_memmodel_limits(unsigned long *start_pfn,",
            "\t\t\t\t\t\tunsigned long *end_pfn)",
            "{",
            "\tunsigned long max_sparsemem_pfn = (PHYSMEM_END + 1) >> PAGE_SHIFT;",
            "",
            "\t/*",
            "\t * Sanity checks - do not allow an architecture to pass",
            "\t * in larger pfns than the maximum scope of sparsemem:",
            "\t */",
            "\tif (*start_pfn > max_sparsemem_pfn) {",
            "\t\tmminit_dprintk(MMINIT_WARNING, \"pfnvalidation\",",
            "\t\t\t\"Start of range %lu -> %lu exceeds SPARSEMEM max %lu\\n\",",
            "\t\t\t*start_pfn, *end_pfn, max_sparsemem_pfn);",
            "\t\tWARN_ON_ONCE(1);",
            "\t\t*start_pfn = max_sparsemem_pfn;",
            "\t\t*end_pfn = max_sparsemem_pfn;",
            "\t} else if (*end_pfn > max_sparsemem_pfn) {",
            "\t\tmminit_dprintk(MMINIT_WARNING, \"pfnvalidation\",",
            "\t\t\t\"End of range %lu -> %lu exceeds SPARSEMEM max %lu\\n\",",
            "\t\t\t*start_pfn, *end_pfn, max_sparsemem_pfn);",
            "\t\tWARN_ON_ONCE(1);",
            "\t\t*end_pfn = max_sparsemem_pfn;",
            "\t}",
            "}",
            "static void __section_mark_present(struct mem_section *ms,",
            "\t\tunsigned long section_nr)",
            "{",
            "\tif (section_nr > __highest_present_section_nr)",
            "\t\t__highest_present_section_nr = section_nr;",
            "",
            "\tms->section_mem_map |= SECTION_MARKED_PRESENT;",
            "}",
            "static inline unsigned long first_present_section_nr(void)",
            "{",
            "\treturn next_present_section_nr(-1);",
            "}",
            "static void subsection_mask_set(unsigned long *map, unsigned long pfn,",
            "\t\tunsigned long nr_pages)",
            "{",
            "\tint idx = subsection_map_index(pfn);",
            "\tint end = subsection_map_index(pfn + nr_pages - 1);",
            "",
            "\tbitmap_set(map, idx, end - idx + 1);",
            "}",
            "void __init subsection_map_init(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\tint end_sec = pfn_to_section_nr(pfn + nr_pages - 1);",
            "\tunsigned long nr, start_sec = pfn_to_section_nr(pfn);",
            "",
            "\tif (!nr_pages)",
            "\t\treturn;",
            "",
            "\tfor (nr = start_sec; nr <= end_sec; nr++) {",
            "\t\tstruct mem_section *ms;",
            "\t\tunsigned long pfns;",
            "",
            "\t\tpfns = min(nr_pages, PAGES_PER_SECTION",
            "\t\t\t\t- (pfn & ~PAGE_SECTION_MASK));",
            "\t\tms = __nr_to_section(nr);",
            "\t\tsubsection_mask_set(ms->usage->subsection_map, pfn, pfns);",
            "",
            "\t\tpr_debug(\"%s: sec: %lu pfns: %lu set(%d, %d)\\n\", __func__, nr,",
            "\t\t\t\tpfns, subsection_map_index(pfn),",
            "\t\t\t\tsubsection_map_index(pfn + pfns - 1));",
            "",
            "\t\tpfn += pfns;",
            "\t\tnr_pages -= pfns;",
            "\t}",
            "}"
          ],
          "function_name": "page_to_nid, set_section_nid, set_section_nid, sparse_index_init, sparse_index_init, sparse_encode_early_nid, sparse_early_nid, mminit_validate_memmodel_limits, __section_mark_present, first_present_section_nr, subsection_mask_set, subsection_map_init",
          "description": "提供区段与节点号转换接口，包括page_to_nid获取节点号，set_section_nid设置区段节点号。实现sparse_index_init初始化区段结构，包含内存分配和有效性检查。定义sparse_encode_early_nid/nid转换函数，以及内存模型验证和子部分掩码操作函数。",
          "similarity": 0.43125468492507935
        },
        {
          "chunk_id": 5,
          "file_path": "mm/sparse.c",
          "start_line": 717,
          "end_line": 839,
          "content": [
            "static void free_map_bootmem(struct page *memmap)",
            "{",
            "\tunsigned long maps_section_nr, removing_section_nr, i;",
            "\tunsigned long magic, nr_pages;",
            "\tstruct page *page = virt_to_page(memmap);",
            "",
            "\tnr_pages = PAGE_ALIGN(PAGES_PER_SECTION * sizeof(struct page))",
            "\t\t>> PAGE_SHIFT;",
            "",
            "\tfor (i = 0; i < nr_pages; i++, page++) {",
            "\t\tmagic = page->index;",
            "",
            "\t\tBUG_ON(magic == NODE_INFO);",
            "",
            "\t\tmaps_section_nr = pfn_to_section_nr(page_to_pfn(page));",
            "\t\tremoving_section_nr = page_private(page);",
            "",
            "\t\t/*",
            "\t\t * When this function is called, the removing section is",
            "\t\t * logical offlined state. This means all pages are isolated",
            "\t\t * from page allocator. If removing section's memmap is placed",
            "\t\t * on the same section, it must not be freed.",
            "\t\t * If it is freed, page allocator may allocate it which will",
            "\t\t * be removed physically soon.",
            "\t\t */",
            "\t\tif (maps_section_nr != removing_section_nr)",
            "\t\t\tput_page_bootmem(page);",
            "\t}",
            "}",
            "static int clear_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\treturn 0;",
            "}",
            "static bool is_subsection_map_empty(struct mem_section *ms)",
            "{",
            "\treturn true;",
            "}",
            "static int fill_subsection_map(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\treturn 0;",
            "}",
            "static void section_deactivate(unsigned long pfn, unsigned long nr_pages,",
            "\t\tstruct vmem_altmap *altmap)",
            "{",
            "\tstruct mem_section *ms = __pfn_to_section(pfn);",
            "\tbool section_is_early = early_section(ms);",
            "\tstruct page *memmap = NULL;",
            "\tbool empty;",
            "",
            "\tif (clear_subsection_map(pfn, nr_pages))",
            "\t\treturn;",
            "",
            "\tempty = is_subsection_map_empty(ms);",
            "\tif (empty) {",
            "\t\tunsigned long section_nr = pfn_to_section_nr(pfn);",
            "",
            "\t\t/*",
            "\t\t * Mark the section invalid so that valid_section()",
            "\t\t * return false. This prevents code from dereferencing",
            "\t\t * ms->usage array.",
            "\t\t */",
            "\t\tms->section_mem_map &= ~SECTION_HAS_MEM_MAP;",
            "",
            "\t\t/*",
            "\t\t * When removing an early section, the usage map is kept (as the",
            "\t\t * usage maps of other sections fall into the same page). It",
            "\t\t * will be re-used when re-adding the section - which is then no",
            "\t\t * longer an early section. If the usage map is PageReserved, it",
            "\t\t * was allocated during boot.",
            "\t\t */",
            "\t\tif (!PageReserved(virt_to_page(ms->usage))) {",
            "\t\t\tkfree_rcu(ms->usage, rcu);",
            "\t\t\tWRITE_ONCE(ms->usage, NULL);",
            "\t\t}",
            "\t\tmemmap = sparse_decode_mem_map(ms->section_mem_map, section_nr);",
            "\t}",
            "",
            "\t/*",
            "\t * The memmap of early sections is always fully populated. See",
            "\t * section_activate() and pfn_valid() .",
            "\t */",
            "\tif (!section_is_early)",
            "\t\tdepopulate_section_memmap(pfn, nr_pages, altmap);",
            "\telse if (memmap)",
            "\t\tfree_map_bootmem(memmap);",
            "",
            "\tif (empty)",
            "\t\tms->section_mem_map = (unsigned long)NULL;",
            "}",
            "int __meminit sparse_add_section(int nid, unsigned long start_pfn,",
            "\t\tunsigned long nr_pages, struct vmem_altmap *altmap,",
            "\t\tstruct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long section_nr = pfn_to_section_nr(start_pfn);",
            "\tstruct mem_section *ms;",
            "\tstruct page *memmap;",
            "\tint ret;",
            "",
            "\tret = sparse_index_init(section_nr, nid);",
            "\tif (ret < 0)",
            "\t\treturn ret;",
            "",
            "\tmemmap = section_activate(nid, start_pfn, nr_pages, altmap, pgmap);",
            "\tif (IS_ERR(memmap))",
            "\t\treturn PTR_ERR(memmap);",
            "",
            "\t/*",
            "\t * Poison uninitialized struct pages in order to catch invalid flags",
            "\t * combinations.",
            "\t */",
            "\tpage_init_poison(memmap, sizeof(struct page) * nr_pages);",
            "",
            "\tms = __nr_to_section(section_nr);",
            "\tset_section_nid(section_nr, nid);",
            "\t__section_mark_present(ms, section_nr);",
            "",
            "\t/* Align memmap to section boundary in the subsection case */",
            "\tif (section_nr_to_pfn(section_nr) != start_pfn)",
            "\t\tmemmap = pfn_to_page(section_nr_to_pfn(section_nr));",
            "\tsparse_init_one_section(ms, section_nr, memmap, ms->usage, 0);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "free_map_bootmem, clear_subsection_map, is_subsection_map_empty, fill_subsection_map, section_deactivate, sparse_add_section",
          "description": "free_map_bootmem 函数遍历指定内存映射区域的 page 结构，跳过与当前移除节（section）相同的物理节号的页面，其余页面通过 put_page_bootmem 释放。clear_subsection_map、is_subsection_map_empty、fill_subsection_map 均为空实现。section_deactivate 处理内存节停用流程，清除子节映射标志，释放早期节的 usage 数据并调用 depopulate_section_memmap 或 free_map_bootmem。sparse_add_section 初始化新内存节，设置节点 ID，标记节为有效，并调整 memmap 地址对齐。",
          "similarity": 0.41255515813827515
        }
      ]
    },
    {
      "source_file": "mm/ksm.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:34:25\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `ksm.c`\n\n---\n\n# ksm.c 技术文档\n\n## 1. 文件概述\n\n`ksm.c` 实现了内核同页合并（Kernel Samepage Merging, KSM）功能，该机制能够动态识别并合并内容完全相同的物理内存页，即使这些页属于不同的进程地址空间且未通过 `fork()` 共享。KSM 通过后台扫描线程周期性地遍历注册的内存区域，利用红黑树（rbtree）结构高效地比对页面内容，将重复页替换为只读的共享页，从而显著减少物理内存占用。此功能特别适用于虚拟化环境中多个相似虚拟机共存的场景。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct ksm_mm_slot`**  \n  表示一个被 KSM 扫描的内存描述符（`mm_struct`）的元数据，包含哈希槽位和反向映射项链表头。\n\n- **`struct ksm_scan`**  \n  全局扫描游标，记录当前扫描进度（包括当前 `mm_slot`、虚拟地址、反向映射项指针及完整扫描轮次计数）。\n\n- **`struct ksm_stable_node`**  \n  稳定红黑树中的节点，代表一个已合并的 KSM 页面。包含：\n  - 红黑树节点或迁移链表指针（联合体复用）\n  - 指向使用该 KSM 页的所有 `rmap_item` 的哈希链表头\n  - 物理页帧号（`kpfn`）或链修剪时间戳\n  - 反向映射项数量（支持链式扩展）\n  - NUMA 节点 ID（若启用 NUMA）\n\n- **`struct ksm_rmap_item`**  \n  反向映射项，跟踪一个虚拟地址到其物理页的映射关系。包含：\n  - 所属 `mm_struct` 和虚拟地址（低比特位用于标志）\n  - 匿名 VMA 指针（稳定状态）或 NUMA 节点 ID（不稳定状态）\n  - 校验和（不稳定状态）\n  - 红黑树节点（不稳定树）或指向 `stable_node` 的指针及哈希链表节点（稳定状态）\n\n### 关键宏定义\n\n- **`STABLE_NODE_CHAIN`**  \n  标识稳定节点为“链”类型（值为 -1024），用于高效管理大量相同内容的 KSM 页副本。\n  \n- **标志位**  \n  - `UNSTABLE_FLAG` (0x100)：标识 `rmap_item` 属于不稳定树\n  - `STABLE_FLAG` (0x200)：标识 `rmap_item` 已链接到稳定树\n  - `SEQNR_MASK` (0x0ff)：用于存储不稳定树序列号的低 8 位\n\n## 3. 关键实现\n\n### 双树架构设计\nKSM 采用**稳定树（stable tree）**与**不稳定树（unstable tree）**协同工作的机制：\n- **稳定树**：存储已确认可合并的只读 KSM 页，因写保护而内容恒定，支持高效精确匹配。\n- **不稳定树**：临时缓存近期未修改的普通页，因内容可能变化而需周期性重建。\n\n### 扫描与合并流程\n1. **增量扫描**：全局游标 `ksm_scan` 遍历所有注册的 `mm_slot` 及其内存区域。\n2. **校验和预筛**：计算页面内容的 `xxhash` 校验和，仅当与上次扫描一致时才尝试插入不稳定树。\n3. **双阶段匹配**：\n   - 优先在**稳定树**中查找完全匹配的 KSM 页\n   - 若未命中，则在**不稳定树**中查找潜在重复页\n4. **树维护策略**：\n   - 不稳定树在每轮全量扫描结束后**完全清空重建**\n   - 稳定树**持久保留**，通过反向映射（rmap）和链式节点优化大规模合并场景\n5. **NUMA 感知**：若 `merge_across_nodes=0`，则为每个 NUMA 节点维护独立的稳定/不稳定树，避免跨节点内存访问开销。\n\n### 内存安全机制\n- **写保护**：合并后的 KSM 页设为只读，任何写操作触发 COW（写时复制）并解除合并。\n- **RMAP 集成**：通过 `anon_vma` 和反向映射链表，在页解绑时高效更新所有相关虚拟地址。\n- **OOM 防护**：在内存压力下可释放 KSM 页以缓解系统压力。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：深度依赖 `mm.h`、`rmap.h`、`pagemap.h` 实现页表操作、反向映射和页生命周期管理。\n- **调度与进程管理**：通过 `sched/mm.h` 获取进程内存上下文，利用 `kthread.h` 创建后台扫描线程。\n- **NUMA 支持**：条件编译依赖 `CONFIG_NUMA`，使用 `numa.h` 实现节点亲和性。\n- **调试与追踪**：集成 `trace/events/ksm.h` 提供运行时事件追踪能力。\n- **哈希算法**：使用 `xxhash.h` 提供高效的内容指纹计算。\n- **内部辅助模块**：依赖 `mm_slot.h` 管理内存描述符槽位，`internal.h` 提供内核 MM 内部接口。\n\n## 5. 使用场景\n\n- **虚拟化环境**：在 KVM/Xen 等 Hypervisor 中合并多个相似虚拟机的内存页（如相同操作系统镜像）。\n- **内存密集型应用**：合并大型应用（如数据库、Web 服务器）中重复的静态数据或零页。\n- **容器化平台**：在 Docker/LXC 等容器运行时中减少同镜像容器的内存占用。\n- **内存超分场景**：在物理内存有限但允许超额分配的系统中提升内存利用率。\n- **开发调试**：通过 `/sys/kernel/mm/ksm/` 接口动态控制扫描速率、合并阈值等参数。",
      "similarity": 0.5242141485214233,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/ksm.c",
          "start_line": 1,
          "end_line": 311,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Memory merging support.",
            " *",
            " * This code enables dynamic sharing of identical pages found in different",
            " * memory areas, even if they are not shared by fork()",
            " *",
            " * Copyright (C) 2008-2009 Red Hat, Inc.",
            " * Authors:",
            " *\tIzik Eidus",
            " *\tAndrea Arcangeli",
            " *\tChris Wright",
            " *\tHugh Dickins",
            " */",
            "",
            "#include <linux/errno.h>",
            "#include <linux/mm.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/fs.h>",
            "#include <linux/mman.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/rwsem.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rmap.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/xxhash.h>",
            "#include <linux/delay.h>",
            "#include <linux/kthread.h>",
            "#include <linux/wait.h>",
            "#include <linux/slab.h>",
            "#include <linux/rbtree.h>",
            "#include <linux/memory.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/swap.h>",
            "#include <linux/ksm.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/freezer.h>",
            "#include <linux/oom.h>",
            "#include <linux/numa.h>",
            "#include <linux/pagewalk.h>",
            "",
            "#include <asm/tlbflush.h>",
            "#include \"internal.h\"",
            "#include \"mm_slot.h\"",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/ksm.h>",
            "",
            "#ifdef CONFIG_NUMA",
            "#define NUMA(x)\t\t(x)",
            "#define DO_NUMA(x)\tdo { (x); } while (0)",
            "#else",
            "#define NUMA(x)\t\t(0)",
            "#define DO_NUMA(x)\tdo { } while (0)",
            "#endif",
            "",
            "/**",
            " * DOC: Overview",
            " *",
            " * A few notes about the KSM scanning process,",
            " * to make it easier to understand the data structures below:",
            " *",
            " * In order to reduce excessive scanning, KSM sorts the memory pages by their",
            " * contents into a data structure that holds pointers to the pages' locations.",
            " *",
            " * Since the contents of the pages may change at any moment, KSM cannot just",
            " * insert the pages into a normal sorted tree and expect it to find anything.",
            " * Therefore KSM uses two data structures - the stable and the unstable tree.",
            " *",
            " * The stable tree holds pointers to all the merged pages (ksm pages), sorted",
            " * by their contents.  Because each such page is write-protected, searching on",
            " * this tree is fully assured to be working (except when pages are unmapped),",
            " * and therefore this tree is called the stable tree.",
            " *",
            " * The stable tree node includes information required for reverse",
            " * mapping from a KSM page to virtual addresses that map this page.",
            " *",
            " * In order to avoid large latencies of the rmap walks on KSM pages,",
            " * KSM maintains two types of nodes in the stable tree:",
            " *",
            " * * the regular nodes that keep the reverse mapping structures in a",
            " *   linked list",
            " * * the \"chains\" that link nodes (\"dups\") that represent the same",
            " *   write protected memory content, but each \"dup\" corresponds to a",
            " *   different KSM page copy of that content",
            " *",
            " * Internally, the regular nodes, \"dups\" and \"chains\" are represented",
            " * using the same struct ksm_stable_node structure.",
            " *",
            " * In addition to the stable tree, KSM uses a second data structure called the",
            " * unstable tree: this tree holds pointers to pages which have been found to",
            " * be \"unchanged for a period of time\".  The unstable tree sorts these pages",
            " * by their contents, but since they are not write-protected, KSM cannot rely",
            " * upon the unstable tree to work correctly - the unstable tree is liable to",
            " * be corrupted as its contents are modified, and so it is called unstable.",
            " *",
            " * KSM solves this problem by several techniques:",
            " *",
            " * 1) The unstable tree is flushed every time KSM completes scanning all",
            " *    memory areas, and then the tree is rebuilt again from the beginning.",
            " * 2) KSM will only insert into the unstable tree, pages whose hash value",
            " *    has not changed since the previous scan of all memory areas.",
            " * 3) The unstable tree is a RedBlack Tree - so its balancing is based on the",
            " *    colors of the nodes and not on their contents, assuring that even when",
            " *    the tree gets \"corrupted\" it won't get out of balance, so scanning time",
            " *    remains the same (also, searching and inserting nodes in an rbtree uses",
            " *    the same algorithm, so we have no overhead when we flush and rebuild).",
            " * 4) KSM never flushes the stable tree, which means that even if it were to",
            " *    take 10 attempts to find a page in the unstable tree, once it is found,",
            " *    it is secured in the stable tree.  (When we scan a new page, we first",
            " *    compare it against the stable tree, and then against the unstable tree.)",
            " *",
            " * If the merge_across_nodes tunable is unset, then KSM maintains multiple",
            " * stable trees and multiple unstable trees: one of each for each NUMA node.",
            " */",
            "",
            "/**",
            " * struct ksm_mm_slot - ksm information per mm that is being scanned",
            " * @slot: hash lookup from mm to mm_slot",
            " * @rmap_list: head for this mm_slot's singly-linked list of rmap_items",
            " */",
            "struct ksm_mm_slot {",
            "\tstruct mm_slot slot;",
            "\tstruct ksm_rmap_item *rmap_list;",
            "};",
            "",
            "/**",
            " * struct ksm_scan - cursor for scanning",
            " * @mm_slot: the current mm_slot we are scanning",
            " * @address: the next address inside that to be scanned",
            " * @rmap_list: link to the next rmap to be scanned in the rmap_list",
            " * @seqnr: count of completed full scans (needed when removing unstable node)",
            " *",
            " * There is only the one ksm_scan instance of this cursor structure.",
            " */",
            "struct ksm_scan {",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tunsigned long address;",
            "\tstruct ksm_rmap_item **rmap_list;",
            "\tunsigned long seqnr;",
            "};",
            "",
            "/**",
            " * struct ksm_stable_node - node of the stable rbtree",
            " * @node: rb node of this ksm page in the stable tree",
            " * @head: (overlaying parent) &migrate_nodes indicates temporarily on that list",
            " * @hlist_dup: linked into the stable_node->hlist with a stable_node chain",
            " * @list: linked into migrate_nodes, pending placement in the proper node tree",
            " * @hlist: hlist head of rmap_items using this ksm page",
            " * @kpfn: page frame number of this ksm page (perhaps temporarily on wrong nid)",
            " * @chain_prune_time: time of the last full garbage collection",
            " * @rmap_hlist_len: number of rmap_item entries in hlist or STABLE_NODE_CHAIN",
            " * @nid: NUMA node id of stable tree in which linked (may not match kpfn)",
            " */",
            "struct ksm_stable_node {",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of stable tree */",
            "\t\tstruct {\t\t/* when listed for migration */",
            "\t\t\tstruct list_head *head;",
            "\t\t\tstruct {",
            "\t\t\t\tstruct hlist_node hlist_dup;",
            "\t\t\t\tstruct list_head list;",
            "\t\t\t};",
            "\t\t};",
            "\t};",
            "\tstruct hlist_head hlist;",
            "\tunion {",
            "\t\tunsigned long kpfn;",
            "\t\tunsigned long chain_prune_time;",
            "\t};",
            "\t/*",
            "\t * STABLE_NODE_CHAIN can be any negative number in",
            "\t * rmap_hlist_len negative range, but better not -1 to be able",
            "\t * to reliably detect underflows.",
            "\t */",
            "#define STABLE_NODE_CHAIN -1024",
            "\tint rmap_hlist_len;",
            "#ifdef CONFIG_NUMA",
            "\tint nid;",
            "#endif",
            "};",
            "",
            "/**",
            " * struct ksm_rmap_item - reverse mapping item for virtual addresses",
            " * @rmap_list: next rmap_item in mm_slot's singly-linked rmap_list",
            " * @anon_vma: pointer to anon_vma for this mm,address, when in stable tree",
            " * @nid: NUMA node id of unstable tree in which linked (may not match page)",
            " * @mm: the memory structure this rmap_item is pointing into",
            " * @address: the virtual address this rmap_item tracks (+ flags in low bits)",
            " * @oldchecksum: previous checksum of the page at that virtual address",
            " * @node: rb node of this rmap_item in the unstable tree",
            " * @head: pointer to stable_node heading this list in the stable tree",
            " * @hlist: link into hlist of rmap_items hanging off that stable_node",
            " */",
            "struct ksm_rmap_item {",
            "\tstruct ksm_rmap_item *rmap_list;",
            "\tunion {",
            "\t\tstruct anon_vma *anon_vma;\t/* when stable */",
            "#ifdef CONFIG_NUMA",
            "\t\tint nid;\t\t/* when node of unstable tree */",
            "#endif",
            "\t};",
            "\tstruct mm_struct *mm;",
            "\tunsigned long address;\t\t/* + low bits used for flags below */",
            "\tunsigned int oldchecksum;\t/* when unstable */",
            "\tunion {",
            "\t\tstruct rb_node node;\t/* when node of unstable tree */",
            "\t\tstruct {\t\t/* when listed from stable tree */",
            "\t\t\tstruct ksm_stable_node *head;",
            "\t\t\tstruct hlist_node hlist;",
            "\t\t};",
            "\t};",
            "};",
            "",
            "#define SEQNR_MASK\t0x0ff\t/* low bits of unstable tree seqnr */",
            "#define UNSTABLE_FLAG\t0x100\t/* is a node of the unstable tree */",
            "#define STABLE_FLAG\t0x200\t/* is listed from the stable tree */",
            "",
            "/* The stable and unstable tree heads */",
            "static struct rb_root one_stable_tree[1] = { RB_ROOT };",
            "static struct rb_root one_unstable_tree[1] = { RB_ROOT };",
            "static struct rb_root *root_stable_tree = one_stable_tree;",
            "static struct rb_root *root_unstable_tree = one_unstable_tree;",
            "",
            "/* Recently migrated nodes of stable tree, pending proper placement */",
            "static LIST_HEAD(migrate_nodes);",
            "#define STABLE_NODE_DUP_HEAD ((struct list_head *)&migrate_nodes.prev)",
            "",
            "#define MM_SLOTS_HASH_BITS 10",
            "static DEFINE_HASHTABLE(mm_slots_hash, MM_SLOTS_HASH_BITS);",
            "",
            "static struct ksm_mm_slot ksm_mm_head = {",
            "\t.slot.mm_node = LIST_HEAD_INIT(ksm_mm_head.slot.mm_node),",
            "};",
            "static struct ksm_scan ksm_scan = {",
            "\t.mm_slot = &ksm_mm_head,",
            "};",
            "",
            "static struct kmem_cache *rmap_item_cache;",
            "static struct kmem_cache *stable_node_cache;",
            "static struct kmem_cache *mm_slot_cache;",
            "",
            "/* The number of pages scanned */",
            "static unsigned long ksm_pages_scanned;",
            "",
            "/* The number of nodes in the stable tree */",
            "static unsigned long ksm_pages_shared;",
            "",
            "/* The number of page slots additionally sharing those nodes */",
            "static unsigned long ksm_pages_sharing;",
            "",
            "/* The number of nodes in the unstable tree */",
            "static unsigned long ksm_pages_unshared;",
            "",
            "/* The number of rmap_items in use: to calculate pages_volatile */",
            "static unsigned long ksm_rmap_items;",
            "",
            "/* The number of stable_node chains */",
            "static unsigned long ksm_stable_node_chains;",
            "",
            "/* The number of stable_node dups linked to the stable_node chains */",
            "static unsigned long ksm_stable_node_dups;",
            "",
            "/* Delay in pruning stale stable_node_dups in the stable_node_chains */",
            "static unsigned int ksm_stable_node_chains_prune_millisecs = 2000;",
            "",
            "/* Maximum number of page slots sharing a stable node */",
            "static int ksm_max_page_sharing = 256;",
            "",
            "/* Number of pages ksmd should scan in one batch */",
            "static unsigned int ksm_thread_pages_to_scan = 100;",
            "",
            "/* Milliseconds ksmd should sleep between batches */",
            "static unsigned int ksm_thread_sleep_millisecs = 20;",
            "",
            "/* Checksum of an empty (zeroed) page */",
            "static unsigned int zero_checksum __read_mostly;",
            "",
            "/* Whether to merge empty (zeroed) pages with actual zero pages */",
            "static bool ksm_use_zero_pages __read_mostly;",
            "",
            "/* The number of zero pages which is placed by KSM */",
            "atomic_long_t ksm_zero_pages = ATOMIC_LONG_INIT(0);",
            "",
            "#ifdef CONFIG_NUMA",
            "/* Zeroed when merging across nodes is not allowed */",
            "static unsigned int ksm_merge_across_nodes = 1;",
            "static int ksm_nr_node_ids = 1;",
            "#else",
            "#define ksm_merge_across_nodes\t1U",
            "#define ksm_nr_node_ids\t\t1",
            "#endif",
            "",
            "#define KSM_RUN_STOP\t0",
            "#define KSM_RUN_MERGE\t1",
            "#define KSM_RUN_UNMERGE\t2",
            "#define KSM_RUN_OFFLINE\t4",
            "static unsigned long ksm_run = KSM_RUN_STOP;",
            "static void wait_while_offlining(void);",
            "",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_thread_wait);",
            "static DECLARE_WAIT_QUEUE_HEAD(ksm_iter_wait);",
            "static DEFINE_MUTEX(ksm_thread_mutex);",
            "static DEFINE_SPINLOCK(ksm_mmlist_lock);",
            "",
            "#define KSM_KMEM_CACHE(__struct, __flags) kmem_cache_create(#__struct,\\",
            "\t\tsizeof(struct __struct), __alignof__(struct __struct),\\",
            "\t\t(__flags), NULL)",
            ""
          ],
          "function_name": null,
          "description": "定义了KSM（内核同一内存页合并）模块的核心数据结构和全局变量，包括稳定树和不稳定树的数据结构、NUMA支持相关定义、哈希表和红黑树操作接口，以及用于跟踪扫描进度的kscan结构。核心功能是建立KSM内存合并算法的基础框架。",
          "similarity": 0.4876437783241272
        },
        {
          "chunk_id": 9,
          "file_path": "mm/ksm.c",
          "start_line": 2608,
          "end_line": 2731,
          "content": [
            "int ksm_enable_merge_any(struct mm_struct *mm)",
            "{",
            "\tint err;",
            "",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn 0;",
            "",
            "\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {",
            "\t\terr = __ksm_enter(mm);",
            "\t\tif (err)",
            "\t\t\treturn err;",
            "\t}",
            "",
            "\tset_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\tksm_add_vmas(mm);",
            "",
            "\treturn 0;",
            "}",
            "int ksm_disable_merge_any(struct mm_struct *mm)",
            "{",
            "\tint err;",
            "",
            "\tif (!test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn 0;",
            "",
            "\terr = ksm_del_vmas(mm);",
            "\tif (err) {",
            "\t\tksm_add_vmas(mm);",
            "\t\treturn err;",
            "\t}",
            "",
            "\tclear_bit(MMF_VM_MERGE_ANY, &mm->flags);",
            "\treturn 0;",
            "}",
            "int ksm_disable(struct mm_struct *mm)",
            "{",
            "\tmmap_assert_write_locked(mm);",
            "",
            "\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags))",
            "\t\treturn 0;",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\treturn ksm_disable_merge_any(mm);",
            "\treturn ksm_del_vmas(mm);",
            "}",
            "int ksm_madvise(struct vm_area_struct *vma, unsigned long start,",
            "\t\tunsigned long end, int advice, unsigned long *vm_flags)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tint err;",
            "",
            "\tswitch (advice) {",
            "\tcase MADV_MERGEABLE:",
            "\t\tif (vma->vm_flags & VM_MERGEABLE)",
            "\t\t\treturn 0;",
            "\t\tif (!vma_ksm_compatible(vma))",
            "\t\t\treturn 0;",
            "",
            "\t\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {",
            "\t\t\terr = __ksm_enter(mm);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "",
            "\t\t*vm_flags |= VM_MERGEABLE;",
            "\t\tbreak;",
            "",
            "\tcase MADV_UNMERGEABLE:",
            "\t\tif (!(*vm_flags & VM_MERGEABLE))",
            "\t\t\treturn 0;\t\t/* just ignore the advice */",
            "",
            "\t\tif (vma->anon_vma) {",
            "\t\t\terr = unmerge_ksm_pages(vma, start, end, true);",
            "\t\t\tif (err)",
            "\t\t\t\treturn err;",
            "\t\t}",
            "",
            "\t\t*vm_flags &= ~VM_MERGEABLE;",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __ksm_enter(struct mm_struct *mm)",
            "{",
            "\tstruct ksm_mm_slot *mm_slot;",
            "\tstruct mm_slot *slot;",
            "\tint needs_wakeup;",
            "",
            "\tmm_slot = mm_slot_alloc(mm_slot_cache);",
            "\tif (!mm_slot)",
            "\t\treturn -ENOMEM;",
            "",
            "\tslot = &mm_slot->slot;",
            "",
            "\t/* Check ksm_run too?  Would need tighter locking */",
            "\tneeds_wakeup = list_empty(&ksm_mm_head.slot.mm_node);",
            "",
            "\tspin_lock(&ksm_mmlist_lock);",
            "\tmm_slot_insert(mm_slots_hash, mm, slot);",
            "\t/*",
            "\t * When KSM_RUN_MERGE (or KSM_RUN_STOP),",
            "\t * insert just behind the scanning cursor, to let the area settle",
            "\t * down a little; when fork is followed by immediate exec, we don't",
            "\t * want ksmd to waste time setting up and tearing down an rmap_list.",
            "\t *",
            "\t * But when KSM_RUN_UNMERGE, it's important to insert ahead of its",
            "\t * scanning cursor, otherwise KSM pages in newly forked mms will be",
            "\t * missed: then we might as well insert at the end of the list.",
            "\t */",
            "\tif (ksm_run & KSM_RUN_UNMERGE)",
            "\t\tlist_add_tail(&slot->mm_node, &ksm_mm_head.slot.mm_node);",
            "\telse",
            "\t\tlist_add_tail(&slot->mm_node, &ksm_scan.mm_slot->slot.mm_node);",
            "\tspin_unlock(&ksm_mmlist_lock);",
            "",
            "\tset_bit(MMF_VM_MERGEABLE, &mm->flags);",
            "\tmmgrab(mm);",
            "",
            "\tif (needs_wakeup)",
            "\t\twake_up_interruptible(&ksm_thread_wait);",
            "",
            "\ttrace_ksm_enter(mm);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ksm_enable_merge_any, ksm_disable_merge_any, ksm_disable, ksm_madvise, __ksm_enter",
          "description": "ksm_enable_merge_any 启用全局合并标志并注册内存区域。ksm_disable_merge_any 禁用合并并清理配置。ksm_disable 移除所有KSM配置。ksm_madvise 处理MADV_MERGEABLE/MADV_UNMERGEABLE建议，调整页面属性。__ksm_enter 注册内存区域到KSM管理系统。",
          "similarity": 0.44828030467033386
        },
        {
          "chunk_id": 13,
          "file_path": "mm/ksm.c",
          "start_line": 3171,
          "end_line": 3288,
          "content": [
            "static ssize_t pages_to_scan_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_thread_pages_to_scan);",
            "}",
            "static ssize_t pages_to_scan_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tunsigned int nr_pages;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &nr_pages);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_thread_pages_to_scan = nr_pages;",
            "",
            "\treturn count;",
            "}",
            "static ssize_t run_show(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\tchar *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_run);",
            "}",
            "static ssize_t run_store(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\t const char *buf, size_t count)",
            "{",
            "\tunsigned int flags;",
            "\tint err;",
            "",
            "\terr = kstrtouint(buf, 10, &flags);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "\tif (flags > KSM_RUN_UNMERGE)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * KSM_RUN_MERGE sets ksmd running, and 0 stops it running.",
            "\t * KSM_RUN_UNMERGE stops it running and unmerges all rmap_items,",
            "\t * breaking COW to free the pages_shared (but leaves mm_slots",
            "\t * on the list for when ksmd may be set running again).",
            "\t */",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_run != flags) {",
            "\t\tksm_run = flags;",
            "\t\tif (flags & KSM_RUN_UNMERGE) {",
            "\t\t\tset_current_oom_origin();",
            "\t\t\terr = unmerge_and_remove_all_rmap_items();",
            "\t\t\tclear_current_oom_origin();",
            "\t\t\tif (err) {",
            "\t\t\t\tksm_run = KSM_RUN_STOP;",
            "\t\t\t\tcount = err;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\tif (flags & KSM_RUN_MERGE)",
            "\t\twake_up_interruptible(&ksm_thread_wait);",
            "",
            "\treturn count;",
            "}",
            "static ssize_t merge_across_nodes_show(struct kobject *kobj,",
            "\t\t\t\t       struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_merge_across_nodes);",
            "}",
            "static ssize_t merge_across_nodes_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tint err;",
            "\tunsigned long knob;",
            "",
            "\terr = kstrtoul(buf, 10, &knob);",
            "\tif (err)",
            "\t\treturn err;",
            "\tif (knob > 1)",
            "\t\treturn -EINVAL;",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_merge_across_nodes != knob) {",
            "\t\tif (ksm_pages_shared || remove_all_stable_nodes())",
            "\t\t\terr = -EBUSY;",
            "\t\telse if (root_stable_tree == one_stable_tree) {",
            "\t\t\tstruct rb_root *buf;",
            "\t\t\t/*",
            "\t\t\t * This is the first time that we switch away from the",
            "\t\t\t * default of merging across nodes: must now allocate",
            "\t\t\t * a buffer to hold as many roots as may be needed.",
            "\t\t\t * Allocate stable and unstable together:",
            "\t\t\t * MAXSMP NODES_SHIFT 10 will use 16kB.",
            "\t\t\t */",
            "\t\t\tbuf = kcalloc(nr_node_ids + nr_node_ids, sizeof(*buf),",
            "\t\t\t\t      GFP_KERNEL);",
            "\t\t\t/* Let us assume that RB_ROOT is NULL is zero */",
            "\t\t\tif (!buf)",
            "\t\t\t\terr = -ENOMEM;",
            "\t\t\telse {",
            "\t\t\t\troot_stable_tree = buf;",
            "\t\t\t\troot_unstable_tree = buf + nr_node_ids;",
            "\t\t\t\t/* Stable tree is empty but not the unstable */",
            "\t\t\t\troot_unstable_tree[0] = one_unstable_tree[0];",
            "\t\t\t}",
            "\t\t}",
            "\t\tif (!err) {",
            "\t\t\tksm_merge_across_nodes = knob;",
            "\t\t\tksm_nr_node_ids = knob ? 1 : nr_node_ids;",
            "\t\t}",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\treturn err ? err : count;",
            "}"
          ],
          "function_name": "pages_to_scan_show, pages_to_scan_store, run_show, run_store, merge_across_nodes_show, merge_across_nodes_store",
          "description": "pages_to_scan_*控制KSMAPI扫描页数；run_*控制KSMAPI运行模式（启动/停止/解合并）；merge_across_nodes_*配置是否允许跨NUMA节点合并，切换时重构稳定树根节点数组",
          "similarity": 0.43711036443710327
        },
        {
          "chunk_id": 14,
          "file_path": "mm/ksm.c",
          "start_line": 3300,
          "end_line": 3407,
          "content": [
            "static ssize_t use_zero_pages_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_use_zero_pages);",
            "}",
            "static ssize_t use_zero_pages_store(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tint err;",
            "\tbool value;",
            "",
            "\terr = kstrtobool(buf, &value);",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\tksm_use_zero_pages = value;",
            "",
            "\treturn count;",
            "}",
            "static ssize_t max_page_sharing_show(struct kobject *kobj,",
            "\t\t\t\t     struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%u\\n\", ksm_max_page_sharing);",
            "}",
            "static ssize_t max_page_sharing_store(struct kobject *kobj,",
            "\t\t\t\t      struct kobj_attribute *attr,",
            "\t\t\t\t      const char *buf, size_t count)",
            "{",
            "\tint err;",
            "\tint knob;",
            "",
            "\terr = kstrtoint(buf, 10, &knob);",
            "\tif (err)",
            "\t\treturn err;",
            "\t/*",
            "\t * When a KSM page is created it is shared by 2 mappings. This",
            "\t * being a signed comparison, it implicitly verifies it's not",
            "\t * negative.",
            "\t */",
            "\tif (knob < 2)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (READ_ONCE(ksm_max_page_sharing) == knob)",
            "\t\treturn count;",
            "",
            "\tmutex_lock(&ksm_thread_mutex);",
            "\twait_while_offlining();",
            "\tif (ksm_max_page_sharing != knob) {",
            "\t\tif (ksm_pages_shared || remove_all_stable_nodes())",
            "\t\t\terr = -EBUSY;",
            "\t\telse",
            "\t\t\tksm_max_page_sharing = knob;",
            "\t}",
            "\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\treturn err ? err : count;",
            "}",
            "static ssize_t pages_scanned_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_scanned);",
            "}",
            "static ssize_t pages_shared_show(struct kobject *kobj,",
            "\t\t\t\t struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_shared);",
            "}",
            "static ssize_t pages_sharing_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_sharing);",
            "}",
            "static ssize_t pages_unshared_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%lu\\n\", ksm_pages_unshared);",
            "}",
            "static ssize_t pages_volatile_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\tlong ksm_pages_volatile;",
            "",
            "\tksm_pages_volatile = ksm_rmap_items - ksm_pages_shared",
            "\t\t\t\t- ksm_pages_sharing - ksm_pages_unshared;",
            "\t/*",
            "\t * It was not worth any locking to calculate that statistic,",
            "\t * but it might therefore sometimes be negative: conceal that.",
            "\t */",
            "\tif (ksm_pages_volatile < 0)",
            "\t\tksm_pages_volatile = 0;",
            "\treturn sysfs_emit(buf, \"%ld\\n\", ksm_pages_volatile);",
            "}",
            "static ssize_t ksm_zero_pages_show(struct kobject *kobj,",
            "\t\t\t\tstruct kobj_attribute *attr, char *buf)",
            "{",
            "\treturn sysfs_emit(buf, \"%ld\\n\", atomic_long_read(&ksm_zero_pages));",
            "}",
            "static ssize_t general_profit_show(struct kobject *kobj,",
            "\t\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\tlong general_profit;",
            "",
            "\tgeneral_profit = (ksm_pages_sharing + atomic_long_read(&ksm_zero_pages)) * PAGE_SIZE -",
            "\t\t\t\tksm_rmap_items * sizeof(struct ksm_rmap_item);",
            "",
            "\treturn sysfs_emit(buf, \"%ld\\n\", general_profit);",
            "}"
          ],
          "function_name": "use_zero_pages_show, use_zero_pages_store, max_page_sharing_show, max_page_sharing_store, pages_scanned_show, pages_shared_show, pages_sharing_show, pages_unshared_show, pages_volatile_show, ksm_zero_pages_show, general_profit_show",
          "description": "use_zero_pages_*控制是否使用零填充页面；max_page_sharing_*设置最大共享页数；pages_*系列接口统计KSMAPI页扫描、共享、分裂等状态；general_profit_*计算整体内存优化收益",
          "similarity": 0.42468273639678955
        },
        {
          "chunk_id": 8,
          "file_path": "mm/ksm.c",
          "start_line": 2487,
          "end_line": 2587,
          "content": [
            "static void ksm_do_scan(unsigned int scan_npages)",
            "{",
            "\tstruct ksm_rmap_item *rmap_item;",
            "\tstruct page *page;",
            "",
            "\twhile (scan_npages-- && likely(!freezing(current))) {",
            "\t\tcond_resched();",
            "\t\trmap_item = scan_get_next_rmap_item(&page);",
            "\t\tif (!rmap_item)",
            "\t\t\treturn;",
            "\t\tcmp_and_merge_page(page, rmap_item);",
            "\t\tput_page(page);",
            "\t\tksm_pages_scanned++;",
            "\t}",
            "}",
            "static int ksmd_should_run(void)",
            "{",
            "\treturn (ksm_run & KSM_RUN_MERGE) && !list_empty(&ksm_mm_head.slot.mm_node);",
            "}",
            "static int ksm_scan_thread(void *nothing)",
            "{",
            "\tunsigned int sleep_ms;",
            "",
            "\tset_freezable();",
            "\tset_user_nice(current, 5);",
            "",
            "\twhile (!kthread_should_stop()) {",
            "\t\tmutex_lock(&ksm_thread_mutex);",
            "\t\twait_while_offlining();",
            "\t\tif (ksmd_should_run())",
            "\t\t\tksm_do_scan(ksm_thread_pages_to_scan);",
            "\t\tmutex_unlock(&ksm_thread_mutex);",
            "",
            "\t\ttry_to_freeze();",
            "",
            "\t\tif (ksmd_should_run()) {",
            "\t\t\tsleep_ms = READ_ONCE(ksm_thread_sleep_millisecs);",
            "\t\t\twait_event_interruptible_timeout(ksm_iter_wait,",
            "\t\t\t\tsleep_ms != READ_ONCE(ksm_thread_sleep_millisecs),",
            "\t\t\t\tmsecs_to_jiffies(sleep_ms));",
            "\t\t} else {",
            "\t\t\twait_event_freezable(ksm_thread_wait,",
            "\t\t\t\tksmd_should_run() || kthread_should_stop());",
            "\t\t}",
            "\t}",
            "\treturn 0;",
            "}",
            "static void __ksm_add_vma(struct vm_area_struct *vma)",
            "{",
            "\tunsigned long vm_flags = vma->vm_flags;",
            "",
            "\tif (vm_flags & VM_MERGEABLE)",
            "\t\treturn;",
            "",
            "\tif (vma_ksm_compatible(vma))",
            "\t\tvm_flags_set(vma, VM_MERGEABLE);",
            "}",
            "static int __ksm_del_vma(struct vm_area_struct *vma)",
            "{",
            "\tint err;",
            "",
            "\tif (!(vma->vm_flags & VM_MERGEABLE))",
            "\t\treturn 0;",
            "",
            "\tif (vma->anon_vma) {",
            "\t\terr = unmerge_ksm_pages(vma, vma->vm_start, vma->vm_end, true);",
            "\t\tif (err)",
            "\t\t\treturn err;",
            "\t}",
            "",
            "\tvm_flags_clear(vma, VM_MERGEABLE);",
            "\treturn 0;",
            "}",
            "void ksm_add_vma(struct vm_area_struct *vma)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "",
            "\tif (test_bit(MMF_VM_MERGE_ANY, &mm->flags))",
            "\t\t__ksm_add_vma(vma);",
            "}",
            "static void ksm_add_vmas(struct mm_struct *mm)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "\tfor_each_vma(vmi, vma)",
            "\t\t__ksm_add_vma(vma);",
            "}",
            "static int ksm_del_vmas(struct mm_struct *mm)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tint err;",
            "",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "\tfor_each_vma(vmi, vma) {",
            "\t\terr = __ksm_del_vma(vma);",
            "\t\tif (err)",
            "\t\t\treturn err;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "ksm_do_scan, ksmd_should_run, ksm_scan_thread, __ksm_add_vma, __ksm_del_vma, ksm_add_vma, ksm_add_vmas, ksm_del_vmas",
          "description": "ksm_do_scan 由KSM线程执行，遍历页面进行合并操作。ksm_scan_thread 是守护线程，周期性扫描内存区域。__ksm_add_vma/__ksm_del_vma 管理VMA标志位，控制是否启用KSM合并。",
          "similarity": 0.41921210289001465
        }
      ]
    },
    {
      "source_file": "mm/mm_init.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:50:02\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mm_init.c`\n\n---\n\n# mm_init.c 技术文档\n\n## 1. 文件概述\n\n`mm_init.c` 是 Linux 内核内存管理子系统（Memory Management, MM）中的一个初始化和调试辅助文件。其主要作用包括：\n\n- 提供内存初始化过程的验证与调试功能（在 `CONFIG_DEBUG_MEMORY_INIT` 启用时）\n- 初始化内存相关的全局参数和 sysfs 接口\n- 解析内核启动命令行参数（如 `kernelcore` 和 `movablecore`），用于控制不可移动与可移动内存区域的分配策略\n- 在 SMP 系统中动态计算 `vm_committed_as` 的批处理阈值，以优化内存提交统计的性能\n\n该文件不直接参与页分配或虚拟内存管理的核心逻辑，而是为内存子系统的正确性验证、配置和可观测性提供支持。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `mminit_verify_zonelist()` | 验证并打印每个 NUMA 节点的 zonelist 结构，用于调试内存区域组织 |\n| `mminit_verify_pageflags_layout()` | 验证 `struct page` 中用于存储节点、区域、节区等元数据的位域布局是否无重叠且对齐正确 |\n| `set_mminit_loglevel()` | 解析 `mminit_loglevel` 内核参数，设置内存初始化调试日志级别 |\n| `mm_compute_batch()` | 根据系统内存总量和 CPU 数量，计算 `vm_committed_as` per-CPU 计数器的批处理阈值 |\n| `mm_compute_batch_notifier()` | 内存热插拔事件回调，重新计算 `vm_committed_as` 批处理值 |\n| `mm_sysfs_init()` | 创建 `/sys/kernel/mm` sysfs 目录，用于暴露内存子系统信息 |\n| `cmdline_parse_core()` | 辅助函数，解析带百分比或字节单位的内存大小参数 |\n| `cmdline_parse_kernelcore()` / `cmdline_parse_movablecore()` | 解析 `kernelcore=` 和 `movablecore=` 内核启动参数 |\n\n### 主要全局变量\n\n| 变量名 | 类型/说明 |\n|--------|---------|\n| `mminit_loglevel` | 调试日志级别（仅当 `CONFIG_DEBUG_MEMORY_INIT` 启用） |\n| `mm_kobj` | 指向 `/sys/kernel/mm` 的 kobject 指针 |\n| `vm_committed_as_batch` | `vm_committed_as` per-CPU 计数器的批处理阈值（SMP） |\n| `required_kernelcore` / `required_kernelcore_percent` | 用户指定的不可移动内存需求（页数或百分比） |\n| `required_movablecore` / `required_movablecore_percent` | 用户指定的可移动内存需求（页数或百分比） |\n| `mirrored_kernelcore` | 是否启用镜像式 kernelcore 布局 |\n| `arch_zone_lowest_possible_pfn[]` / `arch_zone_highest_possible_pfn[]` | 架构定义的各内存区域（ZONE）的 PFN 范围 |\n| `zone_movable_pfn[]` | 各 NUMA 节点上 ZONE_MOVABLE 的起始 PFN |\n| `deferred_struct_pages` | 标记是否延迟初始化 struct page 实例 |\n\n## 3. 关键实现\n\n### 3.1 内存初始化调试（`CONFIG_DEBUG_MEMORY_INIT`）\n\n- **Zonelist 验证**：`mminit_verify_zonelist()` 遍历所有在线 NUMA 节点，打印其“通用”（general）和“本节点优先”（thisnode）两种 zonelist 的组成，帮助开发者确认内存区域的 fallback 顺序是否符合预期。\n- **Page Flags 布局验证**：`mminit_verify_pageflags_layout()` 检查 `struct page` 中用于编码物理位置（section/node/zone）的位域是否：\n  - 总宽度不超过 `BITS_PER_LONG`\n  - 各字段偏移（`_PGSHIFT`）与宽度一致\n  - 位掩码无重叠（通过 `or_mask == add_mask` 验证）\n\n### 3.2 内存区域划分策略\n\n- 通过 `kernelcore=` 和 `movablecore=` 参数，用户可显式指定系统中用于**不可移动分配**（如内核数据结构）和**可移动分配**（如用户页、可迁移 slab）的内存大小。\n- 支持 `kernelcore=mirror` 模式，在支持内存镜像的平台上启用特殊布局。\n- 参数值可为绝对字节数（如 `512M`）或总内存百分比（如 `40%`）。\n\n### 3.3 `vm_committed_as` 批处理优化（SMP）\n\n- `vm_committed_as` 是一个 per-CPU 计数器，跟踪已提交虚拟内存总量。\n- 为减少原子操作开销，当本地计数器变化超过 `vm_committed_as_batch` 时才同步到全局值。\n- `mm_compute_batch()` 根据 overcommit 策略动态调整 batch 大小：\n  - `OVERCOMMIT_NEVER`：batch = 总内存 / CPU数 / 256（约 0.4%）\n  - 其他策略：batch = 总内存 / CPU数 / 4（25%）\n- 注册内存热插拔通知器，确保内存容量变化后重新计算 batch 值。\n\n### 3.4 Sysfs 接口初始化\n\n- `mm_sysfs_init()` 在内核早期创建 `/sys/kernel/mm` 目录，作为内存子系统其他模块（如 compaction、numa、transparent_hugepage 等）注册 sysfs 属性的基础。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/memory.h>`、`<linux/memblock.h>`：内存块和热插拔管理\n  - `<linux/page-isolation.h>`、`<linux/cma.h>`：连续内存分配和页面隔离\n  - `\"internal.h\"`、`\"slab.h\"`：MM 子系统内部接口\n  - `<asm/setup.h>`：架构相关内存布局信息\n- **配置依赖**：\n  - `CONFIG_DEBUG_MEMORY_INIT`：启用调试验证功能\n  - `CONFIG_SMP`：启用 `vm_committed_as_batch` 优化\n  - `CONFIG_SYSFS`：支持 mm sysfs 目录创建\n- **被依赖模块**：\n  - 内存初始化流程（`mm_init()` in `init/main.c`）\n  - 页面分配器（`page_alloc.c`）使用 `zone_movable_pfn` 等变量\n  - 内存热插拔子系统调用 batch 重计算回调\n\n## 5. 使用场景\n\n- **内核开发与调试**：开发者启用 `CONFIG_DEBUG_MEMORY_INIT` 并设置 `mminit_loglevel`，可在启动时验证内存拓扑结构和 page 结构体布局的正确性。\n- **系统部署调优**：管理员通过 `kernelcore=` 或 `movablecore=` 参数，强制划分不可移动/可移动内存区域，以优化透明大页（THP）或避免内存碎片。\n- **高可靠性系统**：使用 `kernelcore=mirror` 在支持的硬件上启用内存镜像，提升容错能力。\n- **大规模 SMP 系统**：自动调整 `vm_committed_as_batch` 减少锁竞争，提升多进程内存密集型应用的性能。\n- **运行时监控**：`/sys/kernel/mm` 为用户空间工具（如 `numastat`、`cma` 调试接口）提供统一入口点。",
      "similarity": 0.5099141597747803,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/mm_init.c",
          "start_line": 320,
          "end_line": 557,
          "content": [
            "static void __init find_usable_zone_for_movable(void)",
            "{",
            "\tint zone_index;",
            "\tfor (zone_index = MAX_NR_ZONES - 1; zone_index >= 0; zone_index--) {",
            "\t\tif (zone_index == ZONE_MOVABLE)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (arch_zone_highest_possible_pfn[zone_index] >",
            "\t\t\t\tarch_zone_lowest_possible_pfn[zone_index])",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tVM_BUG_ON(zone_index == -1);",
            "\tmovable_zone = zone_index;",
            "}",
            "static void __init find_zone_movable_pfns_for_nodes(void)",
            "{",
            "\tint i, nid;",
            "\tunsigned long usable_startpfn;",
            "\tunsigned long kernelcore_node, kernelcore_remaining;",
            "\t/* save the state before borrow the nodemask */",
            "\tnodemask_t saved_node_state = node_states[N_MEMORY];",
            "\tunsigned long totalpages = early_calculate_totalpages();",
            "\tint usable_nodes = nodes_weight(node_states[N_MEMORY]);",
            "\tstruct memblock_region *r;",
            "",
            "\t/* Need to find movable_zone earlier when movable_node is specified. */",
            "\tfind_usable_zone_for_movable();",
            "",
            "\t/*",
            "\t * If movable_node is specified, ignore kernelcore and movablecore",
            "\t * options.",
            "\t */",
            "\tif (movable_node_is_enabled()) {",
            "\t\tfor_each_mem_region(r) {",
            "\t\t\tif (!memblock_is_hotpluggable(r))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tnid = memblock_get_region_node(r);",
            "",
            "\t\t\tusable_startpfn = PFN_DOWN(r->base);",
            "\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?",
            "\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :",
            "\t\t\t\tusable_startpfn;",
            "\t\t}",
            "",
            "\t\tgoto out2;",
            "\t}",
            "",
            "\t/*",
            "\t * If kernelcore=mirror is specified, ignore movablecore option",
            "\t */",
            "\tif (mirrored_kernelcore) {",
            "\t\tbool mem_below_4gb_not_mirrored = false;",
            "",
            "\t\tif (!memblock_has_mirror()) {",
            "\t\t\tpr_warn(\"The system has no mirror memory, ignore kernelcore=mirror.\\n\");",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (is_kdump_kernel()) {",
            "\t\t\tpr_warn(\"The system is under kdump, ignore kernelcore=mirror.\\n\");",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tfor_each_mem_region(r) {",
            "\t\t\tif (memblock_is_mirror(r))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tnid = memblock_get_region_node(r);",
            "",
            "\t\t\tusable_startpfn = memblock_region_memory_base_pfn(r);",
            "",
            "\t\t\tif (usable_startpfn < PHYS_PFN(SZ_4G)) {",
            "\t\t\t\tmem_below_4gb_not_mirrored = true;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?",
            "\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :",
            "\t\t\t\tusable_startpfn;",
            "\t\t}",
            "",
            "\t\tif (mem_below_4gb_not_mirrored)",
            "\t\t\tpr_warn(\"This configuration results in unmirrored kernel memory.\\n\");",
            "",
            "\t\tgoto out2;",
            "\t}",
            "",
            "\t/*",
            "\t * If kernelcore=nn% or movablecore=nn% was specified, calculate the",
            "\t * amount of necessary memory.",
            "\t */",
            "\tif (required_kernelcore_percent)",
            "\t\trequired_kernelcore = (totalpages * 100 * required_kernelcore_percent) /",
            "\t\t\t\t       10000UL;",
            "\tif (required_movablecore_percent)",
            "\t\trequired_movablecore = (totalpages * 100 * required_movablecore_percent) /",
            "\t\t\t\t\t10000UL;",
            "",
            "\t/*",
            "\t * If movablecore= was specified, calculate what size of",
            "\t * kernelcore that corresponds so that memory usable for",
            "\t * any allocation type is evenly spread. If both kernelcore",
            "\t * and movablecore are specified, then the value of kernelcore",
            "\t * will be used for required_kernelcore if it's greater than",
            "\t * what movablecore would have allowed.",
            "\t */",
            "\tif (required_movablecore) {",
            "\t\tunsigned long corepages;",
            "",
            "\t\t/*",
            "\t\t * Round-up so that ZONE_MOVABLE is at least as large as what",
            "\t\t * was requested by the user",
            "\t\t */",
            "\t\trequired_movablecore =",
            "\t\t\troundup(required_movablecore, MAX_ORDER_NR_PAGES);",
            "\t\trequired_movablecore = min(totalpages, required_movablecore);",
            "\t\tcorepages = totalpages - required_movablecore;",
            "",
            "\t\trequired_kernelcore = max(required_kernelcore, corepages);",
            "\t}",
            "",
            "\t/*",
            "\t * If kernelcore was not specified or kernelcore size is larger",
            "\t * than totalpages, there is no ZONE_MOVABLE.",
            "\t */",
            "\tif (!required_kernelcore || required_kernelcore >= totalpages)",
            "\t\tgoto out;",
            "",
            "\t/* usable_startpfn is the lowest possible pfn ZONE_MOVABLE can be at */",
            "\tusable_startpfn = arch_zone_lowest_possible_pfn[movable_zone];",
            "",
            "restart:",
            "\t/* Spread kernelcore memory as evenly as possible throughout nodes */",
            "\tkernelcore_node = required_kernelcore / usable_nodes;",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\tunsigned long start_pfn, end_pfn;",
            "",
            "\t\t/*",
            "\t\t * Recalculate kernelcore_node if the division per node",
            "\t\t * now exceeds what is necessary to satisfy the requested",
            "\t\t * amount of memory for the kernel",
            "\t\t */",
            "\t\tif (required_kernelcore < kernelcore_node)",
            "\t\t\tkernelcore_node = required_kernelcore / usable_nodes;",
            "",
            "\t\t/*",
            "\t\t * As the map is walked, we track how much memory is usable",
            "\t\t * by the kernel using kernelcore_remaining. When it is",
            "\t\t * 0, the rest of the node is usable by ZONE_MOVABLE",
            "\t\t */",
            "\t\tkernelcore_remaining = kernelcore_node;",
            "",
            "\t\t/* Go through each range of PFNs within this node */",
            "\t\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {",
            "\t\t\tunsigned long size_pages;",
            "",
            "\t\t\tstart_pfn = max(start_pfn, zone_movable_pfn[nid]);",
            "\t\t\tif (start_pfn >= end_pfn)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* Account for what is only usable for kernelcore */",
            "\t\t\tif (start_pfn < usable_startpfn) {",
            "\t\t\t\tunsigned long kernel_pages;",
            "\t\t\t\tkernel_pages = min(end_pfn, usable_startpfn)",
            "\t\t\t\t\t\t\t\t- start_pfn;",
            "",
            "\t\t\t\tkernelcore_remaining -= min(kernel_pages,",
            "\t\t\t\t\t\t\tkernelcore_remaining);",
            "\t\t\t\trequired_kernelcore -= min(kernel_pages,",
            "\t\t\t\t\t\t\trequired_kernelcore);",
            "",
            "\t\t\t\t/* Continue if range is now fully accounted */",
            "\t\t\t\tif (end_pfn <= usable_startpfn) {",
            "",
            "\t\t\t\t\t/*",
            "\t\t\t\t\t * Push zone_movable_pfn to the end so",
            "\t\t\t\t\t * that if we have to rebalance",
            "\t\t\t\t\t * kernelcore across nodes, we will",
            "\t\t\t\t\t * not double account here",
            "\t\t\t\t\t */",
            "\t\t\t\t\tzone_movable_pfn[nid] = end_pfn;",
            "\t\t\t\t\tcontinue;",
            "\t\t\t\t}",
            "\t\t\t\tstart_pfn = usable_startpfn;",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * The usable PFN range for ZONE_MOVABLE is from",
            "\t\t\t * start_pfn->end_pfn. Calculate size_pages as the",
            "\t\t\t * number of pages used as kernelcore",
            "\t\t\t */",
            "\t\t\tsize_pages = end_pfn - start_pfn;",
            "\t\t\tif (size_pages > kernelcore_remaining)",
            "\t\t\t\tsize_pages = kernelcore_remaining;",
            "\t\t\tzone_movable_pfn[nid] = start_pfn + size_pages;",
            "",
            "\t\t\t/*",
            "\t\t\t * Some kernelcore has been met, update counts and",
            "\t\t\t * break if the kernelcore for this node has been",
            "\t\t\t * satisfied",
            "\t\t\t */",
            "\t\t\trequired_kernelcore -= min(required_kernelcore,",
            "\t\t\t\t\t\t\t\tsize_pages);",
            "\t\t\tkernelcore_remaining -= size_pages;",
            "\t\t\tif (!kernelcore_remaining)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If there is still required_kernelcore, we do another pass with one",
            "\t * less node in the count. This will push zone_movable_pfn[nid] further",
            "\t * along on the nodes that still have memory until kernelcore is",
            "\t * satisfied",
            "\t */",
            "\tusable_nodes--;",
            "\tif (usable_nodes && required_kernelcore > usable_nodes)",
            "\t\tgoto restart;",
            "",
            "out2:",
            "\t/* Align start of ZONE_MOVABLE on all nids to MAX_ORDER_NR_PAGES */",
            "\tfor (nid = 0; nid < MAX_NUMNODES; nid++) {",
            "\t\tunsigned long start_pfn, end_pfn;",
            "",
            "\t\tzone_movable_pfn[nid] =",
            "\t\t\troundup(zone_movable_pfn[nid], MAX_ORDER_NR_PAGES);",
            "",
            "\t\tget_pfn_range_for_nid(nid, &start_pfn, &end_pfn);",
            "\t\tif (zone_movable_pfn[nid] >= end_pfn)",
            "\t\t\tzone_movable_pfn[nid] = 0;",
            "\t}",
            "",
            "out:",
            "\t/* restore the node_state */",
            "\tnode_states[N_MEMORY] = saved_node_state;",
            "}"
          ],
          "function_name": "find_usable_zone_for_movable, find_zone_movable_pfns_for_nodes",
          "description": "确定可移动内存区域位置，根据内核核心/可移动核心比例分配PFN范围，平衡内存分布并设置ZONE_MOVABLE起始地址",
          "similarity": 0.487576961517334
        },
        {
          "chunk_id": 17,
          "file_path": "mm/mm_init.c",
          "start_line": 2711,
          "end_line": 2806,
          "content": [
            "static void __init mem_init_print_info(void)",
            "{",
            "\tunsigned long physpages, codesize, datasize, rosize, bss_size;",
            "\tunsigned long init_code_size, init_data_size;",
            "",
            "\tphyspages = get_num_physpages();",
            "\tcodesize = _etext - _stext;",
            "\tdatasize = _edata - _sdata;",
            "\trosize = __end_rodata - __start_rodata;",
            "\tbss_size = __bss_stop - __bss_start;",
            "\tinit_data_size = __init_end - __init_begin;",
            "\tinit_code_size = _einittext - _sinittext;",
            "",
            "\t/*",
            "\t * Detect special cases and adjust section sizes accordingly:",
            "\t * 1) .init.* may be embedded into .data sections",
            "\t * 2) .init.text.* may be out of [__init_begin, __init_end],",
            "\t *    please refer to arch/tile/kernel/vmlinux.lds.S.",
            "\t * 3) .rodata.* may be embedded into .text or .data sections.",
            "\t */",
            "#define adj_init_size(start, end, size, pos, adj) \\",
            "\tdo { \\",
            "\t\tif (&start[0] <= &pos[0] && &pos[0] < &end[0] && size > adj) \\",
            "\t\t\tsize -= adj; \\",
            "\t} while (0)",
            "",
            "\tadj_init_size(__init_begin, __init_end, init_data_size,",
            "\t\t     _sinittext, init_code_size);",
            "\tadj_init_size(_stext, _etext, codesize, _sinittext, init_code_size);",
            "\tadj_init_size(_sdata, _edata, datasize, __init_begin, init_data_size);",
            "\tadj_init_size(_stext, _etext, codesize, __start_rodata, rosize);",
            "\tadj_init_size(_sdata, _edata, datasize, __start_rodata, rosize);",
            "",
            "#undef\tadj_init_size",
            "",
            "\tpr_info(\"Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved\"",
            "#ifdef\tCONFIG_HIGHMEM",
            "\t\t\", %luK highmem\"",
            "#endif",
            "\t\t\")\\n\",",
            "\t\tK(nr_free_pages()), K(physpages),",
            "\t\tcodesize / SZ_1K, datasize / SZ_1K, rosize / SZ_1K,",
            "\t\t(init_data_size + init_code_size) / SZ_1K, bss_size / SZ_1K,",
            "\t\tK(physpages - totalram_pages() - totalcma_pages),",
            "\t\tK(totalcma_pages)",
            "#ifdef\tCONFIG_HIGHMEM",
            "\t\t, K(totalhigh_pages())",
            "#endif",
            "\t\t);",
            "}",
            "void __init mm_core_init(void)",
            "{",
            "\t/* Initializations relying on SMP setup */",
            "\tbuild_all_zonelists(NULL);",
            "\tpage_alloc_init_cpuhp();",
            "",
            "\t/*",
            "\t * page_ext requires contiguous pages,",
            "\t * bigger than MAX_PAGE_ORDER unless SPARSEMEM.",
            "\t */",
            "\tpage_ext_init_flatmem();",
            "\tmem_debugging_and_hardening_init();",
            "\tkfence_alloc_pool_and_metadata();",
            "\treport_meminit();",
            "\tkmsan_init_shadow();",
            "\tstack_depot_early_init();",
            "",
            "\t/*",
            "\t * KHO memory setup must happen while memblock is still active, but",
            "\t * as close as possible to buddy initialization",
            "\t */",
            "\tkho_memory_init();",
            "",
            "\tmem_init();",
            "\tmem_init_print_info();",
            "\tkmem_cache_init();",
            "\t/*",
            "\t * page_owner must be initialized after buddy is ready, and also after",
            "\t * slab is ready so that stack_depot_init() works properly",
            "\t */",
            "\tpage_ext_init_flatmem_late();",
            "\tkmemleak_init();",
            "\tptlock_cache_init();",
            "\tpgtable_cache_init();",
            "\tdebug_objects_mem_init();",
            "\tvmalloc_init();",
            "\t/* If no deferred init page_ext now, as vmap is fully initialized */",
            "\tif (!deferred_struct_pages)",
            "\t\tpage_ext_init();",
            "\t/* Should be run before the first non-init thread is created */",
            "\tinit_espfix_bsp();",
            "\t/* Should be run after espfix64 is set up. */",
            "\tpti_init();",
            "\tkmsan_init_runtime();",
            "\tmm_cache_init();",
            "}"
          ],
          "function_name": "mem_init_print_info, mm_core_init",
          "description": "mem_init_print_info计算并打印内存统计信息，包括可用页面数、各段代码数据大小及保留区域。mm_core_init初始化内存核心组件，构建zonelists，初始化slab/kmem_cache，启用调试对象跟踪，设置虚拟内存管理，最后根据是否延迟结构页初始化page_ext模块。",
          "similarity": 0.4834495186805725
        },
        {
          "chunk_id": 9,
          "file_path": "mm/mm_init.c",
          "start_line": 1313,
          "end_line": 1420,
          "content": [
            "static unsigned long __init calc_memmap_size(unsigned long spanned_pages,",
            "\t\t\t\t\t\tunsigned long present_pages)",
            "{",
            "\tunsigned long pages = spanned_pages;",
            "",
            "\t/*",
            "\t * Provide a more accurate estimation if there are holes within",
            "\t * the zone and SPARSEMEM is in use. If there are holes within the",
            "\t * zone, each populated memory region may cost us one or two extra",
            "\t * memmap pages due to alignment because memmap pages for each",
            "\t * populated regions may not be naturally aligned on page boundary.",
            "\t * So the (present_pages >> 4) heuristic is a tradeoff for that.",
            "\t */",
            "\tif (spanned_pages > present_pages + (present_pages >> 4) &&",
            "\t    IS_ENABLED(CONFIG_SPARSEMEM))",
            "\t\tpages = present_pages;",
            "",
            "\treturn PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;",
            "}",
            "static void pgdat_init_split_queue(struct pglist_data *pgdat)",
            "{",
            "\tstruct deferred_split *ds_queue = &pgdat->deferred_split_queue;",
            "",
            "\tspin_lock_init(&ds_queue->split_queue_lock);",
            "\tINIT_LIST_HEAD(&ds_queue->split_queue);",
            "\tds_queue->split_queue_len = 0;",
            "}",
            "static void pgdat_init_split_queue(struct pglist_data *pgdat) {}",
            "static void pgdat_init_kcompactd(struct pglist_data *pgdat)",
            "{",
            "\tinit_waitqueue_head(&pgdat->kcompactd_wait);",
            "}",
            "static void pgdat_init_kcompactd(struct pglist_data *pgdat) {}",
            "static void __meminit pgdat_init_internals(struct pglist_data *pgdat)",
            "{",
            "\tint i;",
            "",
            "\tpgdat_resize_init(pgdat);",
            "\tpgdat_kswapd_lock_init(pgdat);",
            "",
            "\tpgdat_init_split_queue(pgdat);",
            "\tpgdat_init_kcompactd(pgdat);",
            "",
            "\tinit_waitqueue_head(&pgdat->kswapd_wait);",
            "\tinit_waitqueue_head(&pgdat->pfmemalloc_wait);",
            "",
            "\tfor (i = 0; i < NR_VMSCAN_THROTTLE; i++)",
            "\t\tinit_waitqueue_head(&pgdat->reclaim_wait[i]);",
            "",
            "\tpgdat_page_ext_init(pgdat);",
            "\tlruvec_init(&pgdat->__lruvec);",
            "}",
            "static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,",
            "\t\t\t\t\t\t\tunsigned long remaining_pages)",
            "{",
            "\tatomic_long_set(&zone->managed_pages, remaining_pages);",
            "\tzone_set_nid(zone, nid);",
            "\tzone->name = zone_names[idx];",
            "\tzone->zone_pgdat = NODE_DATA(nid);",
            "\tspin_lock_init(&zone->lock);",
            "\tzone_seqlock_init(zone);",
            "\tzone_pcp_init(zone);",
            "}",
            "static void __meminit zone_init_free_lists(struct zone *zone)",
            "{",
            "\tunsigned int order, t;",
            "\tfor_each_migratetype_order(order, t) {",
            "\t\tINIT_LIST_HEAD(&zone->free_area[order].free_list[t]);",
            "\t\tzone->free_area[order].nr_free = 0;",
            "\t}",
            "",
            "#ifdef CONFIG_UNACCEPTED_MEMORY",
            "\tINIT_LIST_HEAD(&zone->unaccepted_pages);",
            "#endif",
            "}",
            "void __meminit init_currently_empty_zone(struct zone *zone,",
            "\t\t\t\t\tunsigned long zone_start_pfn,",
            "\t\t\t\t\tunsigned long size)",
            "{",
            "\tstruct pglist_data *pgdat = zone->zone_pgdat;",
            "\tint zone_idx = zone_idx(zone) + 1;",
            "",
            "\tif (zone_idx > pgdat->nr_zones)",
            "\t\tpgdat->nr_zones = zone_idx;",
            "",
            "\tzone->zone_start_pfn = zone_start_pfn;",
            "",
            "\tmminit_dprintk(MMINIT_TRACE, \"memmap_init\",",
            "\t\t\t\"Initialising map node %d zone %lu pfns %lu -> %lu\\n\",",
            "\t\t\tpgdat->node_id,",
            "\t\t\t(unsigned long)zone_idx(zone),",
            "\t\t\tzone_start_pfn, (zone_start_pfn + size));",
            "",
            "\tzone_init_free_lists(zone);",
            "\tzone->initialized = 1;",
            "}",
            "static unsigned long __init usemap_size(unsigned long zone_start_pfn, unsigned long zonesize)",
            "{",
            "\tunsigned long usemapsize;",
            "",
            "\tzonesize += zone_start_pfn & (pageblock_nr_pages-1);",
            "\tusemapsize = roundup(zonesize, pageblock_nr_pages);",
            "\tusemapsize = usemapsize >> pageblock_order;",
            "\tusemapsize *= NR_PAGEBLOCK_BITS;",
            "\tusemapsize = roundup(usemapsize, BITS_PER_LONG);",
            "",
            "\treturn usemapsize / BITS_PER_BYTE;",
            "}"
          ],
          "function_name": "calc_memmap_size, pgdat_init_split_queue, pgdat_init_split_queue, pgdat_init_kcompactd, pgdat_init_kcompactd, pgdat_init_internals, zone_init_internals, zone_init_free_lists, init_currently_empty_zone, usemap_size",
          "description": "计算内存映射所需大小，初始化split队列、kcompactd等待队列等内部结构，并配置区的自由列表及空闲页面管理机制。",
          "similarity": 0.4819209575653076
        },
        {
          "chunk_id": 6,
          "file_path": "mm/mm_init.c",
          "start_line": 912,
          "end_line": 1017,
          "content": [
            "static void __init memmap_init_zone_range(struct zone *zone,",
            "\t\t\t\t\t  unsigned long start_pfn,",
            "\t\t\t\t\t  unsigned long end_pfn,",
            "\t\t\t\t\t  unsigned long *hole_pfn)",
            "{",
            "\tunsigned long zone_start_pfn = zone->zone_start_pfn;",
            "\tunsigned long zone_end_pfn = zone_start_pfn + zone->spanned_pages;",
            "\tint nid = zone_to_nid(zone), zone_id = zone_idx(zone);",
            "",
            "\tstart_pfn = clamp(start_pfn, zone_start_pfn, zone_end_pfn);",
            "\tend_pfn = clamp(end_pfn, zone_start_pfn, zone_end_pfn);",
            "",
            "\tif (start_pfn >= end_pfn)",
            "\t\treturn;",
            "",
            "\tmemmap_init_range(end_pfn - start_pfn, nid, zone_id, start_pfn,",
            "\t\t\t  zone_end_pfn, MEMINIT_EARLY, NULL, MIGRATE_MOVABLE);",
            "",
            "\tif (*hole_pfn < start_pfn)",
            "\t\tinit_unavailable_range(*hole_pfn, start_pfn, zone_id, nid);",
            "",
            "\t*hole_pfn = end_pfn;",
            "}",
            "static void __init memmap_init(void)",
            "{",
            "\tunsigned long start_pfn, end_pfn;",
            "\tunsigned long hole_pfn = 0;",
            "\tint i, j, zone_id = 0, nid;",
            "",
            "\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {",
            "\t\tstruct pglist_data *node = NODE_DATA(nid);",
            "",
            "\t\tfor (j = 0; j < MAX_NR_ZONES; j++) {",
            "\t\t\tstruct zone *zone = node->node_zones + j;",
            "",
            "\t\t\tif (!populated_zone(zone))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tmemmap_init_zone_range(zone, start_pfn, end_pfn,",
            "\t\t\t\t\t       &hole_pfn);",
            "\t\t\tzone_id = j;",
            "\t\t}",
            "\t}",
            "",
            "#ifdef CONFIG_SPARSEMEM",
            "\t/*",
            "\t * Initialize the memory map for hole in the range [memory_end,",
            "\t * section_end].",
            "\t * Append the pages in this hole to the highest zone in the last",
            "\t * node.",
            "\t * The call to init_unavailable_range() is outside the ifdef to",
            "\t * silence the compiler warining about zone_id set but not used;",
            "\t * for FLATMEM it is a nop anyway",
            "\t */",
            "\tend_pfn = round_up(end_pfn, PAGES_PER_SECTION);",
            "\tif (hole_pfn < end_pfn)",
            "#endif",
            "\t\tinit_unavailable_range(hole_pfn, end_pfn, zone_id, nid);",
            "}",
            "static void __ref __init_zone_device_page(struct page *page, unsigned long pfn,",
            "\t\t\t\t\t  unsigned long zone_idx, int nid,",
            "\t\t\t\t\t  struct dev_pagemap *pgmap)",
            "{",
            "",
            "\t__init_single_page(page, pfn, zone_idx, nid);",
            "",
            "\t/*",
            "\t * Mark page reserved as it will need to wait for onlining",
            "\t * phase for it to be fully associated with a zone.",
            "\t *",
            "\t * We can use the non-atomic __set_bit operation for setting",
            "\t * the flag as we are still initializing the pages.",
            "\t */",
            "\t__SetPageReserved(page);",
            "",
            "\t/*",
            "\t * ZONE_DEVICE pages union ->lru with a ->pgmap back pointer",
            "\t * and zone_device_data.  It is a bug if a ZONE_DEVICE page is",
            "\t * ever freed or placed on a driver-private list.",
            "\t */",
            "\tpage->pgmap = pgmap;",
            "\tpage->zone_device_data = NULL;",
            "",
            "\t/*",
            "\t * Mark the block movable so that blocks are reserved for",
            "\t * movable at startup. This will force kernel allocations",
            "\t * to reserve their blocks rather than leaking throughout",
            "\t * the address space during boot when many long-lived",
            "\t * kernel allocations are made.",
            "\t *",
            "\t * Please note that MEMINIT_HOTPLUG path doesn't clear memmap",
            "\t * because this is done early in section_activate()",
            "\t */",
            "\tif (pageblock_aligned(pfn)) {",
            "\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\t/*",
            "\t * ZONE_DEVICE pages are released directly to the driver page allocator",
            "\t * which will set the page count to 1 when allocating the page.",
            "\t */",
            "\tif (pgmap->type == MEMORY_DEVICE_PRIVATE ||",
            "\t    pgmap->type == MEMORY_DEVICE_COHERENT)",
            "\t\tset_page_count(page, 0);",
            "}"
          ],
          "function_name": "memmap_init_zone_range, memmap_init, __init_zone_device_page",
          "description": "遍历各节点和区，调用memmap_init_range初始化内存映射，处理稀疏内存中洞的不可用范围，并调整ZONE_MOVABLE范围以适应架构需求。",
          "similarity": 0.4780239462852478
        },
        {
          "chunk_id": 2,
          "file_path": "mm/mm_init.c",
          "start_line": 151,
          "end_line": 259,
          "content": [
            "static __init int set_mminit_loglevel(char *str)",
            "{",
            "\tget_option(&str, &mminit_loglevel);",
            "\treturn 0;",
            "}",
            "void mm_compute_batch(int overcommit_policy)",
            "{",
            "\tu64 memsized_batch;",
            "\ts32 nr = num_present_cpus();",
            "\ts32 batch = max_t(s32, nr*2, 32);",
            "\tunsigned long ram_pages = totalram_pages();",
            "",
            "\t/*",
            "\t * For policy OVERCOMMIT_NEVER, set batch size to 0.4% of",
            "\t * (total memory/#cpus), and lift it to 25% for other policies",
            "\t * to easy the possible lock contention for percpu_counter",
            "\t * vm_committed_as, while the max limit is INT_MAX",
            "\t */",
            "\tif (overcommit_policy == OVERCOMMIT_NEVER)",
            "\t\tmemsized_batch = min_t(u64, ram_pages/nr/256, INT_MAX);",
            "\telse",
            "\t\tmemsized_batch = min_t(u64, ram_pages/nr/4, INT_MAX);",
            "",
            "\tvm_committed_as_batch = max_t(s32, memsized_batch, batch);",
            "}",
            "static int __meminit mm_compute_batch_notifier(struct notifier_block *self,",
            "\t\t\t\t\tunsigned long action, void *arg)",
            "{",
            "\tswitch (action) {",
            "\tcase MEM_ONLINE:",
            "\tcase MEM_OFFLINE:",
            "\t\tmm_compute_batch(sysctl_overcommit_memory);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "\treturn NOTIFY_OK;",
            "}",
            "static int __init mm_compute_batch_init(void)",
            "{",
            "\tmm_compute_batch(sysctl_overcommit_memory);",
            "\thotplug_memory_notifier(mm_compute_batch_notifier, MM_COMPUTE_BATCH_PRI);",
            "\treturn 0;",
            "}",
            "static int __init mm_sysfs_init(void)",
            "{",
            "\tmm_kobj = kobject_create_and_add(\"mm\", kernel_kobj);",
            "\tif (!mm_kobj)",
            "\t\treturn -ENOMEM;",
            "",
            "\treturn 0;",
            "}",
            "static int __init cmdline_parse_core(char *p, unsigned long *core,",
            "\t\t\t\t     unsigned long *percent)",
            "{",
            "\tunsigned long long coremem;",
            "\tchar *endptr;",
            "",
            "\tif (!p)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Value may be a percentage of total memory, otherwise bytes */",
            "\tcoremem = simple_strtoull(p, &endptr, 0);",
            "\tif (*endptr == '%') {",
            "\t\t/* Paranoid check for percent values greater than 100 */",
            "\t\tWARN_ON(coremem > 100);",
            "",
            "\t\t*percent = coremem;",
            "\t} else {",
            "\t\tcoremem = memparse(p, &p);",
            "\t\t/* Paranoid check that UL is enough for the coremem value */",
            "\t\tWARN_ON((coremem >> PAGE_SHIFT) > ULONG_MAX);",
            "",
            "\t\t*core = coremem >> PAGE_SHIFT;",
            "\t\t*percent = 0UL;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int __init cmdline_parse_kernelcore(char *p)",
            "{",
            "\t/* parse kernelcore=mirror */",
            "\tif (parse_option_str(p, \"mirror\")) {",
            "\t\tmirrored_kernelcore = true;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn cmdline_parse_core(p, &required_kernelcore,",
            "\t\t\t\t  &required_kernelcore_percent);",
            "}",
            "static int __init cmdline_parse_movablecore(char *p)",
            "{",
            "\treturn cmdline_parse_core(p, &required_movablecore,",
            "\t\t\t\t  &required_movablecore_percent);",
            "}",
            "static unsigned long __init early_calculate_totalpages(void)",
            "{",
            "\tunsigned long totalpages = 0;",
            "\tunsigned long start_pfn, end_pfn;",
            "\tint i, nid;",
            "",
            "\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {",
            "\t\tunsigned long pages = end_pfn - start_pfn;",
            "",
            "\t\ttotalpages += pages;",
            "\t\tif (pages)",
            "\t\t\tnode_set_state(nid, N_MEMORY);",
            "\t}",
            "\treturn totalpages;",
            "}"
          ],
          "function_name": "set_mminit_loglevel, mm_compute_batch, mm_compute_batch_notifier, mm_compute_batch_init, mm_sysfs_init, cmdline_parse_core, cmdline_parse_kernelcore, cmdline_parse_movablecore, early_calculate_totalpages",
          "description": "初始化内存批次计算逻辑，注册内存变化通知回调，解析命令行参数以确定内核核心和可移动内存需求",
          "similarity": 0.4692927896976471
        }
      ]
    }
  ]
}