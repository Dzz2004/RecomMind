{
  "query": "缓冲区缓存",
  "timestamp": "2025-12-26 00:28:53",
  "retrieved_files": [
    {
      "source_file": "kernel/trace/ring_buffer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:07:21\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\ring_buffer.c`\n\n---\n\n# `trace/ring_buffer.c` 技术文档\n\n## 1. 文件概述\n\n`trace/ring_buffer.c` 实现了 Linux 内核中通用的高性能环形缓冲区（ring buffer）机制，主要用于跟踪（tracing）子系统。该缓冲区支持多 CPU 并发写入、单读者或多读者无锁读取，并通过时间戳压缩、事件类型编码和页面交换等技术优化内存使用和性能。该实现是 ftrace、perf 和其他内核跟踪工具的核心基础设施。\n\n## 2. 核心功能\n\n### 主要函数\n- `ring_buffer_print_entry_header()`：输出环形缓冲区条目头部格式说明，用于调试或用户空间解析。\n- `ring_buffer_event_length()`：返回事件有效载荷（payload）的长度，对 TIME_EXTEND 类型自动跳过扩展头。\n- `rb_event_data()`（内联）：返回指向事件实际数据的指针，处理 TIME_EXTEND 和不同长度编码。\n- `rb_event_length()`：返回完整事件结构（含头部）的字节长度。\n- `rb_event_ts_length()`：返回 TIME_EXTEND 事件及其后续数据事件的总长度。\n- `rb_event_data_length()`：计算数据类型事件的总长度（含头部）。\n- `rb_null_event()` / `rb_event_set_padding()`：判断或设置空/填充事件。\n\n### 关键数据结构（隐含或引用）\n- `struct ring_buffer_event`：环形缓冲区中每个事件的通用头部结构。\n- `struct buffer_data_page`：每个 CPU 缓冲区页面的封装，包含数据和元数据。\n- 每 CPU 页面链表：每个 CPU 拥有独立的环形页面链，写者仅写本地 CPU 缓冲区。\n\n### 核心常量与宏\n- `RINGBUF_TYPE_PADDING`、`RINGBUF_TYPE_TIME_EXTEND`、`RINGBUF_TYPE_TIME_STAMP`、`RINGBUF_TYPE_DATA`：事件类型标识。\n- `RB_ALIGNMENT` / `RB_ARCH_ALIGNMENT`：数据对齐策略，根据架构是否支持 64 位对齐访问调整。\n- `RB_MAX_SMALL_DATA`：小数据事件的最大长度（基于 4 字节对齐和类型长度上限）。\n- `TS_MSB` / `ABS_TS_MASK`：用于处理 59 位时间戳的高位截断与恢复。\n\n## 3. 关键实现\n\n### 无锁读写架构\n- **写者**：每个 CPU 只能写入其对应的 per-CPU 缓冲区，通过原子操作和内存屏障保证写入一致性，无需全局锁。\n- **读者**：每个 per-CPU 缓冲区维护一个独立的“reader page”。当 reader page 被读完后，通过原子交换（未来使用 `cmpxchg`）将其与环形缓冲区中的一个页面互换。交换后，原 reader page 不再被写者访问，读者可安全地将其用于 splice、复制或释放。\n\n### 事件编码与压缩\n- 事件头部使用紧凑位域编码：\n  - `type_len`（5 位）：事件类型或小数据长度（≤31）。\n  - `time_delta`（27 位）：相对于前一事件的时间增量。\n  - `array`（32 位）：用于存储大长度值或事件数据。\n- **TIME_EXTEND 事件**：当时间增量超出 27 位或需要绝对时间戳时，插入一个 8 字节的 TIME_EXTEND 事件，后跟实际数据事件。\n- **数据长度编码**：\n  - 若 `type_len > 0` 且 ≤ `RINGBUF_TYPE_DATA_TYPE_LEN_MAX`，则数据长度 = `type_len * RB_ALIGNMENT`，数据从 `array[0]` 开始。\n  - 否则，数据长度存储在 `array[0]`，实际数据从 `array[1]` 开始。\n\n### 时间戳处理\n- 绝对时间戳仅保留低 59 位（`ABS_TS_MASK`），高 5 位（`TS_MSB`）若非零需单独保存并在读取时恢复，以支持长时间运行的跟踪。\n\n### 内存对齐优化\n- 在支持 64 位对齐访问的架构上（`CONFIG_HAVE_64BIT_ALIGNED_ACCESS`），强制 8 字节对齐（`RB_FORCE_8BYTE_ALIGNMENT`），提升访问性能；否则使用 4 字节对齐。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/ring_buffer.h>`：定义公共 API 和数据结构。\n  - `<linux/trace_clock.h>`、`<linux/sched/clock.h>`：提供高精度时间戳源。\n  - `<linux/percpu.h>`：支持 per-CPU 缓冲区分配。\n  - `<linux/spinlock.h>`、`<asm/local.h>`：提供底层原子操作和锁原语。\n  - `<linux/trace_recursion.h>`：防止跟踪递归。\n- **子系统依赖**：\n  - **ftrace**：主要消费者，用于函数跟踪、事件跟踪等。\n  - **perf**：通过 ring buffer 获取性能事件数据。\n  - **Security Module**：通过 `<linux/security.h>` 集成 LSM 钩子（如 trace 访问控制）。\n- **架构依赖**：依赖 `CONFIG_HAVE_64BIT_ALIGNED_ACCESS` 配置项优化对齐策略。\n\n## 5. 使用场景\n\n- **内核跟踪（ftrace）**：记录函数调用、上下文切换、中断等事件，数据写入 per-CPU ring buffer，用户通过 `tracefs` 读取。\n- **性能分析（perf）**：perf 工具通过 ring buffer 接收内核采样事件（如 PMU 中断、软件事件）。\n- **实时监控与调试**：开发者或运维人员通过读取 ring buffer 内容分析系统行为、延迟或错误。\n- **自测试（selftest）**：文件包含自测试逻辑（依赖 `<linux/kthread.h>`），用于验证 ring buffer 功能正确性。\n- **低开销事件记录**：由于其无锁设计和压缩编码，适用于高频事件记录场景（如每秒百万级事件）。",
      "similarity": 0.5848537087440491,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 837,
          "end_line": 965,
          "content": [
            "size_t ring_buffer_nr_pages(struct trace_buffer *buffer, int cpu)",
            "{",
            "\treturn buffer->buffers[cpu]->nr_pages;",
            "}",
            "size_t ring_buffer_nr_dirty_pages(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tsize_t read;",
            "\tsize_t lost;",
            "\tsize_t cnt;",
            "",
            "\tread = local_read(&buffer->buffers[cpu]->pages_read);",
            "\tlost = local_read(&buffer->buffers[cpu]->pages_lost);",
            "\tcnt = local_read(&buffer->buffers[cpu]->pages_touched);",
            "",
            "\tif (WARN_ON_ONCE(cnt < lost))",
            "\t\treturn 0;",
            "",
            "\tcnt -= lost;",
            "",
            "\t/* The reader can read an empty page, but not more than that */",
            "\tif (cnt < read) {",
            "\t\tWARN_ON_ONCE(read > cnt + 1);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn cnt - read;",
            "}",
            "static __always_inline bool full_hit(struct trace_buffer *buffer, int cpu, int full)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];",
            "\tsize_t nr_pages;",
            "\tsize_t dirty;",
            "",
            "\tnr_pages = cpu_buffer->nr_pages;",
            "\tif (!nr_pages || !full)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Add one as dirty will never equal nr_pages, as the sub-buffer",
            "\t * that the writer is on is not counted as dirty.",
            "\t * This is needed if \"buffer_percent\" is set to 100.",
            "\t */",
            "\tdirty = ring_buffer_nr_dirty_pages(buffer, cpu) + 1;",
            "",
            "\treturn (dirty * 100) >= (full * nr_pages);",
            "}",
            "static void rb_wake_up_waiters(struct irq_work *work)",
            "{",
            "\tstruct rb_irq_work *rbwork = container_of(work, struct rb_irq_work, work);",
            "",
            "\twake_up_all(&rbwork->waiters);",
            "\tif (rbwork->full_waiters_pending || rbwork->wakeup_full) {",
            "\t\t/* Only cpu_buffer sets the above flags */",
            "\t\tstruct ring_buffer_per_cpu *cpu_buffer =",
            "\t\t\tcontainer_of(rbwork, struct ring_buffer_per_cpu, irq_work);",
            "",
            "\t\t/* Called from interrupt context */",
            "\t\traw_spin_lock(&cpu_buffer->reader_lock);",
            "\t\trbwork->wakeup_full = false;",
            "\t\trbwork->full_waiters_pending = false;",
            "",
            "\t\t/* Waking up all waiters, they will reset the shortest full */",
            "\t\tcpu_buffer->shortest_full = 0;",
            "\t\traw_spin_unlock(&cpu_buffer->reader_lock);",
            "",
            "\t\twake_up_all(&rbwork->full_waiters);",
            "\t}",
            "}",
            "void ring_buffer_wake_waiters(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct rb_irq_work *rbwork;",
            "",
            "\tif (!buffer)",
            "\t\treturn;",
            "",
            "\tif (cpu == RING_BUFFER_ALL_CPUS) {",
            "",
            "\t\t/* Wake up individual ones too. One level recursion */",
            "\t\tfor_each_buffer_cpu(buffer, cpu)",
            "\t\t\tring_buffer_wake_waiters(buffer, cpu);",
            "",
            "\t\trbwork = &buffer->irq_work;",
            "\t} else {",
            "\t\tif (WARN_ON_ONCE(!buffer->buffers))",
            "\t\t\treturn;",
            "\t\tif (WARN_ON_ONCE(cpu >= nr_cpu_ids))",
            "\t\t\treturn;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t/* The CPU buffer may not have been initialized yet */",
            "\t\tif (!cpu_buffer)",
            "\t\t\treturn;",
            "\t\trbwork = &cpu_buffer->irq_work;",
            "\t}",
            "",
            "\t/* This can be called in any context */",
            "\tirq_work_queue(&rbwork->work);",
            "}",
            "static bool rb_watermark_hit(struct trace_buffer *buffer, int cpu, int full)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tbool ret = false;",
            "",
            "\t/* Reads of all CPUs always waits for any data */",
            "\tif (cpu == RING_BUFFER_ALL_CPUS)",
            "\t\treturn !ring_buffer_empty(buffer);",
            "",
            "\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\tif (!ring_buffer_empty_cpu(buffer, cpu)) {",
            "\t\tunsigned long flags;",
            "\t\tbool pagebusy;",
            "",
            "\t\tif (!full)",
            "\t\t\treturn true;",
            "",
            "\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\tpagebusy = cpu_buffer->reader_page == cpu_buffer->commit_page;",
            "\t\tret = !pagebusy && full_hit(buffer, cpu, full);",
            "",
            "\t\tif (!ret && (!cpu_buffer->shortest_full ||",
            "\t\t\t     cpu_buffer->shortest_full > full)) {",
            "\t\t    cpu_buffer->shortest_full = full;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t}",
            "\treturn ret;",
            "}"
          ],
          "function_name": "ring_buffer_nr_pages, ring_buffer_nr_dirty_pages, full_hit, rb_wake_up_waiters, ring_buffer_wake_waiters, rb_watermark_hit",
          "description": "实现缓冲区页数统计功能，包含满状态检测算法，提供唤醒等待者的中断处理函数，支持全CPU模式和特定CPU的唤醒操作，包含水位标记判断逻辑控制缓冲区回收行为",
          "similarity": 0.6890391111373901
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 357,
          "end_line": 461,
          "content": [
            "static __always_inline unsigned int rb_page_commit(struct buffer_page *bpage)",
            "{",
            "\treturn local_read(&bpage->page->commit);",
            "}",
            "static void free_buffer_page(struct buffer_page *bpage)",
            "{",
            "\tfree_page((unsigned long)bpage->page);",
            "\tkfree(bpage);",
            "}",
            "static inline bool test_time_stamp(u64 delta)",
            "{",
            "\treturn !!(delta & TS_DELTA_TEST);",
            "}",
            "int ring_buffer_print_page_header(struct trace_seq *s)",
            "{",
            "\tstruct buffer_data_page field;",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: u64 timestamp;\\t\"",
            "\t\t\t \"offset:0;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)sizeof(field.time_stamp),",
            "\t\t\t (unsigned int)is_signed_type(u64));",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: local_t commit;\\t\"",
            "\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)offsetof(typeof(field), commit),",
            "\t\t\t (unsigned int)sizeof(field.commit),",
            "\t\t\t (unsigned int)is_signed_type(long));",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: int overwrite;\\t\"",
            "\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)offsetof(typeof(field), commit),",
            "\t\t\t 1,",
            "\t\t\t (unsigned int)is_signed_type(long));",
            "",
            "\ttrace_seq_printf(s, \"\\tfield: char data;\\t\"",
            "\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",",
            "\t\t\t (unsigned int)offsetof(typeof(field), data),",
            "\t\t\t (unsigned int)BUF_PAGE_SIZE,",
            "\t\t\t (unsigned int)is_signed_type(char));",
            "",
            "\treturn !trace_seq_has_overflowed(s);",
            "}",
            "static inline int rb_time_cnt(unsigned long val)",
            "{",
            "\treturn (val >> RB_TIME_SHIFT) & 3;",
            "}",
            "static inline u64 rb_time_val(unsigned long top, unsigned long bottom)",
            "{",
            "\tu64 val;",
            "",
            "\tval = top & RB_TIME_VAL_MASK;",
            "\tval <<= RB_TIME_SHIFT;",
            "\tval |= bottom & RB_TIME_VAL_MASK;",
            "",
            "\treturn val;",
            "}",
            "static inline bool __rb_time_read(rb_time_t *t, u64 *ret, unsigned long *cnt)",
            "{",
            "\tunsigned long top, bottom, msb;",
            "\tunsigned long c;",
            "",
            "\t/*",
            "\t * If the read is interrupted by a write, then the cnt will",
            "\t * be different. Loop until both top and bottom have been read",
            "\t * without interruption.",
            "\t */",
            "\tdo {",
            "\t\tc = local_read(&t->cnt);",
            "\t\ttop = local_read(&t->top);",
            "\t\tbottom = local_read(&t->bottom);",
            "\t\tmsb = local_read(&t->msb);",
            "\t} while (c != local_read(&t->cnt));",
            "",
            "\t*cnt = rb_time_cnt(top);",
            "",
            "\t/* If top, msb or bottom counts don't match, this interrupted a write */",
            "\tif (*cnt != rb_time_cnt(msb) || *cnt != rb_time_cnt(bottom))",
            "\t\treturn false;",
            "",
            "\t/* The shift to msb will lose its cnt bits */",
            "\t*ret = rb_time_val(top, bottom) | ((u64)msb << RB_TIME_MSB_SHIFT);",
            "\treturn true;",
            "}",
            "static bool rb_time_read(rb_time_t *t, u64 *ret)",
            "{",
            "\tunsigned long cnt;",
            "",
            "\treturn __rb_time_read(t, ret, &cnt);",
            "}",
            "static inline unsigned long rb_time_val_cnt(unsigned long val, unsigned long cnt)",
            "{",
            "\treturn (val & RB_TIME_VAL_MASK) | ((cnt & 3) << RB_TIME_SHIFT);",
            "}",
            "static inline void rb_time_split(u64 val, unsigned long *top, unsigned long *bottom,",
            "\t\t\t\t unsigned long *msb)",
            "{",
            "\t*top = (unsigned long)((val >> RB_TIME_SHIFT) & RB_TIME_VAL_MASK);",
            "\t*bottom = (unsigned long)(val & RB_TIME_VAL_MASK);",
            "\t*msb = (unsigned long)(val >> RB_TIME_MSB_SHIFT);",
            "}",
            "static inline void rb_time_val_set(local_t *t, unsigned long val, unsigned long cnt)",
            "{",
            "\tval = rb_time_val_cnt(val, cnt);",
            "\tlocal_set(t, val);",
            "}"
          ],
          "function_name": "rb_page_commit, free_buffer_page, test_time_stamp, ring_buffer_print_page_header, rb_time_cnt, rb_time_val, __rb_time_read, rb_time_read, rb_time_val_cnt, rb_time_split, rb_time_val_set",
          "description": "实现缓冲页提交计数器读取、页面释放、时间戳字段解析与组合操作，包含原子操作辅助函数用于安全的时间戳读写，支持多处理器环境下时间戳的精确捕获",
          "similarity": 0.6388081312179565
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 2052,
          "end_line": 2363,
          "content": [
            "static bool",
            "rb_insert_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct list_head *pages = &cpu_buffer->new_pages;",
            "\tunsigned long flags;",
            "\tbool success;",
            "\tint retries;",
            "",
            "\t/* Can be called at early boot up, where interrupts must not been enabled */",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t/*",
            "\t * We are holding the reader lock, so the reader page won't be swapped",
            "\t * in the ring buffer. Now we are racing with the writer trying to",
            "\t * move head page and the tail page.",
            "\t * We are going to adapt the reader page update process where:",
            "\t * 1. We first splice the start and end of list of new pages between",
            "\t *    the head page and its previous page.",
            "\t * 2. We cmpxchg the prev_page->next to point from head page to the",
            "\t *    start of new pages list.",
            "\t * 3. Finally, we update the head->prev to the end of new list.",
            "\t *",
            "\t * We will try this process 10 times, to make sure that we don't keep",
            "\t * spinning.",
            "\t */",
            "\tretries = 10;",
            "\tsuccess = false;",
            "\twhile (retries--) {",
            "\t\tstruct list_head *head_page, *prev_page, *r;",
            "\t\tstruct list_head *last_page, *first_page;",
            "\t\tstruct list_head *head_page_with_bit;",
            "\t\tstruct buffer_page *hpage = rb_set_head_page(cpu_buffer);",
            "",
            "\t\tif (!hpage)",
            "\t\t\tbreak;",
            "\t\thead_page = &hpage->list;",
            "\t\tprev_page = head_page->prev;",
            "",
            "\t\tfirst_page = pages->next;",
            "\t\tlast_page  = pages->prev;",
            "",
            "\t\thead_page_with_bit = (struct list_head *)",
            "\t\t\t\t     ((unsigned long)head_page | RB_PAGE_HEAD);",
            "",
            "\t\tlast_page->next = head_page_with_bit;",
            "\t\tfirst_page->prev = prev_page;",
            "",
            "\t\tr = cmpxchg(&prev_page->next, head_page_with_bit, first_page);",
            "",
            "\t\tif (r == head_page_with_bit) {",
            "\t\t\t/*",
            "\t\t\t * yay, we replaced the page pointer to our new list,",
            "\t\t\t * now, we just have to update to head page's prev",
            "\t\t\t * pointer to point to end of list",
            "\t\t\t */",
            "\t\t\thead_page->prev = last_page;",
            "\t\t\tsuccess = true;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tif (success)",
            "\t\tINIT_LIST_HEAD(pages);",
            "\t/*",
            "\t * If we weren't successful in adding in new pages, warn and stop",
            "\t * tracing",
            "\t */",
            "\tRB_WARN_ON(cpu_buffer, !success);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "",
            "\t/* free pages if they weren't inserted */",
            "\tif (!success) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\t list) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            "\treturn success;",
            "}",
            "static void rb_update_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tbool success;",
            "",
            "\tif (cpu_buffer->nr_pages_to_update > 0)",
            "\t\tsuccess = rb_insert_pages(cpu_buffer);",
            "\telse",
            "\t\tsuccess = rb_remove_pages(cpu_buffer,",
            "\t\t\t\t\t-cpu_buffer->nr_pages_to_update);",
            "",
            "\tif (success)",
            "\t\tcpu_buffer->nr_pages += cpu_buffer->nr_pages_to_update;",
            "}",
            "static void update_pages_handler(struct work_struct *work)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = container_of(work,",
            "\t\t\tstruct ring_buffer_per_cpu, update_pages_work);",
            "\trb_update_pages(cpu_buffer);",
            "\tcomplete(&cpu_buffer->update_done);",
            "}",
            "int ring_buffer_resize(struct trace_buffer *buffer, unsigned long size,",
            "\t\t\tint cpu_id)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tunsigned long nr_pages;",
            "\tint cpu, err;",
            "",
            "\t/*",
            "\t * Always succeed at resizing a non-existent buffer:",
            "\t */",
            "\tif (!buffer)",
            "\t\treturn 0;",
            "",
            "\t/* Make sure the requested buffer exists */",
            "\tif (cpu_id != RING_BUFFER_ALL_CPUS &&",
            "\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))",
            "\t\treturn 0;",
            "",
            "\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);",
            "",
            "\t/* we need a minimum of two pages */",
            "\tif (nr_pages < 2)",
            "\t\tnr_pages = 2;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "\tatomic_inc(&buffer->resizing);",
            "",
            "\tif (cpu_id == RING_BUFFER_ALL_CPUS) {",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\t\terr = -EBUSY;",
            "\t\t\t\tgoto out_err_unlock;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* calculate the pages to update */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "\t\t\t/*",
            "\t\t\t * nothing more to do for removing pages or no update",
            "\t\t\t */",
            "\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)",
            "\t\t\t\tcontinue;",
            "\t\t\t/*",
            "\t\t\t * to add pages, make sure all new pages can be",
            "\t\t\t * allocated without receiving ENOMEM",
            "\t\t\t */",
            "\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\t\tif (__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t\t&cpu_buffer->new_pages)) {",
            "\t\t\t\t/* not enough memory for new pages */",
            "\t\t\t\terr = -ENOMEM;",
            "\t\t\t\tgoto out_err;",
            "\t\t\t}",
            "",
            "\t\t\tcond_resched();",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "\t\t/*",
            "\t\t * Fire off all the required work handlers",
            "\t\t * We can't schedule on offline CPUs, but it's not necessary",
            "\t\t * since we can change their buffer sizes without any race.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* Can't run something on an offline CPU. */",
            "\t\t\tif (!cpu_online(cpu)) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t\t} else {",
            "\t\t\t\t/* Run directly if possible. */",
            "\t\t\t\tmigrate_disable();",
            "\t\t\t\tif (cpu != smp_processor_id()) {",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t\tschedule_work_on(cpu,",
            "\t\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\t} else {",
            "\t\t\t\t\tupdate_pages_handler(&cpu_buffer->update_pages_work);",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* wait for all the updates to complete */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tif (cpu_online(cpu))",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t}",
            "",
            "\t\tcpus_read_unlock();",
            "\t} else {",
            "\t\tcpu_buffer = buffer->buffers[cpu_id];",
            "",
            "\t\tif (nr_pages == cpu_buffer->nr_pages)",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\terr = -EBUSY;",
            "\t\t\tgoto out_err_unlock;",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "",
            "\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\tif (cpu_buffer->nr_pages_to_update > 0 &&",
            "\t\t\t__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t    &cpu_buffer->new_pages)) {",
            "\t\t\terr = -ENOMEM;",
            "\t\t\tgoto out_err;",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "",
            "\t\t/* Can't run something on an offline CPU. */",
            "\t\tif (!cpu_online(cpu_id))",
            "\t\t\trb_update_pages(cpu_buffer);",
            "\t\telse {",
            "\t\t\t/* Run directly if possible. */",
            "\t\t\tmigrate_disable();",
            "\t\t\tif (cpu_id == smp_processor_id()) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t} else {",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t\tschedule_work_on(cpu_id,",
            "\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\tcpus_read_unlock();",
            "\t}",
            "",
            " out:",
            "\t/*",
            "\t * The ring buffer resize can happen with the ring buffer",
            "\t * enabled, so that the update disturbs the tracing as little",
            "\t * as possible. But if the buffer is disabled, we do not need",
            "\t * to worry about that, and we can take the time to verify",
            "\t * that the buffer is not corrupt.",
            "\t */",
            "\tif (atomic_read(&buffer->record_disabled)) {",
            "\t\tatomic_inc(&buffer->record_disabled);",
            "\t\t/*",
            "\t\t * Even though the buffer was disabled, we must make sure",
            "\t\t * that it is truly disabled before calling rb_check_pages.",
            "\t\t * There could have been a race between checking",
            "\t\t * record_disable and incrementing it.",
            "\t\t */",
            "\t\tsynchronize_rcu();",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tunsigned long flags;",
            "",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\t\trb_check_pages(cpu_buffer);",
            "\t\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t\t}",
            "\t\tatomic_dec(&buffer->record_disabled);",
            "\t}",
            "",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn 0;",
            "",
            " out_err:",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "",
            "\t\tif (list_empty(&cpu_buffer->new_pages))",
            "\t\t\tcontinue;",
            "",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\tlist) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            " out_err_unlock:",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn err;",
            "}"
          ],
          "function_name": "rb_insert_pages, rb_update_pages, update_pages_handler, ring_buffer_resize",
          "description": "实现将新分配的缓冲页插入到环形缓冲区的头部，通过CAS操作确保线程安全地更新链表结构，若失败则释放内存资源。包含调整缓冲区大小的核心逻辑，协调多CPU上的页面分配与更新操作。",
          "similarity": 0.6381856203079224
        },
        {
          "chunk_id": 23,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 5310,
          "end_line": 5415,
          "content": [
            "static void reset_disabled_cpu_buffer(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "",
            "\tif (RB_WARN_ON(cpu_buffer, local_read(&cpu_buffer->committing)))",
            "\t\tgoto out;",
            "",
            "\tarch_spin_lock(&cpu_buffer->lock);",
            "",
            "\trb_reset_cpu(cpu_buffer);",
            "",
            "\tarch_spin_unlock(&cpu_buffer->lock);",
            "",
            " out:",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "void ring_buffer_reset_cpu(struct trace_buffer *buffer, int cpu)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];",
            "",
            "\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\treturn;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tatomic_inc(&cpu_buffer->resize_disabled);",
            "\tatomic_inc(&cpu_buffer->record_disabled);",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\tatomic_dec(&cpu_buffer->record_disabled);",
            "\tatomic_dec(&cpu_buffer->resize_disabled);",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}",
            "void ring_buffer_reset_online_cpus(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tfor_each_online_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\tatomic_add(RESET_BIT, &cpu_buffer->resize_disabled);",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t}",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t/*",
            "\t\t * If a CPU came online during the synchronize_rcu(), then",
            "\t\t * ignore it.",
            "\t\t */",
            "\t\tif (!(atomic_read(&cpu_buffer->resize_disabled) & RESET_BIT))",
            "\t\t\tcontinue;",
            "",
            "\t\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\t\tatomic_dec(&cpu_buffer->record_disabled);",
            "\t\tatomic_sub(RESET_BIT, &cpu_buffer->resize_disabled);",
            "\t}",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}",
            "void ring_buffer_reset(struct trace_buffer *buffer)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tint cpu;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\tatomic_inc(&cpu_buffer->resize_disabled);",
            "\t\tatomic_inc(&cpu_buffer->record_disabled);",
            "\t}",
            "",
            "\t/* Make sure all commits have finished */",
            "\tsynchronize_rcu();",
            "",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\treset_disabled_cpu_buffer(cpu_buffer);",
            "",
            "\t\tatomic_dec(&cpu_buffer->record_disabled);",
            "\t\tatomic_dec(&cpu_buffer->resize_disabled);",
            "\t}",
            "",
            "\tmutex_unlock(&buffer->mutex);",
            "}"
          ],
          "function_name": "reset_disabled_cpu_buffer, ring_buffer_reset_cpu, ring_buffer_reset_online_cpus, ring_buffer_reset",
          "description": "该代码段实现了对跟踪环形缓冲区的重置机制，通过原子操作与RCU同步确保多线程安全。  \n`reset_disabled_cpu_buffer`负责安全地重置指定CPU的缓冲区，`ring_buffer_reset_cpu`、`ring_buffer_reset_online_cpus`和`ring_buffer_reset`分别针对单个CPU、在线CPU及全系统CPU执行重置，均通过原子计数器控制访问权限并阻塞数据提交。  \n由于`rb_reset_cpu`未在当前代码片段中定义，故其具体行为依赖上下文信息。",
          "similarity": 0.6331637501716614
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 45,
          "end_line": 148,
          "content": [
            "int ring_buffer_print_entry_header(struct trace_seq *s)",
            "{",
            "\ttrace_seq_puts(s, \"# compressed entry header\\n\");",
            "\ttrace_seq_puts(s, \"\\ttype_len    :    5 bits\\n\");",
            "\ttrace_seq_puts(s, \"\\ttime_delta  :   27 bits\\n\");",
            "\ttrace_seq_puts(s, \"\\tarray       :   32 bits\\n\");",
            "\ttrace_seq_putc(s, '\\n');",
            "\ttrace_seq_printf(s, \"\\tpadding     : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_PADDING);",
            "\ttrace_seq_printf(s, \"\\ttime_extend : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_TIME_EXTEND);",
            "\ttrace_seq_printf(s, \"\\ttime_stamp : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_TIME_STAMP);",
            "\ttrace_seq_printf(s, \"\\tdata max type_len  == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_DATA_TYPE_LEN_MAX);",
            "",
            "\treturn !trace_seq_has_overflowed(s);",
            "}",
            "static inline bool rb_null_event(struct ring_buffer_event *event)",
            "{",
            "\treturn event->type_len == RINGBUF_TYPE_PADDING && !event->time_delta;",
            "}",
            "static void rb_event_set_padding(struct ring_buffer_event *event)",
            "{",
            "\t/* padding has a NULL time_delta */",
            "\tevent->type_len = RINGBUF_TYPE_PADDING;",
            "\tevent->time_delta = 0;",
            "}",
            "static unsigned",
            "rb_event_data_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned length;",
            "",
            "\tif (event->type_len)",
            "\t\tlength = event->type_len * RB_ALIGNMENT;",
            "\telse",
            "\t\tlength = event->array[0];",
            "\treturn length + RB_EVNT_HDR_SIZE;",
            "}",
            "static inline unsigned",
            "rb_event_length(struct ring_buffer_event *event)",
            "{",
            "\tswitch (event->type_len) {",
            "\tcase RINGBUF_TYPE_PADDING:",
            "\t\tif (rb_null_event(event))",
            "\t\t\t/* undefined */",
            "\t\t\treturn -1;",
            "\t\treturn  event->array[0] + RB_EVNT_HDR_SIZE;",
            "",
            "\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\treturn RB_LEN_TIME_EXTEND;",
            "",
            "\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\treturn RB_LEN_TIME_STAMP;",
            "",
            "\tcase RINGBUF_TYPE_DATA:",
            "\t\treturn rb_event_data_length(event);",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(1);",
            "\t}",
            "\t/* not hit */",
            "\treturn 0;",
            "}",
            "static inline unsigned",
            "rb_event_ts_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned len = 0;",
            "",
            "\tif (extended_time(event)) {",
            "\t\t/* time extends include the data event after it */",
            "\t\tlen = RB_LEN_TIME_EXTEND;",
            "\t\tevent = skip_time_extend(event);",
            "\t}",
            "\treturn len + rb_event_length(event);",
            "}",
            "unsigned ring_buffer_event_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned length;",
            "",
            "\tif (extended_time(event))",
            "\t\tevent = skip_time_extend(event);",
            "",
            "\tlength = rb_event_length(event);",
            "\tif (event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX)",
            "\t\treturn length;",
            "\tlength -= RB_EVNT_HDR_SIZE;",
            "\tif (length > RB_MAX_SMALL_DATA + sizeof(event->array[0]))",
            "                length -= sizeof(event->array[0]);",
            "\treturn length;",
            "}",
            "static u64 rb_event_time_stamp(struct ring_buffer_event *event)",
            "{",
            "\tu64 ts;",
            "",
            "\tts = event->array[0];",
            "\tts <<= TS_SHIFT;",
            "\tts += event->time_delta;",
            "",
            "\treturn ts;",
            "}",
            "static void rb_init_page(struct buffer_data_page *bpage)",
            "{",
            "\tlocal_set(&bpage->commit, 0);",
            "}"
          ],
          "function_name": "ring_buffer_print_entry_header, rb_null_event, rb_event_set_padding, rb_event_data_length, rb_event_length, rb_event_ts_length, ring_buffer_event_length, rb_event_time_stamp, rb_init_page",
          "description": "实现环形缓冲区事件解析功能，包括打印事件头信息、识别空事件、设置填充事件、计算不同事件类型的数据长度及时间戳，提供事件长度和时间戳读取接口",
          "similarity": 0.63089919090271
        }
      ]
    },
    {
      "source_file": "mm/page_alloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_alloc.c`\n\n---\n\n# page_alloc.c 技术文档\n\n## 1. 文件概述\n\n`page_alloc.c` 是 Linux 内核内存管理子系统的核心文件之一，负责物理页面的分配与释放。该文件实现了基于区域（zone）和迁移类型（migratetype）的伙伴系统（Buddy System）内存分配器，管理系统的空闲页链表，并提供高效的页面分配/回收机制。它不处理小对象分配（由 slab/slub/slob 子系统负责），而是专注于以页为单位的大块物理内存管理。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct per_cpu_pages`**：每个 CPU 的每区（per-zone）页面缓存，用于减少锁竞争，提升分配性能。\n- **`node_states[NR_NODE_STATES]`**：全局节点状态掩码数组，跟踪各 NUMA 节点的状态（如在线、有内存等）。\n- **`sysctl_lowmem_reserve_ratio[MAX_NR_ZONES]`**：各内存区域的低内存保留比例，防止高优先级区域耗尽低优先级区域的内存。\n- **`zone_names[]` 和 `migratetype_names[]`**：内存区域和页面迁移类型的名称字符串，用于调试和日志。\n- **`gfp_allowed_mask`**：全局 GFP（Get Free Page）标志掩码，控制启动早期可使用的分配标志。\n\n### 主要函数（部分声明）\n- **`__free_pages_ok()`**：内部页面释放函数，执行实际的伙伴系统合并与链表插入逻辑。\n- 各种页面分配函数（如 `alloc_pages()`、`__alloc_pages()` 等，定义在其他位置但在此文件中实现核心逻辑）。\n- 每 CPU 页面列表操作辅助宏（如 `pcp_spin_lock()`、`pcp_spin_trylock()`）。\n\n### 关键常量与标志\n- **`fpi_t` 类型及标志**：\n  - `FPI_NONE`：无特殊要求。\n  - `FPI_SKIP_REPORT_NOTIFY`：跳过空闲页报告通知。\n  - `FPI_TO_TAIL`：将页面放回空闲链表尾部（用于优化场景如内存热插拔）。\n- **`min_free_kbytes`**：系统保留的最小空闲内存（KB），影响水位线计算。\n\n## 3. 关键实现\n\n### 每 CPU 页面缓存（Per-CPU Page Caching）\n- 通过 `struct per_cpu_pages` 为每个 CPU 维护热/冷页列表，避免频繁访问全局 zone 锁。\n- 使用 `pcpu_spin_lock` 宏族安全地访问每 CPU 数据，结合 `preempt_disable()`（非 RT）或 `migrate_disable()`（RT）防止任务迁移导致访问错误 CPU 的数据。\n- 在 UP 系统上，使用 IRQ 关闭防止重入；在 SMP/RT 系统上依赖自旋锁语义。\n\n### 内存区域（Zone）与 NUMA 支持\n- 支持多种内存区域（DMA、DMA32、Normal、HighMem、Movable、Device），通过 `zone_names` 标识。\n- 实现 `lowmem_reserve_ratio` 机制，确保高区域分配不会耗尽低区域的保留内存（如 ZONE_DMA 为设备保留）。\n- 通过 `node_states` 和 per-CPU 变量（如 `numa_node`、`_numa_mem_`）支持 NUMA 和无内存节点架构。\n\n### 空闲页管理优化\n- **`FPI_TO_TAIL` 标志**：允许将页面放回空闲链表尾部，配合内存打乱（shuffle）或热插拔时批量初始化。\n- **`FPI_SKIP_REPORT_NOTIFY` 标志**：在临时取出并归还页面时不触发空闲页报告机制，减少开销。\n- **水位线与保留内存**：`min_free_kbytes` 控制最低水位，影响 OOM（Out-Of-Memory）决策和内存回收行为。\n\n### 实时内核（PREEMPT_RT）适配\n- 在 RT 内核中使用 `migrate_disable()` 替代 `preempt_disable()`，避免干扰 RT 自旋锁的优先级继承机制。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/highmem.h>`, `\"internal.h\"`\n- **同步机制**：`<linux/spinlock.h>`（隐含）、`<linux/mutex.h>`\n- **NUMA 与拓扑**：`<linux/topology.h>`, `<linux/nodemask.h>`\n- **调试与追踪**：`<linux/kasan.h>`, `<trace/events/kmem.h>`, `<linux/page_owner.h>`\n- **高级特性**：`<linux/compaction.h>`, `<linux/migrate.h>`, `<linux/memcontrol.h>`\n\n### 子系统交互\n- **Slab 分配器**：本文件不处理 kmalloc，由 `slab.c` 等负责。\n- **内存回收**：与 `vmscan.c` 协同，通过水位线触发 reclaim。\n- **内存热插拔**：通过 `memory_hotplug.h` 接口管理动态内存。\n- **OOM Killer**：通过 `oom.h` 和水位线机制触发 OOM。\n- **透明大页（THP）**：与 `khugepaged` 协同进行大页分配。\n\n## 5. 使用场景\n\n- **内核内存分配**：所有以页为单位的内核内存请求（如 `alloc_pages()`）最终由本文件处理。\n- **用户空间缺页处理**：匿名页、文件页的物理页分配。\n- **内存映射（mmap）**：大块物理内存的分配与管理。\n- **内存回收与迁移**：页面回收、压缩（compaction）、迁移（migration）过程中涉及的页面释放与重新分配。\n- **系统启动与热插拔**：初始化内存区域、处理动态添加/移除内存。\n- **实时系统**：在 PREEMPT_RT 内核中提供低延迟的页面分配路径。\n- **调试与监控**：通过 page owner、KASAN、tracepoint 等机制提供内存使用追踪。",
      "similarity": 0.5827658176422119,
      "chunks": [
        {
          "chunk_id": 30,
          "file_path": "mm/page_alloc.c",
          "start_line": 5824,
          "end_line": 5948,
          "content": [
            "unsigned long free_reserved_area(void *start, void *end, int poison, const char *s)",
            "{",
            "\tvoid *pos;",
            "\tunsigned long pages = 0;",
            "",
            "\tstart = (void *)PAGE_ALIGN((unsigned long)start);",
            "\tend = (void *)((unsigned long)end & PAGE_MASK);",
            "\tfor (pos = start; pos < end; pos += PAGE_SIZE, pages++) {",
            "\t\tstruct page *page = virt_to_page(pos);",
            "\t\tvoid *direct_map_addr;",
            "",
            "\t\t/*",
            "\t\t * 'direct_map_addr' might be different from 'pos'",
            "\t\t * because some architectures' virt_to_page()",
            "\t\t * work with aliases.  Getting the direct map",
            "\t\t * address ensures that we get a _writeable_",
            "\t\t * alias for the memset().",
            "\t\t */",
            "\t\tdirect_map_addr = page_address(page);",
            "\t\t/*",
            "\t\t * Perform a kasan-unchecked memset() since this memory",
            "\t\t * has not been initialized.",
            "\t\t */",
            "\t\tdirect_map_addr = kasan_reset_tag(direct_map_addr);",
            "\t\tif ((unsigned int)poison <= 0xFF)",
            "\t\t\tmemset(direct_map_addr, poison, PAGE_SIZE);",
            "",
            "\t\tfree_reserved_page(page);",
            "\t}",
            "",
            "\tif (pages && s)",
            "\t\tpr_info(\"Freeing %s memory: %ldK\\n\", s, K(pages));",
            "",
            "\treturn pages;",
            "}",
            "void free_reserved_page(struct page *page)",
            "{",
            "\tclear_page_tag_ref(page);",
            "\tClearPageReserved(page);",
            "\tinit_page_count(page);",
            "\t__free_page(page);",
            "\tadjust_managed_page_count(page, 1);",
            "}",
            "static int page_alloc_cpu_dead(unsigned int cpu)",
            "{",
            "\tstruct zone *zone;",
            "",
            "\tlru_add_drain_cpu(cpu);",
            "\tmlock_drain_remote(cpu);",
            "\tdrain_pages(cpu);",
            "",
            "\t/*",
            "\t * Spill the event counters of the dead processor",
            "\t * into the current processors event counters.",
            "\t * This artificially elevates the count of the current",
            "\t * processor.",
            "\t */",
            "\tvm_events_fold_cpu(cpu);",
            "",
            "\t/*",
            "\t * Zero the differential counters of the dead processor",
            "\t * so that the vm statistics are consistent.",
            "\t *",
            "\t * This is only okay since the processor is dead and cannot",
            "\t * race with what we are doing.",
            "\t */",
            "\tcpu_vm_stats_fold(cpu);",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tzone_pcp_update(zone, 0);",
            "",
            "\treturn 0;",
            "}",
            "static int page_alloc_cpu_online(unsigned int cpu)",
            "{",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tzone_pcp_update(zone, 1);",
            "\treturn 0;",
            "}",
            "void __init page_alloc_init_cpuhp(void)",
            "{",
            "\tint ret;",
            "",
            "\tret = cpuhp_setup_state_nocalls(CPUHP_PAGE_ALLOC,",
            "\t\t\t\t\t\"mm/page_alloc:pcp\",",
            "\t\t\t\t\tpage_alloc_cpu_online,",
            "\t\t\t\t\tpage_alloc_cpu_dead);",
            "\tWARN_ON(ret < 0);",
            "}",
            "static void calculate_totalreserve_pages(void)",
            "{",
            "\tstruct pglist_data *pgdat;",
            "\tunsigned long reserve_pages = 0;",
            "\tenum zone_type i, j;",
            "",
            "\tfor_each_online_pgdat(pgdat) {",
            "",
            "\t\tpgdat->totalreserve_pages = 0;",
            "",
            "\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {",
            "\t\t\tstruct zone *zone = pgdat->node_zones + i;",
            "\t\t\tlong max = 0;",
            "\t\t\tunsigned long managed_pages = zone_managed_pages(zone);",
            "",
            "\t\t\t/* Find valid and maximum lowmem_reserve in the zone */",
            "\t\t\tfor (j = i; j < MAX_NR_ZONES; j++) {",
            "\t\t\t\tif (zone->lowmem_reserve[j] > max)",
            "\t\t\t\t\tmax = zone->lowmem_reserve[j];",
            "\t\t\t}",
            "",
            "\t\t\t/* we treat the high watermark as reserved pages. */",
            "\t\t\tmax += high_wmark_pages(zone);",
            "",
            "\t\t\tif (max > managed_pages)",
            "\t\t\t\tmax = managed_pages;",
            "",
            "\t\t\tpgdat->totalreserve_pages += max;",
            "",
            "\t\t\treserve_pages += max;",
            "\t\t}",
            "\t}",
            "\ttotalreserve_pages = reserve_pages;",
            "}"
          ],
          "function_name": "free_reserved_area, free_reserved_page, page_alloc_cpu_dead, page_alloc_cpu_online, page_alloc_init_cpuhp, calculate_totalreserve_pages",
          "description": "实现释放保留内存区域，遍历指定范围内的页面进行初始化和释放操作，并维护统计信息。包含释放保留页面、CPU在线/离线回调及计算总保留页数等功能。",
          "similarity": 0.6137986779212952
        },
        {
          "chunk_id": 34,
          "file_path": "mm/page_alloc.c",
          "start_line": 6473,
          "end_line": 6610,
          "content": [
            "static void split_free_pages(struct list_head *list)",
            "{",
            "\tint order;",
            "",
            "\tfor (order = 0; order < NR_PAGE_ORDERS; order++) {",
            "\t\tstruct page *page, *next;",
            "\t\tint nr_pages = 1 << order;",
            "",
            "\t\tlist_for_each_entry_safe(page, next, &list[order], lru) {",
            "\t\t\tint i;",
            "",
            "\t\t\tpost_alloc_hook(page, order, __GFP_MOVABLE);",
            "\t\t\tif (!order)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tsplit_page(page, order);",
            "",
            "\t\t\t/* Add all subpages to the order-0 head, in sequence. */",
            "\t\t\tlist_del(&page->lru);",
            "\t\t\tfor (i = 0; i < nr_pages; i++)",
            "\t\t\t\tlist_add_tail(&page[i].lru, &list[0]);",
            "\t\t}",
            "\t}",
            "}",
            "int alloc_contig_range_noprof(unsigned long start, unsigned long end,",
            "\t\t       unsigned migratetype, gfp_t gfp_mask)",
            "{",
            "\tunsigned long outer_start, outer_end;",
            "\tint ret = 0;",
            "",
            "\tstruct compact_control cc = {",
            "\t\t.nr_migratepages = 0,",
            "\t\t.order = -1,",
            "\t\t.zone = page_zone(pfn_to_page(start)),",
            "\t\t.mode = MIGRATE_SYNC,",
            "\t\t.ignore_skip_hint = true,",
            "\t\t.no_set_skip_hint = true,",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.alloc_contig = true,",
            "\t};",
            "\tINIT_LIST_HEAD(&cc.migratepages);",
            "",
            "\t/*",
            "\t * What we do here is we mark all pageblocks in range as",
            "\t * MIGRATE_ISOLATE.  Because pageblock and max order pages may",
            "\t * have different sizes, and due to the way page allocator",
            "\t * work, start_isolate_page_range() has special handlings for this.",
            "\t *",
            "\t * Once the pageblocks are marked as MIGRATE_ISOLATE, we",
            "\t * migrate the pages from an unaligned range (ie. pages that",
            "\t * we are interested in). This will put all the pages in",
            "\t * range back to page allocator as MIGRATE_ISOLATE.",
            "\t *",
            "\t * When this is done, we take the pages in range from page",
            "\t * allocator removing them from the buddy system.  This way",
            "\t * page allocator will never consider using them.",
            "\t *",
            "\t * This lets us mark the pageblocks back as",
            "\t * MIGRATE_CMA/MIGRATE_MOVABLE so that free pages in the",
            "\t * aligned range but not in the unaligned, original range are",
            "\t * put back to page allocator so that buddy can use them.",
            "\t */",
            "",
            "\tret = start_isolate_page_range(start, end, migratetype, 0, gfp_mask);",
            "\tif (ret)",
            "\t\tgoto done;",
            "",
            "\tdrain_all_pages(cc.zone);",
            "",
            "\t/*",
            "\t * In case of -EBUSY, we'd like to know which page causes problem.",
            "\t * So, just fall through. test_pages_isolated() has a tracepoint",
            "\t * which will report the busy page.",
            "\t *",
            "\t * It is possible that busy pages could become available before",
            "\t * the call to test_pages_isolated, and the range will actually be",
            "\t * allocated.  So, if we fall through be sure to clear ret so that",
            "\t * -EBUSY is not accidentally used or returned to caller.",
            "\t */",
            "\tret = __alloc_contig_migrate_range(&cc, start, end, migratetype);",
            "\tif (ret && ret != -EBUSY)",
            "\t\tgoto done;",
            "\tret = 0;",
            "",
            "\t/*",
            "\t * Pages from [start, end) are within a pageblock_nr_pages",
            "\t * aligned blocks that are marked as MIGRATE_ISOLATE.  What's",
            "\t * more, all pages in [start, end) are free in page allocator.",
            "\t * What we are going to do is to allocate all pages from",
            "\t * [start, end) (that is remove them from page allocator).",
            "\t *",
            "\t * The only problem is that pages at the beginning and at the",
            "\t * end of interesting range may be not aligned with pages that",
            "\t * page allocator holds, ie. they can be part of higher order",
            "\t * pages.  Because of this, we reserve the bigger range and",
            "\t * once this is done free the pages we are not interested in.",
            "\t *",
            "\t * We don't have to hold zone->lock here because the pages are",
            "\t * isolated thus they won't get removed from buddy.",
            "\t */",
            "\touter_start = find_large_buddy(start);",
            "",
            "\t/* Make sure the range is really isolated. */",
            "\tif (test_pages_isolated(outer_start, end, 0)) {",
            "\t\tret = -EBUSY;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\t/* Grab isolated pages from freelists. */",
            "\touter_end = isolate_freepages_range(&cc, outer_start, end);",
            "\tif (!outer_end) {",
            "\t\tret = -EBUSY;",
            "\t\tgoto done;",
            "\t}",
            "",
            "\tif (!(gfp_mask & __GFP_COMP)) {",
            "\t\tsplit_free_pages(cc.freepages);",
            "",
            "\t\t/* Free head and tail (if any) */",
            "\t\tif (start != outer_start)",
            "\t\t\tfree_contig_range(outer_start, start - outer_start);",
            "\t\tif (end != outer_end)",
            "\t\t\tfree_contig_range(end, outer_end - end);",
            "\t} else if (start == outer_start && end == outer_end && is_power_of_2(end - start)) {",
            "\t\tstruct page *head = pfn_to_page(start);",
            "\t\tint order = ilog2(end - start);",
            "",
            "\t\tcheck_new_pages(head, order);",
            "\t\tprep_new_page(head, order, gfp_mask, 0);",
            "\t} else {",
            "\t\tret = -EINVAL;",
            "\t\tWARN(true, \"PFN range: requested [%lu, %lu), allocated [%lu, %lu)\\n\",",
            "\t\t     start, end, outer_start, outer_end);",
            "\t}",
            "done:",
            "\tundo_isolate_page_range(start, end, migratetype);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "split_free_pages, alloc_contig_range_noprof",
          "description": "实现连续物理内存范围分配功能，通过隔离页面块、迁移页面和调整区域状态，确保目标范围内内存可被系统使用并处理分配异常情况。",
          "similarity": 0.6114052534103394
        },
        {
          "chunk_id": 32,
          "file_path": "mm/page_alloc.c",
          "start_line": 6106,
          "end_line": 6209,
          "content": [
            "void calculate_min_free_kbytes(void)",
            "{",
            "\tunsigned long lowmem_kbytes;",
            "\tint new_min_free_kbytes;",
            "",
            "\tlowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE >> 10);",
            "\tnew_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);",
            "",
            "\tif (new_min_free_kbytes > user_min_free_kbytes)",
            "\t\tmin_free_kbytes = clamp(new_min_free_kbytes, 128, 262144);",
            "\telse",
            "\t\tpr_warn(\"min_free_kbytes is not updated to %d because user defined value %d is preferred\\n\",",
            "\t\t\t\tnew_min_free_kbytes, user_min_free_kbytes);",
            "",
            "}",
            "int __meminit init_per_zone_wmark_min(void)",
            "{",
            "\tcalculate_min_free_kbytes();",
            "\tsetup_per_zone_wmarks();",
            "\trefresh_zone_stat_thresholds();",
            "\tsetup_per_zone_lowmem_reserve();",
            "",
            "#ifdef CONFIG_NUMA",
            "\tsetup_min_unmapped_ratio();",
            "\tsetup_min_slab_ratio();",
            "#endif",
            "",
            "\tkhugepaged_min_free_kbytes_update();",
            "",
            "\treturn 0;",
            "}",
            "postcore_initcall(init_per_zone_wmark_min)",
            "",
            "/*",
            " * min_free_kbytes_sysctl_handler - just a wrapper around proc_dointvec() so",
            " *\tthat we can call two helper functions whenever min_free_kbytes",
            " *\tchanges.",
            " */",
            "static int min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tif (write) {",
            "\t\tuser_min_free_kbytes = min_free_kbytes;",
            "\t\tsetup_per_zone_wmarks();",
            "\t}",
            "\treturn 0;",
            "}",
            "static int watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tif (write)",
            "\t\tsetup_per_zone_wmarks();",
            "",
            "\treturn 0;",
            "}",
            "static void setup_min_unmapped_ratio(void)",
            "{",
            "\tpg_data_t *pgdat;",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_online_pgdat(pgdat)",
            "\t\tpgdat->min_unmapped_pages = 0;",
            "",
            "\tfor_each_zone(zone)",
            "\t\tzone->zone_pgdat->min_unmapped_pages += (zone_managed_pages(zone) *",
            "\t\t\t\t\t\t         sysctl_min_unmapped_ratio) / 100;",
            "}",
            "static int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,",
            "\t\tvoid *buffer, size_t *length, loff_t *ppos)",
            "{",
            "\tint rc;",
            "",
            "\trc = proc_dointvec_minmax(table, write, buffer, length, ppos);",
            "\tif (rc)",
            "\t\treturn rc;",
            "",
            "\tsetup_min_unmapped_ratio();",
            "",
            "\treturn 0;",
            "}",
            "static void setup_min_slab_ratio(void)",
            "{",
            "\tpg_data_t *pgdat;",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_online_pgdat(pgdat)",
            "\t\tpgdat->min_slab_pages = 0;",
            "",
            "\tfor_each_zone(zone)",
            "\t\tzone->zone_pgdat->min_slab_pages += (zone_managed_pages(zone) *",
            "\t\t\t\t\t\t     sysctl_min_slab_ratio) / 100;",
            "}"
          ],
          "function_name": "calculate_min_free_kbytes, init_per_zone_wmark_min, min_free_kbytes_sysctl_handler, watermark_scale_factor_sysctl_handler, setup_min_unmapped_ratio, sysctl_min_unmapped_ratio_sysctl_handler, setup_min_slab_ratio",
          "description": "计算并更新最小空闲内存阈值，初始化区域水印参数，处理sysctl接口变更，动态调整内存管理策略参数。",
          "similarity": 0.6000020503997803
        },
        {
          "chunk_id": 28,
          "file_path": "mm/page_alloc.c",
          "start_line": 5556,
          "end_line": 5674,
          "content": [
            "static int zone_highsize(struct zone *zone, int batch, int cpu_online,",
            "\t\t\t int high_fraction)",
            "{",
            "#ifdef CONFIG_MMU",
            "\tint high;",
            "\tint nr_split_cpus;",
            "\tunsigned long total_pages;",
            "",
            "\tif (!high_fraction) {",
            "\t\t/*",
            "\t\t * By default, the high value of the pcp is based on the zone",
            "\t\t * low watermark so that if they are full then background",
            "\t\t * reclaim will not be started prematurely.",
            "\t\t */",
            "\t\ttotal_pages = low_wmark_pages(zone);",
            "\t} else {",
            "\t\t/*",
            "\t\t * If percpu_pagelist_high_fraction is configured, the high",
            "\t\t * value is based on a fraction of the managed pages in the",
            "\t\t * zone.",
            "\t\t */",
            "\t\ttotal_pages = zone_managed_pages(zone) / high_fraction;",
            "\t}",
            "",
            "\t/*",
            "\t * Split the high value across all online CPUs local to the zone. Note",
            "\t * that early in boot that CPUs may not be online yet and that during",
            "\t * CPU hotplug that the cpumask is not yet updated when a CPU is being",
            "\t * onlined. For memory nodes that have no CPUs, split the high value",
            "\t * across all online CPUs to mitigate the risk that reclaim is triggered",
            "\t * prematurely due to pages stored on pcp lists.",
            "\t */",
            "\tnr_split_cpus = cpumask_weight(cpumask_of_node(zone_to_nid(zone))) + cpu_online;",
            "\tif (!nr_split_cpus)",
            "\t\tnr_split_cpus = num_online_cpus();",
            "\thigh = total_pages / nr_split_cpus;",
            "",
            "\t/*",
            "\t * Ensure high is at least batch*4. The multiple is based on the",
            "\t * historical relationship between high and batch.",
            "\t */",
            "\thigh = max(high, batch << 2);",
            "",
            "\treturn high;",
            "#else",
            "\treturn 0;",
            "#endif",
            "}",
            "static void pageset_update(struct per_cpu_pages *pcp, unsigned long high_min,",
            "\t\t\t   unsigned long high_max, unsigned long batch)",
            "{",
            "\tWRITE_ONCE(pcp->batch, batch);",
            "\tWRITE_ONCE(pcp->high_min, high_min);",
            "\tWRITE_ONCE(pcp->high_max, high_max);",
            "}",
            "static void per_cpu_pages_init(struct per_cpu_pages *pcp, struct per_cpu_zonestat *pzstats)",
            "{",
            "\tint pindex;",
            "",
            "\tmemset(pcp, 0, sizeof(*pcp));",
            "\tmemset(pzstats, 0, sizeof(*pzstats));",
            "",
            "\tspin_lock_init(&pcp->lock);",
            "\tfor (pindex = 0; pindex < NR_PCP_LISTS; pindex++)",
            "\t\tINIT_LIST_HEAD(&pcp->lists[pindex]);",
            "",
            "\t/*",
            "\t * Set batch and high values safe for a boot pageset. A true percpu",
            "\t * pageset's initialization will update them subsequently. Here we don't",
            "\t * need to be as careful as pageset_update() as nobody can access the",
            "\t * pageset yet.",
            "\t */",
            "\tpcp->high_min = BOOT_PAGESET_HIGH;",
            "\tpcp->high_max = BOOT_PAGESET_HIGH;",
            "\tpcp->batch = BOOT_PAGESET_BATCH;",
            "\tpcp->free_count = 0;",
            "}",
            "static void __zone_set_pageset_high_and_batch(struct zone *zone, unsigned long high_min,",
            "\t\t\t\t\t      unsigned long high_max, unsigned long batch)",
            "{",
            "\tstruct per_cpu_pages *pcp;",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);",
            "\t\tpageset_update(pcp, high_min, high_max, batch);",
            "\t}",
            "}",
            "static void zone_set_pageset_high_and_batch(struct zone *zone, int cpu_online)",
            "{",
            "\tint new_high_min, new_high_max, new_batch;",
            "",
            "\tnew_batch = max(1, zone_batchsize(zone));",
            "\tif (percpu_pagelist_high_fraction) {",
            "\t\tnew_high_min = zone_highsize(zone, new_batch, cpu_online,",
            "\t\t\t\t\t     percpu_pagelist_high_fraction);",
            "\t\t/*",
            "\t\t * PCP high is tuned manually, disable auto-tuning via",
            "\t\t * setting high_min and high_max to the manual value.",
            "\t\t */",
            "\t\tnew_high_max = new_high_min;",
            "\t} else {",
            "\t\tnew_high_min = zone_highsize(zone, new_batch, cpu_online, 0);",
            "\t\tnew_high_max = zone_highsize(zone, new_batch, cpu_online,",
            "\t\t\t\t\t     MIN_PERCPU_PAGELIST_HIGH_FRACTION);",
            "\t}",
            "",
            "\tif (zone->pageset_high_min == new_high_min &&",
            "\t    zone->pageset_high_max == new_high_max &&",
            "\t    zone->pageset_batch == new_batch)",
            "\t\treturn;",
            "",
            "\tzone->pageset_high_min = new_high_min;",
            "\tzone->pageset_high_max = new_high_max;",
            "\tzone->pageset_batch = new_batch;",
            "",
            "\t__zone_set_pageset_high_and_batch(zone, new_high_min, new_high_max,",
            "\t\t\t\t\t  new_batch);",
            "}"
          ],
          "function_name": "zone_highsize, pageset_update, per_cpu_pages_init, __zone_set_pageset_high_and_batch, zone_set_pageset_high_and_batch",
          "description": "计算并设置各CPU的页集高水位和批量参数，根据缓存数据片大小调整批量标志位，控制PCP列表的行为特性。",
          "similarity": 0.5940933227539062
        },
        {
          "chunk_id": 13,
          "file_path": "mm/page_alloc.c",
          "start_line": 2309,
          "end_line": 2451,
          "content": [
            "int decay_pcp_high(struct zone *zone, struct per_cpu_pages *pcp)",
            "{",
            "\tint high_min, to_drain, batch;",
            "\tint todo = 0;",
            "",
            "\thigh_min = READ_ONCE(pcp->high_min);",
            "\tbatch = READ_ONCE(pcp->batch);",
            "\t/*",
            "\t * Decrease pcp->high periodically to try to free possible",
            "\t * idle PCP pages.  And, avoid to free too many pages to",
            "\t * control latency.  This caps pcp->high decrement too.",
            "\t */",
            "\tif (pcp->high > high_min) {",
            "\t\tpcp->high = max3(pcp->count - (batch << CONFIG_PCP_BATCH_SCALE_MAX),",
            "\t\t\t\t pcp->high - (pcp->high >> 3), high_min);",
            "\t\tif (pcp->high > high_min)",
            "\t\t\ttodo++;",
            "\t}",
            "",
            "\tto_drain = pcp->count - pcp->high;",
            "\tif (to_drain > 0) {",
            "\t\tspin_lock(&pcp->lock);",
            "\t\tfree_pcppages_bulk(zone, to_drain, pcp, 0);",
            "\t\tspin_unlock(&pcp->lock);",
            "\t\ttodo++;",
            "\t}",
            "",
            "\treturn todo;",
            "}",
            "void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp)",
            "{",
            "\tint to_drain, batch;",
            "",
            "\tbatch = READ_ONCE(pcp->batch);",
            "\tto_drain = min(pcp->count, batch);",
            "\tif (to_drain > 0) {",
            "\t\tspin_lock(&pcp->lock);",
            "\t\tfree_pcppages_bulk(zone, to_drain, pcp, 0);",
            "\t\tspin_unlock(&pcp->lock);",
            "\t}",
            "}",
            "static void drain_pages_zone(unsigned int cpu, struct zone *zone)",
            "{",
            "\tstruct per_cpu_pages *pcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);",
            "\tint count;",
            "",
            "\tdo {",
            "\t\tspin_lock(&pcp->lock);",
            "\t\tcount = pcp->count;",
            "\t\tif (count) {",
            "\t\t\tint to_drain = min(count,",
            "\t\t\t\tpcp->batch << CONFIG_PCP_BATCH_SCALE_MAX);",
            "",
            "\t\t\tfree_pcppages_bulk(zone, to_drain, pcp, 0);",
            "\t\t\tcount -= to_drain;",
            "\t\t}",
            "\t\tspin_unlock(&pcp->lock);",
            "\t} while (count);",
            "}",
            "static void drain_pages(unsigned int cpu)",
            "{",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_populated_zone(zone) {",
            "\t\tdrain_pages_zone(cpu, zone);",
            "\t}",
            "}",
            "void drain_local_pages(struct zone *zone)",
            "{",
            "\tint cpu = smp_processor_id();",
            "",
            "\tif (zone)",
            "\t\tdrain_pages_zone(cpu, zone);",
            "\telse",
            "\t\tdrain_pages(cpu);",
            "}",
            "static void __drain_all_pages(struct zone *zone, bool force_all_cpus)",
            "{",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * Allocate in the BSS so we won't require allocation in",
            "\t * direct reclaim path for CONFIG_CPUMASK_OFFSTACK=y",
            "\t */",
            "\tstatic cpumask_t cpus_with_pcps;",
            "",
            "\t/*",
            "\t * Do not drain if one is already in progress unless it's specific to",
            "\t * a zone. Such callers are primarily CMA and memory hotplug and need",
            "\t * the drain to be complete when the call returns.",
            "\t */",
            "\tif (unlikely(!mutex_trylock(&pcpu_drain_mutex))) {",
            "\t\tif (!zone)",
            "\t\t\treturn;",
            "\t\tmutex_lock(&pcpu_drain_mutex);",
            "\t}",
            "",
            "\t/*",
            "\t * We don't care about racing with CPU hotplug event",
            "\t * as offline notification will cause the notified",
            "\t * cpu to drain that CPU pcps and on_each_cpu_mask",
            "\t * disables preemption as part of its processing",
            "\t */",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct per_cpu_pages *pcp;",
            "\t\tstruct zone *z;",
            "\t\tbool has_pcps = false;",
            "",
            "\t\tif (force_all_cpus) {",
            "\t\t\t/*",
            "\t\t\t * The pcp.count check is racy, some callers need a",
            "\t\t\t * guarantee that no cpu is missed.",
            "\t\t\t */",
            "\t\t\thas_pcps = true;",
            "\t\t} else if (zone) {",
            "\t\t\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);",
            "\t\t\tif (pcp->count)",
            "\t\t\t\thas_pcps = true;",
            "\t\t} else {",
            "\t\t\tfor_each_populated_zone(z) {",
            "\t\t\t\tpcp = per_cpu_ptr(z->per_cpu_pageset, cpu);",
            "\t\t\t\tif (pcp->count) {",
            "\t\t\t\t\thas_pcps = true;",
            "\t\t\t\t\tbreak;",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tif (has_pcps)",
            "\t\t\tcpumask_set_cpu(cpu, &cpus_with_pcps);",
            "\t\telse",
            "\t\t\tcpumask_clear_cpu(cpu, &cpus_with_pcps);",
            "\t}",
            "",
            "\tfor_each_cpu(cpu, &cpus_with_pcps) {",
            "\t\tif (zone)",
            "\t\t\tdrain_pages_zone(cpu, zone);",
            "\t\telse",
            "\t\t\tdrain_pages(cpu);",
            "\t}",
            "",
            "\tmutex_unlock(&pcpu_drain_mutex);",
            "}"
          ],
          "function_name": "decay_pcp_high, drain_zone_pages, drain_pages_zone, drain_pages, drain_local_pages, __drain_all_pages",
          "description": "decay_pcp_high周期性降低PCP高速缓存上限，drain_zone_pages等函数实现多CPU间PCP页面的同步释放，保障内存回收和负载均衡。",
          "similarity": 0.5939231514930725
        }
      ]
    },
    {
      "source_file": "kernel/watch_queue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:50:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `watch_queue.c`\n\n---\n\n# watch_queue.c 技术文档\n\n## 文件概述\n\n`watch_queue.c` 实现了 Linux 内核中的**监视队列**（Watch Queue）机制，这是一种基于管道（pipe）构建的通用事件通知系统。该机制允许内核子系统（如文件系统、密钥管理、设备驱动等）向用户空间异步发送结构化通知。用户空间通过创建特殊类型的管道并关联监视队列，即可接收来自内核的各类事件通知。该文件定义了通知的投递、过滤、缓冲管理及与管道集成的核心逻辑。\n\n## 核心功能\n\n### 主要函数\n\n- **`__post_watch_notification()`**  \n  核心通知投递函数。遍历指定 `watch_list` 中所有匹配 `id` 的监视器（`watch`），对每个关联的 `watch_queue` 应用过滤规则、安全检查，并将通知写入底层管道。\n\n- **`post_one_notification()`**  \n  将单个通知写入指定 `watch_queue` 的底层管道缓冲区。负责从预分配的通知页中获取空闲槽位、填充数据、更新管道头指针并唤醒等待读取的进程。\n\n- **`filter_watch_notification()`**  \n  根据 `watch_filter` 中的类型、子类型和信息掩码规则，判断是否允许特定通知通过。\n\n- **`watch_queue_set_size()`**  \n  为监视队列分配预分配的通知缓冲区（页数组和位图），并调整底层管道的环形缓冲区大小。\n\n- **`watch_queue_pipe_buf_release()`**  \n  管道缓冲区释放回调。当用户空间读取完通知后，将对应的通知槽位在位图中标记为空闲，供后续复用。\n\n### 关键数据结构\n\n- **`struct watch_queue`**  \n  表示一个监视队列，包含：\n  - 指向底层 `pipe_inode_info` 的指针\n  - 预分配的通知页数组（`notes`）\n  - 通知槽位空闲位图（`notes_bitmap`）\n  - 通知过滤器（`filter`）\n  - 保护锁（`lock`）\n\n- **`struct watch_notification`**  \n  通用通知记录格式，包含类型（`type`）、子类型（`subtype`）、信息字段（`info`，含长度和ID）及可变负载。\n\n- **`struct watch_filter` / `struct watch_type_filter`**  \n  定义通知过滤规则，支持按类型、子类型及信息字段的位掩码进行精确过滤。\n\n- **`watch_queue_pipe_buf_ops`**  \n  自定义的 `pipe_buf_operations`，用于管理监视队列专用管道缓冲区的生命周期。\n\n## 关键实现\n\n### 基于管道的通知传输\n- 监视队列复用内核管道（`pipe_inode_info`）作为通知传输通道，利用其成熟的读写、轮询、异步通知机制。\n- 通过自定义 `pipe_buf_operations`（`watch_queue_pipe_buf_ops`）实现通知槽位的回收：当用户读取通知后，`release` 回调将对应槽位在 `notes_bitmap` 中置位，标记为空闲。\n\n### 预分配通知缓冲区\n- 通知数据存储在预分配的内核页（`notes`）中，每页划分为多个固定大小（128字节）的槽位（`WATCH_QUEUE_NOTE_SIZE`）。\n- 使用位图（`notes_bitmap`）跟踪槽位使用状态，1 表示空闲。投递通知时通过 `find_first_bit()` 快速查找空闲槽位。\n- 缓冲区大小由用户通过 `watch_queue_set_size()` 设置（1-512个通知），并受管道缓冲区配额限制。\n\n### 通知投递流程\n1. **匹配监视器**：遍历 `watch_list`，查找 `id` 匹配的 `watch`。\n2. **应用过滤**：若队列配置了过滤器，调用 `filter_watch_notification()` 决定是否丢弃。\n3. **安全检查**：调用 LSM 钩子 `security_post_notification()` 进行权限验证。\n4. **写入管道**：\n   - 获取空闲通知槽位，复制通知数据。\n   - 构造 `pipe_buffer` 指向该槽位，设置自定义操作集。\n   - 更新管道 `head` 指针，唤醒等待读取的进程。\n   - 若缓冲区满，标记前一个缓冲区为 `PIPE_BUF_FLAG_LOSS` 表示丢包。\n\n### 并发与同步\n- **RCU 保护**：`watch_list` 和 `watch_queue` 的访问通过 RCU 机制保护，确保遍历时结构体不被释放。\n- **自旋锁**：\n  - `wqueue->lock`：保护 `wqueue` 状态（如 `pipe` 指针有效性）。\n  - `pipe->rd_wait.lock`：保护管道环形缓冲区的读写操作。\n- **原子操作**：管道 `head` 指针使用 `smp_store_release()` 更新，确保与 `pipe_read()` 的同步。\n\n## 依赖关系\n\n- **管道子系统**（`fs/pipe.c`）  \n  依赖管道的核心数据结构（`pipe_inode_info`、`pipe_buffer`）和操作接口（`pipe_buf()`、`pipe_full()`、`generic_pipe_buf_*`）。\n\n- **内存管理**  \n  使用 `alloc_page()`、`kmap_atomic()` 管理通知缓冲区页，`bitmap_alloc()` 管理槽位位图。\n\n- **安全模块**（LSM）  \n  通过 `security_post_notification()` 钩子集成安全策略。\n\n- **用户空间接口**  \n  与 `fs/watch_queue.c` 中的系统调用（如 `watch_queue_set_size()`）协同工作，后者负责创建监视队列并与管道关联。\n\n- **头文件依赖**  \n  `linux/watch_queue.h`（核心数据结构定义）、`linux/pipe_fs_i.h`（管道内部接口）。\n\n## 使用场景\n\n- **文件系统事件监控**  \n  如 `fsnotify` 子系统可通过监视队列向用户空间报告文件访问、修改等事件。\n\n- **密钥管理通知**  \n  内核密钥环（`KEYS`）子系统使用该机制通知密钥状态变更（如过期、撤销）。\n\n- **设备事件上报**  \n  设备驱动可利用监视队列异步上报硬件状态变化或错误事件。\n\n- **通用内核事件分发**  \n  任何需要向特权用户空间守护进程（如 `systemd`）发送结构化事件的内核子系统均可集成此机制。\n\n- **用户空间消费**  \n  应用程序通过 `open(\"/dev/watch_queue\")` 获取监视队列文件描述符，调用 `ioctl()` 设置缓冲区大小和过滤器，然后像读取普通管道一样接收通知。",
      "similarity": 0.5819007754325867,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/watch_queue.c",
          "start_line": 42,
          "end_line": 154,
          "content": [
            "static inline bool lock_wqueue(struct watch_queue *wqueue)",
            "{",
            "\tspin_lock_bh(&wqueue->lock);",
            "\tif (unlikely(!wqueue->pipe)) {",
            "\t\tspin_unlock_bh(&wqueue->lock);",
            "\t\treturn false;",
            "\t}",
            "\treturn true;",
            "}",
            "static inline void unlock_wqueue(struct watch_queue *wqueue)",
            "{",
            "\tspin_unlock_bh(&wqueue->lock);",
            "}",
            "static void watch_queue_pipe_buf_release(struct pipe_inode_info *pipe,",
            "\t\t\t\t\t struct pipe_buffer *buf)",
            "{",
            "\tstruct watch_queue *wqueue = (struct watch_queue *)buf->private;",
            "\tstruct page *page;",
            "\tunsigned int bit;",
            "",
            "\t/* We need to work out which note within the page this refers to, but",
            "\t * the note might have been maximum size, so merely ANDing the offset",
            "\t * off doesn't work.  OTOH, the note must've been more than zero size.",
            "\t */",
            "\tbit = buf->offset + buf->len;",
            "\tif ((bit & (WATCH_QUEUE_NOTE_SIZE - 1)) == 0)",
            "\t\tbit -= WATCH_QUEUE_NOTE_SIZE;",
            "\tbit /= WATCH_QUEUE_NOTE_SIZE;",
            "",
            "\tpage = buf->page;",
            "\tbit += page->index;",
            "",
            "\tset_bit(bit, wqueue->notes_bitmap);",
            "\tgeneric_pipe_buf_release(pipe, buf);",
            "}",
            "static bool post_one_notification(struct watch_queue *wqueue,",
            "\t\t\t\t  struct watch_notification *n)",
            "{",
            "\tvoid *p;",
            "\tstruct pipe_inode_info *pipe = wqueue->pipe;",
            "\tstruct pipe_buffer *buf;",
            "\tstruct page *page;",
            "\tunsigned int head, tail, note, offset, len;",
            "\tbool done = false;",
            "",
            "\tspin_lock_irq(&pipe->rd_wait.lock);",
            "",
            "\thead = pipe->head;",
            "\ttail = pipe->tail;",
            "\tif (pipe_full(head, tail, pipe->ring_size))",
            "\t\tgoto lost;",
            "",
            "\tnote = find_first_bit(wqueue->notes_bitmap, wqueue->nr_notes);",
            "\tif (note >= wqueue->nr_notes)",
            "\t\tgoto lost;",
            "",
            "\tpage = wqueue->notes[note / WATCH_QUEUE_NOTES_PER_PAGE];",
            "\toffset = note % WATCH_QUEUE_NOTES_PER_PAGE * WATCH_QUEUE_NOTE_SIZE;",
            "\tget_page(page);",
            "\tlen = n->info & WATCH_INFO_LENGTH;",
            "\tp = kmap_atomic(page);",
            "\tmemcpy(p + offset, n, len);",
            "\tkunmap_atomic(p);",
            "",
            "\tbuf = pipe_buf(pipe, head);",
            "\tbuf->page = page;",
            "\tbuf->private = (unsigned long)wqueue;",
            "\tbuf->ops = &watch_queue_pipe_buf_ops;",
            "\tbuf->offset = offset;",
            "\tbuf->len = len;",
            "\tbuf->flags = PIPE_BUF_FLAG_WHOLE;",
            "\tsmp_store_release(&pipe->head, head + 1); /* vs pipe_read() */",
            "",
            "\tif (!test_and_clear_bit(note, wqueue->notes_bitmap)) {",
            "\t\tspin_unlock_irq(&pipe->rd_wait.lock);",
            "\t\tBUG();",
            "\t}",
            "\twake_up_interruptible_sync_poll_locked(&pipe->rd_wait, EPOLLIN | EPOLLRDNORM);",
            "\tdone = true;",
            "",
            "out:",
            "\tspin_unlock_irq(&pipe->rd_wait.lock);",
            "\tif (done)",
            "\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);",
            "\treturn done;",
            "",
            "lost:",
            "\tbuf = pipe_buf(pipe, head - 1);",
            "\tbuf->flags |= PIPE_BUF_FLAG_LOSS;",
            "\tgoto out;",
            "}",
            "static bool filter_watch_notification(const struct watch_filter *wf,",
            "\t\t\t\t      const struct watch_notification *n)",
            "{",
            "\tconst struct watch_type_filter *wt;",
            "\tunsigned int st_bits = sizeof(wt->subtype_filter[0]) * 8;",
            "\tunsigned int st_index = n->subtype / st_bits;",
            "\tunsigned int st_bit = 1U << (n->subtype % st_bits);",
            "\tint i;",
            "",
            "\tif (!test_bit(n->type, wf->type_filter))",
            "\t\treturn false;",
            "",
            "\tfor (i = 0; i < wf->nr_filters; i++) {",
            "\t\twt = &wf->filters[i];",
            "\t\tif (n->type == wt->type &&",
            "\t\t    (wt->subtype_filter[st_index] & st_bit) &&",
            "\t\t    (n->info & wt->info_mask) == wt->info_filter)",
            "\t\t\treturn true;",
            "\t}",
            "",
            "\treturn false; /* If there is a filter, the default is to reject. */",
            "}"
          ],
          "function_name": "lock_wqueue, unlock_wqueue, watch_queue_pipe_buf_release, post_one_notification, filter_watch_notification",
          "description": "实现了watch_queue的锁操作、缓冲区释放、通知提交及过滤逻辑。lock_wqueue/unlock_wqueue用于保护队列访问，watch_queue_pipe_buf_release处理缓冲区回收并更新位图，post_one_notification将通知数据写入管道，filter_watch_notification进行类型和子类型的匹配判断。",
          "similarity": 0.5902833342552185
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/watch_queue.c",
          "start_line": 193,
          "end_line": 304,
          "content": [
            "void __post_watch_notification(struct watch_list *wlist,",
            "\t\t\t       struct watch_notification *n,",
            "\t\t\t       const struct cred *cred,",
            "\t\t\t       u64 id)",
            "{",
            "\tconst struct watch_filter *wf;",
            "\tstruct watch_queue *wqueue;",
            "\tstruct watch *watch;",
            "",
            "\tif (((n->info & WATCH_INFO_LENGTH) >> WATCH_INFO_LENGTH__SHIFT) == 0) {",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\trcu_read_lock();",
            "",
            "\thlist_for_each_entry_rcu(watch, &wlist->watchers, list_node) {",
            "\t\tif (watch->id != id)",
            "\t\t\tcontinue;",
            "\t\tn->info &= ~WATCH_INFO_ID;",
            "\t\tn->info |= watch->info_id;",
            "",
            "\t\twqueue = rcu_dereference(watch->queue);",
            "\t\twf = rcu_dereference(wqueue->filter);",
            "\t\tif (wf && !filter_watch_notification(wf, n))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (security_post_notification(watch->cred, cred, n) < 0)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (lock_wqueue(wqueue)) {",
            "\t\t\tpost_one_notification(wqueue, n);",
            "\t\t\tunlock_wqueue(wqueue);",
            "\t\t}",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "}",
            "long watch_queue_set_size(struct pipe_inode_info *pipe, unsigned int nr_notes)",
            "{",
            "\tstruct watch_queue *wqueue = pipe->watch_queue;",
            "\tstruct page **pages;",
            "\tunsigned long *bitmap;",
            "\tunsigned long user_bufs;",
            "\tint ret, i, nr_pages;",
            "",
            "\tif (!wqueue)",
            "\t\treturn -ENODEV;",
            "\tif (wqueue->notes)",
            "\t\treturn -EBUSY;",
            "",
            "\tif (nr_notes < 1 ||",
            "\t    nr_notes > 512) /* TODO: choose a better hard limit */",
            "\t\treturn -EINVAL;",
            "",
            "\tnr_pages = (nr_notes + WATCH_QUEUE_NOTES_PER_PAGE - 1);",
            "\tnr_pages /= WATCH_QUEUE_NOTES_PER_PAGE;",
            "\tuser_bufs = account_pipe_buffers(pipe->user, pipe->nr_accounted, nr_pages);",
            "",
            "\tif (nr_pages > pipe->max_usage &&",
            "\t    (too_many_pipe_buffers_hard(user_bufs) ||",
            "\t     too_many_pipe_buffers_soft(user_bufs)) &&",
            "\t    pipe_is_unprivileged_user()) {",
            "\t\tret = -EPERM;",
            "\t\tgoto error;",
            "\t}",
            "",
            "\tnr_notes = nr_pages * WATCH_QUEUE_NOTES_PER_PAGE;",
            "\tret = pipe_resize_ring(pipe, roundup_pow_of_two(nr_notes));",
            "\tif (ret < 0)",
            "\t\tgoto error;",
            "",
            "\t/*",
            "\t * pipe_resize_ring() does not update nr_accounted for watch_queue",
            "\t * pipes, because the above vastly overprovisions. Set nr_accounted on",
            "\t * and max_usage this pipe to the number that was actually charged to",
            "\t * the user above via account_pipe_buffers.",
            "\t */",
            "\tpipe->max_usage = nr_pages;",
            "\tpipe->nr_accounted = nr_pages;",
            "",
            "\tret = -ENOMEM;",
            "\tpages = kcalloc(sizeof(struct page *), nr_pages, GFP_KERNEL);",
            "\tif (!pages)",
            "\t\tgoto error;",
            "",
            "\tfor (i = 0; i < nr_pages; i++) {",
            "\t\tpages[i] = alloc_page(GFP_KERNEL);",
            "\t\tif (!pages[i])",
            "\t\t\tgoto error_p;",
            "\t\tpages[i]->index = i * WATCH_QUEUE_NOTES_PER_PAGE;",
            "\t}",
            "",
            "\tbitmap = bitmap_alloc(nr_notes, GFP_KERNEL);",
            "\tif (!bitmap)",
            "\t\tgoto error_p;",
            "",
            "\tbitmap_fill(bitmap, nr_notes);",
            "\twqueue->notes = pages;",
            "\twqueue->notes_bitmap = bitmap;",
            "\twqueue->nr_pages = nr_pages;",
            "\twqueue->nr_notes = nr_notes;",
            "\treturn 0;",
            "",
            "error_p:",
            "\twhile (--i >= 0)",
            "\t\t__free_page(pages[i]);",
            "\tkfree(pages);",
            "error:",
            "\t(void) account_pipe_buffers(pipe->user, nr_pages, pipe->nr_accounted);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__post_watch_notification, watch_queue_set_size",
          "description": "__post_watch_notification遍历watch列表并应用过滤器后提交通知，watch_queue_set_size动态调整管道容量，通过计算所需页数和位图分配，限制最大容量为512个笔记，支持扩展性需求。",
          "similarity": 0.49850988388061523
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/watch_queue.c",
          "start_line": 602,
          "end_line": 680,
          "content": [
            "void watch_queue_clear(struct watch_queue *wqueue)",
            "{",
            "\tstruct watch_list *wlist;",
            "\tstruct watch *watch;",
            "\tbool release;",
            "",
            "\trcu_read_lock();",
            "\tspin_lock_bh(&wqueue->lock);",
            "",
            "\t/*",
            "\t * This pipe can be freed by callers like free_pipe_info().",
            "\t * Removing this reference also prevents new notifications.",
            "\t */",
            "\twqueue->pipe = NULL;",
            "",
            "\twhile (!hlist_empty(&wqueue->watches)) {",
            "\t\twatch = hlist_entry(wqueue->watches.first, struct watch, queue_node);",
            "\t\thlist_del_init_rcu(&watch->queue_node);",
            "\t\t/* We now own a ref on the watch. */",
            "\t\tspin_unlock_bh(&wqueue->lock);",
            "",
            "\t\t/* We can't do the next bit under the queue lock as we need to",
            "\t\t * get the list lock - which would cause a deadlock if someone",
            "\t\t * was removing from the opposite direction at the same time or",
            "\t\t * posting a notification.",
            "\t\t */",
            "\t\twlist = rcu_dereference(watch->watch_list);",
            "\t\tif (wlist) {",
            "\t\t\tvoid (*release_watch)(struct watch *);",
            "",
            "\t\t\tspin_lock(&wlist->lock);",
            "",
            "\t\t\trelease = !hlist_unhashed(&watch->list_node);",
            "\t\t\tif (release) {",
            "\t\t\t\thlist_del_init_rcu(&watch->list_node);",
            "\t\t\t\trcu_assign_pointer(watch->watch_list, NULL);",
            "",
            "\t\t\t\t/* We now own a second ref on the watch. */",
            "\t\t\t}",
            "",
            "\t\t\trelease_watch = wlist->release_watch;",
            "\t\t\tspin_unlock(&wlist->lock);",
            "",
            "\t\t\tif (release) {",
            "\t\t\t\tif (release_watch) {",
            "\t\t\t\t\trcu_read_unlock();",
            "\t\t\t\t\t/* This might need to call dput(), so",
            "\t\t\t\t\t * we have to drop all the locks.",
            "\t\t\t\t\t */",
            "\t\t\t\t\t(*release_watch)(watch);",
            "\t\t\t\t\trcu_read_lock();",
            "\t\t\t\t}",
            "\t\t\t\tput_watch(watch);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tput_watch(watch);",
            "\t\tspin_lock_bh(&wqueue->lock);",
            "\t}",
            "",
            "\tspin_unlock_bh(&wqueue->lock);",
            "\trcu_read_unlock();",
            "}",
            "int watch_queue_init(struct pipe_inode_info *pipe)",
            "{",
            "\tstruct watch_queue *wqueue;",
            "",
            "\twqueue = kzalloc(sizeof(*wqueue), GFP_KERNEL);",
            "\tif (!wqueue)",
            "\t\treturn -ENOMEM;",
            "",
            "\twqueue->pipe = pipe;",
            "\tkref_init(&wqueue->usage);",
            "\tspin_lock_init(&wqueue->lock);",
            "\tINIT_HLIST_HEAD(&wqueue->watches);",
            "",
            "\tpipe->watch_queue = wqueue;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "watch_queue_clear, watch_queue_init",
          "description": "该代码实现了监视队列的初始化与清理功能。  \n`watch_queue_clear`通过RCU和自旋锁机制安全地移除所有监视项并释放资源，`watch_queue_init`初始化监视队列结构并绑定至管道对象。  \n上下文不完整：`release_watch`等关键函数依赖外部定义，部分RCU回调逻辑未完全展示。",
          "similarity": 0.4843918979167938
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/watch_queue.c",
          "start_line": 315,
          "end_line": 422,
          "content": [
            "long watch_queue_set_filter(struct pipe_inode_info *pipe,",
            "\t\t\t    struct watch_notification_filter __user *_filter)",
            "{",
            "\tstruct watch_notification_type_filter *tf;",
            "\tstruct watch_notification_filter filter;",
            "\tstruct watch_type_filter *q;",
            "\tstruct watch_filter *wfilter;",
            "\tstruct watch_queue *wqueue = pipe->watch_queue;",
            "\tint ret, nr_filter = 0, i;",
            "",
            "\tif (!wqueue)",
            "\t\treturn -ENODEV;",
            "",
            "\tif (!_filter) {",
            "\t\t/* Remove the old filter */",
            "\t\twfilter = NULL;",
            "\t\tgoto set;",
            "\t}",
            "",
            "\t/* Grab the user's filter specification */",
            "\tif (copy_from_user(&filter, _filter, sizeof(filter)) != 0)",
            "\t\treturn -EFAULT;",
            "\tif (filter.nr_filters == 0 ||",
            "\t    filter.nr_filters > 16 ||",
            "\t    filter.__reserved != 0)",
            "\t\treturn -EINVAL;",
            "",
            "\ttf = memdup_array_user(_filter->filters, filter.nr_filters, sizeof(*tf));",
            "\tif (IS_ERR(tf))",
            "\t\treturn PTR_ERR(tf);",
            "",
            "\tret = -EINVAL;",
            "\tfor (i = 0; i < filter.nr_filters; i++) {",
            "\t\tif ((tf[i].info_filter & ~tf[i].info_mask) ||",
            "\t\t    tf[i].info_mask & WATCH_INFO_LENGTH)",
            "\t\t\tgoto err_filter;",
            "\t\t/* Ignore any unknown types */",
            "\t\tif (tf[i].type >= WATCH_TYPE__NR)",
            "\t\t\tcontinue;",
            "\t\tnr_filter++;",
            "\t}",
            "",
            "\t/* Now we need to build the internal filter from only the relevant",
            "\t * user-specified filters.",
            "\t */",
            "\tret = -ENOMEM;",
            "\twfilter = kzalloc(struct_size(wfilter, filters, nr_filter), GFP_KERNEL);",
            "\tif (!wfilter)",
            "\t\tgoto err_filter;",
            "\twfilter->nr_filters = nr_filter;",
            "",
            "\tq = wfilter->filters;",
            "\tfor (i = 0; i < filter.nr_filters; i++) {",
            "\t\tif (tf[i].type >= WATCH_TYPE__NR)",
            "\t\t\tcontinue;",
            "",
            "\t\tq->type\t\t\t= tf[i].type;",
            "\t\tq->info_filter\t\t= tf[i].info_filter;",
            "\t\tq->info_mask\t\t= tf[i].info_mask;",
            "\t\tq->subtype_filter[0]\t= tf[i].subtype_filter[0];",
            "\t\t__set_bit(q->type, wfilter->type_filter);",
            "\t\tq++;",
            "\t}",
            "",
            "\tkfree(tf);",
            "set:",
            "\tpipe_lock(pipe);",
            "\twfilter = rcu_replace_pointer(wqueue->filter, wfilter,",
            "\t\t\t\t      lockdep_is_held(&pipe->mutex));",
            "\tpipe_unlock(pipe);",
            "\tif (wfilter)",
            "\t\tkfree_rcu(wfilter, rcu);",
            "\treturn 0;",
            "",
            "err_filter:",
            "\tkfree(tf);",
            "\treturn ret;",
            "}",
            "static void __put_watch_queue(struct kref *kref)",
            "{",
            "\tstruct watch_queue *wqueue =",
            "\t\tcontainer_of(kref, struct watch_queue, usage);",
            "\tstruct watch_filter *wfilter;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < wqueue->nr_pages; i++)",
            "\t\t__free_page(wqueue->notes[i]);",
            "\tkfree(wqueue->notes);",
            "\tbitmap_free(wqueue->notes_bitmap);",
            "",
            "\twfilter = rcu_access_pointer(wqueue->filter);",
            "\tif (wfilter)",
            "\t\tkfree_rcu(wfilter, rcu);",
            "\tkfree_rcu(wqueue, rcu);",
            "}",
            "void put_watch_queue(struct watch_queue *wqueue)",
            "{",
            "\tkref_put(&wqueue->usage, __put_watch_queue);",
            "}",
            "static void free_watch(struct rcu_head *rcu)",
            "{",
            "\tstruct watch *watch = container_of(rcu, struct watch, rcu);",
            "",
            "\tput_watch_queue(rcu_access_pointer(watch->queue));",
            "\tatomic_dec(&watch->cred->user->nr_watches);",
            "\tput_cred(watch->cred);",
            "\tkfree(watch);",
            "}"
          ],
          "function_name": "watch_queue_set_filter, __put_watch_queue, put_watch_queue, free_watch",
          "description": "watch_queue_set_filter设置过滤规则并转换为内核内部结构，__put_watch_queue释放watch_queue相关资源包括页面、位图和过滤器，put_watch_queue通过引用计数管理watch_queue生命周期，free_watch执行RCU回调完成最终释放。",
          "similarity": 0.4784236550331116
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/watch_queue.c",
          "start_line": 1,
          "end_line": 41,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/* Watch queue and general notification mechanism, built on pipes",
            " *",
            " * Copyright (C) 2020 Red Hat, Inc. All Rights Reserved.",
            " * Written by David Howells (dhowells@redhat.com)",
            " *",
            " * See Documentation/core-api/watch_queue.rst",
            " */",
            "",
            "#define pr_fmt(fmt) \"watchq: \" fmt",
            "#include <linux/module.h>",
            "#include <linux/init.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/printk.h>",
            "#include <linux/miscdevice.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/poll.h>",
            "#include <linux/uaccess.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/file.h>",
            "#include <linux/security.h>",
            "#include <linux/cred.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/watch_queue.h>",
            "#include <linux/pipe_fs_i.h>",
            "",
            "MODULE_DESCRIPTION(\"Watch queue\");",
            "MODULE_AUTHOR(\"Red Hat, Inc.\");",
            "",
            "#define WATCH_QUEUE_NOTE_SIZE 128",
            "#define WATCH_QUEUE_NOTES_PER_PAGE (PAGE_SIZE / WATCH_QUEUE_NOTE_SIZE)",
            "",
            "/*",
            " * This must be called under the RCU read-lock, which makes",
            " * sure that the wqueue still exists. It can then take the lock,",
            " * and check that the wqueue hasn't been destroyed, which in",
            " * turn makes sure that the notification pipe still exists.",
            " */"
          ],
          "function_name": null,
          "description": "定义了watch_queue模块的头部信息，包含常量WATCH_QUEUE_NOTE_SIZE和NOTES_PER_PAGE，声明模块许可证及作者信息，并引入相关内核头文件，为后续实现提供基础框架。",
          "similarity": 0.470470666885376
        }
      ]
    }
  ]
}