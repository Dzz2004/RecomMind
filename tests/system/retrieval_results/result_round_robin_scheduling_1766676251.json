{
  "query": "round robin scheduling",
  "timestamp": "2025-12-25 23:24:11",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.48071300983428955,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/rt.c",
          "start_line": 776,
          "end_line": 913,
          "content": [
            "static void balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tif (!sched_feat(RT_RUNTIME_SHARE))",
            "\t\treturn;",
            "",
            "\tif (rt_rq->rt_time > rt_rq->rt_runtime) {",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tdo_balance_runtime(rt_rq);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t}",
            "}",
            "static inline void balance_runtime(struct rt_rq *rt_rq) {}",
            "static int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun)",
            "{",
            "\tint i, idle = 1, throttled = 0;",
            "\tconst struct cpumask *span;",
            "",
            "\tspan = sched_rt_period_mask();",
            "",
            "\t/*",
            "\t * FIXME: isolated CPUs should really leave the root task group,",
            "\t * whether they are isolcpus or were isolated via cpusets, lest",
            "\t * the timer run on a CPU which does not service all runqueues,",
            "\t * potentially leaving other CPUs indefinitely throttled.  If",
            "\t * isolation is really required, the user will turn the throttle",
            "\t * off to kill the perturbations it causes anyway.  Meanwhile,",
            "\t * this maintains functionality for boot and/or troubleshooting.",
            "\t */",
            "\tif (rt_b == &root_task_group.rt_bandwidth)",
            "\t\tspan = cpu_online_mask;",
            "",
            "\tfor_each_cpu(i, span) {",
            "\t\tint enqueue = 0;",
            "\t\tstruct rt_rq *rt_rq = sched_rt_period_rt_rq(rt_b, i);",
            "\t\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\t\tstruct rq_flags rf;",
            "\t\tint skip;",
            "",
            "\t\t/*",
            "\t\t * When span == cpu_online_mask, taking each rq->lock",
            "\t\t * can be time-consuming. Try to avoid it when possible.",
            "\t\t */",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\tif (!sched_feat(RT_RUNTIME_SHARE) && rt_rq->rt_runtime != RUNTIME_INF)",
            "\t\t\trt_rq->rt_runtime = rt_b->rt_runtime;",
            "\t\tskip = !rt_rq->rt_time && !rt_rq->rt_nr_running;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tif (skip)",
            "\t\t\tcontinue;",
            "",
            "\t\trq_lock(rq, &rf);",
            "\t\tupdate_rq_clock(rq);",
            "",
            "\t\tif (rt_rq->rt_time) {",
            "\t\t\tu64 runtime;",
            "",
            "\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t\tif (rt_rq->rt_throttled)",
            "\t\t\t\tbalance_runtime(rt_rq);",
            "\t\t\truntime = rt_rq->rt_runtime;",
            "\t\t\trt_rq->rt_time -= min(rt_rq->rt_time, overrun*runtime);",
            "\t\t\tif (rt_rq->rt_throttled && rt_rq->rt_time < runtime) {",
            "\t\t\t\trt_rq->rt_throttled = 0;",
            "\t\t\t\tenqueue = 1;",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * When we're idle and a woken (rt) task is",
            "\t\t\t\t * throttled wakeup_preempt() will set",
            "\t\t\t\t * skip_update and the time between the wakeup",
            "\t\t\t\t * and this unthrottle will get accounted as",
            "\t\t\t\t * 'runtime'.",
            "\t\t\t\t */",
            "\t\t\t\tif (rt_rq->rt_nr_running && rq->curr == rq->idle)",
            "\t\t\t\t\trq_clock_cancel_skipupdate(rq);",
            "\t\t\t}",
            "\t\t\tif (rt_rq->rt_time || rt_rq->rt_nr_running)",
            "\t\t\t\tidle = 0;",
            "\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\t} else if (rt_rq->rt_nr_running) {",
            "\t\t\tidle = 0;",
            "\t\t\tif (!rt_rq_throttled(rt_rq))",
            "\t\t\t\tenqueue = 1;",
            "\t\t}",
            "\t\tif (rt_rq->rt_throttled)",
            "\t\t\tthrottled = 1;",
            "",
            "\t\tif (enqueue)",
            "\t\t\tsched_rt_rq_enqueue(rt_rq);",
            "\t\trq_unlock(rq, &rf);",
            "\t}",
            "",
            "\tif (!throttled && (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF))",
            "\t\treturn 1;",
            "",
            "\treturn idle;",
            "}",
            "static int sched_rt_runtime_exceeded(struct rt_rq *rt_rq)",
            "{",
            "\tu64 runtime = sched_rt_runtime(rt_rq);",
            "",
            "\tif (rt_rq->rt_throttled)",
            "\t\treturn rt_rq_throttled(rt_rq);",
            "",
            "\tif (runtime >= sched_rt_period(rt_rq))",
            "\t\treturn 0;",
            "",
            "\tbalance_runtime(rt_rq);",
            "\truntime = sched_rt_runtime(rt_rq);",
            "\tif (runtime == RUNTIME_INF)",
            "\t\treturn 0;",
            "",
            "\tif (rt_rq->rt_time > runtime) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\t\t/*",
            "\t\t * Don't actually throttle groups that have no runtime assigned",
            "\t\t * but accrue some time due to boosting.",
            "\t\t */",
            "\t\tif (likely(rt_b->rt_runtime)) {",
            "\t\t\trt_rq->rt_throttled = 1;",
            "\t\t\tprintk_deferred_once(\"sched: RT throttling activated\\n\");",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * In case we did anyway, make it go away,",
            "\t\t\t * replenishment is a joke, since it will replenish us",
            "\t\t\t * with exactly 0 ns.",
            "\t\t\t */",
            "\t\t\trt_rq->rt_time = 0;",
            "\t\t}",
            "",
            "\t\tif (rt_rq_throttled(rt_rq)) {",
            "\t\t\tsched_rt_rq_dequeue(rt_rq);",
            "\t\t\treturn 1;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "balance_runtime, balance_runtime, do_sched_rt_period_timer, sched_rt_runtime_exceeded",
          "description": "`balance_runtime`在超时时触发重新平衡，`do_sched_rt_period_timer`周期性调整运行时并检查节流状态，`sched_rt_runtime_exceeded`判断是否超出运行时限制并标记节流",
          "similarity": 0.5123656988143921
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/sched/rt.c",
          "start_line": 670,
          "end_line": 773,
          "content": [
            "static void __disable_runtime(struct rq *rq)",
            "{",
            "\tstruct root_domain *rd = rq->rd;",
            "\trt_rq_iter_t iter;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tif (unlikely(!scheduler_running))",
            "\t\treturn;",
            "",
            "\tfor_each_rt_rq(rt_rq, iter, rq) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\t\ts64 want;",
            "\t\tint i;",
            "",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either we're all inf and nobody needs to borrow, or we're",
            "\t\t * already disabled and thus have nothing to do, or we have",
            "\t\t * exactly the right amount of runtime to take out.",
            "\t\t */",
            "\t\tif (rt_rq->rt_runtime == RUNTIME_INF ||",
            "\t\t\t\trt_rq->rt_runtime == rt_b->rt_runtime)",
            "\t\t\tgoto balanced;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "",
            "\t\t/*",
            "\t\t * Calculate the difference between what we started out with",
            "\t\t * and what we current have, that's the amount of runtime",
            "\t\t * we lend and now have to reclaim.",
            "\t\t */",
            "\t\twant = rt_b->rt_runtime - rt_rq->rt_runtime;",
            "",
            "\t\t/*",
            "\t\t * Greedy reclaim, take back as much as we can.",
            "\t\t */",
            "\t\tfor_each_cpu(i, rd->span) {",
            "\t\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\t\ts64 diff;",
            "",
            "\t\t\t/*",
            "\t\t\t * Can't reclaim from ourselves or disabled runqueues.",
            "\t\t\t */",
            "\t\t\tif (iter == rt_rq || iter->rt_runtime == RUNTIME_INF)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t\tif (want > 0) {",
            "\t\t\t\tdiff = min_t(s64, iter->rt_runtime, want);",
            "\t\t\t\titer->rt_runtime -= diff;",
            "\t\t\t\twant -= diff;",
            "\t\t\t} else {",
            "\t\t\t\titer->rt_runtime -= want;",
            "\t\t\t\twant -= want;",
            "\t\t\t}",
            "\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "",
            "\t\t\tif (!want)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * We cannot be left wanting - that would mean some runtime",
            "\t\t * leaked out of the system.",
            "\t\t */",
            "\t\tWARN_ON_ONCE(want);",
            "balanced:",
            "\t\t/*",
            "\t\t * Disable all the borrow logic by pretending we have inf",
            "\t\t * runtime - in which case borrowing doesn't make sense.",
            "\t\t */",
            "\t\trt_rq->rt_runtime = RUNTIME_INF;",
            "\t\trt_rq->rt_throttled = 0;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "",
            "\t\t/* Make rt_rq available for pick_next_task() */",
            "\t\tsched_rt_rq_enqueue(rt_rq);",
            "\t}",
            "}",
            "static void __enable_runtime(struct rq *rq)",
            "{",
            "\trt_rq_iter_t iter;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tif (unlikely(!scheduler_running))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Reset each runqueue's bandwidth settings",
            "\t */",
            "\tfor_each_rt_rq(rt_rq, iter, rq) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\trt_rq->rt_runtime = rt_b->rt_runtime;",
            "\t\trt_rq->rt_time = 0;",
            "\t\trt_rq->rt_throttled = 0;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "\t}",
            "}"
          ],
          "function_name": "__disable_runtime, __enable_runtime",
          "description": "`__disable_runtime`遍历实时运行队列，将每个队列的运行时设为无穷大以禁用借用逻辑，`__enable_runtime`重置各队列的运行时限制",
          "similarity": 0.5116748809814453
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/rt.c",
          "start_line": 57,
          "end_line": 159,
          "content": [
            "static int __init sched_rt_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", sched_rt_sysctls);",
            "\treturn 0;",
            "}",
            "void init_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_prio_array *array;",
            "\tint i;",
            "",
            "\tarray = &rt_rq->active;",
            "\tfor (i = 0; i < MAX_RT_PRIO; i++) {",
            "\t\tINIT_LIST_HEAD(array->queue + i);",
            "\t\t__clear_bit(i, array->bitmap);",
            "\t}",
            "\t/* delimiter for bitsearch: */",
            "\t__set_bit(MAX_RT_PRIO, array->bitmap);",
            "",
            "#if defined CONFIG_SMP",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->highest_prio.next = MAX_RT_PRIO-1;",
            "\trt_rq->overloaded = 0;",
            "\tplist_head_init(&rt_rq->pushable_tasks);",
            "#endif /* CONFIG_SMP */",
            "\t/* We start is dequeued state, because no RT tasks are queued */",
            "\trt_rq->rt_queued = 0;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq->rt_time = 0;",
            "\trt_rq->rt_throttled = 0;",
            "\trt_rq->rt_runtime = 0;",
            "\traw_spin_lock_init(&rt_rq->rt_runtime_lock);",
            "#endif",
            "}",
            "static enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)",
            "{",
            "\tstruct rt_bandwidth *rt_b =",
            "\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);",
            "\tint idle = 0;",
            "\tint overrun;",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tfor (;;) {",
            "\t\toverrun = hrtimer_forward_now(timer, rt_b->rt_period);",
            "\t\tif (!overrun)",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "\t\tidle = do_sched_rt_period_timer(rt_b, overrun);",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t}",
            "\tif (idle)",
            "\t\trt_b->rt_period_active = 0;",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "",
            "\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;",
            "}",
            "void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)",
            "{",
            "\trt_b->rt_period = ns_to_ktime(period);",
            "\trt_b->rt_runtime = runtime;",
            "",
            "\traw_spin_lock_init(&rt_b->rt_runtime_lock);",
            "",
            "\thrtimer_init(&rt_b->rt_period_timer, CLOCK_MONOTONIC,",
            "\t\t     HRTIMER_MODE_REL_HARD);",
            "\trt_b->rt_period_timer.function = sched_rt_period_timer;",
            "}",
            "static inline void do_start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tif (!rt_b->rt_period_active) {",
            "\t\trt_b->rt_period_active = 1;",
            "\t\t/*",
            "\t\t * SCHED_DEADLINE updates the bandwidth, as a run away",
            "\t\t * RT task with a DL task could hog a CPU. But DL does",
            "\t\t * not reset the period. If a deadline task was running",
            "\t\t * without an RT task running, it can cause RT tasks to",
            "\t\t * throttle when they start up. Kick the timer right away",
            "\t\t * to update the period.",
            "\t\t */",
            "\t\thrtimer_forward_now(&rt_b->rt_period_timer, ns_to_ktime(0));",
            "\t\thrtimer_start_expires(&rt_b->rt_period_timer,",
            "\t\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}",
            "static void start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)",
            "\t\treturn;",
            "",
            "\tdo_start_rt_bandwidth(rt_b);",
            "}",
            "static void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\thrtimer_cancel(&rt_b->rt_period_timer);",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg)",
            "{",
            "\tif (tg->rt_se)",
            "\t\tdestroy_rt_bandwidth(&tg->rt_bandwidth);",
            "}"
          ],
          "function_name": "sched_rt_sysctl_init, init_rt_rq, sched_rt_period_timer, init_rt_bandwidth, do_start_rt_bandwidth, start_rt_bandwidth, destroy_rt_bandwidth, unregister_rt_sched_group",
          "description": "初始化实时调度相关数据结构，管理实时任务周期定时器，控制实时带宽分配与回收，实现基于时间片轮转的调度策略。",
          "similarity": 0.5000835657119751
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1066,
          "end_line": 1170,
          "content": [
            "static void",
            "inc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\t/*",
            "\t * Change rq's cpupri only if rt_rq is the top queue.",
            "\t */",
            "\tif (&rq->rt != rt_rq)",
            "\t\treturn;",
            "#endif",
            "\tif (rq->online && prio < prev_prio)",
            "\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, prio);",
            "}",
            "static void",
            "dec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio)",
            "{",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\t/*",
            "\t * Change rq's cpupri only if rt_rq is the top queue.",
            "\t */",
            "\tif (&rq->rt != rt_rq)",
            "\t\treturn;",
            "#endif",
            "\tif (rq->online && rt_rq->highest_prio.curr != prev_prio)",
            "\t\tcpupri_set(&rq->rd->cpupri, rq->cpu, rt_rq->highest_prio.curr);",
            "}",
            "static inline",
            "void inc_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}",
            "static inline",
            "void dec_rt_prio_smp(struct rt_rq *rt_rq, int prio, int prev_prio) {}",
            "static void",
            "inc_rt_prio(struct rt_rq *rt_rq, int prio)",
            "{",
            "\tint prev_prio = rt_rq->highest_prio.curr;",
            "",
            "\tif (prio < prev_prio)",
            "\t\trt_rq->highest_prio.curr = prio;",
            "",
            "\tinc_rt_prio_smp(rt_rq, prio, prev_prio);",
            "}",
            "static void",
            "dec_rt_prio(struct rt_rq *rt_rq, int prio)",
            "{",
            "\tint prev_prio = rt_rq->highest_prio.curr;",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "",
            "\t\tWARN_ON(prio < prev_prio);",
            "",
            "\t\t/*",
            "\t\t * This may have been our highest task, and therefore",
            "\t\t * we may have some recomputation to do",
            "\t\t */",
            "\t\tif (prio == prev_prio) {",
            "\t\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "",
            "\t\t\trt_rq->highest_prio.curr =",
            "\t\t\t\tsched_find_first_bit(array->bitmap);",
            "\t\t}",
            "",
            "\t} else {",
            "\t\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\t}",
            "",
            "\tdec_rt_prio_smp(rt_rq, prio, prev_prio);",
            "}",
            "static inline void inc_rt_prio(struct rt_rq *rt_rq, int prio) {}",
            "static inline void dec_rt_prio(struct rt_rq *rt_rq, int prio) {}",
            "static void",
            "inc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)",
            "{",
            "\tif (rt_se_boosted(rt_se))",
            "\t\trt_rq->rt_nr_boosted++;",
            "",
            "\tif (rt_rq->tg)",
            "\t\tstart_rt_bandwidth(&rt_rq->tg->rt_bandwidth);",
            "}",
            "static void",
            "dec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)",
            "{",
            "\tif (rt_se_boosted(rt_se))",
            "\t\trt_rq->rt_nr_boosted--;",
            "",
            "\tWARN_ON(!rt_rq->rt_nr_running && rt_rq->rt_nr_boosted);",
            "}",
            "static void",
            "inc_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq)",
            "{",
            "}",
            "static inline",
            "void dec_rt_group(struct sched_rt_entity *rt_se, struct rt_rq *rt_rq) {}",
            "static inline",
            "unsigned int rt_se_nr_running(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *group_rq = group_rt_rq(rt_se);",
            "",
            "\tif (group_rq)",
            "\t\treturn group_rq->rt_nr_running;",
            "\telse",
            "\t\treturn 1;",
            "}"
          ],
          "function_name": "inc_rt_prio_smp, dec_rt_prio_smp, inc_rt_prio_smp, dec_rt_prio_smp, inc_rt_prio, dec_rt_prio, inc_rt_prio, dec_rt_prio, inc_rt_group, dec_rt_group, inc_rt_group, dec_rt_group, rt_se_nr_running",
          "description": "`inc_rt_prio/dec_rt_prio`维护实时队列最高优先级，`inc_rt_group/dec_rt_group`处理调度组的资源计数，`rt_se_nr_running`查询任务所属组的运行数量",
          "similarity": 0.49414247274398804
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.49197566509246826
        }
      ]
    },
    {
      "source_file": "kernel/sched/deadline.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:06:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\deadline.c`\n\n---\n\n# `sched/deadline.c` 技术文档\n\n## 1. 文件概述\n\n`sched/deadline.c` 是 Linux 内核调度器中 **SCHED_DEADLINE** 调度类的核心实现文件。该调度类基于 **最早截止时间优先（Earliest Deadline First, EDF）** 算法，并结合 **恒定带宽服务器（Constant Bandwidth Server, CBS）** 机制，为具有严格实时性要求的任务提供可预测的调度保障。\n\n其核心目标是：  \n- 对于周期性任务，若其实际运行时间不超过所申请的运行时间（runtime），则保证不会错过任何截止时间（deadline）；  \n- 对于非周期性任务、突发任务或试图超出其预留带宽的任务，系统会对其进行节流（throttling），防止其影响其他任务的实时性保障。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct sched_dl_entity`：表示一个 deadline 调度实体，包含任务的运行时间（runtime）、截止期限（deadline）、周期（period）、带宽（dl_bw）等关键参数。\n- `struct dl_rq`：每个 CPU 的 deadline 运行队列，维护该 CPU 上所有 deadline 任务的红黑树、当前带宽使用情况（`this_bw`、`running_bw`）等。\n- `struct dl_bw`：deadline 带宽管理结构，用于跟踪系统或调度域中已分配的总带宽（`total_bw`）。\n\n### 主要函数与辅助宏\n\n#### 调度实体与运行队列关联\n- `dl_task_of(dl_se)`：从 `sched_dl_entity` 获取对应的 `task_struct`（仅适用于普通任务，不适用于服务器实体）。\n- `rq_of_dl_rq(dl_rq)` / `rq_of_dl_se(dl_se)`：获取与 deadline 运行队列或调度实体关联的 `rq`（runqueue）。\n- `dl_rq_of_se(dl_se)`：获取调度实体所属的 `dl_rq`。\n- `on_dl_rq(dl_se)`：判断调度实体是否已在 deadline 运行队列中（通过红黑树节点是否为空判断）。\n\n#### 优先级继承（PI）支持（`CONFIG_RT_MUTEXES`）\n- `pi_of(dl_se)`：获取当前调度实体因优先级继承而提升后的“代理”实体。\n- `is_dl_boosted(dl_se)`：判断该 deadline 实体是否因优先级继承被提升。\n\n#### 带宽管理（SMP 与 UP 差异处理）\n- `dl_bw_of(cpu)`：获取指定 CPU 所属调度域（或本地）的 `dl_bw` 结构。\n- `dl_bw_cpus(cpu)`：返回该 CPU 所在调度域中活跃 CPU 的数量。\n- `dl_bw_capacity(cpu)`：计算调度域的总 CPU 容量（考虑异构 CPU 的 `arch_scale_cpu_capacity`）。\n- `__dl_add()` / `__dl_sub()`：向带宽池中添加或移除任务带宽，并更新 `extra_bw`（用于负载均衡）。\n- `__dl_overflow()`：检查新增带宽是否超出系统/调度域的可用带宽上限。\n\n#### 运行时带宽跟踪\n- `__add_running_bw()` / `__sub_running_bw()`：更新 `dl_rq->running_bw`（当前正在运行的 deadline 任务所消耗的带宽）。\n- `__add_rq_bw()` / `__sub_rq_bw()`：更新 `dl_rq->this_bw`（该运行队列上所有 deadline 任务的总预留带宽）。\n- `add_running_bw()` / `sub_running_bw()` / `add_rq_bw()` / `sub_rq_bw()`：带宽操作的封装，跳过“特殊”调度实体（如服务器）。\n\n#### 其他\n- `dl_server(dl_se)`：判断调度实体是否为 CBS 服务器（而非普通任务）。\n- `dl_bw_visited(cpu, gen)`：用于带宽遍历去重（SMP 场景）。\n\n### 系统控制接口（`CONFIG_SYSCTL`）\n- `sched_deadline_period_max_us`：deadline 任务周期上限（默认 ~4 秒）。\n- `sched_deadline_period_min_us`：deadline 任务周期下限（默认 100 微秒），防止定时器 DoS。\n\n## 3. 关键实现\n\n### EDF + CBS 调度模型\n- 每个 deadline 任务通过 `runtime`、`deadline`、`period` 三个参数定义其资源需求。\n- 调度器按 **绝对截止时间（absolute deadline）** 对任务排序，使用红黑树实现 O(log n) 的调度决策。\n- CBS 机制确保任务即使突发执行，也不会长期占用超过其 `runtime/period` 的 CPU 带宽，超限任务会被 throttled。\n\n### 带宽隔离与全局限制\n- 在 SMP 系统中，deadline 带宽按 **调度域（root domain）** 进行管理，防止跨 CPU 的带宽滥用。\n- 总带宽限制默认为 CPU 总容量的 95%（由 `sysctl_sched_util_clamp_min` 等机制间接控制，具体限制逻辑在带宽分配函数中体现）。\n- `dl_bw->total_bw` 跟踪已分配带宽，`__dl_overflow()` 用于在任务加入时检查是否超限。\n\n### 异构 CPU 支持\n- 通过 `arch_scale_cpu_capacity()` 获取每个 CPU 的相对性能权重。\n- `dl_bw_capacity()` 在异构系统中返回调度域内所有活跃 CPU 的容量总和，用于带宽比例计算（`cap_scale()`）。\n\n### 与 cpufreq 集成\n- 每次 `running_bw` 变化时调用 `cpufreq_update_util()`，通知 CPU 频率调节器当前 deadline 负载，确保满足实时性能需求。\n\n### 优先级继承（PI）\n- 当 deadline 任务因持有 mutex 而阻塞高优先级任务时，通过 `pi_se` 字段临时提升其调度参数，避免优先级反转。\n\n## 4. 依赖关系\n\n- **核心调度框架**：依赖 `kernel/sched/sched.h` 中定义的通用调度结构（如 `rq`、`task_struct`）和宏（如 `SCHED_CAPACITY_SCALE`）。\n- **CPU 拓扑与容量**：依赖 `arch_scale_cpu_capacity()`（由各架构实现）获取 CPU 性能信息。\n- **RCU 机制**：在 SMP 路径中大量使用 `rcu_read_lock_sched_held()` 进行锁依赖检查。\n- **cpufreq 子系统**：通过 `cpufreq_update_util()` 与 CPU 频率调节器交互。\n- **实时互斥锁**：`CONFIG_RT_MUTEXES` 启用时，支持 deadline 任务的优先级继承。\n- **Sysctl 接口**：`CONFIG_SYSCTL` 启用时，提供用户空间可调的 deadline 参数。\n\n## 5. 使用场景\n\n- **工业实时控制**：如机器人控制、数控机床等需要严格周期性和低延迟响应的场景。\n- **音视频处理**：专业音视频采集、编码、播放等对 jitter 敏感的应用。\n- **电信基础设施**：5G 基站、核心网网元中的高优先级信令处理。\n- **汽车电子**：ADAS、自动驾驶系统中的关键任务调度。\n- **科研与高性能计算**：需要确定性执行时间的实验或仿真任务。\n\n用户通过 `sched_setattr(2)` 系统调用设置任务的 `SCHED_DEADLINE` 策略及对应的 `runtime`、`deadline`、`period` 参数，内核则通过本文件实现的调度逻辑确保其满足实时性约束。",
      "similarity": 0.47474730014801025,
      "chunks": [
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2104,
          "end_line": 2251,
          "content": [
            "static void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tif (is_dl_boosted(&p->dl)) {",
            "\t\t/*",
            "\t\t * Because of delays in the detection of the overrun of a",
            "\t\t * thread's runtime, it might be the case that a thread",
            "\t\t * goes to sleep in a rt mutex with negative runtime. As",
            "\t\t * a consequence, the thread will be throttled.",
            "\t\t *",
            "\t\t * While waiting for the mutex, this thread can also be",
            "\t\t * boosted via PI, resulting in a thread that is throttled",
            "\t\t * and boosted at the same time.",
            "\t\t *",
            "\t\t * In this case, the boost overrides the throttle.",
            "\t\t */",
            "\t\tif (p->dl.dl_throttled) {",
            "\t\t\t/*",
            "\t\t\t * The replenish timer needs to be canceled. No",
            "\t\t\t * problem if it fires concurrently: boosted threads",
            "\t\t\t * are ignored in dl_task_timer().",
            "\t\t\t *",
            "\t\t\t * If the timer callback was running (hrtimer_try_to_cancel == -1),",
            "\t\t\t * it will eventually call put_task_struct().",
            "\t\t\t */",
            "\t\t\tif (hrtimer_try_to_cancel(&p->dl.dl_timer) == 1 &&",
            "\t\t\t    !dl_server(&p->dl))",
            "\t\t\t\tput_task_struct(p);",
            "\t\t\tp->dl.dl_throttled = 0;",
            "\t\t}",
            "\t} else if (!dl_prio(p->normal_prio)) {",
            "\t\t/*",
            "\t\t * Special case in which we have a !SCHED_DEADLINE task that is going",
            "\t\t * to be deboosted, but exceeds its runtime while doing so. No point in",
            "\t\t * replenishing it, as it's going to return back to its original",
            "\t\t * scheduling class after this. If it has been throttled, we need to",
            "\t\t * clear the flag, otherwise the task may wake up as throttled after",
            "\t\t * being boosted again with no means to replenish the runtime and clear",
            "\t\t * the throttle.",
            "\t\t */",
            "\t\tp->dl.dl_throttled = 0;",
            "\t\tif (!(flags & ENQUEUE_REPLENISH))",
            "\t\t\tprintk_deferred_once(\"sched: DL de-boosted task PID %d: REPLENISH flag missing\\n\",",
            "\t\t\t\t\t     task_pid_nr(p));",
            "",
            "\t\treturn;",
            "\t}",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_dl(dl_rq_of_se(&p->dl), &p->dl);",
            "",
            "\tif (p->on_rq == TASK_ON_RQ_MIGRATING)",
            "\t\tflags |= ENQUEUE_MIGRATING;",
            "",
            "\tenqueue_dl_entity(&p->dl, flags);",
            "",
            "\tif (dl_server(&p->dl))",
            "\t\treturn;",
            "",
            "\tif (!task_current(rq, p) && !p->dl.dl_throttled && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_dl_task(rq, p);",
            "}",
            "static bool dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tupdate_curr_dl(rq);",
            "",
            "\tif (p->on_rq == TASK_ON_RQ_MIGRATING)",
            "\t\tflags |= DEQUEUE_MIGRATING;",
            "",
            "\tdequeue_dl_entity(&p->dl, flags);",
            "\tif (!p->dl.dl_throttled && !dl_server(&p->dl))",
            "\t\tdequeue_pushable_dl_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void yield_task_dl(struct rq *rq)",
            "{",
            "\t/*",
            "\t * We make the task go to sleep until its current deadline by",
            "\t * forcing its runtime to zero. This way, update_curr_dl() stops",
            "\t * it and the bandwidth timer will wake it up and will give it",
            "\t * new scheduling parameters (thanks to dl_yielded=1).",
            "\t */",
            "\trq->curr->dl.dl_yielded = 1;",
            "",
            "\tupdate_rq_clock(rq);",
            "\tupdate_curr_dl(rq);",
            "\t/*",
            "\t * Tell update_rq_clock() that we've just updated,",
            "\t * so we don't do microscopic update in schedule()",
            "\t * and double the fastpath cost.",
            "\t */",
            "\trq_clock_skip_update(rq);",
            "}",
            "static inline bool dl_task_is_earliest_deadline(struct task_struct *p,",
            "\t\t\t\t\t\t struct rq *rq)",
            "{",
            "\treturn (!rq->dl.dl_nr_running ||",
            "\t\tdl_time_before(p->dl.deadline,",
            "\t\t\t       rq->dl.earliest_dl.curr));",
            "}",
            "static int",
            "select_task_rq_dl(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tbool select_rq;",
            "\tstruct rq *rq;",
            "",
            "\tif (!(flags & WF_TTWU))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If we are dealing with a -deadline task, we must",
            "\t * decide where to wake it up.",
            "\t * If it has a later deadline and the current task",
            "\t * on this rq can't move (provided the waking task",
            "\t * can!) we prefer to send it somewhere else. On the",
            "\t * other hand, if it has a shorter deadline, we",
            "\t * try to make it stay here, it might be important.",
            "\t */",
            "\tselect_rq = unlikely(dl_task(curr)) &&",
            "\t\t    (curr->nr_cpus_allowed < 2 ||",
            "\t\t     !dl_entity_preempt(&p->dl, &curr->dl)) &&",
            "\t\t    p->nr_cpus_allowed > 1;",
            "",
            "\t/*",
            "\t * Take the capacity of the CPU into account to",
            "\t * ensure it fits the requirement of the task.",
            "\t */",
            "\tif (sched_asym_cpucap_active())",
            "\t\tselect_rq |= !dl_task_fits_capacity(p, cpu);",
            "",
            "\tif (select_rq) {",
            "\t\tint target = find_later_rq(p);",
            "",
            "\t\tif (target != -1 &&",
            "\t\t    dl_task_is_earliest_deadline(p, cpu_rq(target)))",
            "\t\t\tcpu = target;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "enqueue_task_dl, dequeue_task_dl, yield_task_dl, dl_task_is_earliest_deadline, select_task_rq_dl",
          "description": "处理截止时间任务的调度决策，包含任务入队出队、抢占检查、CPU选择及负载均衡逻辑，通过dl_task_is_earliest_deadline判断任务截止时间优先级并选择合适CPU",
          "similarity": 0.5203199982643127
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2887,
          "end_line": 3034,
          "content": [
            "static void task_woken_dl(struct rq *rq, struct task_struct *p)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    !test_tsk_need_resched(rq->curr) &&",
            "\t    p->nr_cpus_allowed > 1 &&",
            "\t    dl_task(rq->curr) &&",
            "\t    (rq->curr->nr_cpus_allowed < 2 ||",
            "\t     !dl_entity_preempt(&p->dl, &rq->curr->dl))) {",
            "\t\tpush_dl_tasks(rq);",
            "\t}",
            "}",
            "static void set_cpus_allowed_dl(struct task_struct *p,",
            "\t\t\t\tstruct affinity_context *ctx)",
            "{",
            "\tstruct root_domain *src_rd;",
            "\tstruct rq *rq;",
            "",
            "\tWARN_ON_ONCE(!dl_task(p));",
            "",
            "\trq = task_rq(p);",
            "\tsrc_rd = rq->rd;",
            "\t/*",
            "\t * Migrating a SCHED_DEADLINE task between exclusive",
            "\t * cpusets (different root_domains) entails a bandwidth",
            "\t * update. We already made space for us in the destination",
            "\t * domain (see cpuset_can_attach()).",
            "\t */",
            "\tif (!cpumask_intersects(src_rd->span, ctx->new_mask)) {",
            "\t\tstruct dl_bw *src_dl_b;",
            "",
            "\t\tsrc_dl_b = dl_bw_of(cpu_of(rq));",
            "\t\t/*",
            "\t\t * We now free resources of the root_domain we are migrating",
            "\t\t * off. In the worst case, sched_setattr() may temporary fail",
            "\t\t * until we complete the update.",
            "\t\t */",
            "\t\traw_spin_lock(&src_dl_b->lock);",
            "\t\t__dl_sub(src_dl_b, p->dl.dl_bw, dl_bw_cpus(task_cpu(p)));",
            "\t\traw_spin_unlock(&src_dl_b->lock);",
            "\t}",
            "",
            "\tset_cpus_allowed_common(p, ctx);",
            "}",
            "static void rq_online_dl(struct rq *rq)",
            "{",
            "\tif (rq->dl.overloaded)",
            "\t\tdl_set_overload(rq);",
            "",
            "\tcpudl_set_freecpu(&rq->rd->cpudl, rq->cpu);",
            "\tif (rq->dl.dl_nr_running > 0)",
            "\t\tcpudl_set(&rq->rd->cpudl, rq->cpu, rq->dl.earliest_dl.curr);",
            "}",
            "static void rq_offline_dl(struct rq *rq)",
            "{",
            "\tif (rq->dl.overloaded)",
            "\t\tdl_clear_overload(rq);",
            "",
            "\tcpudl_clear(&rq->rd->cpudl, rq->cpu);",
            "\tcpudl_clear_freecpu(&rq->rd->cpudl, rq->cpu);",
            "}",
            "void __init init_sched_dl_class(void)",
            "{",
            "\tunsigned int i;",
            "",
            "\tfor_each_possible_cpu(i)",
            "\t\tzalloc_cpumask_var_node(&per_cpu(local_cpu_mask_dl, i),",
            "\t\t\t\t\tGFP_KERNEL, cpu_to_node(i));",
            "}",
            "void dl_add_task_root_domain(struct task_struct *p)",
            "{",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "\tstruct dl_bw *dl_b;",
            "",
            "\traw_spin_lock_irqsave(&p->pi_lock, rf.flags);",
            "\tif (!dl_task(p)) {",
            "\t\traw_spin_unlock_irqrestore(&p->pi_lock, rf.flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\trq = __task_rq_lock(p, &rf);",
            "",
            "\tdl_b = &rq->rd->dl_bw;",
            "\traw_spin_lock(&dl_b->lock);",
            "",
            "\t__dl_add(dl_b, p->dl.dl_bw, cpumask_weight(rq->rd->span));",
            "",
            "\traw_spin_unlock(&dl_b->lock);",
            "",
            "\ttask_rq_unlock(rq, p, &rf);",
            "}",
            "void dl_clear_root_domain(struct root_domain *rd)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&rd->dl_bw.lock, flags);",
            "\trd->dl_bw.total_bw = 0;",
            "\traw_spin_unlock_irqrestore(&rd->dl_bw.lock, flags);",
            "}",
            "static void switched_from_dl(struct rq *rq, struct task_struct *p)",
            "{",
            "\t/*",
            "\t * task_non_contending() can start the \"inactive timer\" (if the 0-lag",
            "\t * time is in the future). If the task switches back to dl before",
            "\t * the \"inactive timer\" fires, it can continue to consume its current",
            "\t * runtime using its current deadline. If it stays outside of",
            "\t * SCHED_DEADLINE until the 0-lag time passes, inactive_task_timer()",
            "\t * will reset the task parameters.",
            "\t */",
            "\tif (task_on_rq_queued(p) && p->dl.dl_runtime)",
            "\t\ttask_non_contending(&p->dl);",
            "",
            "\t/*",
            "\t * In case a task is setscheduled out from SCHED_DEADLINE we need to",
            "\t * keep track of that on its cpuset (for correct bandwidth tracking).",
            "\t */",
            "\tdec_dl_tasks_cs(p);",
            "",
            "\tif (!task_on_rq_queued(p)) {",
            "\t\t/*",
            "\t\t * Inactive timer is armed. However, p is leaving DEADLINE and",
            "\t\t * might migrate away from this rq while continuing to run on",
            "\t\t * some other class. We need to remove its contribution from",
            "\t\t * this rq running_bw now, or sub_rq_bw (below) will complain.",
            "\t\t */",
            "\t\tif (p->dl.dl_non_contending)",
            "\t\t\tsub_running_bw(&p->dl, &rq->dl);",
            "\t\tsub_rq_bw(&p->dl, &rq->dl);",
            "\t}",
            "",
            "\t/*",
            "\t * We cannot use inactive_task_timer() to invoke sub_running_bw()",
            "\t * at the 0-lag time, because the task could have been migrated",
            "\t * while SCHED_OTHER in the meanwhile.",
            "\t */",
            "\tif (p->dl.dl_non_contending)",
            "\t\tp->dl.dl_non_contending = 0;",
            "",
            "\t/*",
            "\t * Since this might be the only -deadline task on the rq,",
            "\t * this is the right place to try to pull some other one",
            "\t * from an overloaded CPU, if any.",
            "\t */",
            "\tif (!task_on_rq_queued(p) || rq->dl.dl_nr_running)",
            "\t\treturn;",
            "",
            "\tdeadline_queue_pull_task(rq);",
            "}"
          ],
          "function_name": "task_woken_dl, set_cpus_allowed_dl, rq_online_dl, rq_offline_dl, init_sched_dl_class, dl_add_task_root_domain, dl_clear_root_domain, switched_from_dl",
          "description": "管理SCHED_DEADLINE任务唤醒后的处理、CPU亲和性变更、根域带宽分配及任务状态切换时的资源回收与重新计算",
          "similarity": 0.49869394302368164
        },
        {
          "chunk_id": 16,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2709,
          "end_line": 2879,
          "content": [
            "static int push_dl_task(struct rq *rq)",
            "{",
            "\tstruct task_struct *next_task;",
            "\tstruct rq *later_rq;",
            "\tint ret = 0;",
            "",
            "\tnext_task = pick_next_pushable_dl_task(rq);",
            "\tif (!next_task)",
            "\t\treturn 0;",
            "",
            "retry:",
            "\t/*",
            "\t * If next_task preempts rq->curr, and rq->curr",
            "\t * can move away, it makes sense to just reschedule",
            "\t * without going further in pushing next_task.",
            "\t */",
            "\tif (dl_task(rq->curr) &&",
            "\t    dl_time_before(next_task->dl.deadline, rq->curr->dl.deadline) &&",
            "\t    rq->curr->nr_cpus_allowed > 1) {",
            "\t\tresched_curr(rq);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_migration_disabled(next_task))",
            "\t\treturn 0;",
            "",
            "\tif (WARN_ON(next_task == rq->curr))",
            "\t\treturn 0;",
            "",
            "\t/* We might release rq lock */",
            "\tget_task_struct(next_task);",
            "",
            "\t/* Will lock the rq it'll find */",
            "\tlater_rq = find_lock_later_rq(next_task, rq);",
            "\tif (!later_rq) {",
            "\t\tstruct task_struct *task;",
            "",
            "\t\t/*",
            "\t\t * We must check all this again, since",
            "\t\t * find_lock_later_rq releases rq->lock and it is",
            "\t\t * then possible that next_task has migrated.",
            "\t\t */",
            "\t\ttask = pick_next_pushable_dl_task(rq);",
            "\t\tif (task == next_task) {",
            "\t\t\t/*",
            "\t\t\t * The task is still there. We don't try",
            "\t\t\t * again, some other CPU will pull it when ready.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (!task)",
            "\t\t\t/* No more tasks */",
            "\t\t\tgoto out;",
            "",
            "\t\tput_task_struct(next_task);",
            "\t\tnext_task = task;",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tdeactivate_task(rq, next_task, 0);",
            "\tset_task_cpu(next_task, later_rq->cpu);",
            "\tactivate_task(later_rq, next_task, 0);",
            "\tret = 1;",
            "",
            "\tresched_curr(later_rq);",
            "",
            "\tdouble_unlock_balance(rq, later_rq);",
            "",
            "out:",
            "\tput_task_struct(next_task);",
            "",
            "\treturn ret;",
            "}",
            "static void push_dl_tasks(struct rq *rq)",
            "{",
            "\t/* push_dl_task() will return true if it moved a -deadline task */",
            "\twhile (push_dl_task(rq))",
            "\t\t;",
            "}",
            "static void pull_dl_task(struct rq *this_rq)",
            "{",
            "\tint this_cpu = this_rq->cpu, cpu;",
            "\tstruct task_struct *p, *push_task;",
            "\tbool resched = false;",
            "\tstruct rq *src_rq;",
            "\tu64 dmin = LONG_MAX;",
            "",
            "\tif (likely(!dl_overloaded(this_rq)))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Match the barrier from dl_set_overloaded; this guarantees that if we",
            "\t * see overloaded we must also see the dlo_mask bit.",
            "\t */",
            "\tsmp_rmb();",
            "",
            "\tfor_each_cpu(cpu, this_rq->rd->dlo_mask) {",
            "\t\tif (this_cpu == cpu)",
            "\t\t\tcontinue;",
            "",
            "\t\tsrc_rq = cpu_rq(cpu);",
            "",
            "\t\t/*",
            "\t\t * It looks racy, abd it is! However, as in sched_rt.c,",
            "\t\t * we are fine with this.",
            "\t\t */",
            "\t\tif (this_rq->dl.dl_nr_running &&",
            "\t\t    dl_time_before(this_rq->dl.earliest_dl.curr,",
            "\t\t\t\t   src_rq->dl.earliest_dl.next))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Might drop this_rq->lock */",
            "\t\tpush_task = NULL;",
            "\t\tdouble_lock_balance(this_rq, src_rq);",
            "",
            "\t\t/*",
            "\t\t * If there are no more pullable tasks on the",
            "\t\t * rq, we're done with it.",
            "\t\t */",
            "\t\tif (src_rq->dl.dl_nr_running <= 1)",
            "\t\t\tgoto skip;",
            "",
            "\t\tp = pick_earliest_pushable_dl_task(src_rq, this_cpu);",
            "",
            "\t\t/*",
            "\t\t * We found a task to be pulled if:",
            "\t\t *  - it preempts our current (if there's one),",
            "\t\t *  - it will preempt the last one we pulled (if any).",
            "\t\t */",
            "\t\tif (p && dl_time_before(p->dl.deadline, dmin) &&",
            "\t\t    dl_task_is_earliest_deadline(p, this_rq)) {",
            "\t\t\tWARN_ON(p == src_rq->curr);",
            "\t\t\tWARN_ON(!task_on_rq_queued(p));",
            "",
            "\t\t\t/*",
            "\t\t\t * Then we pull iff p has actually an earlier",
            "\t\t\t * deadline than the current task of its runqueue.",
            "\t\t\t */",
            "\t\t\tif (dl_time_before(p->dl.deadline,",
            "\t\t\t\t\t   src_rq->curr->dl.deadline))",
            "\t\t\t\tgoto skip;",
            "",
            "\t\t\tif (is_migration_disabled(p)) {",
            "\t\t\t\tpush_task = get_push_task(src_rq);",
            "\t\t\t} else {",
            "\t\t\t\tdeactivate_task(src_rq, p, 0);",
            "\t\t\t\tset_task_cpu(p, this_cpu);",
            "\t\t\t\tactivate_task(this_rq, p, 0);",
            "\t\t\t\tdmin = p->dl.deadline;",
            "\t\t\t\tresched = true;",
            "\t\t\t}",
            "",
            "\t\t\t/* Is there any other task even earlier? */",
            "\t\t}",
            "skip:",
            "\t\tdouble_unlock_balance(this_rq, src_rq);",
            "",
            "\t\tif (push_task) {",
            "\t\t\tpreempt_disable();",
            "\t\t\traw_spin_rq_unlock(this_rq);",
            "\t\t\tstop_one_cpu_nowait(src_rq->cpu, push_cpu_stop,",
            "\t\t\t\t\t    push_task, &src_rq->push_work);",
            "\t\t\tpreempt_enable();",
            "\t\t\traw_spin_rq_lock(this_rq);",
            "\t\t}",
            "\t}",
            "",
            "\tif (resched)",
            "\t\tresched_curr(this_rq);",
            "}"
          ],
          "function_name": "push_dl_task, push_dl_tasks, pull_dl_task",
          "description": "实现SCHED_DEADLINE任务的迁移逻辑，通过push_dl_task尝试将任务推送到更晚截止时间的CPU，pull_dl_task从过载CPU拉取任务以平衡负载",
          "similarity": 0.49263331294059753
        },
        {
          "chunk_id": 15,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 2454,
          "end_line": 2581,
          "content": [
            "static void put_prev_task_dl(struct rq *rq, struct task_struct *p, struct task_struct *next)",
            "{",
            "\tstruct sched_dl_entity *dl_se = &p->dl;",
            "\tstruct dl_rq *dl_rq = &rq->dl;",
            "",
            "\tif (on_dl_rq(&p->dl))",
            "\t\tupdate_stats_wait_start_dl(dl_rq, dl_se);",
            "",
            "\tupdate_curr_dl(rq);",
            "",
            "\tupdate_dl_rq_load_avg(rq_clock_pelt(rq), rq, 1);",
            "\tif (on_dl_rq(&p->dl) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_dl_task(rq, p);",
            "}",
            "static void task_tick_dl(struct rq *rq, struct task_struct *p, int queued)",
            "{",
            "\tupdate_curr_dl(rq);",
            "",
            "\tupdate_dl_rq_load_avg(rq_clock_pelt(rq), rq, 1);",
            "\t/*",
            "\t * Even when we have runtime, update_curr_dl() might have resulted in us",
            "\t * not being the leftmost task anymore. In that case NEED_RESCHED will",
            "\t * be set and schedule() will start a new hrtick for the next task.",
            "\t */",
            "\tif (hrtick_enabled_dl(rq) && queued && p->dl.runtime > 0 &&",
            "\t    is_leftmost(&p->dl, &rq->dl))",
            "\t\tstart_hrtick_dl(rq, &p->dl);",
            "}",
            "static void task_fork_dl(struct task_struct *p)",
            "{",
            "\t/*",
            "\t * SCHED_DEADLINE tasks cannot fork and this is achieved through",
            "\t * sched_fork()",
            "\t */",
            "}",
            "static int pick_dl_task(struct rq *rq, struct task_struct *p, int cpu)",
            "{",
            "\tif (!task_on_cpu(rq, p) &&",
            "\t    cpumask_test_cpu(cpu, &p->cpus_mask))",
            "\t\treturn 1;",
            "\treturn 0;",
            "}",
            "static int find_later_rq(struct task_struct *task)",
            "{",
            "\tstruct sched_domain *sd;",
            "\tstruct cpumask *later_mask = this_cpu_cpumask_var_ptr(local_cpu_mask_dl);",
            "\tint this_cpu = smp_processor_id();",
            "\tint cpu = task_cpu(task);",
            "",
            "\t/* Make sure the mask is initialized first */",
            "\tif (unlikely(!later_mask))",
            "\t\treturn -1;",
            "",
            "\tif (task->nr_cpus_allowed == 1)",
            "\t\treturn -1;",
            "",
            "\t/*",
            "\t * We have to consider system topology and task affinity",
            "\t * first, then we can look for a suitable CPU.",
            "\t */",
            "\tif (!cpudl_find(&task_rq(task)->rd->cpudl, task, later_mask))",
            "\t\treturn -1;",
            "",
            "\t/*",
            "\t * If we are here, some targets have been found, including",
            "\t * the most suitable which is, among the runqueues where the",
            "\t * current tasks have later deadlines than the task's one, the",
            "\t * rq with the latest possible one.",
            "\t *",
            "\t * Now we check how well this matches with task's",
            "\t * affinity and system topology.",
            "\t *",
            "\t * The last CPU where the task run is our first",
            "\t * guess, since it is most likely cache-hot there.",
            "\t */",
            "\tif (cpumask_test_cpu(cpu, later_mask))",
            "\t\treturn cpu;",
            "\t/*",
            "\t * Check if this_cpu is to be skipped (i.e., it is",
            "\t * not in the mask) or not.",
            "\t */",
            "\tif (!cpumask_test_cpu(this_cpu, later_mask))",
            "\t\tthis_cpu = -1;",
            "",
            "\trcu_read_lock();",
            "\tfor_each_domain(cpu, sd) {",
            "\t\tif (sd->flags & SD_WAKE_AFFINE) {",
            "\t\t\tint best_cpu;",
            "",
            "\t\t\t/*",
            "\t\t\t * If possible, preempting this_cpu is",
            "\t\t\t * cheaper than migrating.",
            "\t\t\t */",
            "\t\t\tif (this_cpu != -1 &&",
            "\t\t\t    cpumask_test_cpu(this_cpu, sched_domain_span(sd))) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn this_cpu;",
            "\t\t\t}",
            "",
            "\t\t\tbest_cpu = cpumask_any_and_distribute(later_mask,",
            "\t\t\t\t\t\t\t      sched_domain_span(sd));",
            "\t\t\t/*",
            "\t\t\t * Last chance: if a CPU being in both later_mask",
            "\t\t\t * and current sd span is valid, that becomes our",
            "\t\t\t * choice. Of course, the latest possible CPU is",
            "\t\t\t * already under consideration through later_mask.",
            "\t\t\t */",
            "\t\t\tif (best_cpu < nr_cpu_ids) {",
            "\t\t\t\trcu_read_unlock();",
            "\t\t\t\treturn best_cpu;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * At this point, all our guesses failed, we just return",
            "\t * 'something', and let the caller sort the things out.",
            "\t */",
            "\tif (this_cpu != -1)",
            "\t\treturn this_cpu;",
            "",
            "\tcpu = cpumask_any_distribute(later_mask);",
            "\tif (cpu < nr_cpu_ids)",
            "\t\treturn cpu;",
            "",
            "\treturn -1;",
            "}"
          ],
          "function_name": "put_prev_task_dl, task_tick_dl, task_fork_dl, pick_dl_task, find_later_rq",
          "description": "处理SCHED_DEADLINE任务切换时的状态更新和负载均衡，包含任务统计更新、运行队列加载平均值调整及可推送任务的入队操作",
          "similarity": 0.4845747947692871
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/sched/deadline.c",
          "start_line": 1176,
          "end_line": 1341,
          "content": [
            "static void __push_dl_task(struct rq *rq, struct rq_flags *rf)",
            "{",
            "#ifdef CONFIG_SMP",
            "\t/*",
            "\t * Queueing this task back might have overloaded rq, check if we need",
            "\t * to kick someone away.",
            "\t */",
            "\tif (has_pushable_dl_tasks(rq)) {",
            "\t\t/*",
            "\t\t * Nothing relies on rq->lock after this, so its safe to drop",
            "\t\t * rq->lock.",
            "\t\t */",
            "\t\trq_unpin_lock(rq, rf);",
            "\t\tpush_dl_task(rq);",
            "\t\trq_repin_lock(rq, rf);",
            "\t}",
            "#endif",
            "}",
            "static enum hrtimer_restart dl_server_timer(struct hrtimer *timer, struct sched_dl_entity *dl_se)",
            "{",
            "\tstruct rq *rq = rq_of_dl_se(dl_se);",
            "\tu64 fw;",
            "",
            "\tscoped_guard (rq_lock, rq) {",
            "\t\tstruct rq_flags *rf = &scope.rf;",
            "",
            "\t\tif (!dl_se->dl_throttled || !dl_se->dl_runtime)",
            "\t\t\treturn HRTIMER_NORESTART;",
            "",
            "\t\tsched_clock_tick();",
            "\t\tupdate_rq_clock(rq);",
            "",
            "\t\tif (!dl_se->dl_runtime)",
            "\t\t\treturn HRTIMER_NORESTART;",
            "",
            "\t\tif (!dl_se->server_has_tasks(dl_se)) {",
            "\t\t\treplenish_dl_entity(dl_se);",
            "\t\t\treturn HRTIMER_NORESTART;",
            "\t\t}",
            "",
            "\t\tif (dl_se->dl_defer_armed) {",
            "\t\t\t/*",
            "\t\t\t * First check if the server could consume runtime in background.",
            "\t\t\t * If so, it is possible to push the defer timer for this amount",
            "\t\t\t * of time. The dl_server_min_res serves as a limit to avoid",
            "\t\t\t * forwarding the timer for a too small amount of time.",
            "\t\t\t */",
            "\t\t\tif (dl_time_before(rq_clock(dl_se->rq),",
            "\t\t\t\t\t   (dl_se->deadline - dl_se->runtime - dl_server_min_res))) {",
            "",
            "\t\t\t\t/* reset the defer timer */",
            "\t\t\t\tfw = dl_se->deadline - rq_clock(dl_se->rq) - dl_se->runtime;",
            "",
            "\t\t\t\thrtimer_forward_now(timer, ns_to_ktime(fw));",
            "\t\t\t\treturn HRTIMER_RESTART;",
            "\t\t\t}",
            "",
            "\t\t\tdl_se->dl_defer_running = 1;",
            "\t\t}",
            "",
            "\t\tenqueue_dl_entity(dl_se, ENQUEUE_REPLENISH);",
            "",
            "\t\tif (!dl_task(dl_se->rq->curr) || dl_entity_preempt(dl_se, &dl_se->rq->curr->dl))",
            "\t\t\tresched_curr(rq);",
            "",
            "\t\t__push_dl_task(rq, rf);",
            "\t}",
            "",
            "\treturn HRTIMER_NORESTART;",
            "}",
            "static enum hrtimer_restart dl_task_timer(struct hrtimer *timer)",
            "{",
            "\tstruct sched_dl_entity *dl_se = container_of(timer,",
            "\t\t\t\t\t\t     struct sched_dl_entity,",
            "\t\t\t\t\t\t     dl_timer);",
            "\tstruct task_struct *p;",
            "\tstruct rq_flags rf;",
            "\tstruct rq *rq;",
            "",
            "\tif (dl_server(dl_se))",
            "\t\treturn dl_server_timer(timer, dl_se);",
            "",
            "\tp = dl_task_of(dl_se);",
            "\trq = task_rq_lock(p, &rf);",
            "",
            "\t/*",
            "\t * The task might have changed its scheduling policy to something",
            "\t * different than SCHED_DEADLINE (through switched_from_dl()).",
            "\t */",
            "\tif (!dl_task(p))",
            "\t\tgoto unlock;",
            "",
            "\t/*",
            "\t * The task might have been boosted by someone else and might be in the",
            "\t * boosting/deboosting path, its not throttled.",
            "\t */",
            "\tif (is_dl_boosted(dl_se))",
            "\t\tgoto unlock;",
            "",
            "\t/*",
            "\t * Spurious timer due to start_dl_timer() race; or we already received",
            "\t * a replenishment from rt_mutex_setprio().",
            "\t */",
            "\tif (!dl_se->dl_throttled)",
            "\t\tgoto unlock;",
            "",
            "\tsched_clock_tick();",
            "\tupdate_rq_clock(rq);",
            "",
            "\t/*",
            "\t * If the throttle happened during sched-out; like:",
            "\t *",
            "\t *   schedule()",
            "\t *     deactivate_task()",
            "\t *       dequeue_task_dl()",
            "\t *         update_curr_dl()",
            "\t *           start_dl_timer()",
            "\t *         __dequeue_task_dl()",
            "\t *     prev->on_rq = 0;",
            "\t *",
            "\t * We can be both throttled and !queued. Replenish the counter",
            "\t * but do not enqueue -- wait for our wakeup to do that.",
            "\t */",
            "\tif (!task_on_rq_queued(p)) {",
            "\t\treplenish_dl_entity(dl_se);",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "#ifdef CONFIG_SMP",
            "\tif (unlikely(!rq->online)) {",
            "\t\t/*",
            "\t\t * If the runqueue is no longer available, migrate the",
            "\t\t * task elsewhere. This necessarily changes rq.",
            "\t\t */",
            "\t\tlockdep_unpin_lock(__rq_lockp(rq), rf.cookie);",
            "\t\trq = dl_task_offline_migration(rq, p);",
            "\t\trf.cookie = lockdep_pin_lock(__rq_lockp(rq));",
            "\t\tupdate_rq_clock(rq);",
            "",
            "\t\t/*",
            "\t\t * Now that the task has been migrated to the new RQ and we",
            "\t\t * have that locked, proceed as normal and enqueue the task",
            "\t\t * there.",
            "\t\t */",
            "\t}",
            "#endif",
            "",
            "\tenqueue_task_dl(rq, p, ENQUEUE_REPLENISH);",
            "\tif (dl_task(rq->curr))",
            "\t\twakeup_preempt_dl(rq, p, 0);",
            "\telse",
            "\t\tresched_curr(rq);",
            "",
            "\t__push_dl_task(rq, &rf);",
            "",
            "unlock:",
            "\ttask_rq_unlock(rq, p, &rf);",
            "",
            "\t/*",
            "\t * This can free the task_struct, including this hrtimer, do not touch",
            "\t * anything related to that after this.",
            "\t */",
            "\tput_task_struct(p);",
            "",
            "\treturn HRTIMER_NORESTART;",
            "}"
          ],
          "function_name": "__push_dl_task, dl_server_timer, dl_task_timer",
          "description": "处理截止时间任务的推送逻辑和定时器回调，通过服务器定时器管理延迟任务重排，协调多核环境下的任务迁移与抢占。",
          "similarity": 0.4798821806907654
        }
      ]
    },
    {
      "source_file": "kernel/sched/cpudeadline.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:02:17\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cpudeadline.c`\n\n---\n\n# `sched/cpudeadline.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cpudeadline.c` 是 Linux 内核调度器中用于 **全局 CPU 截止时间（Deadline）管理** 的核心实现文件。该文件维护一个 **最大堆（max-heap）数据结构**，用于高效追踪系统中所有运行 SCHED_DEADLINE 任务的 CPU 上 **最早截止时间（earliest deadline）** 的任务信息。通过此结构，调度器可快速找到最适合迁移或唤醒 deadline 任务的目标 CPU，从而满足实时调度的截止时间约束。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct cpudl`**：CPU 截止时间管理上下文，包含：\n  - `elements[]`：堆数组，每个元素为 `struct cpudl_item`，记录 CPU ID 和其上最早 deadline\n  - `size`：堆中有效元素数量\n  - `free_cpus`：位图，标记当前无 deadline 任务的 CPU\n  - `lock`：保护堆操作的自旋锁\n- **`struct cpudl_item`**：堆中单个元素，包含：\n  - `cpu`：CPU ID\n  - `dl`：该 CPU 上最早 deadline 时间戳\n  - `idx`：该 CPU 在堆中的索引（用于快速定位）\n\n### 主要函数\n| 函数 | 功能描述 |\n|------|----------|\n| `cpudl_init()` / `cpudl_cleanup()` | 初始化/销毁 `cpudl` 结构 |\n| `cpudl_set()` | 更新指定 CPU 的最早 deadline，并维护堆性质 |\n| `cpudl_clear()` | 从堆中移除指定 CPU（如 CPU 下线或无 deadline 任务） |\n| `cpudl_find()` | 查找满足任务截止时间约束的最佳 CPU |\n| `cpudl_set_freecpu()` / `cpudl_clear_freecpu()` | 设置/清除 CPU 的“空闲”状态（无 deadline 任务） |\n| `cpudl_heapify_up()` / `cpudl_heapify_down()` | 堆调整函数，维护最大堆性质 |\n\n## 3. 关键实现\n\n### 堆结构设计\n- 使用 **数组实现的最大堆**，堆顶（`elements[0]`）始终保存 **系统中最早的 deadline**。\n- 每个 CPU 在堆中有唯一索引（`idx`），支持 **O(log n)** 时间复杂度的插入、删除和更新操作。\n- 堆比较基于 `dl_time_before(a, b)` 宏（定义在 `sched/deadline.h`），判断 `a` 是否早于 `b`。\n\n### 核心操作逻辑\n- **`cpudl_set()`**：\n  - 若 CPU 首次加入堆（`idx == IDX_INVALID`），将其插入堆尾并上滤（`heapify_up`）。\n  - 否则直接更新 deadline 值，并根据新旧值关系选择上滤或下滤（`heapify`）。\n- **`cpudl_clear()`**：\n  - 用堆尾元素覆盖待删除元素，缩小堆大小，再调整堆结构。\n  - 将 CPU 标记为“空闲”（加入 `free_cpus` 位图）。\n- **`cpudl_find()`**：\n  - **优先使用 `free_cpus`**：若存在空闲 CPU 且满足任务亲和性，则从中选择。\n    - 在异构系统（`sched_asym_cpucap_active()`）中，进一步筛选满足任务计算能力需求（`dl_task_fits_capacity()`）的 CPU。\n    - 若无满足条件的空闲 CPU，则选择能力最强的 CPU。\n  - **回退到堆顶 CPU**：若无空闲 CPU，则检查堆顶 CPU 的 deadline 是否晚于任务 deadline，若是则选择该 CPU。\n\n### 并发控制\n- 所有堆操作均通过 `raw_spin_lock_irqsave()` 保护，确保多 CPU 环境下的数据一致性。\n- 调用者需持有对应 CPU 的运行队列锁（`cpu_rq(cpu)->lock`），避免与调度器主逻辑冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `linux/sched.h`：`task_struct`, `cpumask`\n  - `linux/sched/deadline.h`：`sched_dl_entity`, `dl_time_before()`, `dl_task_fits_capacity()`\n  - `linux/cpumask.h`：CPU 位图操作\n  - `linux/kernel.h`：`kcalloc()`, `WARN_ON()`\n- **功能依赖**：\n  - **SCHED_DEADLINE 调度类**：提供 deadline 任务的核心调度逻辑。\n  - **CPU 拓扑与能力模型**：`arch_scale_cpu_capacity()` 用于异构系统 CPU 能力评估。\n  - **调度域（sched_domain）**：`free_cpus` 的管理与调度域 CPU 列表关联。\n\n## 5. 使用场景\n\n- **Deadline 任务负载均衡**：\n  - 当新 deadline 任务被创建或唤醒时，`find_later_rq()` 调用 `cpudl_find()` 选择迁移目标 CPU。\n- **CPU 热插拔**：\n  - CPU 下线时调用 `cpudl_clear()` 移除其 deadline 信息。\n  - CPU 上线时通过 `cpudl_set_freecpu()` 标记为空闲。\n- **任务迁移**：\n  - 调度器在跨 CPU 迁移 deadline 任务前，通过 `cpudl_find()` 验证目标 CPU 的 deadline 约束。\n- **异构系统优化**：\n  - 在 ARM big.LITTLE 等架构中，结合 CPU 计算能力筛选满足任务需求的 CPU，避免小核过载。",
      "similarity": 0.4704552888870239,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/cpudeadline.c",
          "start_line": 10,
          "end_line": 147,
          "content": [
            "static inline int parent(int i)",
            "{",
            "\treturn (i - 1) >> 1;",
            "}",
            "static inline int left_child(int i)",
            "{",
            "\treturn (i << 1) + 1;",
            "}",
            "static inline int right_child(int i)",
            "{",
            "\treturn (i << 1) + 2;",
            "}",
            "static void cpudl_heapify_down(struct cpudl *cp, int idx)",
            "{",
            "\tint l, r, largest;",
            "",
            "\tint orig_cpu = cp->elements[idx].cpu;",
            "\tu64 orig_dl = cp->elements[idx].dl;",
            "",
            "\tif (left_child(idx) >= cp->size)",
            "\t\treturn;",
            "",
            "\t/* adapted from lib/prio_heap.c */",
            "\twhile (1) {",
            "\t\tu64 largest_dl;",
            "",
            "\t\tl = left_child(idx);",
            "\t\tr = right_child(idx);",
            "\t\tlargest = idx;",
            "\t\tlargest_dl = orig_dl;",
            "",
            "\t\tif ((l < cp->size) && dl_time_before(orig_dl,",
            "\t\t\t\t\t\tcp->elements[l].dl)) {",
            "\t\t\tlargest = l;",
            "\t\t\tlargest_dl = cp->elements[l].dl;",
            "\t\t}",
            "\t\tif ((r < cp->size) && dl_time_before(largest_dl,",
            "\t\t\t\t\t\tcp->elements[r].dl))",
            "\t\t\tlargest = r;",
            "",
            "\t\tif (largest == idx)",
            "\t\t\tbreak;",
            "",
            "\t\t/* pull largest child onto idx */",
            "\t\tcp->elements[idx].cpu = cp->elements[largest].cpu;",
            "\t\tcp->elements[idx].dl = cp->elements[largest].dl;",
            "\t\tcp->elements[cp->elements[idx].cpu].idx = idx;",
            "\t\tidx = largest;",
            "\t}",
            "\t/* actual push down of saved original values orig_* */",
            "\tcp->elements[idx].cpu = orig_cpu;",
            "\tcp->elements[idx].dl = orig_dl;",
            "\tcp->elements[cp->elements[idx].cpu].idx = idx;",
            "}",
            "static void cpudl_heapify_up(struct cpudl *cp, int idx)",
            "{",
            "\tint p;",
            "",
            "\tint orig_cpu = cp->elements[idx].cpu;",
            "\tu64 orig_dl = cp->elements[idx].dl;",
            "",
            "\tif (idx == 0)",
            "\t\treturn;",
            "",
            "\tdo {",
            "\t\tp = parent(idx);",
            "\t\tif (dl_time_before(orig_dl, cp->elements[p].dl))",
            "\t\t\tbreak;",
            "\t\t/* pull parent onto idx */",
            "\t\tcp->elements[idx].cpu = cp->elements[p].cpu;",
            "\t\tcp->elements[idx].dl = cp->elements[p].dl;",
            "\t\tcp->elements[cp->elements[idx].cpu].idx = idx;",
            "\t\tidx = p;",
            "\t} while (idx != 0);",
            "\t/* actual push up of saved original values orig_* */",
            "\tcp->elements[idx].cpu = orig_cpu;",
            "\tcp->elements[idx].dl = orig_dl;",
            "\tcp->elements[cp->elements[idx].cpu].idx = idx;",
            "}",
            "static void cpudl_heapify(struct cpudl *cp, int idx)",
            "{",
            "\tif (idx > 0 && dl_time_before(cp->elements[parent(idx)].dl,",
            "\t\t\t\tcp->elements[idx].dl))",
            "\t\tcpudl_heapify_up(cp, idx);",
            "\telse",
            "\t\tcpudl_heapify_down(cp, idx);",
            "}",
            "static inline int cpudl_maximum(struct cpudl *cp)",
            "{",
            "\treturn cp->elements[0].cpu;",
            "}",
            "int cpudl_find(struct cpudl *cp, struct task_struct *p,",
            "\t       struct cpumask *later_mask)",
            "{",
            "\tconst struct sched_dl_entity *dl_se = &p->dl;",
            "",
            "\tif (later_mask &&",
            "\t    cpumask_and(later_mask, cp->free_cpus, &p->cpus_mask)) {",
            "\t\tunsigned long cap, max_cap = 0;",
            "\t\tint cpu, max_cpu = -1;",
            "",
            "\t\tif (!sched_asym_cpucap_active())",
            "\t\t\treturn 1;",
            "",
            "\t\t/* Ensure the capacity of the CPUs fits the task. */",
            "\t\tfor_each_cpu(cpu, later_mask) {",
            "\t\t\tif (!dl_task_fits_capacity(p, cpu)) {",
            "\t\t\t\tcpumask_clear_cpu(cpu, later_mask);",
            "",
            "\t\t\t\tcap = arch_scale_cpu_capacity(cpu);",
            "",
            "\t\t\t\tif (cap > max_cap ||",
            "\t\t\t\t    (cpu == task_cpu(p) && cap == max_cap)) {",
            "\t\t\t\t\tmax_cap = cap;",
            "\t\t\t\t\tmax_cpu = cpu;",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tif (cpumask_empty(later_mask))",
            "\t\t\tcpumask_set_cpu(max_cpu, later_mask);",
            "",
            "\t\treturn 1;",
            "\t} else {",
            "\t\tint best_cpu = cpudl_maximum(cp);",
            "",
            "\t\tWARN_ON(best_cpu != -1 && !cpu_present(best_cpu));",
            "",
            "\t\tif (cpumask_test_cpu(best_cpu, &p->cpus_mask) &&",
            "\t\t    dl_time_before(dl_se->deadline, cp->elements[0].dl)) {",
            "\t\t\tif (later_mask)",
            "\t\t\t\tcpumask_set_cpu(best_cpu, later_mask);",
            "",
            "\t\t\treturn 1;",
            "\t\t}",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "parent, left_child, right_child, cpudl_heapify_down, cpudl_heapify_up, cpudl_heapify, cpudl_maximum, cpudl_find",
          "description": "实现了基于数组的堆结构辅助函数及堆化操作，包含父节点/子节点索引计算、堆下滤/上滤算法、最大值查询及任务 CPU 选择逻辑，用于维护截止时间优先级队列。",
          "similarity": 0.4643024802207947
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/cpudeadline.c",
          "start_line": 173,
          "end_line": 262,
          "content": [
            "void cpudl_clear(struct cpudl *cp, int cpu)",
            "{",
            "\tint old_idx, new_cpu;",
            "\tunsigned long flags;",
            "",
            "\tWARN_ON(!cpu_present(cpu));",
            "",
            "\traw_spin_lock_irqsave(&cp->lock, flags);",
            "",
            "\told_idx = cp->elements[cpu].idx;",
            "\tif (old_idx == IDX_INVALID) {",
            "\t\t/*",
            "\t\t * Nothing to remove if old_idx was invalid.",
            "\t\t * This could happen if a rq_offline_dl is",
            "\t\t * called for a CPU without -dl tasks running.",
            "\t\t */",
            "\t} else {",
            "\t\tnew_cpu = cp->elements[cp->size - 1].cpu;",
            "\t\tcp->elements[old_idx].dl = cp->elements[cp->size - 1].dl;",
            "\t\tcp->elements[old_idx].cpu = new_cpu;",
            "\t\tcp->size--;",
            "\t\tcp->elements[new_cpu].idx = old_idx;",
            "\t\tcp->elements[cpu].idx = IDX_INVALID;",
            "\t\tcpudl_heapify(cp, old_idx);",
            "",
            "\t\tcpumask_set_cpu(cpu, cp->free_cpus);",
            "\t}",
            "\traw_spin_unlock_irqrestore(&cp->lock, flags);",
            "}",
            "void cpudl_set(struct cpudl *cp, int cpu, u64 dl)",
            "{",
            "\tint old_idx;",
            "\tunsigned long flags;",
            "",
            "\tWARN_ON(!cpu_present(cpu));",
            "",
            "\traw_spin_lock_irqsave(&cp->lock, flags);",
            "",
            "\told_idx = cp->elements[cpu].idx;",
            "\tif (old_idx == IDX_INVALID) {",
            "\t\tint new_idx = cp->size++;",
            "",
            "\t\tcp->elements[new_idx].dl = dl;",
            "\t\tcp->elements[new_idx].cpu = cpu;",
            "\t\tcp->elements[cpu].idx = new_idx;",
            "\t\tcpudl_heapify_up(cp, new_idx);",
            "\t\tcpumask_clear_cpu(cpu, cp->free_cpus);",
            "\t} else {",
            "\t\tcp->elements[old_idx].dl = dl;",
            "\t\tcpudl_heapify(cp, old_idx);",
            "\t}",
            "",
            "\traw_spin_unlock_irqrestore(&cp->lock, flags);",
            "}",
            "void cpudl_set_freecpu(struct cpudl *cp, int cpu)",
            "{",
            "\tcpumask_set_cpu(cpu, cp->free_cpus);",
            "}",
            "void cpudl_clear_freecpu(struct cpudl *cp, int cpu)",
            "{",
            "\tcpumask_clear_cpu(cpu, cp->free_cpus);",
            "}",
            "int cpudl_init(struct cpudl *cp)",
            "{",
            "\tint i;",
            "",
            "\traw_spin_lock_init(&cp->lock);",
            "\tcp->size = 0;",
            "",
            "\tcp->elements = kcalloc(nr_cpu_ids,",
            "\t\t\t       sizeof(struct cpudl_item),",
            "\t\t\t       GFP_KERNEL);",
            "\tif (!cp->elements)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (!zalloc_cpumask_var(&cp->free_cpus, GFP_KERNEL)) {",
            "\t\tkfree(cp->elements);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tfor_each_possible_cpu(i)",
            "\t\tcp->elements[i].idx = IDX_INVALID;",
            "",
            "\treturn 0;",
            "}",
            "void cpudl_cleanup(struct cpudl *cp)",
            "{",
            "\tfree_cpumask_var(cp->free_cpus);",
            "\tkfree(cp->elements);",
            "}"
          ],
          "function_name": "cpudl_clear, cpudl_set, cpudl_set_freecpu, cpudl_clear_freecpu, cpudl_init, cpudl_cleanup",
          "description": "提供了 CPU 截止时间数据的增删改查接口，包含堆初始化/清理、CPU 状态更新、截止时间设置与清除操作，通过锁保护保证并发安全性和堆结构一致性。",
          "similarity": 0.4541444778442383
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/cpudeadline.c",
          "start_line": 1,
          "end_line": 9,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  kernel/sched/cpudeadline.c",
            " *",
            " *  Global CPU deadline management",
            " *",
            " *  Author: Juri Lelli <j.lelli@sssup.it>",
            " */",
            ""
          ],
          "function_name": null,
          "description": "此代码块为 cpudeadline 模块的头文件，声明了 GPLv2 许可证并提供模块的基本文档说明，核心功能是定义全局 CPU 截止时间管理机制。",
          "similarity": 0.4117153286933899
        }
      ]
    }
  ]
}