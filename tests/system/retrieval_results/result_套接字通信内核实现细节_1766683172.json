{
  "query": "套接字通信内核实现细节",
  "timestamp": "2025-12-26 01:19:32",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/qspinlock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:45:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\qspinlock.c`\n\n---\n\n# `locking/qspinlock.c` 技术文档\n\n## 1. 文件概述\n\n`qspinlock.c` 实现了 Linux 内核中的 **排队自旋锁（Queued Spinlock）**，这是一种高性能、可扩展的自旋锁机制，旨在替代传统的 ticket spinlock。该实现基于经典的 **MCS 锁（Mellor-Crummey and Scott lock）** 算法，但针对 Linux 内核的 `spinlock_t` 限制（仅 4 字节）进行了高度优化和压缩，同时保留了原有自旋锁的 API 兼容性。其核心目标是在多核系统中减少缓存行争用（cache line bouncing），提升高并发场景下的锁性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct qnode`**  \n  每 CPU 的队列节点结构，封装了 `mcs_spinlock` 节点，并在启用 `CONFIG_PARAVIRT_SPINLOCKS` 时预留额外空间用于半虚拟化支持。每个 CPU 最多维护 `MAX_NODES=4` 个节点，对应最多 4 层嵌套上下文（task、softirq、hardirq、NMI）。\n\n- **`qnodes[MAX_NODES]`**  \n  每 CPU 对齐分配的 `qnode` 数组，确保在 64 位架构上恰好占用一个 64 字节缓存行（半虚拟化模式下占用两个）。\n\n### 关键辅助函数\n\n- **`encode_tail(cpu, idx)`**  \n  将 CPU 编号（+1 以区分无尾状态）和嵌套索引编码为 32 位尾部值，用于表示队列尾节点。\n\n- **`decode_tail(tail)`**  \n  解码尾部值，返回对应的 `mcs_spinlock` 节点指针。\n\n- **`grab_mcs_node(base, idx)`**  \n  从基础 MCS 节点指针偏移获取指定索引的节点。\n\n### 核心锁操作函数（内联）\n\n- **`clear_pending(lock)`**  \n  清除锁的 pending 位（`*,1,* → *,0,*`）。\n\n- **`clear_pending_set_locked(lock)`**  \n  同时清除 pending 位并设置 locked 位，完成锁获取（`*,1,0 → *,0,1`）。\n\n- **`xchg_tail(lock, tail)`**  \n  原子交换锁的尾部字段，返回旧尾部值，用于将当前节点加入等待队列。\n\n- **`queued_fetch_set_pending_acquire(lock)`**  \n  原子获取锁的当前值并设置 pending 位（`*,*,* → *,1,*`），带有获取语义。\n\n- **`set_locked(lock)`**  \n  直接设置 locked 位以获取锁（`*,*,0 → *,0,1`）。\n\n> 注：上述函数根据 `_Q_PENDING_BITS` 是否等于 8 分为两种实现路径，分别优化字节访问和原子位操作。\n\n## 3. 关键实现\n\n### 锁状态压缩设计\n- 传统 MCS 锁需 8 字节尾指针 + 8 字节 next 指针，但 Linux 要求 `spinlock_t` 仅占 4 字节。\n- 本实现将锁状态压缩为 32 位：\n  - **1 字节 locked 字段**：表示锁是否被持有（优化字节写性能）。\n  - **1 字节 pending 字段**：表示是否有第二个竞争者（避免频繁队列操作）。\n  - **2 字节 tail 字段**：编码 `(cpu+1, idx)`，其中 `idx ∈ [0,3]` 表示嵌套层级。\n- 通过 `cpu+1` 编码区分“无尾”（0）和“CPU 0 的尾节点”。\n\n### 快速路径优化\n- **第一个竞争者**：直接自旋在 `locked` 位，无需分配 MCS 节点。\n- **第二个竞争者**：设置 `pending` 位，避免立即进入慢速队列路径。\n- **第三个及以上竞争者**：才真正进入 MCS 队列，通过 `xchg_tail` 原子更新尾指针。\n\n### 嵌套上下文支持\n- 利用每 CPU 的 `qnodes[4]` 数组支持最多 4 层嵌套（task/softirq/hardirq/NMI）。\n- 通过 `idx` 参数在嵌套时选择不同节点，避免递归死锁。\n\n### 架构适配\n- 针对 `_Q_PENDING_BITS == 8`（如 x86）使用字节级原子操作（`WRITE_ONCE`）。\n- 其他架构使用通用原子位操作（`atomic_fetch_or_acquire` 等）。\n- 依赖架构支持 8/16 位原子操作。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/smp.h>`, `<linux/percpu.h>`：SMP 和每 CPU 变量支持。\n  - `<asm/qspinlock.h>`：架构相关的锁布局定义（如 `_Q_*_MASK`）。\n  - `\"mcs_spinlock.h\"`：MCS 锁基础实现。\n  - `\"qspinlock_stat.h\"`：锁统计信息（若启用）。\n- **配置依赖**：\n  - `CONFIG_PARAVIRT_SPINLOCKS`：半虚拟化自旋锁支持（扩展 `qnode` 大小）。\n- **架构要求**：必须支持 8/16 位原子操作（如 x86、ARM64）。\n\n## 5. 使用场景\n\n- **内核通用自旋锁**：作为 `spin_lock()`/`spin_unlock()` 的底层实现，广泛用于内核临界区保护。\n- **高并发场景**：在多核系统中显著优于传统 ticket spinlock，尤其适用于锁竞争激烈的子系统（如内存管理、调度器、文件系统）。\n- **中断上下文**：支持在 hardirq/NMI 等嵌套上下文中安全使用。\n- **半虚拟化环境**：通过 `CONFIG_PARAVIRT_SPINLOCKS` 与 hypervisor 协作减少自旋开销（如 KVM、Xen）。",
      "similarity": 0.5779294967651367,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 116,
          "end_line": 435,
          "content": [
            "static inline __pure u32 encode_tail(int cpu, int idx)",
            "{",
            "\tu32 tail;",
            "",
            "\ttail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;",
            "\ttail |= idx << _Q_TAIL_IDX_OFFSET; /* assume < 4 */",
            "",
            "\treturn tail;",
            "}",
            "static __always_inline void clear_pending(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->pending, 0);",
            "}",
            "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->locked_pending, _Q_LOCKED_VAL);",
            "}",
            "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)",
            "{",
            "\t/*",
            "\t * We can use relaxed semantics since the caller ensures that the",
            "\t * MCS node is properly initialized before updating the tail.",
            "\t */",
            "\treturn (u32)xchg_relaxed(&lock->tail,",
            "\t\t\t\t tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;",
            "}",
            "static __always_inline void clear_pending(struct qspinlock *lock)",
            "{",
            "\tatomic_andnot(_Q_PENDING_VAL, &lock->val);",
            "}",
            "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)",
            "{",
            "\tatomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);",
            "}",
            "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)",
            "{",
            "\tu32 old, new, val = atomic_read(&lock->val);",
            "",
            "\tfor (;;) {",
            "\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;",
            "\t\t/*",
            "\t\t * We can use relaxed semantics since the caller ensures that",
            "\t\t * the MCS node is properly initialized before updating the",
            "\t\t * tail.",
            "\t\t */",
            "\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);",
            "\t\tif (old == val)",
            "\t\t\tbreak;",
            "",
            "\t\tval = old;",
            "\t}",
            "\treturn old;",
            "}",
            "static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)",
            "{",
            "\treturn atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);",
            "}",
            "static __always_inline void set_locked(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);",
            "}",
            "static __always_inline void __pv_init_node(struct mcs_spinlock *node) { }",
            "static __always_inline void __pv_wait_node(struct mcs_spinlock *node,",
            "\t\t\t\t\t   struct mcs_spinlock *prev) { }",
            "static __always_inline void __pv_kick_node(struct qspinlock *lock,",
            "\t\t\t\t\t   struct mcs_spinlock *node) { }",
            "static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,",
            "\t\t\t\t\t\t   struct mcs_spinlock *node)",
            "\t\t\t\t\t\t   { return 0; }",
            "void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)",
            "{",
            "\tstruct mcs_spinlock *prev, *next, *node;",
            "\tu32 old, tail;",
            "\tint idx;",
            "",
            "\tBUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));",
            "",
            "\tif (pv_enabled())",
            "\t\tgoto pv_queue;",
            "",
            "\tif (virt_spin_lock(lock))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Wait for in-progress pending->locked hand-overs with a bounded",
            "\t * number of spins so that we guarantee forward progress.",
            "\t *",
            "\t * 0,1,0 -> 0,0,1",
            "\t */",
            "\tif (val == _Q_PENDING_VAL) {",
            "\t\tint cnt = _Q_PENDING_LOOPS;",
            "\t\tval = atomic_cond_read_relaxed(&lock->val,",
            "\t\t\t\t\t       (VAL != _Q_PENDING_VAL) || !cnt--);",
            "\t}",
            "",
            "\t/*",
            "\t * If we observe any contention; queue.",
            "\t */",
            "\tif (val & ~_Q_LOCKED_MASK)",
            "\t\tgoto queue;",
            "",
            "\t/*",
            "\t * trylock || pending",
            "\t *",
            "\t * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock",
            "\t */",
            "\tval = queued_fetch_set_pending_acquire(lock);",
            "",
            "\t/*",
            "\t * If we observe contention, there is a concurrent locker.",
            "\t *",
            "\t * Undo and queue; our setting of PENDING might have made the",
            "\t * n,0,0 -> 0,0,0 transition fail and it will now be waiting",
            "\t * on @next to become !NULL.",
            "\t */",
            "\tif (unlikely(val & ~_Q_LOCKED_MASK)) {",
            "",
            "\t\t/* Undo PENDING if we set it. */",
            "\t\tif (!(val & _Q_PENDING_MASK))",
            "\t\t\tclear_pending(lock);",
            "",
            "\t\tgoto queue;",
            "\t}",
            "",
            "\t/*",
            "\t * We're pending, wait for the owner to go away.",
            "\t *",
            "\t * 0,1,1 -> *,1,0",
            "\t *",
            "\t * this wait loop must be a load-acquire such that we match the",
            "\t * store-release that clears the locked bit and create lock",
            "\t * sequentiality; this is because not all",
            "\t * clear_pending_set_locked() implementations imply full",
            "\t * barriers.",
            "\t */",
            "\tif (val & _Q_LOCKED_MASK)",
            "\t\tsmp_cond_load_acquire(&lock->locked, !VAL);",
            "",
            "\t/*",
            "\t * take ownership and clear the pending bit.",
            "\t *",
            "\t * 0,1,0 -> 0,0,1",
            "\t */",
            "\tclear_pending_set_locked(lock);",
            "\tlockevent_inc(lock_pending);",
            "\treturn;",
            "",
            "\t/*",
            "\t * End of pending bit optimistic spinning and beginning of MCS",
            "\t * queuing.",
            "\t */",
            "queue:",
            "\tlockevent_inc(lock_slowpath);",
            "pv_queue:",
            "\tnode = this_cpu_ptr(&qnodes[0].mcs);",
            "\tidx = node->count++;",
            "\ttail = encode_tail(smp_processor_id(), idx);",
            "",
            "\ttrace_contention_begin(lock, LCB_F_SPIN);",
            "",
            "\t/*",
            "\t * 4 nodes are allocated based on the assumption that there will",
            "\t * not be nested NMIs taking spinlocks. That may not be true in",
            "\t * some architectures even though the chance of needing more than",
            "\t * 4 nodes will still be extremely unlikely. When that happens,",
            "\t * we fall back to spinning on the lock directly without using",
            "\t * any MCS node. This is not the most elegant solution, but is",
            "\t * simple enough.",
            "\t */",
            "\tif (unlikely(idx >= MAX_NODES)) {",
            "\t\tlockevent_inc(lock_no_node);",
            "\t\twhile (!queued_spin_trylock(lock))",
            "\t\t\tcpu_relax();",
            "\t\tgoto release;",
            "\t}",
            "",
            "\tnode = grab_mcs_node(node, idx);",
            "",
            "\t/*",
            "\t * Keep counts of non-zero index values:",
            "\t */",
            "\tlockevent_cond_inc(lock_use_node2 + idx - 1, idx);",
            "",
            "\t/*",
            "\t * Ensure that we increment the head node->count before initialising",
            "\t * the actual node. If the compiler is kind enough to reorder these",
            "\t * stores, then an IRQ could overwrite our assignments.",
            "\t */",
            "\tbarrier();",
            "",
            "\tnode->locked = 0;",
            "\tnode->next = NULL;",
            "\tpv_init_node(node);",
            "",
            "\t/*",
            "\t * We touched a (possibly) cold cacheline in the per-cpu queue node;",
            "\t * attempt the trylock once more in the hope someone let go while we",
            "\t * weren't watching.",
            "\t */",
            "\tif (queued_spin_trylock(lock))",
            "\t\tgoto release;",
            "",
            "\t/*",
            "\t * Ensure that the initialisation of @node is complete before we",
            "\t * publish the updated tail via xchg_tail() and potentially link",
            "\t * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\t/*",
            "\t * Publish the updated tail.",
            "\t * We have already touched the queueing cacheline; don't bother with",
            "\t * pending stuff.",
            "\t *",
            "\t * p,*,* -> n,*,*",
            "\t */",
            "\told = xchg_tail(lock, tail);",
            "\tnext = NULL;",
            "",
            "\t/*",
            "\t * if there was a previous node; link it and wait until reaching the",
            "\t * head of the waitqueue.",
            "\t */",
            "\tif (old & _Q_TAIL_MASK) {",
            "\t\tprev = decode_tail(old);",
            "",
            "\t\t/* Link @node into the waitqueue. */",
            "\t\tWRITE_ONCE(prev->next, node);",
            "",
            "\t\tpv_wait_node(node, prev);",
            "\t\tarch_mcs_spin_lock_contended(&node->locked);",
            "",
            "\t\t/*",
            "\t\t * While waiting for the MCS lock, the next pointer may have",
            "\t\t * been set by another lock waiter. We optimistically load",
            "\t\t * the next pointer & prefetch the cacheline for writing",
            "\t\t * to reduce latency in the upcoming MCS unlock operation.",
            "\t\t */",
            "\t\tnext = READ_ONCE(node->next);",
            "\t\tif (next)",
            "\t\t\tprefetchw(next);",
            "\t}",
            "",
            "\t/*",
            "\t * we're at the head of the waitqueue, wait for the owner & pending to",
            "\t * go away.",
            "\t *",
            "\t * *,x,y -> *,0,0",
            "\t *",
            "\t * this wait loop must use a load-acquire such that we match the",
            "\t * store-release that clears the locked bit and create lock",
            "\t * sequentiality; this is because the set_locked() function below",
            "\t * does not imply a full barrier.",
            "\t *",
            "\t * The PV pv_wait_head_or_lock function, if active, will acquire",
            "\t * the lock and return a non-zero value. So we have to skip the",
            "\t * atomic_cond_read_acquire() call. As the next PV queue head hasn't",
            "\t * been designated yet, there is no way for the locked value to become",
            "\t * _Q_SLOW_VAL. So both the set_locked() and the",
            "\t * atomic_cmpxchg_relaxed() calls will be safe.",
            "\t *",
            "\t * If PV isn't active, 0 will be returned instead.",
            "\t *",
            "\t */",
            "\tif ((val = pv_wait_head_or_lock(lock, node)))",
            "\t\tgoto locked;",
            "",
            "\tval = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));",
            "",
            "locked:",
            "\t/*",
            "\t * claim the lock:",
            "\t *",
            "\t * n,0,0 -> 0,0,1 : lock, uncontended",
            "\t * *,*,0 -> *,*,1 : lock, contended",
            "\t *",
            "\t * If the queue head is the only one in the queue (lock value == tail)",
            "\t * and nobody is pending, clear the tail code and grab the lock.",
            "\t * Otherwise, we only need to grab the lock.",
            "\t */",
            "",
            "\t/*",
            "\t * In the PV case we might already have _Q_LOCKED_VAL set, because",
            "\t * of lock stealing; therefore we must also allow:",
            "\t *",
            "\t * n,0,1 -> 0,0,1",
            "\t *",
            "\t * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the",
            "\t *       above wait condition, therefore any concurrent setting of",
            "\t *       PENDING will make the uncontended transition fail.",
            "\t */",
            "\tif ((val & _Q_TAIL_MASK) == tail) {",
            "\t\tif (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))",
            "\t\t\tgoto release; /* No contention */",
            "\t}",
            "",
            "\t/*",
            "\t * Either somebody is queued behind us or _Q_PENDING_VAL got set",
            "\t * which will then detect the remaining tail and queue behind us",
            "\t * ensuring we'll see a @next.",
            "\t */",
            "\tset_locked(lock);",
            "",
            "\t/*",
            "\t * contended path; wait for next if not observed yet, release.",
            "\t */",
            "\tif (!next)",
            "\t\tnext = smp_cond_load_relaxed(&node->next, (VAL));",
            "",
            "\tarch_mcs_spin_unlock_contended(&next->locked);",
            "\tpv_kick_node(lock, next);",
            "",
            "release:",
            "\ttrace_contention_end(lock, 0);",
            "",
            "\t/*",
            "\t * release the node",
            "\t */",
            "\t__this_cpu_dec(qnodes[0].mcs.count);",
            "}"
          ],
          "function_name": "encode_tail, clear_pending, clear_pending_set_locked, xchg_tail, clear_pending, clear_pending_set_locked, xchg_tail, queued_fetch_set_pending_acquire, set_locked, __pv_init_node, __pv_wait_node, __pv_kick_node, __pv_wait_head_or_lock, queued_spin_lock_slowpath",
          "description": "实现了qspinlock的核心状态转换函数和慢路径获取逻辑，包含尾部编码、挂起状态清除、锁状态设置等原子操作，并通过MCS队列处理锁竞争，支持硬中断、软中断等嵌套场景的递归控制",
          "similarity": 0.5686929225921631
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 1,
          "end_line": 115,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Queued spinlock",
            " *",
            " * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.",
            " * (C) Copyright 2013-2014,2018 Red Hat, Inc.",
            " * (C) Copyright 2015 Intel Corp.",
            " * (C) Copyright 2015 Hewlett-Packard Enterprise Development LP",
            " *",
            " * Authors: Waiman Long <longman@redhat.com>",
            " *          Peter Zijlstra <peterz@infradead.org>",
            " */",
            "",
            "#ifndef _GEN_PV_LOCK_SLOWPATH",
            "",
            "#include <linux/smp.h>",
            "#include <linux/bug.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/percpu.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/mutex.h>",
            "#include <linux/prefetch.h>",
            "#include <asm/byteorder.h>",
            "#include <asm/qspinlock.h>",
            "#include <trace/events/lock.h>",
            "",
            "/*",
            " * Include queued spinlock statistics code",
            " */",
            "#include \"qspinlock_stat.h\"",
            "",
            "/*",
            " * The basic principle of a queue-based spinlock can best be understood",
            " * by studying a classic queue-based spinlock implementation called the",
            " * MCS lock. A copy of the original MCS lock paper (\"Algorithms for Scalable",
            " * Synchronization on Shared-Memory Multiprocessors by Mellor-Crummey and",
            " * Scott\") is available at",
            " *",
            " * https://bugzilla.kernel.org/show_bug.cgi?id=206115",
            " *",
            " * This queued spinlock implementation is based on the MCS lock, however to",
            " * make it fit the 4 bytes we assume spinlock_t to be, and preserve its",
            " * existing API, we must modify it somehow.",
            " *",
            " * In particular; where the traditional MCS lock consists of a tail pointer",
            " * (8 bytes) and needs the next pointer (another 8 bytes) of its own node to",
            " * unlock the next pending (next->locked), we compress both these: {tail,",
            " * next->locked} into a single u32 value.",
            " *",
            " * Since a spinlock disables recursion of its own context and there is a limit",
            " * to the contexts that can nest; namely: task, softirq, hardirq, nmi. As there",
            " * are at most 4 nesting levels, it can be encoded by a 2-bit number. Now",
            " * we can encode the tail by combining the 2-bit nesting level with the cpu",
            " * number. With one byte for the lock value and 3 bytes for the tail, only a",
            " * 32-bit word is now needed. Even though we only need 1 bit for the lock,",
            " * we extend it to a full byte to achieve better performance for architectures",
            " * that support atomic byte write.",
            " *",
            " * We also change the first spinner to spin on the lock bit instead of its",
            " * node; whereby avoiding the need to carry a node from lock to unlock, and",
            " * preserving existing lock API. This also makes the unlock code simpler and",
            " * faster.",
            " *",
            " * N.B. The current implementation only supports architectures that allow",
            " *      atomic operations on smaller 8-bit and 16-bit data types.",
            " *",
            " */",
            "",
            "#include \"mcs_spinlock.h\"",
            "#define MAX_NODES\t4",
            "",
            "/*",
            " * On 64-bit architectures, the mcs_spinlock structure will be 16 bytes in",
            " * size and four of them will fit nicely in one 64-byte cacheline. For",
            " * pvqspinlock, however, we need more space for extra data. To accommodate",
            " * that, we insert two more long words to pad it up to 32 bytes. IOW, only",
            " * two of them can fit in a cacheline in this case. That is OK as it is rare",
            " * to have more than 2 levels of slowpath nesting in actual use. We don't",
            " * want to penalize pvqspinlocks to optimize for a rare case in native",
            " * qspinlocks.",
            " */",
            "struct qnode {",
            "\tstruct mcs_spinlock mcs;",
            "#ifdef CONFIG_PARAVIRT_SPINLOCKS",
            "\tlong reserved[2];",
            "#endif",
            "};",
            "",
            "/*",
            " * The pending bit spinning loop count.",
            " * This heuristic is used to limit the number of lockword accesses",
            " * made by atomic_cond_read_relaxed when waiting for the lock to",
            " * transition out of the \"== _Q_PENDING_VAL\" state. We don't spin",
            " * indefinitely because there's no guarantee that we'll make forward",
            " * progress.",
            " */",
            "#ifndef _Q_PENDING_LOOPS",
            "#define _Q_PENDING_LOOPS\t1",
            "#endif",
            "",
            "/*",
            " * Per-CPU queue node structures; we can never have more than 4 nested",
            " * contexts: task, softirq, hardirq, nmi.",
            " *",
            " * Exactly fits one 64-byte cacheline on a 64-bit architecture.",
            " *",
            " * PV doubles the storage and uses the second cacheline for PV state.",
            " */",
            "static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);",
            "",
            "/*",
            " * We must be able to distinguish between no-tail and the tail at 0:0,",
            " * therefore increment the cpu number by one.",
            " */",
            ""
          ],
          "function_name": null,
          "description": "定义了qspinlock的队列节点结构体qnode及其per-CPU数组，用于支持paravirtualization的锁机制，通过压缩尾部指针与锁状态到单个32位值，结合MCS锁算法实现可扩展同步",
          "similarity": 0.5107113718986511
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 590,
          "end_line": 594,
          "content": [
            "static __init int parse_nopvspin(char *arg)",
            "{",
            "\tnopvspin = true;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "parse_nopvspin",
          "description": "解析内核启动参数以禁用paravirtualization锁机制的初始化函数，通过设置nopvspin标志位控制是否启用特定的虚拟化架构优化特性",
          "similarity": 0.5008218884468079
        }
      ]
    },
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.5702670812606812,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.5948442220687866
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.5224155187606812
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.4990350604057312
        }
      ]
    },
    {
      "source_file": "kernel/irq/irqdesc.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:59:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\irqdesc.c`\n\n---\n\n# `irq/irqdesc.c` 技术文档\n\n## 1. 文件概述\n\n`irq/irqdesc.c` 是 Linux 内核通用中断子系统（Generic IRQ）的核心实现文件之一，负责中断描述符（`struct irq_desc`）的分配、初始化、管理和释放。该文件实现了中断描述符的生命周期管理，包括在稀疏 IRQ（`CONFIG_SPARSE_IRQ`）配置下的动态分配机制，以及与 SMP（对称多处理）相关的中断亲和性（affinity）管理。它为上层中断处理（如设备驱动注册中断处理函数）和底层硬件中断控制器（通过 `irq_chip`）之间提供了统一的抽象层。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct irq_desc`**：中断描述符，代表一个逻辑中断号（IRQ number），包含中断状态、处理函数、统计信息、锁、亲和性掩码等。\n- **`struct irq_data`**：嵌入在 `irq_desc` 中，包含与硬件中断控制器相关的数据（如 `irq_chip`、`hwirq`、`irq_domain` 等）。\n- **`struct irq_common_data`**：`irq_desc` 和 `irq_data` 共享的数据，如 MSI 描述符、亲和性掩码等。\n- **`sparse_irqs`**：基于 Maple Tree 的稀疏 IRQ 描述符存储结构，用于动态分配 IRQ 号。\n\n### 主要函数\n- **`init_desc()`**：初始化一个 `irq_desc` 实例，包括分配 per-CPU 统计结构、SMP 掩码、初始化锁和默认值。\n- **`desc_set_defaults()`**：设置 `irq_desc` 的默认初始状态（如禁用、屏蔽、默认处理函数为 `handle_bad_irq`）。\n- **`alloc_masks()` / `free_masks()` / `desc_smp_init()`**：SMP 相关的亲和性掩码（affinity、effective_affinity、pending_mask）的分配、释放和初始化。\n- **`irq_find_free_area()` / `irq_find_at_or_after()`**：在稀疏 IRQ 模式下查找可用的 IRQ 号范围或下一个可用 IRQ。\n- **`irq_insert_desc()` / `delete_irq_desc()`**：将 `irq_desc` 插入或从稀疏 IRQ 的 Maple Tree 中删除。\n- **`init_irq_default_affinity()`**：初始化默认的中断亲和性掩码（通常为所有 CPU）。\n- **`irq_kobj_release()` 及相关 sysfs 属性函数**：实现 IRQ 描述符的 sysfs 接口（如 `per_cpu_count`、`chip_name`、`hwirq` 等）。\n\n### 全局变量\n- **`nr_irqs`**：系统支持的最大 IRQ 数量，可被平台代码覆盖。\n- **`irq_default_affinity`**：默认的中断亲和性 CPU 掩码（SMP 模式下）。\n- **`irq_desc_lock_class`**：用于 lockdep 的 IRQ 描述符自旋锁的统一锁类。\n\n## 3. 关键实现\n\n### 稀疏 IRQ 管理（`CONFIG_SPARSE_IRQ`）\n- 使用 **Maple Tree** 数据结构（`sparse_irqs`）替代传统的静态数组，支持动态分配 IRQ 描述符。\n- `irq_find_free_area()` 利用 Maple Tree 的空闲区间查找功能，高效分配连续的 IRQ 号。\n- `irq_insert_desc()` 和 `delete_irq_desc()` 通过 RCU 安全地插入/删除描述符，支持运行时 IRQ 的动态增删。\n- 每个 `irq_desc` 作为独立的 kobject，通过 sysfs 暴露属性（如中断计数、芯片名称等）。\n\n### SMP 中断亲和性\n- **亲和性掩码**：每个 IRQ 可配置其允许运行的 CPU 集合（`affinity`），支持负载均衡和局部性优化。\n- **有效亲和性**（`effective_affinity`）：实际生效的亲和性（可能受中断迁移或 pending 状态影响）。\n- **Pending 掩码**（`pending_mask`）：用于在中断迁移过程中暂存中断事件。\n- 启动参数 `irqaffinity=` 可设置全局默认亲和性，但至少包含引导 CPU 以防配置错误。\n\n### 描述符初始化\n- `init_desc()` 完成描述符的完整初始化：\n  - 分配 per-CPU 中断统计结构（`kstat_irqs`）。\n  - 初始化 SMP 相关掩码（若启用）。\n  - 设置自旋锁（带 lockdep 类）和互斥锁（`request_mutex`）。\n  - 调用 `desc_set_defaults()` 设置默认状态（禁用、屏蔽、无效处理函数）。\n  - 初始化 RCU 回调（用于稀疏 IRQ 的延迟释放）。\n\n### 锁与并发控制\n- **`desc->lock`**：raw spinlock，保护描述符关键字段（如状态、处理函数），在中断上下文中使用。\n- **`desc->request_mutex`**：mutex，用于串行化中断请求/释放操作（如 `request_irq()`）。\n- **Maple Tree 操作**：通过外部互斥锁（`sparse_irq_lock`）和 RCU 保证并发安全。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/irq.h>`、`<linux/interrupt.h>`：IRQ 子系统核心 API 和数据结构。\n  - `<linux/irqdomain.h>`：硬件中断号（hwirq）到逻辑 IRQ 号的映射。\n  - `<linux/maple_tree.h>`：稀疏 IRQ 的底层存储实现。\n  - `<linux/sysfs.h>`：sysfs 属性支持。\n  - `\"internals.h\"`：IRQ 子系统内部函数和宏。\n- **配置依赖**：\n  - `CONFIG_SMP`：启用多处理器支持（亲和性掩码管理）。\n  - `CONFIG_SPARSE_IRQ`：启用动态 IRQ 分配（替代静态数组）。\n  - `CONFIG_GENERIC_PENDING_IRQ` / `CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK`：扩展的 SMP 中断管理功能。\n- **模块交互**：\n  - **中断控制器驱动**：通过 `irq_chip` 操作硬件，依赖 `irq_desc` 提供的抽象。\n  - **设备驱动**：通过 `request_irq()` 等接口注册中断处理函数，操作 `irq_desc`。\n  - **电源管理**：通过 `wakeup` 属性控制中断的唤醒能力。\n\n## 5. 使用场景\n\n- **系统启动阶段**：\n  - 初始化默认中断亲和性（`init_irq_default_affinity()`）。\n  - 预分配或动态创建平台所需的 IRQ 描述符（通过 `alloc_descs()` 等）。\n- **设备驱动加载/卸载**：\n  - 动态分配 IRQ 描述符（稀疏 IRQ 模式下通过 `irq_alloc_desc()`）。\n  - 注册/注销中断处理函数（修改 `handle_irq` 和 action 链表）。\n- **运行时中断管理**：\n  - 修改中断亲和性（`/proc/irq/<n>/smp_affinity`）。\n  - 查询中断统计信息（`/proc/interrupts`，通过 per-CPU 计数）。\n  - 通过 sysfs 查看 IRQ 属性（芯片名称、硬件 IRQ 号、触发类型等）。\n- **中断迁移**（SMP）：\n  - 在 CPU 热插拔或负载均衡时，更新 `affinity` 和 `pending_mask`。\n- **错误处理**：\n  - 未处理的中断由 `handle_bad_irq` 处理，记录到 `irqs_unhandled`。",
      "similarity": 0.5699388980865479,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 27,
          "end_line": 127,
          "content": [
            "static int __init irq_affinity_setup(char *str)",
            "{",
            "\talloc_bootmem_cpumask_var(&irq_default_affinity);",
            "\tcpulist_parse(str, irq_default_affinity);",
            "\t/*",
            "\t * Set at least the boot cpu. We don't want to end up with",
            "\t * bugreports caused by random commandline masks",
            "\t */",
            "\tcpumask_set_cpu(smp_processor_id(), irq_default_affinity);",
            "\treturn 1;",
            "}",
            "static void __init init_irq_default_affinity(void)",
            "{",
            "\tif (!cpumask_available(irq_default_affinity))",
            "\t\tzalloc_cpumask_var(&irq_default_affinity, GFP_NOWAIT);",
            "\tif (cpumask_empty(irq_default_affinity))",
            "\t\tcpumask_setall(irq_default_affinity);",
            "}",
            "static void __init init_irq_default_affinity(void)",
            "{",
            "}",
            "static int alloc_masks(struct irq_desc *desc, int node)",
            "{",
            "\tif (!zalloc_cpumask_var_node(&desc->irq_common_data.affinity,",
            "\t\t\t\t     GFP_KERNEL, node))",
            "\t\treturn -ENOMEM;",
            "",
            "#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK",
            "\tif (!zalloc_cpumask_var_node(&desc->irq_common_data.effective_affinity,",
            "\t\t\t\t     GFP_KERNEL, node)) {",
            "\t\tfree_cpumask_var(desc->irq_common_data.affinity);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "#endif",
            "",
            "#ifdef CONFIG_GENERIC_PENDING_IRQ",
            "\tif (!zalloc_cpumask_var_node(&desc->pending_mask, GFP_KERNEL, node)) {",
            "#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK",
            "\t\tfree_cpumask_var(desc->irq_common_data.effective_affinity);",
            "#endif",
            "\t\tfree_cpumask_var(desc->irq_common_data.affinity);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "#endif",
            "\treturn 0;",
            "}",
            "static void desc_smp_init(struct irq_desc *desc, int node,",
            "\t\t\t  const struct cpumask *affinity)",
            "{",
            "\tif (!affinity)",
            "\t\taffinity = irq_default_affinity;",
            "\tcpumask_copy(desc->irq_common_data.affinity, affinity);",
            "",
            "#ifdef CONFIG_GENERIC_PENDING_IRQ",
            "\tcpumask_clear(desc->pending_mask);",
            "#endif",
            "#ifdef CONFIG_NUMA",
            "\tdesc->irq_common_data.node = node;",
            "#endif",
            "}",
            "static void free_masks(struct irq_desc *desc)",
            "{",
            "#ifdef CONFIG_GENERIC_PENDING_IRQ",
            "\tfree_cpumask_var(desc->pending_mask);",
            "#endif",
            "\tfree_cpumask_var(desc->irq_common_data.affinity);",
            "#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK",
            "\tfree_cpumask_var(desc->irq_common_data.effective_affinity);",
            "#endif",
            "}",
            "static inline int",
            "alloc_masks(struct irq_desc *desc, int node) { return 0; }",
            "static inline void",
            "desc_smp_init(struct irq_desc *desc, int node, const struct cpumask *affinity) { }",
            "static inline void free_masks(struct irq_desc *desc) { }",
            "static void desc_set_defaults(unsigned int irq, struct irq_desc *desc, int node,",
            "\t\t\t      const struct cpumask *affinity, struct module *owner)",
            "{",
            "\tint cpu;",
            "",
            "\tdesc->irq_common_data.handler_data = NULL;",
            "\tdesc->irq_common_data.msi_desc = NULL;",
            "",
            "\tdesc->irq_data.common = &desc->irq_common_data;",
            "\tdesc->irq_data.irq = irq;",
            "\tdesc->irq_data.chip = &no_irq_chip;",
            "\tdesc->irq_data.chip_data = NULL;",
            "\tirq_settings_clr_and_set(desc, ~0, _IRQ_DEFAULT_INIT_FLAGS);",
            "\tirqd_set(&desc->irq_data, IRQD_IRQ_DISABLED);",
            "\tirqd_set(&desc->irq_data, IRQD_IRQ_MASKED);",
            "\tdesc->handle_irq = handle_bad_irq;",
            "\tdesc->depth = 1;",
            "\tdesc->irq_count = 0;",
            "\tdesc->irqs_unhandled = 0;",
            "\tdesc->tot_count = 0;",
            "\tdesc->name = NULL;",
            "\tdesc->owner = owner;",
            "\tfor_each_possible_cpu(cpu)",
            "\t\t*per_cpu_ptr(desc->kstat_irqs, cpu) = (struct irqstat) { };",
            "\tdesc_smp_init(desc, node, affinity);",
            "}"
          ],
          "function_name": "irq_affinity_setup, init_irq_default_affinity, init_irq_default_affinity, alloc_masks, desc_smp_init, free_masks, alloc_masks, desc_smp_init, free_masks, desc_set_defaults",
          "description": "包含中断亲和性初始化与内存分配相关函数，负责设置默认CPU亲和掩码、分配irq_desc结构体的affinity字段、初始化SMP相关信息及释放相关资源。存在多处函数重载实现，体现不同配置条件下的差异化处理。",
          "similarity": 0.5536587238311768
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 1,
          "end_line": 26,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 1992, 1998-2006 Linus Torvalds, Ingo Molnar",
            " * Copyright (C) 2005-2006, Thomas Gleixner, Russell King",
            " *",
            " * This file contains the interrupt descriptor management code. Detailed",
            " * information is available in Documentation/core-api/genericirq.rst",
            " *",
            " */",
            "#include <linux/irq.h>",
            "#include <linux/slab.h>",
            "#include <linux/export.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/maple_tree.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/sysfs.h>",
            "",
            "#include \"internals.h\"",
            "",
            "/*",
            " * lockdep: we want to handle all irq_desc locks as a single lock-class:",
            " */",
            "static struct lock_class_key irq_desc_lock_class;",
            "",
            "#if defined(CONFIG_SMP)"
          ],
          "function_name": null,
          "description": "此代码块定义了中断描述符管理模块的头部信息，声明了用于锁分类的key变量irq_desc_lock_class，并包含SMP支持的相关代码片段。由于上下文不完整，无法确定所有功能细节。",
          "similarity": 0.5183483958244324
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 422,
          "end_line": 523,
          "content": [
            "static void irq_sysfs_add(int irq, struct irq_desc *desc) {}",
            "static void irq_sysfs_del(struct irq_desc *desc) {}",
            "void irq_lock_sparse(void)",
            "{",
            "\tmutex_lock(&sparse_irq_lock);",
            "}",
            "void irq_unlock_sparse(void)",
            "{",
            "\tmutex_unlock(&sparse_irq_lock);",
            "}",
            "static void irq_kobj_release(struct kobject *kobj)",
            "{",
            "\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);",
            "",
            "\tfree_masks(desc);",
            "\tfree_percpu(desc->kstat_irqs);",
            "\tkfree(desc);",
            "}",
            "static void delayed_free_desc(struct rcu_head *rhp)",
            "{",
            "\tstruct irq_desc *desc = container_of(rhp, struct irq_desc, rcu);",
            "",
            "\tkobject_put(&desc->kobj);",
            "}",
            "static void free_desc(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tirq_remove_debugfs_entry(desc);",
            "\tunregister_irq_proc(irq, desc);",
            "",
            "\t/*",
            "\t * sparse_irq_lock protects also show_interrupts() and",
            "\t * kstat_irq_usr(). Once we deleted the descriptor from the",
            "\t * sparse tree we can free it. Access in proc will fail to",
            "\t * lookup the descriptor.",
            "\t *",
            "\t * The sysfs entry must be serialized against a concurrent",
            "\t * irq_sysfs_init() as well.",
            "\t */",
            "\tirq_sysfs_del(desc);",
            "\tdelete_irq_desc(irq);",
            "",
            "\t/*",
            "\t * We free the descriptor, masks and stat fields via RCU. That",
            "\t * allows demultiplex interrupts to do rcu based management of",
            "\t * the child interrupts.",
            "\t * This also allows us to use rcu in kstat_irqs_usr().",
            "\t */",
            "\tcall_rcu(&desc->rcu, delayed_free_desc);",
            "}",
            "static int alloc_descs(unsigned int start, unsigned int cnt, int node,",
            "\t\t       const struct irq_affinity_desc *affinity,",
            "\t\t       struct module *owner)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint i;",
            "",
            "\t/* Validate affinity mask(s) */",
            "\tif (affinity) {",
            "\t\tfor (i = 0; i < cnt; i++) {",
            "\t\t\tif (cpumask_empty(&affinity[i].mask))",
            "\t\t\t\treturn -EINVAL;",
            "\t\t}",
            "\t}",
            "",
            "\tfor (i = 0; i < cnt; i++) {",
            "\t\tconst struct cpumask *mask = NULL;",
            "\t\tunsigned int flags = 0;",
            "",
            "\t\tif (affinity) {",
            "\t\t\tif (affinity->is_managed) {",
            "\t\t\t\tflags = IRQD_AFFINITY_MANAGED |",
            "\t\t\t\t\tIRQD_MANAGED_SHUTDOWN;",
            "\t\t\t}",
            "\t\t\tflags |= IRQD_AFFINITY_SET;",
            "\t\t\tmask = &affinity->mask;",
            "\t\t\tnode = cpu_to_node(cpumask_first(mask));",
            "\t\t\taffinity++;",
            "\t\t}",
            "",
            "\t\tdesc = alloc_desc(start + i, node, flags, mask, owner);",
            "\t\tif (!desc)",
            "\t\t\tgoto err;",
            "\t\tirq_insert_desc(start + i, desc);",
            "\t\tirq_sysfs_add(start + i, desc);",
            "\t\tirq_add_debugfs_entry(start + i, desc);",
            "\t}",
            "\treturn start;",
            "",
            "err:",
            "\tfor (i--; i >= 0; i--)",
            "\t\tfree_desc(start + i);",
            "\treturn -ENOMEM;",
            "}",
            "static int irq_expand_nr_irqs(unsigned int nr)",
            "{",
            "\tif (nr > MAX_SPARSE_IRQS)",
            "\t\treturn -ENOMEM;",
            "\tnr_irqs = nr;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "irq_sysfs_add, irq_sysfs_del, irq_lock_sparse, irq_unlock_sparse, irq_kobj_release, delayed_free_desc, free_desc, alloc_descs, irq_expand_nr_irqs",
          "description": "包含中断描述符的延迟释放机制与批量分配逻辑，利用RCU机制安全释放资源，实现中断描述符的动态扩展与回收，支持多CPU环境下对中断资源的高效管理。",
          "similarity": 0.5170746445655823
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 278,
          "end_line": 385,
          "content": [
            "static ssize_t type_show(struct kobject *kobj,",
            "\t\t\t struct kobj_attribute *attr, char *buf)",
            "{",
            "\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);",
            "\tssize_t ret = 0;",
            "",
            "\traw_spin_lock_irq(&desc->lock);",
            "\tret = sprintf(buf, \"%s\\n\",",
            "\t\t      irqd_is_level_type(&desc->irq_data) ? \"level\" : \"edge\");",
            "\traw_spin_unlock_irq(&desc->lock);",
            "",
            "\treturn ret;",
            "",
            "}",
            "static ssize_t wakeup_show(struct kobject *kobj,",
            "\t\t\t   struct kobj_attribute *attr, char *buf)",
            "{",
            "\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);",
            "\tssize_t ret = 0;",
            "",
            "\traw_spin_lock_irq(&desc->lock);",
            "\tret = sprintf(buf, \"%s\\n\",",
            "\t\t      irqd_is_wakeup_set(&desc->irq_data) ? \"enabled\" : \"disabled\");",
            "\traw_spin_unlock_irq(&desc->lock);",
            "",
            "\treturn ret;",
            "",
            "}",
            "static ssize_t name_show(struct kobject *kobj,",
            "\t\t\t struct kobj_attribute *attr, char *buf)",
            "{",
            "\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);",
            "\tssize_t ret = 0;",
            "",
            "\traw_spin_lock_irq(&desc->lock);",
            "\tif (desc->name)",
            "\t\tret = scnprintf(buf, PAGE_SIZE, \"%s\\n\", desc->name);",
            "\traw_spin_unlock_irq(&desc->lock);",
            "",
            "\treturn ret;",
            "}",
            "static ssize_t actions_show(struct kobject *kobj,",
            "\t\t\t    struct kobj_attribute *attr, char *buf)",
            "{",
            "\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);",
            "\tstruct irqaction *action;",
            "\tssize_t ret = 0;",
            "\tchar *p = \"\";",
            "",
            "\traw_spin_lock_irq(&desc->lock);",
            "\tfor_each_action_of_desc(desc, action) {",
            "\t\tret += scnprintf(buf + ret, PAGE_SIZE - ret, \"%s%s\",",
            "\t\t\t\t p, action->name);",
            "\t\tp = \",\";",
            "\t}",
            "\traw_spin_unlock_irq(&desc->lock);",
            "",
            "\tif (ret)",
            "\t\tret += scnprintf(buf + ret, PAGE_SIZE - ret, \"\\n\");",
            "",
            "\treturn ret;",
            "}",
            "static void irq_sysfs_add(int irq, struct irq_desc *desc)",
            "{",
            "\tif (irq_kobj_base) {",
            "\t\t/*",
            "\t\t * Continue even in case of failure as this is nothing",
            "\t\t * crucial and failures in the late irq_sysfs_init()",
            "\t\t * cannot be rolled back.",
            "\t\t */",
            "\t\tif (kobject_add(&desc->kobj, irq_kobj_base, \"%d\", irq))",
            "\t\t\tpr_warn(\"Failed to add kobject for irq %d\\n\", irq);",
            "\t\telse",
            "\t\t\tdesc->istate |= IRQS_SYSFS;",
            "\t}",
            "}",
            "static void irq_sysfs_del(struct irq_desc *desc)",
            "{",
            "\t/*",
            "\t * Only invoke kobject_del() when kobject_add() was successfully",
            "\t * invoked for the descriptor. This covers both early boot, where",
            "\t * sysfs is not initialized yet, and the case of a failed",
            "\t * kobject_add() invocation.",
            "\t */",
            "\tif (desc->istate & IRQS_SYSFS)",
            "\t\tkobject_del(&desc->kobj);",
            "}",
            "static int __init irq_sysfs_init(void)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint irq;",
            "",
            "\t/* Prevent concurrent irq alloc/free */",
            "\tirq_lock_sparse();",
            "",
            "\tirq_kobj_base = kobject_create_and_add(\"irq\", kernel_kobj);",
            "\tif (!irq_kobj_base) {",
            "\t\tirq_unlock_sparse();",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\t/* Add the already allocated interrupts */",
            "\tfor_each_irq_desc(irq, desc)",
            "\t\tirq_sysfs_add(irq, desc);",
            "\tirq_unlock_sparse();",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "type_show, wakeup_show, name_show, actions_show, irq_sysfs_add, irq_sysfs_del, irq_sysfs_init",
          "description": "通过sysfs接口暴露中断属性信息，包含中断类型、唤醒状态、名称、连接动作等展示函数，以及管理sysfs节点的添加删除操作，实现了中断设备的系统级调试信息导出功能。",
          "similarity": 0.5082836151123047
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 151,
          "end_line": 252,
          "content": [
            "static int irq_find_free_area(unsigned int from, unsigned int cnt)",
            "{",
            "\tMA_STATE(mas, &sparse_irqs, 0, 0);",
            "",
            "\tif (mas_empty_area(&mas, from, MAX_SPARSE_IRQS, cnt))",
            "\t\treturn -ENOSPC;",
            "\treturn mas.index;",
            "}",
            "static unsigned int irq_find_at_or_after(unsigned int offset)",
            "{",
            "\tunsigned long index = offset;",
            "\tstruct irq_desc *desc;",
            "",
            "\tguard(rcu)();",
            "\tdesc = mt_find(&sparse_irqs, &index, nr_irqs);",
            "",
            "\treturn desc ? irq_desc_get_irq(desc) : nr_irqs;",
            "}",
            "static void irq_insert_desc(unsigned int irq, struct irq_desc *desc)",
            "{",
            "\tMA_STATE(mas, &sparse_irqs, irq, irq);",
            "\tWARN_ON(mas_store_gfp(&mas, desc, GFP_KERNEL) != 0);",
            "}",
            "static void delete_irq_desc(unsigned int irq)",
            "{",
            "\tMA_STATE(mas, &sparse_irqs, irq, irq);",
            "\tmas_erase(&mas);",
            "}",
            "static int init_desc(struct irq_desc *desc, int irq, int node,",
            "\t\t     unsigned int flags,",
            "\t\t     const struct cpumask *affinity,",
            "\t\t     struct module *owner)",
            "{",
            "\tdesc->kstat_irqs = alloc_percpu(struct irqstat);",
            "\tif (!desc->kstat_irqs)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (alloc_masks(desc, node)) {",
            "\t\tfree_percpu(desc->kstat_irqs);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\traw_spin_lock_init(&desc->lock);",
            "\tlockdep_set_class(&desc->lock, &irq_desc_lock_class);",
            "\tmutex_init(&desc->request_mutex);",
            "\tinit_waitqueue_head(&desc->wait_for_threads);",
            "\tdesc_set_defaults(irq, desc, node, affinity, owner);",
            "\tirqd_set(&desc->irq_data, flags);",
            "\tirq_resend_init(desc);",
            "#ifdef CONFIG_SPARSE_IRQ",
            "\tkobject_init(&desc->kobj, &irq_kobj_type);",
            "\tinit_rcu_head(&desc->rcu);",
            "#endif",
            "",
            "\treturn 0;",
            "}",
            "static ssize_t per_cpu_count_show(struct kobject *kobj,",
            "\t\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);",
            "\tssize_t ret = 0;",
            "\tchar *p = \"\";",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tunsigned int c = irq_desc_kstat_cpu(desc, cpu);",
            "",
            "\t\tret += scnprintf(buf + ret, PAGE_SIZE - ret, \"%s%u\", p, c);",
            "\t\tp = \",\";",
            "\t}",
            "",
            "\tret += scnprintf(buf + ret, PAGE_SIZE - ret, \"\\n\");",
            "\treturn ret;",
            "}",
            "static ssize_t chip_name_show(struct kobject *kobj,",
            "\t\t\t      struct kobj_attribute *attr, char *buf)",
            "{",
            "\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);",
            "\tssize_t ret = 0;",
            "",
            "\traw_spin_lock_irq(&desc->lock);",
            "\tif (desc->irq_data.chip && desc->irq_data.chip->name) {",
            "\t\tret = scnprintf(buf, PAGE_SIZE, \"%s\\n\",",
            "\t\t\t\tdesc->irq_data.chip->name);",
            "\t}",
            "\traw_spin_unlock_irq(&desc->lock);",
            "",
            "\treturn ret;",
            "}",
            "static ssize_t hwirq_show(struct kobject *kobj,",
            "\t\t\t  struct kobj_attribute *attr, char *buf)",
            "{",
            "\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);",
            "\tssize_t ret = 0;",
            "",
            "\traw_spin_lock_irq(&desc->lock);",
            "\tif (desc->irq_data.domain)",
            "\t\tret = sprintf(buf, \"%lu\\n\", desc->irq_data.hwirq);",
            "\traw_spin_unlock_irq(&desc->lock);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "irq_find_free_area, irq_find_at_or_after, irq_insert_desc, delete_irq_desc, init_desc, per_cpu_count_show, chip_name_show, hwirq_show",
          "description": "实现中断描述符的动态管理操作，包含寻找可用中断区域、插入/删除描述符、初始化中断描述符及其统计信息等功能，同时提供了sysfs接口用于暴露中断类型、唤醒状态等调试信息。",
          "similarity": 0.5058563947677612
        }
      ]
    }
  ]
}