{
  "query": "微内核操作系统例子",
  "timestamp": "2025-12-26 01:52:49",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.5873374938964844,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.5339772701263428
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.48622262477874756
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.4527606666088104
        }
      ]
    },
    {
      "source_file": "mm/mm_init.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:50:02\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mm_init.c`\n\n---\n\n# mm_init.c 技术文档\n\n## 1. 文件概述\n\n`mm_init.c` 是 Linux 内核内存管理子系统（Memory Management, MM）中的一个初始化和调试辅助文件。其主要作用包括：\n\n- 提供内存初始化过程的验证与调试功能（在 `CONFIG_DEBUG_MEMORY_INIT` 启用时）\n- 初始化内存相关的全局参数和 sysfs 接口\n- 解析内核启动命令行参数（如 `kernelcore` 和 `movablecore`），用于控制不可移动与可移动内存区域的分配策略\n- 在 SMP 系统中动态计算 `vm_committed_as` 的批处理阈值，以优化内存提交统计的性能\n\n该文件不直接参与页分配或虚拟内存管理的核心逻辑，而是为内存子系统的正确性验证、配置和可观测性提供支持。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `mminit_verify_zonelist()` | 验证并打印每个 NUMA 节点的 zonelist 结构，用于调试内存区域组织 |\n| `mminit_verify_pageflags_layout()` | 验证 `struct page` 中用于存储节点、区域、节区等元数据的位域布局是否无重叠且对齐正确 |\n| `set_mminit_loglevel()` | 解析 `mminit_loglevel` 内核参数，设置内存初始化调试日志级别 |\n| `mm_compute_batch()` | 根据系统内存总量和 CPU 数量，计算 `vm_committed_as` per-CPU 计数器的批处理阈值 |\n| `mm_compute_batch_notifier()` | 内存热插拔事件回调，重新计算 `vm_committed_as` 批处理值 |\n| `mm_sysfs_init()` | 创建 `/sys/kernel/mm` sysfs 目录，用于暴露内存子系统信息 |\n| `cmdline_parse_core()` | 辅助函数，解析带百分比或字节单位的内存大小参数 |\n| `cmdline_parse_kernelcore()` / `cmdline_parse_movablecore()` | 解析 `kernelcore=` 和 `movablecore=` 内核启动参数 |\n\n### 主要全局变量\n\n| 变量名 | 类型/说明 |\n|--------|---------|\n| `mminit_loglevel` | 调试日志级别（仅当 `CONFIG_DEBUG_MEMORY_INIT` 启用） |\n| `mm_kobj` | 指向 `/sys/kernel/mm` 的 kobject 指针 |\n| `vm_committed_as_batch` | `vm_committed_as` per-CPU 计数器的批处理阈值（SMP） |\n| `required_kernelcore` / `required_kernelcore_percent` | 用户指定的不可移动内存需求（页数或百分比） |\n| `required_movablecore` / `required_movablecore_percent` | 用户指定的可移动内存需求（页数或百分比） |\n| `mirrored_kernelcore` | 是否启用镜像式 kernelcore 布局 |\n| `arch_zone_lowest_possible_pfn[]` / `arch_zone_highest_possible_pfn[]` | 架构定义的各内存区域（ZONE）的 PFN 范围 |\n| `zone_movable_pfn[]` | 各 NUMA 节点上 ZONE_MOVABLE 的起始 PFN |\n| `deferred_struct_pages` | 标记是否延迟初始化 struct page 实例 |\n\n## 3. 关键实现\n\n### 3.1 内存初始化调试（`CONFIG_DEBUG_MEMORY_INIT`）\n\n- **Zonelist 验证**：`mminit_verify_zonelist()` 遍历所有在线 NUMA 节点，打印其“通用”（general）和“本节点优先”（thisnode）两种 zonelist 的组成，帮助开发者确认内存区域的 fallback 顺序是否符合预期。\n- **Page Flags 布局验证**：`mminit_verify_pageflags_layout()` 检查 `struct page` 中用于编码物理位置（section/node/zone）的位域是否：\n  - 总宽度不超过 `BITS_PER_LONG`\n  - 各字段偏移（`_PGSHIFT`）与宽度一致\n  - 位掩码无重叠（通过 `or_mask == add_mask` 验证）\n\n### 3.2 内存区域划分策略\n\n- 通过 `kernelcore=` 和 `movablecore=` 参数，用户可显式指定系统中用于**不可移动分配**（如内核数据结构）和**可移动分配**（如用户页、可迁移 slab）的内存大小。\n- 支持 `kernelcore=mirror` 模式，在支持内存镜像的平台上启用特殊布局。\n- 参数值可为绝对字节数（如 `512M`）或总内存百分比（如 `40%`）。\n\n### 3.3 `vm_committed_as` 批处理优化（SMP）\n\n- `vm_committed_as` 是一个 per-CPU 计数器，跟踪已提交虚拟内存总量。\n- 为减少原子操作开销，当本地计数器变化超过 `vm_committed_as_batch` 时才同步到全局值。\n- `mm_compute_batch()` 根据 overcommit 策略动态调整 batch 大小：\n  - `OVERCOMMIT_NEVER`：batch = 总内存 / CPU数 / 256（约 0.4%）\n  - 其他策略：batch = 总内存 / CPU数 / 4（25%）\n- 注册内存热插拔通知器，确保内存容量变化后重新计算 batch 值。\n\n### 3.4 Sysfs 接口初始化\n\n- `mm_sysfs_init()` 在内核早期创建 `/sys/kernel/mm` 目录，作为内存子系统其他模块（如 compaction、numa、transparent_hugepage 等）注册 sysfs 属性的基础。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/memory.h>`、`<linux/memblock.h>`：内存块和热插拔管理\n  - `<linux/page-isolation.h>`、`<linux/cma.h>`：连续内存分配和页面隔离\n  - `\"internal.h\"`、`\"slab.h\"`：MM 子系统内部接口\n  - `<asm/setup.h>`：架构相关内存布局信息\n- **配置依赖**：\n  - `CONFIG_DEBUG_MEMORY_INIT`：启用调试验证功能\n  - `CONFIG_SMP`：启用 `vm_committed_as_batch` 优化\n  - `CONFIG_SYSFS`：支持 mm sysfs 目录创建\n- **被依赖模块**：\n  - 内存初始化流程（`mm_init()` in `init/main.c`）\n  - 页面分配器（`page_alloc.c`）使用 `zone_movable_pfn` 等变量\n  - 内存热插拔子系统调用 batch 重计算回调\n\n## 5. 使用场景\n\n- **内核开发与调试**：开发者启用 `CONFIG_DEBUG_MEMORY_INIT` 并设置 `mminit_loglevel`，可在启动时验证内存拓扑结构和 page 结构体布局的正确性。\n- **系统部署调优**：管理员通过 `kernelcore=` 或 `movablecore=` 参数，强制划分不可移动/可移动内存区域，以优化透明大页（THP）或避免内存碎片。\n- **高可靠性系统**：使用 `kernelcore=mirror` 在支持的硬件上启用内存镜像，提升容错能力。\n- **大规模 SMP 系统**：自动调整 `vm_committed_as_batch` 减少锁竞争，提升多进程内存密集型应用的性能。\n- **运行时监控**：`/sys/kernel/mm` 为用户空间工具（如 `numastat`、`cma` 调试接口）提供统一入口点。",
      "similarity": 0.5775420069694519,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/mm_init.c",
          "start_line": 151,
          "end_line": 259,
          "content": [
            "static __init int set_mminit_loglevel(char *str)",
            "{",
            "\tget_option(&str, &mminit_loglevel);",
            "\treturn 0;",
            "}",
            "void mm_compute_batch(int overcommit_policy)",
            "{",
            "\tu64 memsized_batch;",
            "\ts32 nr = num_present_cpus();",
            "\ts32 batch = max_t(s32, nr*2, 32);",
            "\tunsigned long ram_pages = totalram_pages();",
            "",
            "\t/*",
            "\t * For policy OVERCOMMIT_NEVER, set batch size to 0.4% of",
            "\t * (total memory/#cpus), and lift it to 25% for other policies",
            "\t * to easy the possible lock contention for percpu_counter",
            "\t * vm_committed_as, while the max limit is INT_MAX",
            "\t */",
            "\tif (overcommit_policy == OVERCOMMIT_NEVER)",
            "\t\tmemsized_batch = min_t(u64, ram_pages/nr/256, INT_MAX);",
            "\telse",
            "\t\tmemsized_batch = min_t(u64, ram_pages/nr/4, INT_MAX);",
            "",
            "\tvm_committed_as_batch = max_t(s32, memsized_batch, batch);",
            "}",
            "static int __meminit mm_compute_batch_notifier(struct notifier_block *self,",
            "\t\t\t\t\tunsigned long action, void *arg)",
            "{",
            "\tswitch (action) {",
            "\tcase MEM_ONLINE:",
            "\tcase MEM_OFFLINE:",
            "\t\tmm_compute_batch(sysctl_overcommit_memory);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "\treturn NOTIFY_OK;",
            "}",
            "static int __init mm_compute_batch_init(void)",
            "{",
            "\tmm_compute_batch(sysctl_overcommit_memory);",
            "\thotplug_memory_notifier(mm_compute_batch_notifier, MM_COMPUTE_BATCH_PRI);",
            "\treturn 0;",
            "}",
            "static int __init mm_sysfs_init(void)",
            "{",
            "\tmm_kobj = kobject_create_and_add(\"mm\", kernel_kobj);",
            "\tif (!mm_kobj)",
            "\t\treturn -ENOMEM;",
            "",
            "\treturn 0;",
            "}",
            "static int __init cmdline_parse_core(char *p, unsigned long *core,",
            "\t\t\t\t     unsigned long *percent)",
            "{",
            "\tunsigned long long coremem;",
            "\tchar *endptr;",
            "",
            "\tif (!p)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Value may be a percentage of total memory, otherwise bytes */",
            "\tcoremem = simple_strtoull(p, &endptr, 0);",
            "\tif (*endptr == '%') {",
            "\t\t/* Paranoid check for percent values greater than 100 */",
            "\t\tWARN_ON(coremem > 100);",
            "",
            "\t\t*percent = coremem;",
            "\t} else {",
            "\t\tcoremem = memparse(p, &p);",
            "\t\t/* Paranoid check that UL is enough for the coremem value */",
            "\t\tWARN_ON((coremem >> PAGE_SHIFT) > ULONG_MAX);",
            "",
            "\t\t*core = coremem >> PAGE_SHIFT;",
            "\t\t*percent = 0UL;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int __init cmdline_parse_kernelcore(char *p)",
            "{",
            "\t/* parse kernelcore=mirror */",
            "\tif (parse_option_str(p, \"mirror\")) {",
            "\t\tmirrored_kernelcore = true;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn cmdline_parse_core(p, &required_kernelcore,",
            "\t\t\t\t  &required_kernelcore_percent);",
            "}",
            "static int __init cmdline_parse_movablecore(char *p)",
            "{",
            "\treturn cmdline_parse_core(p, &required_movablecore,",
            "\t\t\t\t  &required_movablecore_percent);",
            "}",
            "static unsigned long __init early_calculate_totalpages(void)",
            "{",
            "\tunsigned long totalpages = 0;",
            "\tunsigned long start_pfn, end_pfn;",
            "\tint i, nid;",
            "",
            "\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {",
            "\t\tunsigned long pages = end_pfn - start_pfn;",
            "",
            "\t\ttotalpages += pages;",
            "\t\tif (pages)",
            "\t\t\tnode_set_state(nid, N_MEMORY);",
            "\t}",
            "\treturn totalpages;",
            "}"
          ],
          "function_name": "set_mminit_loglevel, mm_compute_batch, mm_compute_batch_notifier, mm_compute_batch_init, mm_sysfs_init, cmdline_parse_core, cmdline_parse_kernelcore, cmdline_parse_movablecore, early_calculate_totalpages",
          "description": "初始化内存批次计算逻辑，注册内存变化通知回调，解析命令行参数以确定内核核心和可移动内存需求",
          "similarity": 0.5706126689910889
        },
        {
          "chunk_id": 17,
          "file_path": "mm/mm_init.c",
          "start_line": 2711,
          "end_line": 2806,
          "content": [
            "static void __init mem_init_print_info(void)",
            "{",
            "\tunsigned long physpages, codesize, datasize, rosize, bss_size;",
            "\tunsigned long init_code_size, init_data_size;",
            "",
            "\tphyspages = get_num_physpages();",
            "\tcodesize = _etext - _stext;",
            "\tdatasize = _edata - _sdata;",
            "\trosize = __end_rodata - __start_rodata;",
            "\tbss_size = __bss_stop - __bss_start;",
            "\tinit_data_size = __init_end - __init_begin;",
            "\tinit_code_size = _einittext - _sinittext;",
            "",
            "\t/*",
            "\t * Detect special cases and adjust section sizes accordingly:",
            "\t * 1) .init.* may be embedded into .data sections",
            "\t * 2) .init.text.* may be out of [__init_begin, __init_end],",
            "\t *    please refer to arch/tile/kernel/vmlinux.lds.S.",
            "\t * 3) .rodata.* may be embedded into .text or .data sections.",
            "\t */",
            "#define adj_init_size(start, end, size, pos, adj) \\",
            "\tdo { \\",
            "\t\tif (&start[0] <= &pos[0] && &pos[0] < &end[0] && size > adj) \\",
            "\t\t\tsize -= adj; \\",
            "\t} while (0)",
            "",
            "\tadj_init_size(__init_begin, __init_end, init_data_size,",
            "\t\t     _sinittext, init_code_size);",
            "\tadj_init_size(_stext, _etext, codesize, _sinittext, init_code_size);",
            "\tadj_init_size(_sdata, _edata, datasize, __init_begin, init_data_size);",
            "\tadj_init_size(_stext, _etext, codesize, __start_rodata, rosize);",
            "\tadj_init_size(_sdata, _edata, datasize, __start_rodata, rosize);",
            "",
            "#undef\tadj_init_size",
            "",
            "\tpr_info(\"Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved\"",
            "#ifdef\tCONFIG_HIGHMEM",
            "\t\t\", %luK highmem\"",
            "#endif",
            "\t\t\")\\n\",",
            "\t\tK(nr_free_pages()), K(physpages),",
            "\t\tcodesize / SZ_1K, datasize / SZ_1K, rosize / SZ_1K,",
            "\t\t(init_data_size + init_code_size) / SZ_1K, bss_size / SZ_1K,",
            "\t\tK(physpages - totalram_pages() - totalcma_pages),",
            "\t\tK(totalcma_pages)",
            "#ifdef\tCONFIG_HIGHMEM",
            "\t\t, K(totalhigh_pages())",
            "#endif",
            "\t\t);",
            "}",
            "void __init mm_core_init(void)",
            "{",
            "\t/* Initializations relying on SMP setup */",
            "\tbuild_all_zonelists(NULL);",
            "\tpage_alloc_init_cpuhp();",
            "",
            "\t/*",
            "\t * page_ext requires contiguous pages,",
            "\t * bigger than MAX_PAGE_ORDER unless SPARSEMEM.",
            "\t */",
            "\tpage_ext_init_flatmem();",
            "\tmem_debugging_and_hardening_init();",
            "\tkfence_alloc_pool_and_metadata();",
            "\treport_meminit();",
            "\tkmsan_init_shadow();",
            "\tstack_depot_early_init();",
            "",
            "\t/*",
            "\t * KHO memory setup must happen while memblock is still active, but",
            "\t * as close as possible to buddy initialization",
            "\t */",
            "\tkho_memory_init();",
            "",
            "\tmem_init();",
            "\tmem_init_print_info();",
            "\tkmem_cache_init();",
            "\t/*",
            "\t * page_owner must be initialized after buddy is ready, and also after",
            "\t * slab is ready so that stack_depot_init() works properly",
            "\t */",
            "\tpage_ext_init_flatmem_late();",
            "\tkmemleak_init();",
            "\tptlock_cache_init();",
            "\tpgtable_cache_init();",
            "\tdebug_objects_mem_init();",
            "\tvmalloc_init();",
            "\t/* If no deferred init page_ext now, as vmap is fully initialized */",
            "\tif (!deferred_struct_pages)",
            "\t\tpage_ext_init();",
            "\t/* Should be run before the first non-init thread is created */",
            "\tinit_espfix_bsp();",
            "\t/* Should be run after espfix64 is set up. */",
            "\tpti_init();",
            "\tkmsan_init_runtime();",
            "\tmm_cache_init();",
            "}"
          ],
          "function_name": "mem_init_print_info, mm_core_init",
          "description": "mem_init_print_info计算并打印内存统计信息，包括可用页面数、各段代码数据大小及保留区域。mm_core_init初始化内存核心组件，构建zonelists，初始化slab/kmem_cache，启用调试对象跟踪，设置虚拟内存管理，最后根据是否延迟结构页初始化page_ext模块。",
          "similarity": 0.5490623712539673
        },
        {
          "chunk_id": 6,
          "file_path": "mm/mm_init.c",
          "start_line": 912,
          "end_line": 1017,
          "content": [
            "static void __init memmap_init_zone_range(struct zone *zone,",
            "\t\t\t\t\t  unsigned long start_pfn,",
            "\t\t\t\t\t  unsigned long end_pfn,",
            "\t\t\t\t\t  unsigned long *hole_pfn)",
            "{",
            "\tunsigned long zone_start_pfn = zone->zone_start_pfn;",
            "\tunsigned long zone_end_pfn = zone_start_pfn + zone->spanned_pages;",
            "\tint nid = zone_to_nid(zone), zone_id = zone_idx(zone);",
            "",
            "\tstart_pfn = clamp(start_pfn, zone_start_pfn, zone_end_pfn);",
            "\tend_pfn = clamp(end_pfn, zone_start_pfn, zone_end_pfn);",
            "",
            "\tif (start_pfn >= end_pfn)",
            "\t\treturn;",
            "",
            "\tmemmap_init_range(end_pfn - start_pfn, nid, zone_id, start_pfn,",
            "\t\t\t  zone_end_pfn, MEMINIT_EARLY, NULL, MIGRATE_MOVABLE);",
            "",
            "\tif (*hole_pfn < start_pfn)",
            "\t\tinit_unavailable_range(*hole_pfn, start_pfn, zone_id, nid);",
            "",
            "\t*hole_pfn = end_pfn;",
            "}",
            "static void __init memmap_init(void)",
            "{",
            "\tunsigned long start_pfn, end_pfn;",
            "\tunsigned long hole_pfn = 0;",
            "\tint i, j, zone_id = 0, nid;",
            "",
            "\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {",
            "\t\tstruct pglist_data *node = NODE_DATA(nid);",
            "",
            "\t\tfor (j = 0; j < MAX_NR_ZONES; j++) {",
            "\t\t\tstruct zone *zone = node->node_zones + j;",
            "",
            "\t\t\tif (!populated_zone(zone))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tmemmap_init_zone_range(zone, start_pfn, end_pfn,",
            "\t\t\t\t\t       &hole_pfn);",
            "\t\t\tzone_id = j;",
            "\t\t}",
            "\t}",
            "",
            "#ifdef CONFIG_SPARSEMEM",
            "\t/*",
            "\t * Initialize the memory map for hole in the range [memory_end,",
            "\t * section_end].",
            "\t * Append the pages in this hole to the highest zone in the last",
            "\t * node.",
            "\t * The call to init_unavailable_range() is outside the ifdef to",
            "\t * silence the compiler warining about zone_id set but not used;",
            "\t * for FLATMEM it is a nop anyway",
            "\t */",
            "\tend_pfn = round_up(end_pfn, PAGES_PER_SECTION);",
            "\tif (hole_pfn < end_pfn)",
            "#endif",
            "\t\tinit_unavailable_range(hole_pfn, end_pfn, zone_id, nid);",
            "}",
            "static void __ref __init_zone_device_page(struct page *page, unsigned long pfn,",
            "\t\t\t\t\t  unsigned long zone_idx, int nid,",
            "\t\t\t\t\t  struct dev_pagemap *pgmap)",
            "{",
            "",
            "\t__init_single_page(page, pfn, zone_idx, nid);",
            "",
            "\t/*",
            "\t * Mark page reserved as it will need to wait for onlining",
            "\t * phase for it to be fully associated with a zone.",
            "\t *",
            "\t * We can use the non-atomic __set_bit operation for setting",
            "\t * the flag as we are still initializing the pages.",
            "\t */",
            "\t__SetPageReserved(page);",
            "",
            "\t/*",
            "\t * ZONE_DEVICE pages union ->lru with a ->pgmap back pointer",
            "\t * and zone_device_data.  It is a bug if a ZONE_DEVICE page is",
            "\t * ever freed or placed on a driver-private list.",
            "\t */",
            "\tpage->pgmap = pgmap;",
            "\tpage->zone_device_data = NULL;",
            "",
            "\t/*",
            "\t * Mark the block movable so that blocks are reserved for",
            "\t * movable at startup. This will force kernel allocations",
            "\t * to reserve their blocks rather than leaking throughout",
            "\t * the address space during boot when many long-lived",
            "\t * kernel allocations are made.",
            "\t *",
            "\t * Please note that MEMINIT_HOTPLUG path doesn't clear memmap",
            "\t * because this is done early in section_activate()",
            "\t */",
            "\tif (pageblock_aligned(pfn)) {",
            "\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\t/*",
            "\t * ZONE_DEVICE pages are released directly to the driver page allocator",
            "\t * which will set the page count to 1 when allocating the page.",
            "\t */",
            "\tif (pgmap->type == MEMORY_DEVICE_PRIVATE ||",
            "\t    pgmap->type == MEMORY_DEVICE_COHERENT)",
            "\t\tset_page_count(page, 0);",
            "}"
          ],
          "function_name": "memmap_init_zone_range, memmap_init, __init_zone_device_page",
          "description": "遍历各节点和区，调用memmap_init_range初始化内存映射，处理稀疏内存中洞的不可用范围，并调整ZONE_MOVABLE范围以适应架构需求。",
          "similarity": 0.5488988161087036
        },
        {
          "chunk_id": 9,
          "file_path": "mm/mm_init.c",
          "start_line": 1313,
          "end_line": 1420,
          "content": [
            "static unsigned long __init calc_memmap_size(unsigned long spanned_pages,",
            "\t\t\t\t\t\tunsigned long present_pages)",
            "{",
            "\tunsigned long pages = spanned_pages;",
            "",
            "\t/*",
            "\t * Provide a more accurate estimation if there are holes within",
            "\t * the zone and SPARSEMEM is in use. If there are holes within the",
            "\t * zone, each populated memory region may cost us one or two extra",
            "\t * memmap pages due to alignment because memmap pages for each",
            "\t * populated regions may not be naturally aligned on page boundary.",
            "\t * So the (present_pages >> 4) heuristic is a tradeoff for that.",
            "\t */",
            "\tif (spanned_pages > present_pages + (present_pages >> 4) &&",
            "\t    IS_ENABLED(CONFIG_SPARSEMEM))",
            "\t\tpages = present_pages;",
            "",
            "\treturn PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;",
            "}",
            "static void pgdat_init_split_queue(struct pglist_data *pgdat)",
            "{",
            "\tstruct deferred_split *ds_queue = &pgdat->deferred_split_queue;",
            "",
            "\tspin_lock_init(&ds_queue->split_queue_lock);",
            "\tINIT_LIST_HEAD(&ds_queue->split_queue);",
            "\tds_queue->split_queue_len = 0;",
            "}",
            "static void pgdat_init_split_queue(struct pglist_data *pgdat) {}",
            "static void pgdat_init_kcompactd(struct pglist_data *pgdat)",
            "{",
            "\tinit_waitqueue_head(&pgdat->kcompactd_wait);",
            "}",
            "static void pgdat_init_kcompactd(struct pglist_data *pgdat) {}",
            "static void __meminit pgdat_init_internals(struct pglist_data *pgdat)",
            "{",
            "\tint i;",
            "",
            "\tpgdat_resize_init(pgdat);",
            "\tpgdat_kswapd_lock_init(pgdat);",
            "",
            "\tpgdat_init_split_queue(pgdat);",
            "\tpgdat_init_kcompactd(pgdat);",
            "",
            "\tinit_waitqueue_head(&pgdat->kswapd_wait);",
            "\tinit_waitqueue_head(&pgdat->pfmemalloc_wait);",
            "",
            "\tfor (i = 0; i < NR_VMSCAN_THROTTLE; i++)",
            "\t\tinit_waitqueue_head(&pgdat->reclaim_wait[i]);",
            "",
            "\tpgdat_page_ext_init(pgdat);",
            "\tlruvec_init(&pgdat->__lruvec);",
            "}",
            "static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,",
            "\t\t\t\t\t\t\tunsigned long remaining_pages)",
            "{",
            "\tatomic_long_set(&zone->managed_pages, remaining_pages);",
            "\tzone_set_nid(zone, nid);",
            "\tzone->name = zone_names[idx];",
            "\tzone->zone_pgdat = NODE_DATA(nid);",
            "\tspin_lock_init(&zone->lock);",
            "\tzone_seqlock_init(zone);",
            "\tzone_pcp_init(zone);",
            "}",
            "static void __meminit zone_init_free_lists(struct zone *zone)",
            "{",
            "\tunsigned int order, t;",
            "\tfor_each_migratetype_order(order, t) {",
            "\t\tINIT_LIST_HEAD(&zone->free_area[order].free_list[t]);",
            "\t\tzone->free_area[order].nr_free = 0;",
            "\t}",
            "",
            "#ifdef CONFIG_UNACCEPTED_MEMORY",
            "\tINIT_LIST_HEAD(&zone->unaccepted_pages);",
            "#endif",
            "}",
            "void __meminit init_currently_empty_zone(struct zone *zone,",
            "\t\t\t\t\tunsigned long zone_start_pfn,",
            "\t\t\t\t\tunsigned long size)",
            "{",
            "\tstruct pglist_data *pgdat = zone->zone_pgdat;",
            "\tint zone_idx = zone_idx(zone) + 1;",
            "",
            "\tif (zone_idx > pgdat->nr_zones)",
            "\t\tpgdat->nr_zones = zone_idx;",
            "",
            "\tzone->zone_start_pfn = zone_start_pfn;",
            "",
            "\tmminit_dprintk(MMINIT_TRACE, \"memmap_init\",",
            "\t\t\t\"Initialising map node %d zone %lu pfns %lu -> %lu\\n\",",
            "\t\t\tpgdat->node_id,",
            "\t\t\t(unsigned long)zone_idx(zone),",
            "\t\t\tzone_start_pfn, (zone_start_pfn + size));",
            "",
            "\tzone_init_free_lists(zone);",
            "\tzone->initialized = 1;",
            "}",
            "static unsigned long __init usemap_size(unsigned long zone_start_pfn, unsigned long zonesize)",
            "{",
            "\tunsigned long usemapsize;",
            "",
            "\tzonesize += zone_start_pfn & (pageblock_nr_pages-1);",
            "\tusemapsize = roundup(zonesize, pageblock_nr_pages);",
            "\tusemapsize = usemapsize >> pageblock_order;",
            "\tusemapsize *= NR_PAGEBLOCK_BITS;",
            "\tusemapsize = roundup(usemapsize, BITS_PER_LONG);",
            "",
            "\treturn usemapsize / BITS_PER_BYTE;",
            "}"
          ],
          "function_name": "calc_memmap_size, pgdat_init_split_queue, pgdat_init_split_queue, pgdat_init_kcompactd, pgdat_init_kcompactd, pgdat_init_internals, zone_init_internals, zone_init_free_lists, init_currently_empty_zone, usemap_size",
          "description": "计算内存映射所需大小，初始化split队列、kcompactd等待队列等内部结构，并配置区的自由列表及空闲页面管理机制。",
          "similarity": 0.541648268699646
        },
        {
          "chunk_id": 16,
          "file_path": "mm/mm_init.c",
          "start_line": 2425,
          "end_line": 2546,
          "content": [
            "static unsigned long __init arch_reserved_kernel_pages(void)",
            "{",
            "\treturn 0;",
            "}",
            "void __init set_dma_reserve(unsigned long new_dma_reserve)",
            "{",
            "\tdma_reserve = new_dma_reserve;",
            "}",
            "void __init memblock_free_pages(struct page *page, unsigned long pfn,",
            "\t\t\t\t\t\t\tunsigned int order)",
            "{",
            "\tif (IS_ENABLED(CONFIG_DEFERRED_STRUCT_PAGE_INIT)) {",
            "\t\tint nid = early_pfn_to_nid(pfn);",
            "",
            "\t\tif (!early_page_initialised(pfn, nid))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\tif (!kmsan_memblock_free_pages(page, order)) {",
            "\t\t/* KMSAN will take care of these pages. */",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* pages were reserved and not allocated */",
            "\tclear_page_tag_ref(page);",
            "\t__free_pages_core(page, order, MEMINIT_EARLY);",
            "}",
            "static int __init early_init_on_alloc(char *buf)",
            "{",
            "",
            "\treturn kstrtobool(buf, &_init_on_alloc_enabled_early);",
            "}",
            "static int __init early_init_on_free(char *buf)",
            "{",
            "\treturn kstrtobool(buf, &_init_on_free_enabled_early);",
            "}",
            "static void __init mem_debugging_and_hardening_init(void)",
            "{",
            "\tbool page_poisoning_requested = false;",
            "\tbool want_check_pages = false;",
            "",
            "#ifdef CONFIG_PAGE_POISONING",
            "\t/*",
            "\t * Page poisoning is debug page alloc for some arches. If",
            "\t * either of those options are enabled, enable poisoning.",
            "\t */",
            "\tif (page_poisoning_enabled() ||",
            "\t     (!IS_ENABLED(CONFIG_ARCH_SUPPORTS_DEBUG_PAGEALLOC) &&",
            "\t      debug_pagealloc_enabled())) {",
            "\t\tstatic_branch_enable(&_page_poisoning_enabled);",
            "\t\tpage_poisoning_requested = true;",
            "\t\twant_check_pages = true;",
            "\t}",
            "#endif",
            "",
            "\tif ((_init_on_alloc_enabled_early || _init_on_free_enabled_early) &&",
            "\t    page_poisoning_requested) {",
            "\t\tpr_info(\"mem auto-init: CONFIG_PAGE_POISONING is on, \"",
            "\t\t\t\"will take precedence over init_on_alloc and init_on_free\\n\");",
            "\t\t_init_on_alloc_enabled_early = false;",
            "\t\t_init_on_free_enabled_early = false;",
            "\t}",
            "",
            "\tif (_init_on_alloc_enabled_early) {",
            "\t\twant_check_pages = true;",
            "\t\tstatic_branch_enable(&init_on_alloc);",
            "\t} else {",
            "\t\tstatic_branch_disable(&init_on_alloc);",
            "\t}",
            "",
            "\tif (_init_on_free_enabled_early) {",
            "\t\twant_check_pages = true;",
            "\t\tstatic_branch_enable(&init_on_free);",
            "\t} else {",
            "\t\tstatic_branch_disable(&init_on_free);",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_KMSAN) &&",
            "\t    (_init_on_alloc_enabled_early || _init_on_free_enabled_early))",
            "\t\tpr_info(\"mem auto-init: please make sure init_on_alloc and init_on_free are disabled when running KMSAN\\n\");",
            "",
            "#ifdef CONFIG_DEBUG_PAGEALLOC",
            "\tif (debug_pagealloc_enabled()) {",
            "\t\twant_check_pages = true;",
            "\t\tstatic_branch_enable(&_debug_pagealloc_enabled);",
            "",
            "\t\tif (debug_guardpage_minorder())",
            "\t\t\tstatic_branch_enable(&_debug_guardpage_enabled);",
            "\t}",
            "#endif",
            "",
            "\t/*",
            "\t * Any page debugging or hardening option also enables sanity checking",
            "\t * of struct pages being allocated or freed. With CONFIG_DEBUG_VM it's",
            "\t * enabled already.",
            "\t */",
            "\tif (!IS_ENABLED(CONFIG_DEBUG_VM) && want_check_pages)",
            "\t\tstatic_branch_enable(&check_pages_enabled);",
            "}",
            "static void __init report_meminit(void)",
            "{",
            "\tconst char *stack;",
            "",
            "\tif (IS_ENABLED(CONFIG_INIT_STACK_ALL_PATTERN))",
            "\t\tstack = \"all(pattern)\";",
            "\telse if (IS_ENABLED(CONFIG_INIT_STACK_ALL_ZERO))",
            "\t\tstack = \"all(zero)\";",
            "\telse if (IS_ENABLED(CONFIG_GCC_PLUGIN_STRUCTLEAK_BYREF_ALL))",
            "\t\tstack = \"byref_all(zero)\";",
            "\telse if (IS_ENABLED(CONFIG_GCC_PLUGIN_STRUCTLEAK_BYREF))",
            "\t\tstack = \"byref(zero)\";",
            "\telse if (IS_ENABLED(CONFIG_GCC_PLUGIN_STRUCTLEAK_USER))",
            "\t\tstack = \"__user(zero)\";",
            "\telse",
            "\t\tstack = \"off\";",
            "",
            "\tpr_info(\"mem auto-init: stack:%s, heap alloc:%s, heap free:%s\\n\",",
            "\t\tstack, want_init_on_alloc(GFP_KERNEL) ? \"on\" : \"off\",",
            "\t\twant_init_on_free() ? \"on\" : \"off\");",
            "\tif (want_init_on_free())",
            "\t\tpr_info(\"mem auto-init: clearing system memory may take some time...\\n\");",
            "}"
          ],
          "function_name": "arch_reserved_kernel_pages, set_dma_reserve, memblock_free_pages, early_init_on_alloc, early_init_on_free, mem_debugging_and_hardening_init, report_meminit",
          "description": "arch_reserved_kernel_pages返回0表示无内核保留物理页。set_dma_reserve设置DMA保留页面数量。memblock_free_pages释放早期分配的内存块，清除保留标记并调用底层释放函数。early_init_on_alloc/early_init_on_free控制早期内存初始化开关。mem_debugging_and_hardening_init启用页面毒化、堆检查等安全机制，优先级高于早期初始化选项。report_meminit打印内存自动初始化配置信息。",
          "similarity": 0.5258140563964844
        }
      ]
    },
    {
      "source_file": "kernel/fork.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:30:07\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `fork.c`\n\n---\n\n# fork.c 技术文档\n\n## 1. 文件概述\n\n`fork.c` 是 Linux 内核中实现进程创建（fork）系统调用的核心源文件。该文件包含了创建新进程所需的所有辅助例程，负责复制父进程的资源（如内存、文件描述符、信号处理等）以生成子进程。虽然 fork 逻辑本身概念简单，但其涉及的内存管理（尤其是写时复制 COW 机制）极为复杂，实际内存页的复制由 `mm/memory.c` 中的 `copy_page_range()` 等函数处理。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `total_forks`: 累计系统自启动以来创建的进程总数\n- `nr_threads`: 当前系统中的线程总数（不包括 idle 线程）\n- `max_threads`: 可配置的线程数量上限（默认为 `FUTEX_TID_MASK`）\n- `process_counts`: 每 CPU 的进程计数器（per-CPU 变量）\n- `tasklist_lock`: 保护任务链表的读写锁（全局任务列表的同步原语）\n\n### 关键辅助函数\n- `nr_processes()`: 计算系统中所有进程的总数（聚合各 CPU 的 `process_counts`）\n- `arch_release_task_struct()`: 架构相关的 task_struct 释放钩子（弱符号，默认为空）\n- `alloc_task_struct_node()` / `free_task_struct()`: 分配/释放 `task_struct` 结构（基于 slab 分配器）\n- `alloc_thread_stack_node()` / `thread_stack_delayed_free()`: 分配/延迟释放线程内核栈（支持 `CONFIG_VMAP_STACK`）\n\n### 核心数据结构\n- `resident_page_types[]`: 用于内存统计的页面类型名称映射数组\n- `vm_stack`: 用于 RCU 延迟释放的虚拟内存栈封装结构\n- `cached_stacks[NR_CACHED_STACKS]`: 每 CPU 的内核栈缓存（减少频繁 vmalloc/vfree 开销）\n\n## 3. 关键实现\n\n### 进程/线程计数管理\n- 使用 per-CPU 变量 `process_counts` 避免全局锁竞争\n- 全局计数器 `nr_threads` 和 `total_forks` 由 `tasklist_lock` 保护\n- `nr_processes()` 通过遍历所有可能的 CPU 聚合计数\n\n### 内核栈分配策略（`CONFIG_VMAP_STACK`）\n- **缓存机制**：每个 CPU 缓存最多 2 个已释放的栈（`NR_CACHED_STACKS`），减少 TLB 刷新和 vmalloc 开销\n- **内存分配**：\n  - 优先从本地缓存获取栈\n  - 缓存未命中时使用 `__vmalloc_node_range()` 分配连续虚拟地址空间\n  - 显式禁用 `__GFP_ACCOUNT`（因后续手动进行 memcg 计费）\n- **安全清理**：\n  - 重用栈时清零内存（`memset(stack, 0, THREAD_SIZE)`）\n  - KASAN 消毒（`kasan_unpoison_range`）和标签重置\n- **延迟释放**：\n  - 通过 RCU 机制延迟释放栈（`call_rcu`）\n  - 释放时尝试回填缓存，失败则直接 `vfree`\n\n### 内存控制组（memcg）集成\n- 手动对栈的每个物理页进行 memcg 计费（`memcg_kmem_charge_page`）\n- 计费失败时回滚已计费页面（`memcg_kmem_uncharge_page`）\n- 确保内核栈内存纳入 cgroup 内存限制\n\n### 锁与同步\n- `tasklist_lock` 作为全局任务列表的保护锁（读写锁）\n- 提供 `lockdep_tasklist_lock_is_held()` 供 RCU 锁验证使用\n- RCU 用于安全延迟释放内核栈资源\n\n## 4. 依赖关系\n\n### 内核子系统依赖\n- **内存管理 (MM)**：`<linux/mm.h>`, `<linux/vmalloc.h>`, `<linux/memcontrol.h>`\n- **调度器 (Scheduler)**：`<linux/sched/*.h>`, 任务状态和 CPU 绑定\n- **安全模块**：`<linux/security.h>`, `<linux/capability.h>`, `<linux/seccomp.h>`\n- **命名空间**：`<linux/nsproxy.h>`（UTS, IPC, PID, 网络等）\n- **文件系统**：`<linux/fs.h>`, `<linux/fdtable.h>`（文件描述符复制）\n- **跟踪与调试**：`<trace/events/sched.h>`, `<linux/ftrace.h>`, KASAN/KMSAN\n\n### 架构相关依赖\n- `<asm/pgalloc.h>`：页表分配\n- `<asm/mmu_context.h>`：MMU 上下文切换\n- `<asm/tlbflush.h>`：TLB 刷新操作\n- 架构特定的 `THREAD_SIZE` 和栈对齐要求\n\n### 配置选项依赖\n- `CONFIG_VMAP_STACK`：启用虚拟内存分配内核栈\n- `CONFIG_PROVE_RCU`：RCU 锁验证支持\n- `CONFIG_ARCH_TASK_STRUCT_ALLOCATOR`：架构自定义 task_struct 分配器\n- `CONFIG_MEMCG_KMEM`：内核内存 cgroup 支持\n\n## 5. 使用场景\n\n### 进程创建路径\n- **系统调用入口**：`sys_fork()`, `sys_vfork()`, `sys_clone()` 最终调用 `_do_fork()`\n- **内核线程创建**：`kthread_create()` 通过 `kernel_thread()` 触发 fork 逻辑\n- **容器/命名空间初始化**：新 PID/UTS/IPC 命名空间创建时伴随进程 fork\n\n### 资源复制关键点\n- **内存描述符 (mm_struct)**：通过 `dup_mm()` 复制地址空间（COW 页表）\n- **文件描述符表**：`dup_fd()` 复制打开文件表\n- **信号处理**：复制信号掩码和处理函数\n- **POSIX 定时器/异步 I/O**：复制相关上下文（如 `aio`, `posix-timers`）\n\n### 特殊场景处理\n- **写时复制优化**：避免物理内存立即复制，提升 fork 性能\n- **OOM Killer 集成**：在内存不足时参与进程选择\n- **审计与监控**：通过 `audit_alloc()` 和 `proc` 文件系统暴露进程信息\n- **实时性保障**：RT 任务 fork 时保持调度策略和优先级",
      "similarity": 0.5712631344795227,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/fork.c",
          "start_line": 666,
          "end_line": 845,
          "content": [
            "static __latent_entropy int dup_mmap(struct mm_struct *mm,",
            "\t\t\t\t\tstruct mm_struct *oldmm)",
            "{",
            "\tstruct vm_area_struct *mpnt, *tmp;",
            "\tint retval;",
            "\tunsigned long charge = 0;",
            "\tLIST_HEAD(uf);",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "",
            "\tuprobe_start_dup_mmap();",
            "\tif (mmap_write_lock_killable(oldmm)) {",
            "\t\tretval = -EINTR;",
            "\t\tgoto fail_uprobe_end;",
            "\t}",
            "\tflush_cache_dup_mm(oldmm);",
            "\tuprobe_dup_mmap(oldmm, mm);",
            "\t/*",
            "\t * Not linked in yet - no deadlock potential:",
            "\t */",
            "\tmmap_write_lock_nested(mm, SINGLE_DEPTH_NESTING);",
            "",
            "\t/* No ordering required: file already has been exposed. */",
            "\tdup_mm_exe_file(mm, oldmm);",
            "",
            "\tmm->total_vm = oldmm->total_vm;",
            "\tmm->data_vm = oldmm->data_vm;",
            "\tmm->exec_vm = oldmm->exec_vm;",
            "\tmm->stack_vm = oldmm->stack_vm;",
            "",
            "\t/* Use __mt_dup() to efficiently build an identical maple tree. */",
            "\tretval = __mt_dup(&oldmm->mm_mt, &mm->mm_mt, GFP_KERNEL);",
            "\tif (unlikely(retval))",
            "\t\tgoto out;",
            "",
            "\tmt_clear_in_rcu(vmi.mas.tree);",
            "\tfor_each_vma(vmi, mpnt) {",
            "\t\tstruct file *file;",
            "",
            "\t\tvma_start_write(mpnt);",
            "\t\tif (mpnt->vm_flags & VM_DONTCOPY) {",
            "\t\t\tretval = vma_iter_clear_gfp(&vmi, mpnt->vm_start,",
            "\t\t\t\t\t\t    mpnt->vm_end, GFP_KERNEL);",
            "\t\t\tif (retval)",
            "\t\t\t\tgoto loop_out;",
            "",
            "\t\t\tvm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tcharge = 0;",
            "\t\t/*",
            "\t\t * Don't duplicate many vmas if we've been oom-killed (for",
            "\t\t * example)",
            "\t\t */",
            "\t\tif (fatal_signal_pending(current)) {",
            "\t\t\tretval = -EINTR;",
            "\t\t\tgoto loop_out;",
            "\t\t}",
            "\t\tif (mpnt->vm_flags & VM_ACCOUNT) {",
            "\t\t\tunsigned long len = vma_pages(mpnt);",
            "",
            "\t\t\tif (security_vm_enough_memory_mm(oldmm, len)) /* sic */",
            "\t\t\t\tgoto fail_nomem;",
            "\t\t\tcharge = len;",
            "\t\t}",
            "\t\ttmp = vm_area_dup(mpnt);",
            "\t\tif (!tmp)",
            "\t\t\tgoto fail_nomem;",
            "",
            "\t\t/* track_pfn_copy() will later take care of copying internal state. */",
            "\t\tif (unlikely(tmp->vm_flags & VM_PFNMAP))",
            "\t\t\tuntrack_pfn_clear(tmp);",
            "",
            "\t\tretval = vma_dup_policy(mpnt, tmp);",
            "\t\tif (retval)",
            "\t\t\tgoto fail_nomem_policy;",
            "\t\ttmp->vm_mm = mm;",
            "\t\tretval = dup_userfaultfd(tmp, &uf);",
            "\t\tif (retval)",
            "\t\t\tgoto fail_nomem_anon_vma_fork;",
            "\t\tif (tmp->vm_flags & VM_WIPEONFORK) {",
            "\t\t\t/*",
            "\t\t\t * VM_WIPEONFORK gets a clean slate in the child.",
            "\t\t\t * Don't prepare anon_vma until fault since we don't",
            "\t\t\t * copy page for current vma.",
            "\t\t\t */",
            "\t\t\ttmp->anon_vma = NULL;",
            "\t\t} else if (anon_vma_fork(tmp, mpnt))",
            "\t\t\tgoto fail_nomem_anon_vma_fork;",
            "\t\tvm_flags_clear(tmp, VM_LOCKED_MASK);",
            "\t\tfile = tmp->vm_file;",
            "\t\tif (file) {",
            "\t\t\tstruct address_space *mapping = file->f_mapping;",
            "",
            "\t\t\tget_file(file);",
            "\t\t\ti_mmap_lock_write(mapping);",
            "\t\t\tif (vma_is_shared_maywrite(tmp))",
            "\t\t\t\tmapping_allow_writable(mapping);",
            "\t\t\tflush_dcache_mmap_lock(mapping);",
            "\t\t\t/* insert tmp into the share list, just after mpnt */",
            "\t\t\tvma_interval_tree_insert_after(tmp, mpnt,",
            "\t\t\t\t\t&mapping->i_mmap);",
            "\t\t\tflush_dcache_mmap_unlock(mapping);",
            "\t\t\ti_mmap_unlock_write(mapping);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Copy/update hugetlb private vma information.",
            "\t\t */",
            "\t\tif (is_vm_hugetlb_page(tmp))",
            "\t\t\thugetlb_dup_vma_private(tmp);",
            "",
            "\t\t/*",
            "\t\t * Link the vma into the MT. After using __mt_dup(), memory",
            "\t\t * allocation is not necessary here, so it cannot fail.",
            "\t\t */",
            "\t\tvma_iter_bulk_store(&vmi, tmp);",
            "",
            "\t\tmm->map_count++;",
            "\t\tif (!(tmp->vm_flags & VM_WIPEONFORK))",
            "\t\t\tretval = copy_page_range(tmp, mpnt);",
            "",
            "\t\tif (tmp->vm_ops && tmp->vm_ops->open)",
            "\t\t\ttmp->vm_ops->open(tmp);",
            "",
            "\t\tif (retval) {",
            "\t\t\tmpnt = vma_next(&vmi);",
            "\t\t\tgoto loop_out;",
            "\t\t}",
            "\t}",
            "\t/* a new mm has just been created */",
            "\tretval = arch_dup_mmap(oldmm, mm);",
            "loop_out:",
            "\tvma_iter_free(&vmi);",
            "\tif (!retval) {",
            "\t\tmt_set_in_rcu(vmi.mas.tree);",
            "\t\tksm_fork(mm, oldmm);",
            "\t\tkhugepaged_fork(mm, oldmm);",
            "\t} else {",
            "",
            "\t\t/*",
            "\t\t * The entire maple tree has already been duplicated. If the",
            "\t\t * mmap duplication fails, mark the failure point with",
            "\t\t * XA_ZERO_ENTRY. In exit_mmap(), if this marker is encountered,",
            "\t\t * stop releasing VMAs that have not been duplicated after this",
            "\t\t * point.",
            "\t\t */",
            "\t\tif (mpnt) {",
            "\t\t\tmas_set_range(&vmi.mas, mpnt->vm_start, mpnt->vm_end - 1);",
            "\t\t\tmas_store(&vmi.mas, XA_ZERO_ENTRY);",
            "\t\t\t/* Avoid OOM iterating a broken tree */",
            "\t\t\tset_bit(MMF_OOM_SKIP, &mm->flags);",
            "\t\t}",
            "\t\t/*",
            "\t\t * The mm_struct is going to exit, but the locks will be dropped",
            "\t\t * first.  Set the mm_struct as unstable is advisable as it is",
            "\t\t * not fully initialised.",
            "\t\t */",
            "\t\tset_bit(MMF_UNSTABLE, &mm->flags);",
            "\t}",
            "out:",
            "\tmmap_write_unlock(mm);",
            "\tflush_tlb_mm(oldmm);",
            "\tmmap_write_unlock(oldmm);",
            "\tif (!retval)",
            "\t\tdup_userfaultfd_complete(&uf);",
            "\telse",
            "\t\tdup_userfaultfd_fail(&uf);",
            "fail_uprobe_end:",
            "\tuprobe_end_dup_mmap();",
            "\treturn retval;",
            "",
            "fail_nomem_anon_vma_fork:",
            "\tmpol_put(vma_policy(tmp));",
            "fail_nomem_policy:",
            "\tvm_area_free(tmp);",
            "fail_nomem:",
            "\tretval = -ENOMEM;",
            "\tvm_unacct_memory(charge);",
            "\tgoto loop_out;",
            "}"
          ],
          "function_name": "dup_mmap",
          "description": "实现进程fork时的内存映射复制逻辑，深度遍历原进程的VMA结构创建副本，处理共享文件映射、hugetlb页等特殊内存类型，并管理OOM异常情况下的失败恢复。",
          "similarity": 0.5475765466690063
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/fork.c",
          "start_line": 3121,
          "end_line": 3229,
          "content": [
            "static bool clone3_args_valid(struct kernel_clone_args *kargs)",
            "{",
            "\t/* Verify that no unknown flags are passed along. */",
            "\tif (kargs->flags &",
            "\t    ~(CLONE_LEGACY_FLAGS | CLONE_CLEAR_SIGHAND | CLONE_INTO_CGROUP))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * - make the CLONE_DETACHED bit reusable for clone3",
            "\t * - make the CSIGNAL bits reusable for clone3",
            "\t */",
            "\tif (kargs->flags & (CLONE_DETACHED | (CSIGNAL & (~CLONE_NEWTIME))))",
            "\t\treturn false;",
            "",
            "\tif ((kargs->flags & (CLONE_SIGHAND | CLONE_CLEAR_SIGHAND)) ==",
            "\t    (CLONE_SIGHAND | CLONE_CLEAR_SIGHAND))",
            "\t\treturn false;",
            "",
            "\tif ((kargs->flags & (CLONE_THREAD | CLONE_PARENT)) &&",
            "\t    kargs->exit_signal)",
            "\t\treturn false;",
            "",
            "\tif (!clone3_stack_valid(kargs))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "void walk_process_tree(struct task_struct *top, proc_visitor visitor, void *data)",
            "{",
            "\tstruct task_struct *leader, *parent, *child;",
            "\tint res;",
            "",
            "\tread_lock(&tasklist_lock);",
            "\tleader = top = top->group_leader;",
            "down:",
            "\tfor_each_thread(leader, parent) {",
            "\t\tlist_for_each_entry(child, &parent->children, sibling) {",
            "\t\t\tres = visitor(child, data);",
            "\t\t\tif (res) {",
            "\t\t\t\tif (res < 0)",
            "\t\t\t\t\tgoto out;",
            "\t\t\t\tleader = child;",
            "\t\t\t\tgoto down;",
            "\t\t\t}",
            "up:",
            "\t\t\t;",
            "\t\t}",
            "\t}",
            "",
            "\tif (leader != top) {",
            "\t\tchild = leader;",
            "\t\tparent = child->real_parent;",
            "\t\tleader = parent->group_leader;",
            "\t\tgoto up;",
            "\t}",
            "out:",
            "\tread_unlock(&tasklist_lock);",
            "}",
            "static void sighand_ctor(void *data)",
            "{",
            "\tstruct sighand_struct *sighand = data;",
            "",
            "\tspin_lock_init(&sighand->siglock);",
            "\tinit_waitqueue_head(&sighand->signalfd_wqh);",
            "}",
            "void __init mm_cache_init(void)",
            "{",
            "\tunsigned int mm_size;",
            "",
            "\t/*",
            "\t * The mm_cpumask is located at the end of mm_struct, and is",
            "\t * dynamically sized based on the maximum CPU number this system",
            "\t * can have, taking hotplug into account (nr_cpu_ids).",
            "\t */",
            "\tmm_size = sizeof(struct mm_struct) + cpumask_size() + mm_cid_size();",
            "",
            "\tmm_cachep = kmem_cache_create_usercopy(\"mm_struct\",",
            "\t\t\tmm_size, ARCH_MIN_MMSTRUCT_ALIGN,",
            "\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,",
            "\t\t\toffsetof(struct mm_struct, saved_auxv),",
            "\t\t\tsizeof_field(struct mm_struct, saved_auxv),",
            "\t\t\tNULL);",
            "}",
            "void __init proc_caches_init(void)",
            "{",
            "\tsighand_cachep = kmem_cache_create(\"sighand_cache\",",
            "\t\t\tsizeof(struct sighand_struct), 0,",
            "\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_TYPESAFE_BY_RCU|",
            "\t\t\tSLAB_ACCOUNT, sighand_ctor);",
            "\tsignal_cachep = kmem_cache_create(\"signal_cache\",",
            "\t\t\tsizeof(struct signal_struct), 0,",
            "\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,",
            "\t\t\tNULL);",
            "\tfiles_cachep = kmem_cache_create(\"files_cache\",",
            "\t\t\tsizeof(struct files_struct), 0,",
            "\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,",
            "\t\t\tNULL);",
            "\tfs_cachep = kmem_cache_create(\"fs_cache\",",
            "\t\t\tsizeof(struct fs_struct), 0,",
            "\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,",
            "\t\t\tNULL);",
            "",
            "\tvm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC|SLAB_ACCOUNT);",
            "#ifdef CONFIG_PER_VMA_LOCK",
            "\tvma_lock_cachep = KMEM_CACHE(vma_lock, SLAB_PANIC|SLAB_ACCOUNT);",
            "#endif",
            "\tmmap_init();",
            "\tnsproxy_cache_init();",
            "}"
          ],
          "function_name": "clone3_args_valid, walk_process_tree, sighand_ctor, mm_cache_init, proc_caches_init",
          "description": "验证clone3参数合法性，遍历进程树执行回调函数，初始化信号处理手柄构造函数及内存管理结构体缓存，注册信号、文件描述符等结构体的Slab缓存",
          "similarity": 0.5395541191101074
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/fork.c",
          "start_line": 2924,
          "end_line": 3037,
          "content": [
            "pid_t kernel_thread(int (*fn)(void *), void *arg, const char *name,",
            "\t\t    unsigned long flags)",
            "{",
            "\tstruct kernel_clone_args args = {",
            "\t\t.flags\t\t= ((lower_32_bits(flags) | CLONE_VM |",
            "\t\t\t\t    CLONE_UNTRACED) & ~CSIGNAL),",
            "\t\t.exit_signal\t= (lower_32_bits(flags) & CSIGNAL),",
            "\t\t.fn\t\t= fn,",
            "\t\t.fn_arg\t\t= arg,",
            "\t\t.name\t\t= name,",
            "\t\t.kthread\t= 1,",
            "\t};",
            "",
            "\treturn kernel_clone(&args);",
            "}",
            "pid_t user_mode_thread(int (*fn)(void *), void *arg, unsigned long flags)",
            "{",
            "\tstruct kernel_clone_args args = {",
            "\t\t.flags\t\t= ((lower_32_bits(flags) | CLONE_VM |",
            "\t\t\t\t    CLONE_UNTRACED) & ~CSIGNAL),",
            "\t\t.exit_signal\t= (lower_32_bits(flags) & CSIGNAL),",
            "\t\t.fn\t\t= fn,",
            "\t\t.fn_arg\t\t= arg,",
            "\t};",
            "",
            "\treturn kernel_clone(&args);",
            "}",
            "noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,",
            "\t\t\t\t\t      struct clone_args __user *uargs,",
            "\t\t\t\t\t      size_t usize)",
            "{",
            "\tint err;",
            "\tstruct clone_args args;",
            "\tpid_t *kset_tid = kargs->set_tid;",
            "",
            "\tBUILD_BUG_ON(offsetofend(struct clone_args, tls) !=",
            "\t\t     CLONE_ARGS_SIZE_VER0);",
            "\tBUILD_BUG_ON(offsetofend(struct clone_args, set_tid_size) !=",
            "\t\t     CLONE_ARGS_SIZE_VER1);",
            "\tBUILD_BUG_ON(offsetofend(struct clone_args, cgroup) !=",
            "\t\t     CLONE_ARGS_SIZE_VER2);",
            "\tBUILD_BUG_ON(sizeof(struct clone_args) != CLONE_ARGS_SIZE_VER2);",
            "",
            "\tif (unlikely(usize > PAGE_SIZE))",
            "\t\treturn -E2BIG;",
            "\tif (unlikely(usize < CLONE_ARGS_SIZE_VER0))",
            "\t\treturn -EINVAL;",
            "",
            "\terr = copy_struct_from_user(&args, sizeof(args), uargs, usize);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (unlikely(args.set_tid_size > MAX_PID_NS_LEVEL))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (unlikely(!args.set_tid && args.set_tid_size > 0))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (unlikely(args.set_tid && args.set_tid_size == 0))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Verify that higher 32bits of exit_signal are unset and that",
            "\t * it is a valid signal",
            "\t */",
            "\tif (unlikely((args.exit_signal & ~((u64)CSIGNAL)) ||",
            "\t\t     !valid_signal(args.exit_signal)))",
            "\t\treturn -EINVAL;",
            "",
            "\tif ((args.flags & CLONE_INTO_CGROUP) &&",
            "\t    (args.cgroup > INT_MAX || usize < CLONE_ARGS_SIZE_VER2))",
            "\t\treturn -EINVAL;",
            "",
            "\t*kargs = (struct kernel_clone_args){",
            "\t\t.flags\t\t= args.flags,",
            "\t\t.pidfd\t\t= u64_to_user_ptr(args.pidfd),",
            "\t\t.child_tid\t= u64_to_user_ptr(args.child_tid),",
            "\t\t.parent_tid\t= u64_to_user_ptr(args.parent_tid),",
            "\t\t.exit_signal\t= args.exit_signal,",
            "\t\t.stack\t\t= args.stack,",
            "\t\t.stack_size\t= args.stack_size,",
            "\t\t.tls\t\t= args.tls,",
            "\t\t.set_tid_size\t= args.set_tid_size,",
            "\t\t.cgroup\t\t= args.cgroup,",
            "\t};",
            "",
            "\tif (args.set_tid &&",
            "\t\tcopy_from_user(kset_tid, u64_to_user_ptr(args.set_tid),",
            "\t\t\t(kargs->set_tid_size * sizeof(pid_t))))",
            "\t\treturn -EFAULT;",
            "",
            "\tkargs->set_tid = kset_tid;",
            "",
            "\treturn 0;",
            "}",
            "static inline bool clone3_stack_valid(struct kernel_clone_args *kargs)",
            "{",
            "\tif (kargs->stack == 0) {",
            "\t\tif (kargs->stack_size > 0)",
            "\t\t\treturn false;",
            "\t} else {",
            "\t\tif (kargs->stack_size == 0)",
            "\t\t\treturn false;",
            "",
            "\t\tif (!access_ok((void __user *)kargs->stack, kargs->stack_size))",
            "\t\t\treturn false;",
            "",
            "#if !defined(CONFIG_STACK_GROWSUP)",
            "\t\tkargs->stack += kargs->stack_size;",
            "#endif",
            "\t}",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "kernel_thread, user_mode_thread, copy_clone_args_from_user, clone3_stack_valid",
          "description": "提供内核线程与用户线程创建接口，解析并验证clone3参数，转换用户空间clone_args到内核结构体，校验栈地址有效性",
          "similarity": 0.5347715616226196
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/fork.c",
          "start_line": 2176,
          "end_line": 2282,
          "content": [
            "static void rv_task_fork(struct task_struct *p)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < RV_PER_TASK_MONITORS; i++)",
            "\t\tp->rv[i].da_mon.monitoring = false;",
            "}",
            "static inline void init_idle_pids(struct task_struct *idle)",
            "{",
            "\tenum pid_type type;",
            "",
            "\tfor (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {",
            "\t\tINIT_HLIST_NODE(&idle->pid_links[type]); /* not really needed */",
            "\t\tinit_task_pid(idle, type, &init_struct_pid);",
            "\t}",
            "}",
            "static int idle_dummy(void *dummy)",
            "{",
            "\t/* This function is never called */",
            "\treturn 0;",
            "}",
            "pid_t kernel_clone(struct kernel_clone_args *args)",
            "{",
            "\tu64 clone_flags = args->flags;",
            "\tstruct completion vfork;",
            "\tstruct pid *pid;",
            "\tstruct task_struct *p;",
            "\tint trace = 0;",
            "\tpid_t nr;",
            "",
            "\t/*",
            "\t * For legacy clone() calls, CLONE_PIDFD uses the parent_tid argument",
            "\t * to return the pidfd. Hence, CLONE_PIDFD and CLONE_PARENT_SETTID are",
            "\t * mutually exclusive. With clone3() CLONE_PIDFD has grown a separate",
            "\t * field in struct clone_args and it still doesn't make sense to have",
            "\t * them both point at the same memory location. Performing this check",
            "\t * here has the advantage that we don't need to have a separate helper",
            "\t * to check for legacy clone().",
            "\t */",
            "\tif ((args->flags & CLONE_PIDFD) &&",
            "\t    (args->flags & CLONE_PARENT_SETTID) &&",
            "\t    (args->pidfd == args->parent_tid))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Determine whether and which event to report to ptracer.  When",
            "\t * called from kernel_thread or CLONE_UNTRACED is explicitly",
            "\t * requested, no event is reported; otherwise, report if the event",
            "\t * for the type of forking is enabled.",
            "\t */",
            "\tif (!(clone_flags & CLONE_UNTRACED)) {",
            "\t\tif (clone_flags & CLONE_VFORK)",
            "\t\t\ttrace = PTRACE_EVENT_VFORK;",
            "\t\telse if (args->exit_signal != SIGCHLD)",
            "\t\t\ttrace = PTRACE_EVENT_CLONE;",
            "\t\telse",
            "\t\t\ttrace = PTRACE_EVENT_FORK;",
            "",
            "\t\tif (likely(!ptrace_event_enabled(current, trace)))",
            "\t\t\ttrace = 0;",
            "\t}",
            "",
            "\tp = copy_process(NULL, trace, NUMA_NO_NODE, args);",
            "\tadd_latent_entropy();",
            "",
            "\tif (IS_ERR(p))",
            "\t\treturn PTR_ERR(p);",
            "",
            "\t/*",
            "\t * Do this prior waking up the new thread - the thread pointer",
            "\t * might get invalid after that point, if the thread exits quickly.",
            "\t */",
            "\ttrace_sched_process_fork(current, p);",
            "",
            "\tpid = get_task_pid(p, PIDTYPE_PID);",
            "\tnr = pid_vnr(pid);",
            "",
            "\tif (clone_flags & CLONE_PARENT_SETTID)",
            "\t\tput_user(nr, args->parent_tid);",
            "",
            "\tif (clone_flags & CLONE_VFORK) {",
            "\t\tp->vfork_done = &vfork;",
            "\t\tinit_completion(&vfork);",
            "\t\tget_task_struct(p);",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_LRU_GEN_WALKS_MMU) && !(clone_flags & CLONE_VM)) {",
            "\t\t/* lock the task to synchronize with memcg migration */",
            "\t\ttask_lock(p);",
            "\t\tlru_gen_add_mm(p->mm);",
            "\t\ttask_unlock(p);",
            "\t}",
            "",
            "\twake_up_new_task(p);",
            "",
            "\t/* forking complete and child started to run, tell ptracer */",
            "\tif (unlikely(trace))",
            "\t\tptrace_event_pid(trace, pid);",
            "",
            "\tif (clone_flags & CLONE_VFORK) {",
            "\t\tif (!wait_for_vfork_done(p, &vfork))",
            "\t\t\tptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);",
            "\t}",
            "",
            "\tput_pid(pid);",
            "\treturn nr;",
            "}"
          ],
          "function_name": "rv_task_fork, init_idle_pids, idle_dummy, kernel_clone",
          "description": "实现kernel_clone核心逻辑，创建新进程并处理克隆标志，管理子进程启动、vfork等待及进程树遍历，包含空闲任务PID初始化与RV监控器重置",
          "similarity": 0.534342885017395
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/fork.c",
          "start_line": 847,
          "end_line": 965,
          "content": [
            "static inline int mm_alloc_pgd(struct mm_struct *mm)",
            "{",
            "\tmm->pgd = pgd_alloc(mm);",
            "\tif (unlikely(!mm->pgd))",
            "\t\treturn -ENOMEM;",
            "\treturn 0;",
            "}",
            "static inline void mm_free_pgd(struct mm_struct *mm)",
            "{",
            "\tpgd_free(mm, mm->pgd);",
            "}",
            "static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)",
            "{",
            "\tmmap_write_lock(oldmm);",
            "\tdup_mm_exe_file(mm, oldmm);",
            "\tmmap_write_unlock(oldmm);",
            "\treturn 0;",
            "}",
            "static void check_mm(struct mm_struct *mm)",
            "{",
            "\tint i;",
            "",
            "\tBUILD_BUG_ON_MSG(ARRAY_SIZE(resident_page_types) != NR_MM_COUNTERS,",
            "\t\t\t \"Please make sure 'struct resident_page_types[]' is updated as well\");",
            "",
            "\tfor (i = 0; i < NR_MM_COUNTERS; i++) {",
            "\t\tlong x = percpu_counter_sum(&mm->rss_stat[i]);",
            "",
            "\t\tif (unlikely(x))",
            "\t\t\tpr_alert(\"BUG: Bad rss-counter state mm:%p type:%s val:%ld\\n\",",
            "\t\t\t\t mm, resident_page_types[i], x);",
            "\t}",
            "",
            "\tif (mm_pgtables_bytes(mm))",
            "\t\tpr_alert(\"BUG: non-zero pgtables_bytes on freeing mm: %ld\\n\",",
            "\t\t\t\tmm_pgtables_bytes(mm));",
            "",
            "#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !defined(CONFIG_SPLIT_PMD_PTLOCKS)",
            "\tVM_BUG_ON_MM(mm->pmd_huge_pte, mm);",
            "#endif",
            "}",
            "static void do_check_lazy_tlb(void *arg)",
            "{",
            "\tstruct mm_struct *mm = arg;",
            "",
            "\tWARN_ON_ONCE(current->active_mm == mm);",
            "}",
            "static void do_shoot_lazy_tlb(void *arg)",
            "{",
            "\tstruct mm_struct *mm = arg;",
            "",
            "\tif (current->active_mm == mm) {",
            "\t\tWARN_ON_ONCE(current->mm);",
            "\t\tcurrent->active_mm = &init_mm;",
            "\t\tswitch_mm(mm, &init_mm, current);",
            "\t}",
            "}",
            "static void cleanup_lazy_tlbs(struct mm_struct *mm)",
            "{",
            "\tif (!IS_ENABLED(CONFIG_MMU_LAZY_TLB_SHOOTDOWN)) {",
            "\t\t/*",
            "\t\t * In this case, lazy tlb mms are refounted and would not reach",
            "\t\t * __mmdrop until all CPUs have switched away and mmdrop()ed.",
            "\t\t */",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Lazy mm shootdown does not refcount \"lazy tlb mm\" usage, rather it",
            "\t * requires lazy mm users to switch to another mm when the refcount",
            "\t * drops to zero, before the mm is freed. This requires IPIs here to",
            "\t * switch kernel threads to init_mm.",
            "\t *",
            "\t * archs that use IPIs to flush TLBs can piggy-back that lazy tlb mm",
            "\t * switch with the final userspace teardown TLB flush which leaves the",
            "\t * mm lazy on this CPU but no others, reducing the need for additional",
            "\t * IPIs here. There are cases where a final IPI is still required here,",
            "\t * such as the final mmdrop being performed on a different CPU than the",
            "\t * one exiting, or kernel threads using the mm when userspace exits.",
            "\t *",
            "\t * IPI overheads have not found to be expensive, but they could be",
            "\t * reduced in a number of possible ways, for example (roughly",
            "\t * increasing order of complexity):",
            "\t * - The last lazy reference created by exit_mm() could instead switch",
            "\t *   to init_mm, however it's probable this will run on the same CPU",
            "\t *   immediately afterwards, so this may not reduce IPIs much.",
            "\t * - A batch of mms requiring IPIs could be gathered and freed at once.",
            "\t * - CPUs store active_mm where it can be remotely checked without a",
            "\t *   lock, to filter out false-positives in the cpumask.",
            "\t * - After mm_users or mm_count reaches zero, switching away from the",
            "\t *   mm could clear mm_cpumask to reduce some IPIs, perhaps together",
            "\t *   with some batching or delaying of the final IPIs.",
            "\t * - A delayed freeing and RCU-like quiescing sequence based on mm",
            "\t *   switching to avoid IPIs completely.",
            "\t */",
            "\ton_each_cpu_mask(mm_cpumask(mm), do_shoot_lazy_tlb, (void *)mm, 1);",
            "\tif (IS_ENABLED(CONFIG_DEBUG_VM_SHOOT_LAZIES))",
            "\t\ton_each_cpu(do_check_lazy_tlb, (void *)mm, 1);",
            "}",
            "void __mmdrop(struct mm_struct *mm)",
            "{",
            "\tBUG_ON(mm == &init_mm);",
            "\tWARN_ON_ONCE(mm == current->mm);",
            "",
            "\t/* Ensure no CPUs are using this as their lazy tlb mm */",
            "\tcleanup_lazy_tlbs(mm);",
            "",
            "\tWARN_ON_ONCE(mm == current->active_mm);",
            "\tmm_free_pgd(mm);",
            "\tdestroy_context(mm);",
            "\tmmu_notifier_subscriptions_destroy(mm);",
            "\tcheck_mm(mm);",
            "\tput_user_ns(mm->user_ns);",
            "\tmm_pasid_drop(mm);",
            "\tmm_destroy_cid(mm);",
            "\tpercpu_counter_destroy_many(mm->rss_stat, NR_MM_COUNTERS);",
            "",
            "\tfree_mm(mm);",
            "}"
          ],
          "function_name": "mm_alloc_pgd, mm_free_pgd, dup_mmap, check_mm, do_check_lazy_tlb, do_shoot_lazy_tlb, cleanup_lazy_tlbs, __mmdrop",
          "description": "实现mm结构体的页目录表分配与释放，dup_mmap复制mmap信息，check_mm验证内存计数器状态，清理延迟TLB射杀并安全释放mm资源。",
          "similarity": 0.5291656255722046
        }
      ]
    }
  ]
}