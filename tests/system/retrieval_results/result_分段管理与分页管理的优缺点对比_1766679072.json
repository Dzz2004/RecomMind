{
  "query": "分段管理与分页管理的优缺点对比",
  "timestamp": "2025-12-26 00:11:12",
  "retrieved_files": [
    {
      "source_file": "mm/khugepaged.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:26:37\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `khugepaged.c`\n\n---\n\n# khugepaged.c 技术文档\n\n## 1. 文件概述\n\n`khugepaged.c` 是 Linux 内核中透明大页（Transparent Huge Page, THP）子系统的核心组件之一，负责在后台异步地将符合条件的小页（4KB）合并为大页（通常为 2MB 的 PMD 级别大页）。该文件实现了名为 `khugepaged` 的内核线程及其相关扫描、合并逻辑，旨在提升内存访问性能并减少 TLB 压力。通过周期性扫描进程地址空间，识别可合并区域，并尝试分配和填充大页，从而优化系统整体内存效率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enum scan_result`**  \n  定义了页面扫描过程中可能返回的各种结果状态码，用于控制合并流程的决策（如失败原因、成功条件等）。\n\n- **`struct collapse_control`**  \n  控制页面折叠（collapse）过程的上下文信息，包括是否由 `khugepaged` 发起、各 NUMA 节点的负载统计及分配回退掩码。\n\n- **`struct khugepaged_mm_slot`**  \n  表示正在被 `khugepaged` 扫描的每个 `mm_struct`（进程地址空间）的元数据槽位，继承自通用 `mm_slot` 结构。\n\n- **`struct khugepaged_scan`**  \n  全局扫描游标，记录当前扫描的 `mm` 列表头、当前 `mm_slot` 及下一次扫描的虚拟地址。\n\n### 全局变量\n\n- `khugepaged_thread`：指向后台 `khugepaged` 内核线程的 `task_struct`。\n- `khugepaged_pages_to_scan`：每次扫描迭代处理的 PTE 或 VMA 数量。\n- `khugepaged_scan_sleep_millisecs` / `khugepaged_alloc_sleep_millisecs`：控制扫描与内存分配的休眠间隔。\n- `khugepaged_max_ptes_none/swap/shared`：限制在合并过程中允许存在的未映射、交换或共享 PTE 的最大数量。\n- `mm_slots_hash`：哈希表，用于快速查找正在被扫描的 `mm_struct`。\n- `khugepaged_scan`：全局唯一的扫描状态结构体。\n\n### Sysfs 接口（CONFIG_SYSFS）\n\n提供用户空间可配置参数：\n- `scan_sleep_millisecs`：扫描间隔\n- `alloc_sleep_millisecs`：分配失败后的重试间隔\n- `pages_to_scan`：每次扫描的页数\n- `pages_collapsed` / `full_scans`：只读统计信息\n- `defrag`：是否启用内存碎片整理\n- `max_ptes_none` / `max_ptes_swap`：控制合并容忍度\n\n## 3. 关键实现\n\n### 后台扫描机制\n- 使用单一线程 `khugepaged` 循环遍历所有注册到 `mm_slots_hash` 中的进程地址空间。\n- 每次从 `khugepaged_scan.mm_head` 列表中取出一个 `mm_slot`，按虚拟地址顺序扫描其 VMA 区域。\n- 扫描粒度由 `khugepaged_pages_to_scan` 控制，默认为 4096 页（8×512），每轮扫描后休眠 `khugepaged_scan_sleep_millisecs` 毫秒。\n\n### 大页合并条件\n- 仅对支持透明大页的 VMA（如匿名私有映射）进行处理。\n- 检查目标 2MB 区域内：\n  - 已映射的小页数量足够多；\n  - 未映射（none）、交换（swap）或共享（shared）的 PTE 数量不超过 `khugepaged_max_ptes_*` 阈值；\n  - 所有页面满足可合并条件（如非 KSM、非 compound、已加入 LRU、引用计数合适等）。\n- 若满足条件，则分配一个新大页，复制小页内容，并更新页表。\n\n### 内存分配与回退策略\n- 优先在本地 NUMA 节点分配大页。\n- 若分配失败且启用了 `defrag`，则尝试内存压缩（compaction）。\n- 支持基于 `alloc_nmask` 的跨节点分配回退。\n\n### 并发与同步\n- 使用 `khugepaged_mutex` 保护关键操作（如添加/移除 mm slot）。\n- 通过 `mm_slot` 机制确保同一 `mm` 不被重复扫描。\n- 利用 RCU 和页锁（`trylock_page()`）避免与用户态访问或其它内核路径冲突。\n\n### 统计与追踪\n- 更新 `khugepaged_pages_collapsed` 和 `khugepaged_full_scans` 等统计计数器。\n- 集成 `trace/events/huge_memory.h` 提供详细的合并事件追踪点。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/rmap.h>`、`<linux/swap.h>` 等，进行页表遍历、反向映射、页面迁移等操作。\n- **透明大页框架**：与 `huge_memory.c` 协同工作，共享 THP 配置标志（如 `TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG`）。\n- **KSM（Kernel Samepage Merging）**：检查页面是否已被 KSM 标记，避免合并 KSM 页面。\n- **Userfaultfd**：检测 `UFFD_WP`（用户态写保护）标记，防止非法合并。\n- **NUMA 与内存策略**：使用 `nodemask_t` 和 NUMA 感知分配。\n- **内核线程与调度**：基于 `kthread` 框架实现后台任务，支持 freezer（挂起/恢复）。\n- **Sysfs**：通过 sysfs 向用户空间暴露 tunable 参数（需 `CONFIG_SYSFS`）。\n\n## 5. 使用场景\n\n- **通用服务器负载**：在数据库、虚拟化、大数据处理等内存密集型应用中，自动提升 TLB 覆盖率，降低缺页开销。\n- **延迟敏感型应用**：通过后台预合并，避免运行时同步分配大页导致的延迟毛刺。\n- **内存碎片整理**：配合 `defrag` 选项，在内存紧张时主动整理碎片以促进大页分配。\n- **动态调优**：管理员可通过 sysfs 实时调整扫描频率、合并激进程度等参数，平衡性能与内存开销。\n- **NUMA 系统优化**：在多节点系统中，结合本地分配策略提升内存访问局部性。",
      "similarity": 0.5663670301437378,
      "chunks": [
        {
          "chunk_id": 8,
          "file_path": "mm/khugepaged.c",
          "start_line": 1101,
          "end_line": 1261,
          "content": [
            "static int collapse_huge_page(struct mm_struct *mm, unsigned long address,",
            "\t\t\t      int referenced, int unmapped,",
            "\t\t\t      struct collapse_control *cc)",
            "{",
            "\tLIST_HEAD(compound_pagelist);",
            "\tpmd_t *pmd, _pmd;",
            "\tpte_t *pte;",
            "\tpgtable_t pgtable;",
            "\tstruct folio *folio;",
            "\tspinlock_t *pmd_ptl, *pte_ptl;",
            "\tint result = SCAN_FAIL;",
            "\tstruct vm_area_struct *vma;",
            "\tstruct mmu_notifier_range range;",
            "",
            "\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);",
            "",
            "\t/*",
            "\t * Before allocating the hugepage, release the mmap_lock read lock.",
            "\t * The allocation can take potentially a long time if it involves",
            "\t * sync compaction, and we do not need to hold the mmap_lock during",
            "\t * that. We will recheck the vma after taking it again in write mode.",
            "\t */",
            "\tmmap_read_unlock(mm);",
            "",
            "\tresult = alloc_charge_folio(&folio, mm, cc);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_nolock;",
            "",
            "\tmmap_read_lock(mm);",
            "\tresult = hugepage_vma_revalidate(mm, address, true, &vma, cc);",
            "\tif (result != SCAN_SUCCEED) {",
            "\t\tmmap_read_unlock(mm);",
            "\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tresult = find_pmd_or_thp_or_none(mm, address, &pmd);",
            "\tif (result != SCAN_SUCCEED) {",
            "\t\tmmap_read_unlock(mm);",
            "\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tif (unmapped) {",
            "\t\t/*",
            "\t\t * __collapse_huge_page_swapin will return with mmap_lock",
            "\t\t * released when it fails. So we jump out_nolock directly in",
            "\t\t * that case.  Continuing to collapse causes inconsistency.",
            "\t\t */",
            "\t\tresult = __collapse_huge_page_swapin(mm, vma, address, pmd,",
            "\t\t\t\t\t\t     referenced);",
            "\t\tif (result != SCAN_SUCCEED)",
            "\t\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tmmap_read_unlock(mm);",
            "\t/*",
            "\t * Prevent all access to pagetables with the exception of",
            "\t * gup_fast later handled by the ptep_clear_flush and the VM",
            "\t * handled by the anon_vma lock + PG_lock.",
            "\t *",
            "\t * UFFDIO_MOVE is prevented to race as well thanks to the",
            "\t * mmap_lock.",
            "\t */",
            "\tmmap_write_lock(mm);",
            "\tresult = hugepage_vma_revalidate(mm, address, true, &vma, cc);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_up_write;",
            "\t/* check if the pmd is still valid */",
            "\tresult = check_pmd_still_valid(mm, address, pmd);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_up_write;",
            "",
            "\tvma_start_write(vma);",
            "\tanon_vma_lock_write(vma->anon_vma);",
            "",
            "\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, address,",
            "\t\t\t\taddress + HPAGE_PMD_SIZE);",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "",
            "\tpmd_ptl = pmd_lock(mm, pmd); /* probably unnecessary */",
            "\t/*",
            "\t * This removes any huge TLB entry from the CPU so we won't allow",
            "\t * huge and small TLB entries for the same virtual address to",
            "\t * avoid the risk of CPU bugs in that area.",
            "\t *",
            "\t * Parallel GUP-fast is fine since GUP-fast will back off when",
            "\t * it detects PMD is changed.",
            "\t */",
            "\t_pmd = pmdp_collapse_flush(vma, address, pmd);",
            "\tspin_unlock(pmd_ptl);",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "\ttlb_remove_table_sync_one();",
            "",
            "\tpte = pte_offset_map_lock(mm, &_pmd, address, &pte_ptl);",
            "\tif (pte) {",
            "\t\tresult = __collapse_huge_page_isolate(vma, address, pte, cc,",
            "\t\t\t\t\t\t      &compound_pagelist);",
            "\t\tspin_unlock(pte_ptl);",
            "\t} else {",
            "\t\tresult = SCAN_PMD_NULL;",
            "\t}",
            "",
            "\tif (unlikely(result != SCAN_SUCCEED)) {",
            "\t\tif (pte)",
            "\t\t\tpte_unmap(pte);",
            "\t\tspin_lock(pmd_ptl);",
            "\t\tBUG_ON(!pmd_none(*pmd));",
            "\t\t/*",
            "\t\t * We can only use set_pmd_at when establishing",
            "\t\t * hugepmds and never for establishing regular pmds that",
            "\t\t * points to regular pagetables. Use pmd_populate for that",
            "\t\t */",
            "\t\tpmd_populate(mm, pmd, pmd_pgtable(_pmd));",
            "\t\tspin_unlock(pmd_ptl);",
            "\t\tanon_vma_unlock_write(vma->anon_vma);",
            "\t\tgoto out_up_write;",
            "\t}",
            "",
            "\t/*",
            "\t * All pages are isolated and locked so anon_vma rmap",
            "\t * can't run anymore.",
            "\t */",
            "\tanon_vma_unlock_write(vma->anon_vma);",
            "",
            "\tresult = __collapse_huge_page_copy(pte, folio, pmd, _pmd,",
            "\t\t\t\t\t   vma, address, pte_ptl,",
            "\t\t\t\t\t   &compound_pagelist);",
            "\tpte_unmap(pte);",
            "\tif (unlikely(result != SCAN_SUCCEED))",
            "\t\tgoto out_up_write;",
            "",
            "\t/*",
            "\t * The smp_wmb() inside __folio_mark_uptodate() ensures the",
            "\t * copy_huge_page writes become visible before the set_pmd_at()",
            "\t * write.",
            "\t */",
            "\t__folio_mark_uptodate(folio);",
            "\tpgtable = pmd_pgtable(_pmd);",
            "",
            "\t_pmd = mk_huge_pmd(&folio->page, vma->vm_page_prot);",
            "\t_pmd = maybe_pmd_mkwrite(pmd_mkdirty(_pmd), vma);",
            "",
            "\tspin_lock(pmd_ptl);",
            "\tBUG_ON(!pmd_none(*pmd));",
            "\tfolio_add_new_anon_rmap(folio, vma, address, RMAP_EXCLUSIVE);",
            "\tfolio_add_lru_vma(folio, vma);",
            "\tpgtable_trans_huge_deposit(mm, pmd, pgtable);",
            "\tset_pmd_at(mm, address, pmd, _pmd);",
            "\tupdate_mmu_cache_pmd(vma, address, pmd);",
            "\tspin_unlock(pmd_ptl);",
            "",
            "\tfolio = NULL;",
            "",
            "\tresult = SCAN_SUCCEED;",
            "out_up_write:",
            "\tmmap_write_unlock(mm);",
            "out_nolock:",
            "\tif (folio)",
            "\t\tfolio_put(folio);",
            "\ttrace_mm_collapse_huge_page(mm, result == SCAN_SUCCEED, result);",
            "\treturn result;",
            "}"
          ],
          "function_name": "collapse_huge_page",
          "description": "执行大页合并主流程，包括页表隔离、数据复制及新PMD设置",
          "similarity": 0.5778748989105225
        },
        {
          "chunk_id": 5,
          "file_path": "mm/khugepaged.c",
          "start_line": 715,
          "end_line": 826,
          "content": [
            "static void __collapse_huge_page_copy_succeeded(pte_t *pte,",
            "\t\t\t\t\t\tstruct vm_area_struct *vma,",
            "\t\t\t\t\t\tunsigned long address,",
            "\t\t\t\t\t\tspinlock_t *ptl,",
            "\t\t\t\t\t\tstruct list_head *compound_pagelist)",
            "{",
            "\tstruct folio *src, *tmp;",
            "\tpte_t *_pte;",
            "\tpte_t pteval;",
            "",
            "\tfor (_pte = pte; _pte < pte + HPAGE_PMD_NR;",
            "\t     _pte++, address += PAGE_SIZE) {",
            "\t\tpteval = ptep_get(_pte);",
            "\t\tif (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {",
            "\t\t\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, 1);",
            "\t\t\tif (is_zero_pfn(pte_pfn(pteval))) {",
            "\t\t\t\t/*",
            "\t\t\t\t * ptl mostly unnecessary.",
            "\t\t\t\t */",
            "\t\t\t\tspin_lock(ptl);",
            "\t\t\t\tptep_clear(vma->vm_mm, address, _pte);",
            "\t\t\t\tspin_unlock(ptl);",
            "\t\t\t\tksm_might_unmap_zero_page(vma->vm_mm, pteval);",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tstruct page *src_page = pte_page(pteval);",
            "",
            "\t\t\tsrc = page_folio(src_page);",
            "\t\t\tif (!folio_test_large(src))",
            "\t\t\t\trelease_pte_folio(src);",
            "\t\t\t/*",
            "\t\t\t * ptl mostly unnecessary, but preempt has to",
            "\t\t\t * be disabled to update the per-cpu stats",
            "\t\t\t * inside folio_remove_rmap_pte().",
            "\t\t\t */",
            "\t\t\tspin_lock(ptl);",
            "\t\t\tptep_clear(vma->vm_mm, address, _pte);",
            "\t\t\tfolio_remove_rmap_pte(src, src_page, vma);",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\tfree_page_and_swap_cache(src_page);",
            "\t\t}",
            "\t}",
            "",
            "\tlist_for_each_entry_safe(src, tmp, compound_pagelist, lru) {",
            "\t\tlist_del(&src->lru);",
            "\t\tnode_stat_sub_folio(src, NR_ISOLATED_ANON +",
            "\t\t\t\tfolio_is_file_lru(src));",
            "\t\tfolio_unlock(src);",
            "\t\tfree_swap_cache(src);",
            "\t\tfolio_putback_lru(src);",
            "\t}",
            "}",
            "static void __collapse_huge_page_copy_failed(pte_t *pte,",
            "\t\t\t\t\t     pmd_t *pmd,",
            "\t\t\t\t\t     pmd_t orig_pmd,",
            "\t\t\t\t\t     struct vm_area_struct *vma,",
            "\t\t\t\t\t     struct list_head *compound_pagelist)",
            "{",
            "\tspinlock_t *pmd_ptl;",
            "",
            "\t/*",
            "\t * Re-establish the PMD to point to the original page table",
            "\t * entry. Restoring PMD needs to be done prior to releasing",
            "\t * pages. Since pages are still isolated and locked here,",
            "\t * acquiring anon_vma_lock_write is unnecessary.",
            "\t */",
            "\tpmd_ptl = pmd_lock(vma->vm_mm, pmd);",
            "\tpmd_populate(vma->vm_mm, pmd, pmd_pgtable(orig_pmd));",
            "\tspin_unlock(pmd_ptl);",
            "\t/*",
            "\t * Release both raw and compound pages isolated",
            "\t * in __collapse_huge_page_isolate.",
            "\t */",
            "\trelease_pte_pages(pte, pte + HPAGE_PMD_NR, compound_pagelist);",
            "}",
            "static int __collapse_huge_page_copy(pte_t *pte, struct folio *folio,",
            "\t\tpmd_t *pmd, pmd_t orig_pmd, struct vm_area_struct *vma,",
            "\t\tunsigned long address, spinlock_t *ptl,",
            "\t\tstruct list_head *compound_pagelist)",
            "{",
            "\tunsigned int i;",
            "\tint result = SCAN_SUCCEED;",
            "",
            "\t/*",
            "\t * Copying pages' contents is subject to memory poison at any iteration.",
            "\t */",
            "\tfor (i = 0; i < HPAGE_PMD_NR; i++) {",
            "\t\tpte_t pteval = ptep_get(pte + i);",
            "\t\tstruct page *page = folio_page(folio, i);",
            "\t\tunsigned long src_addr = address + i * PAGE_SIZE;",
            "\t\tstruct page *src_page;",
            "",
            "\t\tif (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {",
            "\t\t\tclear_user_highpage(page, src_addr);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tsrc_page = pte_page(pteval);",
            "\t\tif (copy_mc_user_highpage(page, src_page, src_addr, vma)) {",
            "\t\t\tresult = SCAN_COPY_MC;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tif (likely(result == SCAN_SUCCEED))",
            "\t\t__collapse_huge_page_copy_succeeded(pte, vma, address, ptl,",
            "\t\t\t\t\t\t    compound_pagelist);",
            "\telse",
            "\t\t__collapse_huge_page_copy_failed(pte, pmd, orig_pmd, vma,",
            "\t\t\t\t\t\t compound_pagelist);",
            "",
            "\treturn result;",
            "}"
          ],
          "function_name": "__collapse_huge_page_copy_succeeded, __collapse_huge_page_copy_failed, __collapse_huge_page_copy",
          "description": "处理将多个小页合并为大页后的成功与失败路径，成功时释放源页面并更新统计，失败时恢复原页表项",
          "similarity": 0.5388646721839905
        },
        {
          "chunk_id": 7,
          "file_path": "mm/khugepaged.c",
          "start_line": 980,
          "end_line": 1090,
          "content": [
            "static int check_pmd_still_valid(struct mm_struct *mm,",
            "\t\t\t\t unsigned long address,",
            "\t\t\t\t pmd_t *pmd)",
            "{",
            "\tpmd_t *new_pmd;",
            "\tint result = find_pmd_or_thp_or_none(mm, address, &new_pmd);",
            "",
            "\tif (result != SCAN_SUCCEED)",
            "\t\treturn result;",
            "\tif (new_pmd != pmd)",
            "\t\treturn SCAN_FAIL;",
            "\treturn SCAN_SUCCEED;",
            "}",
            "static int __collapse_huge_page_swapin(struct mm_struct *mm,",
            "\t\t\t\t       struct vm_area_struct *vma,",
            "\t\t\t\t       unsigned long haddr, pmd_t *pmd,",
            "\t\t\t\t       int referenced)",
            "{",
            "\tint swapped_in = 0;",
            "\tvm_fault_t ret = 0;",
            "\tunsigned long address, end = haddr + (HPAGE_PMD_NR * PAGE_SIZE);",
            "\tint result;",
            "\tpte_t *pte = NULL;",
            "\tspinlock_t *ptl;",
            "",
            "\tfor (address = haddr; address < end; address += PAGE_SIZE) {",
            "\t\tstruct vm_fault vmf = {",
            "\t\t\t.vma = vma,",
            "\t\t\t.address = address,",
            "\t\t\t.pgoff = linear_page_index(vma, address),",
            "\t\t\t.flags = FAULT_FLAG_ALLOW_RETRY,",
            "\t\t\t.pmd = pmd,",
            "\t\t};",
            "",
            "\t\tif (!pte++) {",
            "\t\t\tpte = pte_offset_map_nolock(mm, pmd, address, &ptl);",
            "\t\t\tif (!pte) {",
            "\t\t\t\tmmap_read_unlock(mm);",
            "\t\t\t\tresult = SCAN_PMD_NULL;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tvmf.orig_pte = ptep_get_lockless(pte);",
            "\t\tif (!is_swap_pte(vmf.orig_pte))",
            "\t\t\tcontinue;",
            "",
            "\t\tvmf.pte = pte;",
            "\t\tvmf.ptl = ptl;",
            "\t\tret = do_swap_page(&vmf);",
            "\t\t/* Which unmaps pte (after perhaps re-checking the entry) */",
            "\t\tpte = NULL;",
            "",
            "\t\t/*",
            "\t\t * do_swap_page returns VM_FAULT_RETRY with released mmap_lock.",
            "\t\t * Note we treat VM_FAULT_RETRY as VM_FAULT_ERROR here because",
            "\t\t * we do not retry here and swap entry will remain in pagetable",
            "\t\t * resulting in later failure.",
            "\t\t */",
            "\t\tif (ret & VM_FAULT_RETRY) {",
            "\t\t\t/* Likely, but not guaranteed, that page lock failed */",
            "\t\t\tresult = SCAN_PAGE_LOCK;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tif (ret & VM_FAULT_ERROR) {",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\tresult = SCAN_FAIL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tswapped_in++;",
            "\t}",
            "",
            "\tif (pte)",
            "\t\tpte_unmap(pte);",
            "",
            "\t/* Drain LRU cache to remove extra pin on the swapped in pages */",
            "\tif (swapped_in)",
            "\t\tlru_add_drain();",
            "",
            "\tresult = SCAN_SUCCEED;",
            "out:",
            "\ttrace_mm_collapse_huge_page_swapin(mm, swapped_in, referenced, result);",
            "\treturn result;",
            "}",
            "static int alloc_charge_folio(struct folio **foliop, struct mm_struct *mm,",
            "\t\t\t      struct collapse_control *cc)",
            "{",
            "\tgfp_t gfp = (cc->is_khugepaged ? alloc_hugepage_khugepaged_gfpmask() :",
            "\t\t     GFP_TRANSHUGE);",
            "\tint node = hpage_collapse_find_target_node(cc);",
            "\tstruct folio *folio;",
            "",
            "\tfolio = __folio_alloc(gfp, HPAGE_PMD_ORDER, node, &cc->alloc_nmask);",
            "\tif (!folio) {",
            "\t\t*foliop = NULL;",
            "\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);",
            "\t\treturn SCAN_ALLOC_HUGE_PAGE_FAIL;",
            "\t}",
            "",
            "\tcount_vm_event(THP_COLLAPSE_ALLOC);",
            "\tif (unlikely(mem_cgroup_charge(folio, mm, gfp))) {",
            "\t\tfolio_put(folio);",
            "\t\t*foliop = NULL;",
            "\t\treturn SCAN_CGROUP_CHARGE_FAIL;",
            "\t}",
            "",
            "\tcount_memcg_folio_events(folio, THP_COLLAPSE_ALLOC, 1);",
            "",
            "\t*foliop = folio;",
            "\treturn SCAN_SUCCEED;",
            "}"
          ],
          "function_name": "check_pmd_still_valid, __collapse_huge_page_swapin, alloc_charge_folio",
          "description": "验证PMD有效性、交换页加载及大页分配的辅助函数",
          "similarity": 0.5334613919258118
        },
        {
          "chunk_id": 3,
          "file_path": "mm/khugepaged.c",
          "start_line": 422,
          "end_line": 525,
          "content": [
            "static bool hugepage_pmd_enabled(void)",
            "{",
            "\t/*",
            "\t * We cover the anon, shmem and the file-backed case here; file-backed",
            "\t * hugepages, when configured in, are determined by the global control.",
            "\t * Anon pmd-sized hugepages are determined by the pmd-size control.",
            "\t * Shmem pmd-sized hugepages are also determined by its pmd-size control,",
            "\t * except when the global shmem_huge is set to SHMEM_HUGE_DENY.",
            "\t */",
            "\tif (IS_ENABLED(CONFIG_READ_ONLY_THP_FOR_FS) &&",
            "\t    hugepage_global_enabled())",
            "\t\treturn true;",
            "\tif (test_bit(PMD_ORDER, &huge_anon_orders_always))",
            "\t\treturn true;",
            "\tif (test_bit(PMD_ORDER, &huge_anon_orders_madvise))",
            "\t\treturn true;",
            "\tif (test_bit(PMD_ORDER, &huge_anon_orders_inherit) &&",
            "\t    hugepage_global_enabled())",
            "\t\treturn true;",
            "\tif (IS_ENABLED(CONFIG_SHMEM) && shmem_hpage_pmd_enabled())",
            "\t\treturn true;",
            "\treturn false;",
            "}",
            "void __khugepaged_enter(struct mm_struct *mm)",
            "{",
            "\tstruct khugepaged_mm_slot *mm_slot;",
            "\tstruct mm_slot *slot;",
            "\tint wakeup;",
            "",
            "\t/* __khugepaged_exit() must not run from under us */",
            "\tVM_BUG_ON_MM(hpage_collapse_test_exit(mm), mm);",
            "\tif (unlikely(test_and_set_bit(MMF_VM_HUGEPAGE, &mm->flags)))",
            "\t\treturn;",
            "",
            "\tmm_slot = mm_slot_alloc(mm_slot_cache);",
            "\tif (!mm_slot)",
            "\t\treturn;",
            "",
            "\tslot = &mm_slot->slot;",
            "",
            "\tspin_lock(&khugepaged_mm_lock);",
            "\tmm_slot_insert(mm_slots_hash, mm, slot);",
            "\t/*",
            "\t * Insert just behind the scanning cursor, to let the area settle",
            "\t * down a little.",
            "\t */",
            "\twakeup = list_empty(&khugepaged_scan.mm_head);",
            "\tlist_add_tail(&slot->mm_node, &khugepaged_scan.mm_head);",
            "\tspin_unlock(&khugepaged_mm_lock);",
            "",
            "\tmmgrab(mm);",
            "\tif (wakeup)",
            "\t\twake_up_interruptible(&khugepaged_wait);",
            "}",
            "void khugepaged_enter_vma(struct vm_area_struct *vma,",
            "\t\t\t  unsigned long vm_flags)",
            "{",
            "\tif (!test_bit(MMF_VM_HUGEPAGE, &vma->vm_mm->flags) &&",
            "\t    hugepage_pmd_enabled()) {",
            "\t\tif (thp_vma_allowable_order(vma, vm_flags, TVA_ENFORCE_SYSFS,",
            "\t\t\t\t\t    PMD_ORDER))",
            "\t\t\t__khugepaged_enter(vma->vm_mm);",
            "\t}",
            "}",
            "void __khugepaged_exit(struct mm_struct *mm)",
            "{",
            "\tstruct khugepaged_mm_slot *mm_slot;",
            "\tstruct mm_slot *slot;",
            "\tint free = 0;",
            "",
            "\tspin_lock(&khugepaged_mm_lock);",
            "\tslot = mm_slot_lookup(mm_slots_hash, mm);",
            "\tmm_slot = mm_slot_entry(slot, struct khugepaged_mm_slot, slot);",
            "\tif (mm_slot && khugepaged_scan.mm_slot != mm_slot) {",
            "\t\thash_del(&slot->hash);",
            "\t\tlist_del(&slot->mm_node);",
            "\t\tfree = 1;",
            "\t}",
            "\tspin_unlock(&khugepaged_mm_lock);",
            "",
            "\tif (free) {",
            "\t\tclear_bit(MMF_VM_HUGEPAGE, &mm->flags);",
            "\t\tmm_slot_free(mm_slot_cache, mm_slot);",
            "\t\tmmdrop(mm);",
            "\t} else if (mm_slot) {",
            "\t\t/*",
            "\t\t * This is required to serialize against",
            "\t\t * hpage_collapse_test_exit() (which is guaranteed to run",
            "\t\t * under mmap sem read mode). Stop here (after we return all",
            "\t\t * pagetables will be destroyed) until khugepaged has finished",
            "\t\t * working on the pagetables under the mmap_lock.",
            "\t\t */",
            "\t\tmmap_write_lock(mm);",
            "\t\tmmap_write_unlock(mm);",
            "\t}",
            "}",
            "static void release_pte_folio(struct folio *folio)",
            "{",
            "\tnode_stat_mod_folio(folio,",
            "\t\t\tNR_ISOLATED_ANON + folio_is_file_lru(folio),",
            "\t\t\t-folio_nr_pages(folio));",
            "\tfolio_unlock(folio);",
            "\tfolio_putback_lru(folio);",
            "}"
          ],
          "function_name": "hugepage_pmd_enabled, __khugepaged_enter, khugepaged_enter_vma, __khugepaged_exit, release_pte_folio",
          "description": "判断当前系统是否支持大页；管理mm结构与VMA的关联关系，通过锁保护实现大页合并的注册与注销；提供释放页表项所关联页面的函数。",
          "similarity": 0.5305315852165222
        },
        {
          "chunk_id": 15,
          "file_path": "mm/khugepaged.c",
          "start_line": 2726,
          "end_line": 2874,
          "content": [
            "void khugepaged_min_free_kbytes_update(void)",
            "{",
            "\tmutex_lock(&khugepaged_mutex);",
            "\tif (hugepage_pmd_enabled() && khugepaged_thread)",
            "\t\tset_recommended_min_free_kbytes();",
            "\tmutex_unlock(&khugepaged_mutex);",
            "}",
            "bool current_is_khugepaged(void)",
            "{",
            "\treturn kthread_func(current) == khugepaged;",
            "}",
            "static int madvise_collapse_errno(enum scan_result r)",
            "{",
            "\t/*",
            "\t * MADV_COLLAPSE breaks from existing madvise(2) conventions to provide",
            "\t * actionable feedback to caller, so they may take an appropriate",
            "\t * fallback measure depending on the nature of the failure.",
            "\t */",
            "\tswitch (r) {",
            "\tcase SCAN_ALLOC_HUGE_PAGE_FAIL:",
            "\t\treturn -ENOMEM;",
            "\tcase SCAN_CGROUP_CHARGE_FAIL:",
            "\tcase SCAN_EXCEED_NONE_PTE:",
            "\t\treturn -EBUSY;",
            "\t/* Resource temporary unavailable - trying again might succeed */",
            "\tcase SCAN_PAGE_COUNT:",
            "\tcase SCAN_PAGE_LOCK:",
            "\tcase SCAN_PAGE_LRU:",
            "\tcase SCAN_DEL_PAGE_LRU:",
            "\tcase SCAN_PAGE_FILLED:",
            "\t\treturn -EAGAIN;",
            "\t/*",
            "\t * Other: Trying again likely not to succeed / error intrinsic to",
            "\t * specified memory range. khugepaged likely won't be able to collapse",
            "\t * either.",
            "\t */",
            "\tdefault:",
            "\t\treturn -EINVAL;",
            "\t}",
            "}",
            "int madvise_collapse(struct vm_area_struct *vma, struct vm_area_struct **prev,",
            "\t\t     unsigned long start, unsigned long end)",
            "{",
            "\tstruct collapse_control *cc;",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tunsigned long hstart, hend, addr;",
            "\tint thps = 0, last_fail = SCAN_FAIL;",
            "\tbool mmap_locked = true;",
            "",
            "\tBUG_ON(vma->vm_start > start);",
            "\tBUG_ON(vma->vm_end < end);",
            "",
            "\t*prev = vma;",
            "",
            "\tif (!thp_vma_allowable_order(vma, vma->vm_flags, 0, PMD_ORDER))",
            "\t\treturn -EINVAL;",
            "",
            "\tcc = kmalloc(sizeof(*cc), GFP_KERNEL);",
            "\tif (!cc)",
            "\t\treturn -ENOMEM;",
            "\tcc->is_khugepaged = false;",
            "",
            "\tmmgrab(mm);",
            "\tlru_add_drain_all();",
            "",
            "\thstart = (start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;",
            "\thend = end & HPAGE_PMD_MASK;",
            "",
            "\tfor (addr = hstart; addr < hend; addr += HPAGE_PMD_SIZE) {",
            "\t\tint result = SCAN_FAIL;",
            "",
            "\t\tif (!mmap_locked) {",
            "\t\t\tcond_resched();",
            "\t\t\tmmap_read_lock(mm);",
            "\t\t\tmmap_locked = true;",
            "\t\t\tresult = hugepage_vma_revalidate(mm, addr, false, &vma,",
            "\t\t\t\t\t\t\t cc);",
            "\t\t\tif (result  != SCAN_SUCCEED) {",
            "\t\t\t\tlast_fail = result;",
            "\t\t\t\tgoto out_nolock;",
            "\t\t\t}",
            "",
            "\t\t\thend = min(hend, vma->vm_end & HPAGE_PMD_MASK);",
            "\t\t}",
            "\t\tmmap_assert_locked(mm);",
            "\t\tmemset(cc->node_load, 0, sizeof(cc->node_load));",
            "\t\tnodes_clear(cc->alloc_nmask);",
            "\t\tif (IS_ENABLED(CONFIG_SHMEM) && !vma_is_anonymous(vma)) {",
            "\t\t\tstruct file *file = get_file(vma->vm_file);",
            "\t\t\tpgoff_t pgoff = linear_page_index(vma, addr);",
            "",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\tmmap_locked = false;",
            "\t\t\tresult = hpage_collapse_scan_file(mm, addr, file, pgoff,",
            "\t\t\t\t\t\t\t  cc);",
            "\t\t\tfput(file);",
            "\t\t} else {",
            "\t\t\tresult = hpage_collapse_scan_pmd(mm, vma, addr,",
            "\t\t\t\t\t\t\t &mmap_locked, cc);",
            "\t\t}",
            "\t\tif (!mmap_locked)",
            "\t\t\t*prev = NULL;  /* Tell caller we dropped mmap_lock */",
            "",
            "handle_result:",
            "\t\tswitch (result) {",
            "\t\tcase SCAN_SUCCEED:",
            "\t\tcase SCAN_PMD_MAPPED:",
            "\t\t\t++thps;",
            "\t\t\tbreak;",
            "\t\tcase SCAN_PTE_MAPPED_HUGEPAGE:",
            "\t\t\tBUG_ON(mmap_locked);",
            "\t\t\tBUG_ON(*prev);",
            "\t\t\tmmap_read_lock(mm);",
            "\t\t\tresult = collapse_pte_mapped_thp(mm, addr, true);",
            "\t\t\tmmap_read_unlock(mm);",
            "\t\t\tgoto handle_result;",
            "\t\t/* Whitelisted set of results where continuing OK */",
            "\t\tcase SCAN_PMD_NULL:",
            "\t\tcase SCAN_PTE_NON_PRESENT:",
            "\t\tcase SCAN_PTE_UFFD_WP:",
            "\t\tcase SCAN_PAGE_RO:",
            "\t\tcase SCAN_LACK_REFERENCED_PAGE:",
            "\t\tcase SCAN_PAGE_NULL:",
            "\t\tcase SCAN_PAGE_COUNT:",
            "\t\tcase SCAN_PAGE_LOCK:",
            "\t\tcase SCAN_PAGE_COMPOUND:",
            "\t\tcase SCAN_PAGE_LRU:",
            "\t\tcase SCAN_DEL_PAGE_LRU:",
            "\t\t\tlast_fail = result;",
            "\t\t\tbreak;",
            "\t\tdefault:",
            "\t\t\tlast_fail = result;",
            "\t\t\t/* Other error, exit */",
            "\t\t\tgoto out_maybelock;",
            "\t\t}",
            "\t}",
            "",
            "out_maybelock:",
            "\t/* Caller expects us to hold mmap_lock on return */",
            "\tif (!mmap_locked)",
            "\t\tmmap_read_lock(mm);",
            "out_nolock:",
            "\tmmap_assert_locked(mm);",
            "\tmmdrop(mm);",
            "\tkfree(cc);",
            "",
            "\treturn thps == ((hend - hstart) >> HPAGE_PMD_SHIFT) ? 0",
            "\t\t\t: madvise_collapse_errno(last_fail);",
            "}"
          ],
          "function_name": "khugepaged_min_free_kbytes_update, current_is_khugepaged, madvise_collapse_errno, madvise_collapse",
          "description": "该代码段实现了与HugeTLB大页管理相关的功能。  \n`khugepaged_min_free_kbytes_update` 根据大页启用状态动态调整系统最小空闲内存阈值；`current_is_khugepaged` 用于判断当前进程是否为khugepaged线程；`madvise_collapse` 执行内存区域的大页折叠操作，通过遍历地址范围尝试合并普通页为大页，并根据扫描结果返回对应错误码。  \n注：代码上下文不完整，部分逻辑分支未展示，且`madvise_collapse`中涉及的辅助函数如`hpage_collapse_scan_file`等未在片段中出现。",
          "similarity": 0.5250304937362671
        }
      ]
    },
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.5614719390869141,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.48964548110961914
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.4638311266899109
        },
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.4427073001861572
        }
      ]
    },
    {
      "source_file": "mm/debug_page_alloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:54:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `debug_page_alloc.c`\n\n---\n\n# debug_page_alloc.c 技术文档\n\n## 1. 文件概述\n\n`debug_page_alloc.c` 是 Linux 内核中用于支持页分配调试功能的核心实现文件。该文件主要提供两个关键调试机制：\n\n- **页分配调试（debug_pagealloc）**：在内存分配/释放时对页面进行特殊标记和保护，用于检测内存越界访问、重复释放等错误。\n- **守护页（guard page）机制**：在分配的大块内存前后插入不可访问的“守护页”，用于捕获缓冲区溢出等内存破坏问题。\n\n该文件通过内核启动参数控制调试功能的启用状态和行为参数，并提供底层页标志操作接口供内存管理子系统调用。\n\n## 2. 核心功能\n\n### 全局变量\n- `_debug_guardpage_minorder`：守护页机制生效的最小分配阶数阈值\n- `_debug_pagealloc_enabled_early`：早期启动阶段页分配调试的启用状态\n- `_debug_pagealloc_enabled`：运行时页分配调试功能的静态键开关\n- `_debug_guardpage_enabled`：守护页功能的静态键开关\n\n### 函数接口\n- `early_debug_pagealloc()`：解析 `debug_pagealloc=` 内核启动参数\n- `debug_guardpage_minorder_setup()`：解析 `debug_guardpage_minorder=` 内核启动参数\n- `__set_page_guard()`：为指定页面设置守护页标志和相关属性\n- `__clear_page_guard()`：清除页面的守护页标志和相关属性\n\n### 宏定义\n- `debug_guardpage_minorder()`：获取当前守护页最小阶数值（内联函数）\n\n## 3. 关键实现\n\n### 启动参数处理\n- **`debug_pagealloc` 参数**：通过 `early_param()` 在内核早期初始化阶段解析布尔值参数，控制 `_debug_pagealloc_enabled_early` 的初始状态\n- **`debug_guardpage_minorder` 参数**：解析无符号长整型参数，验证其有效性（0 ≤ value ≤ MAX_PAGE_ORDER/2），设置 `_debug_guardpage_minorder` 全局变量\n\n### 守护页管理\n- **设置守护页 (`__set_page_guard`)**：\n  - 仅当请求的分配阶数小于 `_debug_guardpage_minorder` 时才启用守护页\n  - 设置 `PG_guard` 页面标志位\n  - 初始化 buddy_list 为空链表（防止误用）\n  - 将分配阶数存储在 page->private 字段中\n- **清除守护页 (`__clear_page_guard`)**：\n  - 清除 `PG_guard` 标志位\n  - 将 page->private 重置为 0\n\n### 静态键优化\n- 使用 `DEFINE_STATIC_KEY_FALSE` 定义运行时开关，避免调试代码路径的性能开销\n- 通过 `EXPORT_SYMBOL` 导出符号供其他内核模块使用\n\n## 4. 依赖关系\n\n### 头文件依赖\n- `<linux/mm.h>`：提供核心内存管理数据结构和函数声明\n- `<linux/page-isolation.h>`：提供页面隔离相关功能（如 `__SetPageGuard` 等宏定义）\n\n### 内核配置依赖\n- `CONFIG_DEBUG_PAGEALLOC_ENABLE_DEFAULT`：控制默认是否启用页分配调试功能\n- `MAX_PAGE_ORDER`：定义最大分配阶数常量，用于参数验证\n\n### 符号导出\n- `_debug_pagealloc_enabled_early` 和 `_debug_pagealloc_enabled` 被导出，供内存管理子系统（如伙伴系统）查询调试状态\n\n## 5. 使用场景\n\n### 内存错误检测\n- 在开发和调试阶段启用 `debug_pagealloc=1`，可检测内存越界写入、使用已释放内存等问题\n- 守护页机制特别适用于捕获大内存块分配时的缓冲区溢出错误\n\n### 性能敏感环境\n- 默认情况下调试功能关闭，避免运行时性能开销\n- 通过静态键机制确保禁用时调试代码路径完全不执行\n\n### 内核启动配置\n- 系统管理员可通过内核命令行参数动态调整调试行为：\n  - `debug_pagealloc=1` 启用页分配调试\n  - `debug_guardpage_minorder=N` 设置守护页触发的最小分配阶数\n\n### 内存管理子系统集成\n- 伙伴系统（buddy allocator）在页面分配/释放时调用 `__set_page_guard()` 和 `__clear_page_guard()` 管理守护页状态\n- 页面迁移、内存热插拔等子系统依赖此模块提供的页面状态管理功能",
      "similarity": 0.5461349487304688,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/debug_page_alloc.c",
          "start_line": 1,
          "end_line": 14,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/mm.h>",
            "#include <linux/page-isolation.h>",
            "",
            "unsigned int _debug_guardpage_minorder;",
            "",
            "bool _debug_pagealloc_enabled_early __read_mostly",
            "\t\t\t= IS_ENABLED(CONFIG_DEBUG_PAGEALLOC_ENABLE_DEFAULT);",
            "EXPORT_SYMBOL(_debug_pagealloc_enabled_early);",
            "DEFINE_STATIC_KEY_FALSE(_debug_pagealloc_enabled);",
            "EXPORT_SYMBOL(_debug_pagealloc_enabled);",
            "",
            "DEFINE_STATIC_KEY_FALSE(_debug_guardpage_enabled);",
            ""
          ],
          "function_name": null,
          "description": "定义调试页面分配相关全局变量及符号导出，用于控制早期调试功能启用状态和静态键，支持动态开关调试页保护机制",
          "similarity": 0.48381200432777405
        },
        {
          "chunk_id": 1,
          "file_path": "mm/debug_page_alloc.c",
          "start_line": 15,
          "end_line": 46,
          "content": [
            "static int __init early_debug_pagealloc(char *buf)",
            "{",
            "\treturn kstrtobool(buf, &_debug_pagealloc_enabled_early);",
            "}",
            "static int __init debug_guardpage_minorder_setup(char *buf)",
            "{",
            "\tunsigned long res;",
            "",
            "\tif (kstrtoul(buf, 10, &res) < 0 ||  res > MAX_PAGE_ORDER / 2) {",
            "\t\tpr_err(\"Bad debug_guardpage_minorder value\\n\");",
            "\t\treturn 0;",
            "\t}",
            "\t_debug_guardpage_minorder = res;",
            "\tpr_info(\"Setting debug_guardpage_minorder to %lu\\n\", res);",
            "\treturn 0;",
            "}",
            "bool __set_page_guard(struct zone *zone, struct page *page, unsigned int order)",
            "{",
            "\tif (order >= debug_guardpage_minorder())",
            "\t\treturn false;",
            "",
            "\t__SetPageGuard(page);",
            "\tINIT_LIST_HEAD(&page->buddy_list);",
            "\tset_page_private(page, order);",
            "",
            "\treturn true;",
            "}",
            "void __clear_page_guard(struct zone *zone, struct page *page, unsigned int order)",
            "{",
            "\t__ClearPageGuard(page);",
            "\tset_page_private(page, 0);",
            "}"
          ],
          "function_name": "early_debug_pagealloc, debug_guardpage_minorder_setup, __set_page_guard, __clear_page_guard",
          "description": "实现早期调试参数解析、最小订单值设置及页保护状态管理逻辑，通过订单阈值控制是否对页面添加守护标志并修改页面私有数据",
          "similarity": 0.46791893243789673
        }
      ]
    }
  ]
}