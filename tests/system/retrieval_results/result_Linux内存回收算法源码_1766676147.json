{
  "query": "Linux内存回收算法源码",
  "timestamp": "2025-12-25 23:22:27",
  "retrieved_files": [
    {
      "source_file": "mm/vmscan.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:33:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmscan.c`\n\n---\n\n# vmscan.c 技术文档\n\n## 1. 文件概述\n\n`vmscan.c` 是 Linux 内核内存管理子系统中的核心文件，主要负责**页面回收（page reclaim）**机制的实现。该文件实现了内核在内存压力下如何选择并释放不再活跃或可回收的物理页帧（pages），以维持系统可用内存水位。其核心功能包括：\n\n- 实现 `kswapd` 内核线程，用于后台异步回收内存\n- 提供直接回收（direct reclaim）路径，供分配器在内存不足时同步触发\n- 管理匿名页（anonymous pages）和文件缓存页（file-backed pages）的回收策略\n- 支持基于内存控制组（memcg）的层级化内存回收\n- 与交换（swap）、压缩（compaction）、OOM killer 等子系统协同工作\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct scan_control`**  \n  页面回收上下文控制结构，包含本次回收操作的所有参数和状态：\n  - `nr_to_reclaim`：目标回收页数\n  - `target_mem_cgroup`：目标内存 cgroup（用于 memcg 回收）\n  - `may_unmap` / `may_swap` / `may_writepage`：控制是否允许解除映射、交换、写回\n  - `priority`：扫描优先级（0~12，值越低压力越大）\n  - `order`：请求分配的阶数（影响回收激进程度）\n  - `nr_scanned` / `nr_reclaimed`：已扫描和已回收页数统计\n  - `anon_cost` / `file_cost`：用于平衡匿名页与文件页回收比例\n\n- **全局变量**\n  - `vm_swappiness`（默认 60）：控制系统倾向于回收匿名页（需 swap）还是文件页（可丢弃）\n\n### 主要函数（部分在代码片段中体现）\n\n- `cgroup_reclaim()` / `root_reclaim()`：判断当前回收是否针对特定 memcg 或全局\n- `writeback_throttling_sane()`：判断是否可使用标准脏页限流机制\n- `set_task_reclaim_state()` / `flush_reclaim_state()`：管理任务的 slab 回收状态\n- （注：核心回收函数如 `shrink_lruvec()`、`kswapd()` 等未在片段中展示）\n\n## 3. 关键实现\n\n### 内存回收控制逻辑\n\n- **回收目标决策**：通过 `scan_control` 结构传递回收上下文，区分直接回收（分配失败触发）与 kswapd 后台回收。\n- **LRU 链管理**：利用 `prefetchw_prev_lru_folio` 宏优化 LRU 链遍历时的 CPU 缓存预取性能。\n- **Memcg 集成**：\n  - 若 `target_mem_cgroup` 非空，则优先回收该 cgroup 的内存\n  - 支持 `memory.low` 保护机制：当常规回收无法满足需求且跳过受保护 cgroup 时，会触发二次强制回收（`memcg_low_reclaim`）\n- **脏页处理策略**：\n  - 在传统 memcg 模式下，禁用标准 `balance_dirty_pages()` 限流，改用直接阻塞回收（`writeback_throttling_sane()` 判断）\n  - 通过 `may_writepage` 控制是否在 laptop mode 下批量写回脏页\n\n### 回收统计与状态同步\n\n- **Slab 回收计数**：通过 `reclaim_state` 结构将非 LRU 回收（如 slab 释放）计入全局统计，但**仅在全局回收时计入**，避免 memcg 回收时高估实际效果导致欠回收。\n- **PSI/Trace 集成**：包含 `<trace/events/vmscan.h>` 用于性能分析，支持压力状态指示器（PSI）监控内存压力。\n\n## 4. 依赖关系\n\n### 头文件依赖\n\n- **核心内存管理**：`<linux/mm.h>`, `<linux/gfp.h>`, `<linux/swap.h>`, `<linux/vmstat.h>`\n- **LRU 与反向映射**：`<linux/rmap.h>`, `<linux/pagemap.h>`\n- **内存控制组**：`<linux/memcontrol.h>`\n- **IO 与写回**：`<linux/writeback.h>`, `<linux/backing-dev.h>`\n- **压缩与迁移**：`<linux/compaction.h>`, `<linux/migrate.h>`\n- **体系结构相关**：`<asm/tlbflush.h>`\n\n### 子系统交互\n\n- **Swap 子系统**：通过 `swapops.h` 和 `swap.h` 实现匿名页换出\n- **Slab 分配器**：通过 `reclaim_state` 接收 slab 回收通知\n- **OOM Killer**：当回收无法释放足够内存时触发\n- **Khugepaged**：大页合并/拆分与回收协同\n- **Memory Tiering**：支持分层内存架构中的页降级（demotion）控制\n\n## 5. 使用场景\n\n- **内存分配失败时的直接回收**：当 `alloc_pages()` 等分配函数无法满足请求时，同步调用回收路径。\n- **kswapd 后台回收**：当空闲内存低于 `watermark[low]` 时，唤醒 `kswapd` 线程异步回收至 `watermark[high]`。\n- **Memcg 内存超限时的层级回收**：当某个 cgroup 超过其内存限制时，仅回收该 cgroup 及其子树的页面。\n- **系统休眠（Hibernation）**：通过 `hibernation_mode` 标志优化休眠过程中的内存回收。\n- **主动内存回收（Proactive Reclaim）**：用户空间通过 `memory.reclaim` 接口触发预清回收。\n- **内存压缩准备**：当 `compaction_ready` 置位时，回收操作会为后续内存压缩腾出连续空间。",
      "similarity": 0.6351697444915771,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/vmscan.c",
          "start_line": 343,
          "end_line": 443,
          "content": [
            "unsigned long zone_reclaimable_pages(struct zone *zone)",
            "{",
            "\tunsigned long nr;",
            "",
            "\tnr = zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_FILE) +",
            "\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_FILE);",
            "\tif (can_reclaim_anon_pages(NULL, zone_to_nid(zone), NULL))",
            "\t\tnr += zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_ANON) +",
            "\t\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_ANON);",
            "\t/*",
            "\t * If there are no reclaimable file-backed or anonymous pages,",
            "\t * ensure zones with sufficient free pages are not skipped.",
            "\t * This prevents zones like DMA32 from being ignored in reclaim",
            "\t * scenarios where they can still help alleviate memory pressure.",
            "\t */",
            "\tif (nr == 0)",
            "\t\tnr = zone_page_state_snapshot(zone, NR_FREE_PAGES);",
            "\treturn nr;",
            "}",
            "static unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru,",
            "\t\t\t\t     int zone_idx)",
            "{",
            "\tunsigned long size = 0;",
            "\tint zid;",
            "",
            "\tfor (zid = 0; zid <= zone_idx; zid++) {",
            "\t\tstruct zone *zone = &lruvec_pgdat(lruvec)->node_zones[zid];",
            "",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!mem_cgroup_disabled())",
            "\t\t\tsize += mem_cgroup_get_zone_lru_size(lruvec, lru, zid);",
            "\t\telse",
            "\t\t\tsize += zone_page_state(zone, NR_ZONE_LRU_BASE + lru);",
            "\t}",
            "\treturn size;",
            "}",
            "static unsigned long drop_slab_node(int nid)",
            "{",
            "\tunsigned long freed = 0;",
            "\tstruct mem_cgroup *memcg = NULL;",
            "",
            "\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);",
            "\tdo {",
            "\t\tfreed += shrink_slab(GFP_KERNEL, nid, memcg, 0);",
            "\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);",
            "",
            "\treturn freed;",
            "}",
            "void drop_slab(void)",
            "{",
            "\tint nid;",
            "\tint shift = 0;",
            "\tunsigned long freed;",
            "",
            "\tdo {",
            "\t\tfreed = 0;",
            "\t\tfor_each_online_node(nid) {",
            "\t\t\tif (fatal_signal_pending(current))",
            "\t\t\t\treturn;",
            "",
            "\t\t\tfreed += drop_slab_node(nid);",
            "\t\t}",
            "\t} while ((freed >> shift++) > 1);",
            "}",
            "static int reclaimer_offset(void)",
            "{",
            "\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=",
            "\t\t\tPGDEMOTE_DIRECT - PGDEMOTE_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=",
            "\t\t\tPGDEMOTE_KHUGEPAGED - PGDEMOTE_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=",
            "\t\t\tPGSCAN_DIRECT - PGSCAN_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=",
            "\t\t\tPGSCAN_KHUGEPAGED - PGSCAN_KSWAPD);",
            "",
            "\tif (current_is_kswapd())",
            "\t\treturn 0;",
            "\tif (current_is_khugepaged())",
            "\t\treturn PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD;",
            "\treturn PGSTEAL_DIRECT - PGSTEAL_KSWAPD;",
            "}",
            "static inline int is_page_cache_freeable(struct folio *folio)",
            "{",
            "\t/*",
            "\t * A freeable page cache folio is referenced only by the caller",
            "\t * that isolated the folio, the page cache and optional filesystem",
            "\t * private data at folio->private.",
            "\t */",
            "\treturn folio_ref_count(folio) - folio_test_private(folio) ==",
            "\t\t1 + folio_nr_pages(folio);",
            "}",
            "static void handle_write_error(struct address_space *mapping,",
            "\t\t\t\tstruct folio *folio, int error)",
            "{",
            "\tfolio_lock(folio);",
            "\tif (folio_mapping(folio) == mapping)",
            "\t\tmapping_set_error(mapping, error);",
            "\tfolio_unlock(folio);",
            "}"
          ],
          "function_name": "zone_reclaimable_pages, lruvec_lru_size, drop_slab_node, drop_slab, reclaimer_offset, is_page_cache_freeable, handle_write_error",
          "description": "实现区域可回收页数计算、Slab对象释放及回收偏移量调整逻辑，通过遍历各内存区域统计潜在可回收页数，提供Slab内存碎片回收机制并维护回收进程优先级偏移量。",
          "similarity": 0.6565396785736084
        },
        {
          "chunk_id": 11,
          "file_path": "mm/vmscan.c",
          "start_line": 2156,
          "end_line": 2324,
          "content": [
            "unsigned long reclaim_pages(struct list_head *folio_list, bool ignore_references)",
            "{",
            "\tint nid;",
            "\tunsigned int nr_reclaimed = 0;",
            "\tLIST_HEAD(node_folio_list);",
            "\tunsigned int noreclaim_flag;",
            "",
            "\tif (list_empty(folio_list))",
            "\t\treturn nr_reclaimed;",
            "",
            "\tnoreclaim_flag = memalloc_noreclaim_save();",
            "",
            "\tnid = folio_nid(lru_to_folio(folio_list));",
            "\tdo {",
            "\t\tstruct folio *folio = lru_to_folio(folio_list);",
            "",
            "\t\tif (nid == folio_nid(folio)) {",
            "\t\t\tfolio_clear_active(folio);",
            "\t\t\tlist_move(&folio->lru, &node_folio_list);",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tnr_reclaimed += reclaim_folio_list(&node_folio_list, NODE_DATA(nid),",
            "\t\t\t\t\t\t   ignore_references);",
            "\t\tnid = folio_nid(lru_to_folio(folio_list));",
            "\t} while (!list_empty(folio_list));",
            "",
            "\tnr_reclaimed += reclaim_folio_list(&node_folio_list, NODE_DATA(nid), ignore_references);",
            "",
            "\tmemalloc_noreclaim_restore(noreclaim_flag);",
            "",
            "\treturn nr_reclaimed;",
            "}",
            "static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,",
            "\t\t\t\t struct lruvec *lruvec, struct scan_control *sc)",
            "{",
            "\tif (is_active_lru(lru)) {",
            "\t\tif (sc->may_deactivate & (1 << is_file_lru(lru)))",
            "\t\t\tshrink_active_list(nr_to_scan, lruvec, sc, lru);",
            "\t\telse",
            "\t\t\tsc->skipped_deactivate = 1;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn shrink_inactive_list(nr_to_scan, lruvec, sc, lru);",
            "}",
            "static bool inactive_is_low(struct lruvec *lruvec, enum lru_list inactive_lru)",
            "{",
            "\tenum lru_list active_lru = inactive_lru + LRU_ACTIVE;",
            "\tunsigned long inactive, active;",
            "\tunsigned long inactive_ratio;",
            "\tunsigned long gb;",
            "",
            "\tinactive = lruvec_page_state(lruvec, NR_LRU_BASE + inactive_lru);",
            "\tactive = lruvec_page_state(lruvec, NR_LRU_BASE + active_lru);",
            "",
            "\tgb = (inactive + active) >> (30 - PAGE_SHIFT);",
            "\tif (gb)",
            "\t\tinactive_ratio = int_sqrt(10 * gb);",
            "\telse",
            "\t\tinactive_ratio = 1;",
            "",
            "\treturn inactive * inactive_ratio < active;",
            "}",
            "static void prepare_scan_control(pg_data_t *pgdat, struct scan_control *sc)",
            "{",
            "\tunsigned long file;",
            "\tstruct lruvec *target_lruvec;",
            "",
            "\tif (lru_gen_enabled())",
            "\t\treturn;",
            "",
            "\ttarget_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat);",
            "",
            "\t/*",
            "\t * Flush the memory cgroup stats, so that we read accurate per-memcg",
            "\t * lruvec stats for heuristics.",
            "\t */",
            "\tmem_cgroup_flush_stats(sc->target_mem_cgroup);",
            "",
            "\t/*",
            "\t * Determine the scan balance between anon and file LRUs.",
            "\t */",
            "\tspin_lock_irq(&target_lruvec->lru_lock);",
            "\tsc->anon_cost = target_lruvec->anon_cost;",
            "\tsc->file_cost = target_lruvec->file_cost;",
            "\tspin_unlock_irq(&target_lruvec->lru_lock);",
            "",
            "\t/*",
            "\t * Target desirable inactive:active list ratios for the anon",
            "\t * and file LRU lists.",
            "\t */",
            "\tif (!sc->force_deactivate) {",
            "\t\tunsigned long refaults;",
            "",
            "\t\t/*",
            "\t\t * When refaults are being observed, it means a new",
            "\t\t * workingset is being established. Deactivate to get",
            "\t\t * rid of any stale active pages quickly.",
            "\t\t */",
            "\t\trefaults = lruvec_page_state(target_lruvec,",
            "\t\t\t\tWORKINGSET_ACTIVATE_ANON);",
            "\t\tif (refaults != target_lruvec->refaults[WORKINGSET_ANON] ||",
            "\t\t\tinactive_is_low(target_lruvec, LRU_INACTIVE_ANON))",
            "\t\t\tsc->may_deactivate |= DEACTIVATE_ANON;",
            "\t\telse",
            "\t\t\tsc->may_deactivate &= ~DEACTIVATE_ANON;",
            "",
            "\t\trefaults = lruvec_page_state(target_lruvec,",
            "\t\t\t\tWORKINGSET_ACTIVATE_FILE);",
            "\t\tif (refaults != target_lruvec->refaults[WORKINGSET_FILE] ||",
            "\t\t    inactive_is_low(target_lruvec, LRU_INACTIVE_FILE))",
            "\t\t\tsc->may_deactivate |= DEACTIVATE_FILE;",
            "\t\telse",
            "\t\t\tsc->may_deactivate &= ~DEACTIVATE_FILE;",
            "\t} else",
            "\t\tsc->may_deactivate = DEACTIVATE_ANON | DEACTIVATE_FILE;",
            "",
            "\t/*",
            "\t * If we have plenty of inactive file pages that aren't",
            "\t * thrashing, try to reclaim those first before touching",
            "\t * anonymous pages.",
            "\t */",
            "\tfile = lruvec_page_state(target_lruvec, NR_INACTIVE_FILE);",
            "\tif (file >> sc->priority && !(sc->may_deactivate & DEACTIVATE_FILE))",
            "\t\tsc->cache_trim_mode = 1;",
            "\telse",
            "\t\tsc->cache_trim_mode = 0;",
            "",
            "\t/*",
            "\t * Prevent the reclaimer from falling into the cache trap: as",
            "\t * cache pages start out inactive, every cache fault will tip",
            "\t * the scan balance towards the file LRU.  And as the file LRU",
            "\t * shrinks, so does the window for rotation from references.",
            "\t * This means we have a runaway feedback loop where a tiny",
            "\t * thrashing file LRU becomes infinitely more attractive than",
            "\t * anon pages.  Try to detect this based on file LRU size.",
            "\t */",
            "\tif (!cgroup_reclaim(sc)) {",
            "\t\tunsigned long total_high_wmark = 0;",
            "\t\tunsigned long free, anon;",
            "\t\tint z;",
            "",
            "\t\tfree = sum_zone_node_page_state(pgdat->node_id, NR_FREE_PAGES);",
            "\t\tfile = node_page_state(pgdat, NR_ACTIVE_FILE) +",
            "\t\t\t   node_page_state(pgdat, NR_INACTIVE_FILE);",
            "",
            "\t\tfor (z = 0; z < MAX_NR_ZONES; z++) {",
            "\t\t\tstruct zone *zone = &pgdat->node_zones[z];",
            "",
            "\t\t\tif (!managed_zone(zone))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\ttotal_high_wmark += high_wmark_pages(zone);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Consider anon: if that's low too, this isn't a",
            "\t\t * runaway file reclaim problem, but rather just",
            "\t\t * extreme pressure. Reclaim as per usual then.",
            "\t\t */",
            "\t\tanon = node_page_state(pgdat, NR_INACTIVE_ANON);",
            "",
            "\t\tsc->file_is_tiny =",
            "\t\t\tfile + free <= total_high_wmark &&",
            "\t\t\t!(sc->may_deactivate & DEACTIVATE_ANON) &&",
            "\t\t\tanon >> sc->priority;",
            "\t}",
            "}"
          ],
          "function_name": "reclaim_pages, shrink_list, inactive_is_low, prepare_scan_control",
          "description": "该代码块包含`reclaim_pages`用于跨节点回收页面，`shrink_list`根据LRU类型选择回收策略，`inactive_is_low`检测非活动列表是否低于阈值，`prepare_scan_control`配置扫描控制参数，包括基于swap空间、内存组保护及文件/匿名页比例的扫描权重计算。",
          "similarity": 0.6306161880493164
        },
        {
          "chunk_id": 1,
          "file_path": "mm/vmscan.c",
          "start_line": 195,
          "end_line": 305,
          "content": [
            "static bool cgroup_reclaim(struct scan_control *sc)",
            "{",
            "\treturn sc->target_mem_cgroup;",
            "}",
            "static bool root_reclaim(struct scan_control *sc)",
            "{",
            "\treturn !sc->target_mem_cgroup || mem_cgroup_is_root(sc->target_mem_cgroup);",
            "}",
            "static bool writeback_throttling_sane(struct scan_control *sc)",
            "{",
            "\tif (!cgroup_reclaim(sc))",
            "\t\treturn true;",
            "#ifdef CONFIG_CGROUP_WRITEBACK",
            "\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys))",
            "\t\treturn true;",
            "#endif",
            "\treturn false;",
            "}",
            "static bool cgroup_reclaim(struct scan_control *sc)",
            "{",
            "\treturn false;",
            "}",
            "static bool root_reclaim(struct scan_control *sc)",
            "{",
            "\treturn true;",
            "}",
            "static bool writeback_throttling_sane(struct scan_control *sc)",
            "{",
            "\treturn true;",
            "}",
            "static void set_task_reclaim_state(struct task_struct *task,",
            "\t\t\t\t   struct reclaim_state *rs)",
            "{",
            "\t/* Check for an overwrite */",
            "\tWARN_ON_ONCE(rs && task->reclaim_state);",
            "",
            "\t/* Check for the nulling of an already-nulled member */",
            "\tWARN_ON_ONCE(!rs && !task->reclaim_state);",
            "",
            "\ttask->reclaim_state = rs;",
            "}",
            "static void flush_reclaim_state(struct scan_control *sc)",
            "{",
            "\t/*",
            "\t * Currently, reclaim_state->reclaimed includes three types of pages",
            "\t * freed outside of vmscan:",
            "\t * (1) Slab pages.",
            "\t * (2) Clean file pages from pruned inodes (on highmem systems).",
            "\t * (3) XFS freed buffer pages.",
            "\t *",
            "\t * For all of these cases, we cannot universally link the pages to a",
            "\t * single memcg. For example, a memcg-aware shrinker can free one object",
            "\t * charged to the target memcg, causing an entire page to be freed.",
            "\t * If we count the entire page as reclaimed from the memcg, we end up",
            "\t * overestimating the reclaimed amount (potentially under-reclaiming).",
            "\t *",
            "\t * Only count such pages for global reclaim to prevent under-reclaiming",
            "\t * from the target memcg; preventing unnecessary retries during memcg",
            "\t * charging and false positives from proactive reclaim.",
            "\t *",
            "\t * For uncommon cases where the freed pages were actually mostly",
            "\t * charged to the target memcg, we end up underestimating the reclaimed",
            "\t * amount. This should be fine. The freed pages will be uncharged",
            "\t * anyway, even if they are not counted here properly, and we will be",
            "\t * able to make forward progress in charging (which is usually in a",
            "\t * retry loop).",
            "\t *",
            "\t * We can go one step further, and report the uncharged objcg pages in",
            "\t * memcg reclaim, to make reporting more accurate and reduce",
            "\t * underestimation, but it's probably not worth the complexity for now.",
            "\t */",
            "\tif (current->reclaim_state && root_reclaim(sc)) {",
            "\t\tsc->nr_reclaimed += current->reclaim_state->reclaimed;",
            "\t\tcurrent->reclaim_state->reclaimed = 0;",
            "\t}",
            "}",
            "static bool can_demote(int nid, struct scan_control *sc)",
            "{",
            "\tif (!numa_demotion_enabled)",
            "\t\treturn false;",
            "\tif (sc && sc->no_demotion)",
            "\t\treturn false;",
            "\tif (next_demotion_node(nid) == NUMA_NO_NODE)",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static inline bool can_reclaim_anon_pages(struct mem_cgroup *memcg,",
            "\t\t\t\t\t  int nid,",
            "\t\t\t\t\t  struct scan_control *sc)",
            "{",
            "\tif (memcg == NULL) {",
            "\t\t/*",
            "\t\t * For non-memcg reclaim, is there",
            "\t\t * space in any swap device?",
            "\t\t */",
            "\t\tif (get_nr_swap_pages() > 0)",
            "\t\t\treturn true;",
            "\t} else {",
            "\t\t/* Is the memcg below its swap limit? */",
            "\t\tif (mem_cgroup_get_nr_swap_pages(memcg) > 0)",
            "\t\t\treturn true;",
            "\t}",
            "",
            "\t/*",
            "\t * The page can not be swapped.",
            "\t *",
            "\t * Can it be reclaimed from this node via demotion?",
            "\t */",
            "\treturn can_demote(nid, sc);",
            "}"
          ],
          "function_name": "cgroup_reclaim, root_reclaim, writeback_throttling_sane, cgroup_reclaim, root_reclaim, writeback_throttling_sane, set_task_reclaim_state, flush_reclaim_state, can_demote, can_reclaim_anon_pages",
          "description": "提供内存组回收判定逻辑与回收状态管理接口，包含判断是否针对特定内存组回收、是否允许写回操作、设置/刷新任务回收状态等功能，支持多层级回收场景决策。",
          "similarity": 0.6228225231170654
        },
        {
          "chunk_id": 44,
          "file_path": "mm/vmscan.c",
          "start_line": 7321,
          "end_line": 7463,
          "content": [
            "static int __init kswapd_init(void)",
            "{",
            "\tint nid;",
            "",
            "\tswap_setup();",
            "\tfor_each_node_state(nid, N_MEMORY)",
            " \t\tkswapd_run(nid);",
            "\treturn 0;",
            "}",
            "static inline unsigned long node_unmapped_file_pages(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long file_mapped = node_page_state(pgdat, NR_FILE_MAPPED);",
            "\tunsigned long file_lru = node_page_state(pgdat, NR_INACTIVE_FILE) +",
            "\t\tnode_page_state(pgdat, NR_ACTIVE_FILE);",
            "",
            "\t/*",
            "\t * It's possible for there to be more file mapped pages than",
            "\t * accounted for by the pages on the file LRU lists because",
            "\t * tmpfs pages accounted for as ANON can also be FILE_MAPPED",
            "\t */",
            "\treturn (file_lru > file_mapped) ? (file_lru - file_mapped) : 0;",
            "}",
            "static unsigned long node_pagecache_reclaimable(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long nr_pagecache_reclaimable;",
            "\tunsigned long delta = 0;",
            "",
            "\t/*",
            "\t * If RECLAIM_UNMAP is set, then all file pages are considered",
            "\t * potentially reclaimable. Otherwise, we have to worry about",
            "\t * pages like swapcache and node_unmapped_file_pages() provides",
            "\t * a better estimate",
            "\t */",
            "\tif (node_reclaim_mode & RECLAIM_UNMAP)",
            "\t\tnr_pagecache_reclaimable = node_page_state(pgdat, NR_FILE_PAGES);",
            "\telse",
            "\t\tnr_pagecache_reclaimable = node_unmapped_file_pages(pgdat);",
            "",
            "\t/* If we can't clean pages, remove dirty pages from consideration */",
            "\tif (!(node_reclaim_mode & RECLAIM_WRITE))",
            "\t\tdelta += node_page_state(pgdat, NR_FILE_DIRTY);",
            "",
            "\t/* Watch for any possible underflows due to delta */",
            "\tif (unlikely(delta > nr_pagecache_reclaimable))",
            "\t\tdelta = nr_pagecache_reclaimable;",
            "",
            "\treturn nr_pagecache_reclaimable - delta;",
            "}",
            "static int __node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)",
            "{",
            "\t/* Minimum pages needed in order to stay on node */",
            "\tconst unsigned long nr_pages = 1 << order;",
            "\tstruct task_struct *p = current;",
            "\tunsigned int noreclaim_flag;",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.order = order,",
            "\t\t.priority = NODE_RECLAIM_PRIORITY,",
            "\t\t.may_writepage = !!(node_reclaim_mode & RECLAIM_WRITE),",
            "\t\t.may_unmap = !!(node_reclaim_mode & RECLAIM_UNMAP),",
            "\t\t.may_swap = 1,",
            "\t\t.reclaim_idx = gfp_zone(gfp_mask),",
            "\t};",
            "\tunsigned long pflags;",
            "",
            "\ttrace_mm_vmscan_node_reclaim_begin(pgdat->node_id, order,",
            "\t\t\t\t\t   sc.gfp_mask);",
            "",
            "\tcond_resched();",
            "\tpsi_memstall_enter(&pflags);",
            "\tfs_reclaim_acquire(sc.gfp_mask);",
            "\t/*",
            "\t * We need to be able to allocate from the reserves for RECLAIM_UNMAP",
            "\t */",
            "\tnoreclaim_flag = memalloc_noreclaim_save();",
            "\tset_task_reclaim_state(p, &sc.reclaim_state);",
            "",
            "\tif (node_pagecache_reclaimable(pgdat) > pgdat->min_unmapped_pages ||",
            "\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) > pgdat->min_slab_pages) {",
            "\t\t/*",
            "\t\t * Free memory by calling shrink node with increasing",
            "\t\t * priorities until we have enough memory freed.",
            "\t\t */",
            "\t\tdo {",
            "\t\t\tshrink_node(pgdat, &sc);",
            "\t\t} while (sc.nr_reclaimed < nr_pages && --sc.priority >= 0);",
            "\t}",
            "",
            "\tset_task_reclaim_state(p, NULL);",
            "\tmemalloc_noreclaim_restore(noreclaim_flag);",
            "\tfs_reclaim_release(sc.gfp_mask);",
            "\tpsi_memstall_leave(&pflags);",
            "",
            "\ttrace_mm_vmscan_node_reclaim_end(sc.nr_reclaimed);",
            "",
            "\treturn sc.nr_reclaimed >= nr_pages;",
            "}",
            "int node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)",
            "{",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Node reclaim reclaims unmapped file backed pages and",
            "\t * slab pages if we are over the defined limits.",
            "\t *",
            "\t * A small portion of unmapped file backed pages is needed for",
            "\t * file I/O otherwise pages read by file I/O will be immediately",
            "\t * thrown out if the node is overallocated. So we do not reclaim",
            "\t * if less than a specified percentage of the node is used by",
            "\t * unmapped file backed pages.",
            "\t */",
            "\tif (node_pagecache_reclaimable(pgdat) <= pgdat->min_unmapped_pages &&",
            "\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) <=",
            "\t    pgdat->min_slab_pages)",
            "\t\treturn NODE_RECLAIM_FULL;",
            "",
            "\t/*",
            "\t * Do not scan if the allocation should not be delayed.",
            "\t */",
            "\tif (!gfpflags_allow_blocking(gfp_mask) || (current->flags & PF_MEMALLOC))",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\t/*",
            "\t * Only run node reclaim on the local node or on nodes that do not",
            "\t * have associated processors. This will favor the local processor",
            "\t * over remote processors and spread off node memory allocations",
            "\t * as wide as possible.",
            "\t */",
            "\tif (node_state(pgdat->node_id, N_CPU) && pgdat->node_id != numa_node_id())",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\tif (test_and_set_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags))",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\tret = __node_reclaim(pgdat, gfp_mask, order);",
            "\tclear_bit_unlock(PGDAT_RECLAIM_LOCKED, &pgdat->flags);",
            "",
            "\tif (!ret)",
            "\t\tcount_vm_event(PGSCAN_ZONE_RECLAIM_FAILED);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "kswapd_init, node_unmapped_file_pages, node_pagecache_reclaimable, __node_reclaim, node_reclaim",
          "description": "实现节点级别内存回收策略，kswapd_init初始化kswapd线程，node_unmapped_file_pages计算未映射文件页，node_pagecache_reclaimable评估可回收页面数量，__node_reclaim执行节点级回收，node_reclaim决定是否触发回收操作。",
          "similarity": 0.6176149249076843
        },
        {
          "chunk_id": 24,
          "file_path": "mm/vmscan.c",
          "start_line": 4269,
          "end_line": 4377,
          "content": [
            "void lru_gen_soft_reclaim(struct mem_cgroup *memcg, int nid)",
            "{",
            "\tstruct lruvec *lruvec = get_lruvec(memcg, nid);",
            "",
            "\t/* see the comment on MEMCG_NR_GENS */",
            "\tif (READ_ONCE(lruvec->lrugen.seg) != MEMCG_LRU_HEAD)",
            "\t\tlru_gen_rotate_memcg(lruvec, MEMCG_LRU_HEAD);",
            "}",
            "static bool sort_folio(struct lruvec *lruvec, struct folio *folio, struct scan_control *sc,",
            "\t\t       int tier_idx)",
            "{",
            "\tbool success;",
            "\tint gen = folio_lru_gen(folio);",
            "\tint type = folio_is_file_lru(folio);",
            "\tint zone = folio_zonenum(folio);",
            "\tint delta = folio_nr_pages(folio);",
            "\tint refs = folio_lru_refs(folio);",
            "\tint tier = lru_tier_from_refs(refs);",
            "\tstruct lru_gen_folio *lrugen = &lruvec->lrugen;",
            "",
            "\tVM_WARN_ON_ONCE_FOLIO(gen >= MAX_NR_GENS, folio);",
            "",
            "\t/* unevictable */",
            "\tif (!folio_evictable(folio)) {",
            "\t\tsuccess = lru_gen_del_folio(lruvec, folio, true);",
            "\t\tVM_WARN_ON_ONCE_FOLIO(!success, folio);",
            "\t\tfolio_set_unevictable(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "\t\t__count_vm_events(UNEVICTABLE_PGCULLED, delta);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* dirty lazyfree */",
            "\tif (type == LRU_GEN_FILE && folio_test_anon(folio) && folio_test_dirty(folio)) {",
            "\t\tsuccess = lru_gen_del_folio(lruvec, folio, true);",
            "\t\tVM_WARN_ON_ONCE_FOLIO(!success, folio);",
            "\t\tfolio_set_swapbacked(folio);",
            "\t\tlruvec_add_folio_tail(lruvec, folio);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* promoted */",
            "\tif (gen != lru_gen_from_seq(lrugen->min_seq[type])) {",
            "\t\tlist_move(&folio->lru, &lrugen->folios[gen][type][zone]);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* protected */",
            "\tif (tier > tier_idx || refs == BIT(LRU_REFS_WIDTH)) {",
            "\t\tint hist = lru_hist_from_seq(lrugen->min_seq[type]);",
            "",
            "\t\tgen = folio_inc_gen(lruvec, folio, false);",
            "\t\tlist_move_tail(&folio->lru, &lrugen->folios[gen][type][zone]);",
            "",
            "\t\tWRITE_ONCE(lrugen->protected[hist][type][tier - 1],",
            "\t\t\t   lrugen->protected[hist][type][tier - 1] + delta);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* ineligible */",
            "\tif (zone > sc->reclaim_idx) {",
            "\t\tgen = folio_inc_gen(lruvec, folio, false);",
            "\t\tlist_move_tail(&folio->lru, &lrugen->folios[gen][type][zone]);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/* waiting for writeback */",
            "\tif (folio_test_locked(folio) || folio_test_writeback(folio) ||",
            "\t    (type == LRU_GEN_FILE && folio_test_dirty(folio))) {",
            "\t\tgen = folio_inc_gen(lruvec, folio, true);",
            "\t\tlist_move(&folio->lru, &lrugen->folios[gen][type][zone]);",
            "\t\treturn true;",
            "\t}",
            "",
            "\treturn false;",
            "}",
            "static bool isolate_folio(struct lruvec *lruvec, struct folio *folio, struct scan_control *sc)",
            "{",
            "\tbool success;",
            "",
            "\t/* swap constrained */",
            "\tif (!(sc->gfp_mask & __GFP_IO) &&",
            "\t    (folio_test_dirty(folio) ||",
            "\t     (folio_test_anon(folio) && !folio_test_swapcache(folio))))",
            "\t\treturn false;",
            "",
            "\t/* raced with release_pages() */",
            "\tif (!folio_try_get(folio))",
            "\t\treturn false;",
            "",
            "\t/* raced with another isolation */",
            "\tif (!folio_test_clear_lru(folio)) {",
            "\t\tfolio_put(folio);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/* see the comment on MAX_NR_TIERS */",
            "\tif (!folio_test_referenced(folio))",
            "\t\tset_mask_bits(&folio->flags, LRU_REFS_MASK | LRU_REFS_FLAGS, 0);",
            "",
            "\t/* for shrink_folio_list() */",
            "\tfolio_clear_reclaim(folio);",
            "\tfolio_clear_referenced(folio);",
            "",
            "\tsuccess = lru_gen_del_folio(lruvec, folio, true);",
            "\tVM_WARN_ON_ONCE_FOLIO(!success, folio);",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "lru_gen_soft_reclaim, sort_folio, isolate_folio",
          "description": "该代码段实现了内存组（memcg）的LRU分级管理逻辑，核心功能是通过`lru_gen_soft_reclaim`触发LRU链表旋转，`sort_folio`根据页面属性（如脏/匿名/引用位）将其归类至不同层级的LRU队列，`isolate_folio`则负责安全隔离可回收页面并更新统计信息。三者协同完成基于生成代数（gen）的页面分类与回收决策。",
          "similarity": 0.617326021194458
        }
      ]
    },
    {
      "source_file": "mm/list_lru.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:35:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `list_lru.c`\n\n---\n\n# list_lru.c 技术文档\n\n## 1. 文件概述\n\n`list_lru.c` 实现了 Linux 内核中通用的 **List-based LRU（Least Recently Used）基础设施**，用于管理可回收对象的双向链表。该机制支持按 NUMA 节点（node）和内存控制组（memcg）进行细粒度组织，便于内存压力下的高效回收。主要服务于 slab 分配器等子系统，作为 shrinker 框架的一部分，在内存紧张时协助释放非活跃对象。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct list_lru`：顶层 LRU 管理结构，包含 per-node 的 `list_lru_node`\n- `struct list_lru_node`：每个 NUMA 节点对应的 LRU 节点，含自旋锁和总项数\n- `struct list_lru_one`：实际存储对象链表和计数的单元（per-memcg per-node）\n- `struct list_lru_memcg`：当启用 `CONFIG_MEMCG` 时，为每个 memcg 存储 per-node 的 `list_lru_one`\n\n### 主要导出函数\n- `list_lru_add()` / `list_lru_add_obj()`：向 LRU 添加对象\n- `list_lru_del()` / `list_lru_del_obj()`：从 LRU 删除对象\n- `list_lru_isolate()` / `list_lru_isolate_move()`：在回收过程中隔离对象\n- `list_lru_count_one()` / `list_lru_count_node()`：查询 LRU 中对象数量\n- `list_lru_walk_one()` / `list_lru_walk_node()`：遍历并处理 LRU 中的对象（用于 shrinker 回调）\n\n### 内部辅助函数\n- `list_lru_from_memcg_idx()`：根据 memcg ID 获取对应的 `list_lru_one`\n- `__list_lru_walk_one()`：带锁的 LRU 遍历核心逻辑\n- `list_lru_register()` / `list_lru_unregister()`：注册/注销 memcg-aware 的 LRU（用于全局追踪）\n\n## 3. 关键实现\n\n### 内存控制组（memcg）支持\n- 通过 `CONFIG_MEMCG` 条件编译控制 memcg 相关逻辑\n- 使用 XArray (`lru->xa`) 动态存储每个 memcg 对应的 `list_lru_memcg` 结构\n- 每个 memcg 在每个 NUMA 节点上拥有独立的 `list_lru_one`，实现资源隔离\n- 全局 `memcg_list_lrus` 链表和 `list_lrus_mutex` 用于跟踪所有 memcg-aware 的 LRU 实例\n\n### 并发控制\n- 每个 NUMA 节点 (`list_lru_node`) 拥有独立的自旋锁 (`nlru->lock`)\n- 所有对 LRU 链表的操作（增、删、遍历）均在对应节点锁保护下进行\n- 提供 `_irq` 版本的遍历函数（`list_lru_walk_one_irq`）用于中断上下文\n\n### 回收遍历机制\n- `list_lru_walk_*` 函数接受回调函数 `isolate`，由调用者定义回收策略\n- 回调返回值控制遍历行为：\n  - `LRU_REMOVED`：成功移除\n  - `LRU_REMOVED_RETRY`：移除后需重新开始遍历（锁曾被释放）\n  - `LRU_RETRY`：未移除但需重新开始遍历\n  - `LRU_ROTATE`：将对象移到链表尾部（标记为最近使用）\n  - `LRU_SKIP`：跳过当前对象\n  - `LRU_STOP`：立即停止遍历\n- 通过 `nr_to_walk` 限制单次遍历的最大对象数，防止长时间持锁\n\n### Shrinker 集成\n- 当向空的 `list_lru_one` 添加首个对象时，调用 `set_shrinker_bit()` 标记该 memcg/node 需要被 shrinker 处理\n- `lru_shrinker_id()` 返回关联的 shrinker ID，用于通知内存回收子系统\n\n### 对象归属识别\n- `list_lru_add_obj()` / `list_lru_del_obj()` 通过 `mem_cgroup_from_slab_obj()` 自动获取对象所属的 memcg\n- 使用 `page_to_nid(virt_to_page(item))` 确定对象所在的 NUMA 节点\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/list_lru.h>`：定义核心数据结构和 API\n  - `<linux/memcontrol.h>`：memcg 相关接口（如 `memcg_kmem_id`）\n  - `\"slab.h\"` 和 `\"internal.h\"`：slab 分配器内部接口（如 `mem_cgroup_from_slab_obj`）\n- **配置依赖**：\n  - `CONFIG_MEMCG`：决定是否编译 memcg 相关代码\n  - `CONFIG_NUMA`：影响 per-node 数据结构的大小（通过 `nr_node_ids`）\n- **子系统依赖**：\n  - Slab 分配器：作为主要使用者，管理可回收 slab 对象\n  - Memory Control Group (memcg)：提供内存隔离和记账\n  - Shrinker 框架：通过 shrinker 回调触发 LRU 遍历回收\n\n## 5. 使用场景\n\n- **Slab 对象回收**：当系统内存压力大时，shrinker 通过 `list_lru_walk_*` 遍历 inactive slab 对象链表，释放可回收对象\n- **Per-memcg 内存限制**：在 cgroup 内存超限时，仅遍历该 memcg 对应的 LRU 部分，实现精确回收\n- **NUMA 感知管理**：按 NUMA 节点分离 LRU 链表，减少远程内存访问，提升性能\n- **通用 LRU 容器**：任何需要按 LRU 策略管理可回收对象的内核子系统均可使用此基础设施（如 dentry、inode 缓存等）",
      "similarity": 0.6263279914855957,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "mm/list_lru.c",
          "start_line": 425,
          "end_line": 551,
          "content": [
            "static void memcg_reparent_list_lru(struct list_lru *lru,",
            "\t\t\t\t    int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tint i;",
            "",
            "\tfor_each_node(i)",
            "\t\tmemcg_reparent_list_lru_node(lru, i, src_idx, dst_memcg);",
            "",
            "\tmemcg_list_lru_free(lru, src_idx);",
            "}",
            "void memcg_reparent_list_lrus(struct mem_cgroup *memcg, struct mem_cgroup *parent)",
            "{",
            "\tstruct cgroup_subsys_state *css;",
            "\tstruct list_lru *lru;",
            "\tint src_idx = memcg->kmemcg_id;",
            "",
            "\t/*",
            "\t * Change kmemcg_id of this cgroup and all its descendants to the",
            "\t * parent's id, and then move all entries from this cgroup's list_lrus",
            "\t * to ones of the parent.",
            "\t *",
            "\t * After we have finished, all list_lrus corresponding to this cgroup",
            "\t * are guaranteed to remain empty. So we can safely free this cgroup's",
            "\t * list lrus in memcg_list_lru_free().",
            "\t *",
            "\t * Changing ->kmemcg_id to the parent can prevent memcg_list_lru_alloc()",
            "\t * from allocating list lrus for this cgroup after memcg_list_lru_free()",
            "\t * call.",
            "\t */",
            "\trcu_read_lock();",
            "\tcss_for_each_descendant_pre(css, &memcg->css) {",
            "\t\tstruct mem_cgroup *child;",
            "",
            "\t\tchild = mem_cgroup_from_css(css);",
            "\t\tWRITE_ONCE(child->kmemcg_id, parent->kmemcg_id);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_for_each_entry(lru, &memcg_list_lrus, list)",
            "\t\tmemcg_reparent_list_lru(lru, src_idx, parent);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static inline bool memcg_list_lru_allocated(struct mem_cgroup *memcg,",
            "\t\t\t\t\t    struct list_lru *lru)",
            "{",
            "\tint idx = memcg->kmemcg_id;",
            "",
            "\treturn idx < 0 || xa_load(&lru->xa, idx);",
            "}",
            "int memcg_list_lru_alloc(struct mem_cgroup *memcg, struct list_lru *lru,",
            "\t\t\t gfp_t gfp)",
            "{",
            "\tint i;",
            "\tunsigned long flags;",
            "\tstruct list_lru_memcg_table {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tstruct mem_cgroup *memcg;",
            "\t} *table;",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "",
            "\tif (!list_lru_memcg_aware(lru) || memcg_list_lru_allocated(memcg, lru))",
            "\t\treturn 0;",
            "",
            "\tgfp &= GFP_RECLAIM_MASK;",
            "\ttable = kmalloc_array(memcg->css.cgroup->level, sizeof(*table), gfp);",
            "\tif (!table)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * Because the list_lru can be reparented to the parent cgroup's",
            "\t * list_lru, we should make sure that this cgroup and all its",
            "\t * ancestors have allocated list_lru_memcg.",
            "\t */",
            "\tfor (i = 0; memcg; memcg = parent_mem_cgroup(memcg), i++) {",
            "\t\tif (memcg_list_lru_allocated(memcg, lru))",
            "\t\t\tbreak;",
            "",
            "\t\ttable[i].memcg = memcg;",
            "\t\ttable[i].mlru = memcg_init_list_lru_one(gfp);",
            "\t\tif (!table[i].mlru) {",
            "\t\t\twhile (i--)",
            "\t\t\t\tkfree(table[i].mlru);",
            "\t\t\tkfree(table);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t}",
            "",
            "\txas_lock_irqsave(&xas, flags);",
            "\twhile (i--) {",
            "\t\tint index = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\tstruct list_lru_memcg *mlru = table[i].mlru;",
            "",
            "\t\txas_set(&xas, index);",
            "retry:",
            "\t\tif (unlikely(index < 0 || xas_error(&xas) || xas_load(&xas))) {",
            "\t\t\tkfree(mlru);",
            "\t\t} else {",
            "\t\t\txas_store(&xas, mlru);",
            "\t\t\tif (xas_error(&xas) == -ENOMEM) {",
            "\t\t\t\txas_unlock_irqrestore(&xas, flags);",
            "\t\t\t\tif (xas_nomem(&xas, gfp))",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\txas_lock_irqsave(&xas, flags);",
            "\t\t\t\t/*",
            "\t\t\t\t * The xas lock has been released, this memcg",
            "\t\t\t\t * can be reparented before us. So reload",
            "\t\t\t\t * memcg id. More details see the comments",
            "\t\t\t\t * in memcg_reparent_list_lrus().",
            "\t\t\t\t */",
            "\t\t\t\tindex = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\t\t\tif (index < 0)",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\telse if (!xas_error(&xas) && index != xas.xa_index)",
            "\t\t\t\t\txas_set(&xas, index);",
            "\t\t\t\tgoto retry;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\t/* xas_nomem() is used to free memory instead of memory allocation. */",
            "\tif (xas.xa_alloc)",
            "\t\txas_nomem(&xas, gfp);",
            "\txas_unlock_irqrestore(&xas, flags);",
            "\tkfree(table);",
            "",
            "\treturn xas_error(&xas);",
            "}"
          ],
          "function_name": "memcg_reparent_list_lru, memcg_reparent_list_lrus, memcg_list_lru_allocated, memcg_list_lru_alloc",
          "description": "实现内存组层级间的LRU列表迁移与分配机制，包含递归子组处理、动态分配/释放LRU结构体及冲突解决逻辑。",
          "similarity": 0.5880492925643921
        },
        {
          "chunk_id": 5,
          "file_path": "mm/list_lru.c",
          "start_line": 556,
          "end_line": 605,
          "content": [
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "}",
            "int __list_lru_init(struct list_lru *lru, bool memcg_aware,",
            "\t\t    struct lock_class_key *key, struct shrinker *shrinker)",
            "{",
            "\tint i;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (shrinker)",
            "\t\tlru->shrinker_id = shrinker->id;",
            "\telse",
            "\t\tlru->shrinker_id = -1;",
            "#endif",
            "",
            "\tlru->node = kcalloc(nr_node_ids, sizeof(*lru->node), GFP_KERNEL);",
            "\tif (!lru->node)",
            "\t\treturn -ENOMEM;",
            "",
            "\tfor_each_node(i) {",
            "\t\tspin_lock_init(&lru->node[i].lock);",
            "\t\tif (key)",
            "\t\t\tlockdep_set_class(&lru->node[i].lock, key);",
            "\t\tinit_one_lru(&lru->node[i].lru);",
            "\t}",
            "",
            "\tmemcg_init_list_lru(lru, memcg_aware);",
            "\tlist_lru_register(lru);",
            "",
            "\treturn 0;",
            "}",
            "void list_lru_destroy(struct list_lru *lru)",
            "{",
            "\t/* Already destroyed or not yet initialized? */",
            "\tif (!lru->node)",
            "\t\treturn;",
            "",
            "\tlist_lru_unregister(lru);",
            "",
            "\tmemcg_destroy_list_lru(lru);",
            "\tkfree(lru->node);",
            "\tlru->node = NULL;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tlru->shrinker_id = -1;",
            "#endif",
            "}"
          ],
          "function_name": "memcg_init_list_lru, memcg_destroy_list_lru, __list_lru_init, list_lru_destroy",
          "description": "该代码段实现了基于内存控制组（MEMCG）的LRU列表管理功能。  \n`__list_lru_init` 初始化 `list_lru` 结构体并注册到系统，其中包含 MEMCG 相关的 shrinker ID 设置及节点锁初始化；`list_lru_destroy` 反向清理资源，但 `memcg_init_list_lru` 和 `memcg_destroy_list_lru` 的具体实现缺失，上下文不完整。",
          "similarity": 0.5638083219528198
        },
        {
          "chunk_id": 3,
          "file_path": "mm/list_lru.c",
          "start_line": 289,
          "end_line": 400,
          "content": [
            "unsigned long",
            "list_lru_walk_one_irq(struct list_lru *lru, int nid, struct mem_cgroup *memcg,",
            "\t\t      list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t      unsigned long *nr_to_walk)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tunsigned long ret;",
            "",
            "\tspin_lock_irq(&nlru->lock);",
            "\tret = __list_lru_walk_one(lru, nid, memcg_kmem_id(memcg), isolate,",
            "\t\t\t\t  cb_arg, nr_to_walk);",
            "\tspin_unlock_irq(&nlru->lock);",
            "\treturn ret;",
            "}",
            "unsigned long list_lru_walk_node(struct list_lru *lru, int nid,",
            "\t\t\t\t list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t\t\t unsigned long *nr_to_walk)",
            "{",
            "\tlong isolated = 0;",
            "",
            "\tisolated += list_lru_walk_one(lru, nid, NULL, isolate, cb_arg,",
            "\t\t\t\t      nr_to_walk);",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (*nr_to_walk > 0 && list_lru_memcg_aware(lru)) {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tunsigned long index;",
            "",
            "\t\txa_for_each(&lru->xa, index, mlru) {",
            "\t\t\tstruct list_lru_node *nlru = &lru->node[nid];",
            "",
            "\t\t\tspin_lock(&nlru->lock);",
            "\t\t\tisolated += __list_lru_walk_one(lru, nid, index,",
            "\t\t\t\t\t\t\tisolate, cb_arg,",
            "\t\t\t\t\t\t\tnr_to_walk);",
            "\t\t\tspin_unlock(&nlru->lock);",
            "",
            "\t\t\tif (*nr_to_walk <= 0)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "#endif",
            "",
            "\treturn isolated;",
            "}",
            "static void init_one_lru(struct list_lru_one *l)",
            "{",
            "\tINIT_LIST_HEAD(&l->list);",
            "\tl->nr_items = 0;",
            "}",
            "static void memcg_list_lru_free(struct list_lru *lru, int src_idx)",
            "{",
            "\tstruct list_lru_memcg *mlru = xa_erase_irq(&lru->xa, src_idx);",
            "",
            "\t/*",
            "\t * The __list_lru_walk_one() can walk the list of this node.",
            "\t * We need kvfree_rcu() here. And the walking of the list",
            "\t * is under lru->node[nid]->lock, which can serve as a RCU",
            "\t * read-side critical section.",
            "\t */",
            "\tif (mlru)",
            "\t\tkvfree_rcu(mlru, rcu);",
            "}",
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "\tif (memcg_aware)",
            "\t\txa_init_flags(&lru->xa, XA_FLAGS_LOCK_IRQ);",
            "\tlru->memcg_aware = memcg_aware;",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "\tstruct list_lru_memcg *mlru;",
            "",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\txas_lock_irq(&xas);",
            "\txas_for_each(&xas, mlru, ULONG_MAX) {",
            "\t\tkfree(mlru);",
            "\t\txas_store(&xas, NULL);",
            "\t}",
            "\txas_unlock_irq(&xas);",
            "}",
            "static void memcg_reparent_list_lru_node(struct list_lru *lru, int nid,",
            "\t\t\t\t\t int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tint dst_idx = dst_memcg->kmemcg_id;",
            "\tstruct list_lru_one *src, *dst;",
            "",
            "\t/*",
            "\t * Since list_lru_{add,del} may be called under an IRQ-safe lock,",
            "\t * we have to use IRQ-safe primitives here to avoid deadlock.",
            "\t */",
            "\tspin_lock_irq(&nlru->lock);",
            "",
            "\tsrc = list_lru_from_memcg_idx(lru, nid, src_idx);",
            "\tif (!src)",
            "\t\tgoto out;",
            "\tdst = list_lru_from_memcg_idx(lru, nid, dst_idx);",
            "",
            "\tlist_splice_init(&src->list, &dst->list);",
            "",
            "\tif (src->nr_items) {",
            "\t\tdst->nr_items += src->nr_items;",
            "\t\tset_shrinker_bit(dst_memcg, nid, lru_shrinker_id(lru));",
            "\t\tsrc->nr_items = 0;",
            "\t}",
            "out:",
            "\tspin_unlock_irq(&nlru->lock);",
            "}"
          ],
          "function_name": "list_lru_walk_one_irq, list_lru_walk_node, init_one_lru, memcg_list_lru_free, memcg_init_list_lru, memcg_destroy_list_lru, memcg_reparent_list_lru_node",
          "description": "包含LRU节点初始化、内存组间列表迁移、资源释放等高级操作，涉及XA表管理、中断安全锁操作及内存组重新归属处理。",
          "similarity": 0.5512700080871582
        },
        {
          "chunk_id": 1,
          "file_path": "mm/list_lru.c",
          "start_line": 22,
          "end_line": 129,
          "content": [
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn lru->memcg_aware;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_add(&lru->list, &memcg_list_lrus);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_del(&lru->list);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn lru->shrinker_id;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn -1;",
            "}",
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn false;",
            "}",
            "bool list_lru_add(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_add_tail(item, &l->list);",
            "\t\t/* Set shrinker bit if the first element was added */",
            "\t\tif (!l->nr_items++)",
            "\t\t\tset_shrinker_bit(memcg, nid, lru_shrinker_id(lru));",
            "\t\tnlru->nr_items++;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_add_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_add(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_add(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "bool list_lru_del(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (!list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_del_init(item);",
            "\t\tl->nr_items--;",
            "\t\tnlru->nr_items--;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_del_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_del(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_del(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "list_lru_memcg_aware, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_memcg_aware, list_lru_add, list_lru_add_obj, list_lru_del, list_lru_del_obj",
          "description": "实现了LRU列表的添加/删除操作，支持MemCG感知的节点和内存组粒度管理，包含处理多核、内存组切换及RCU安全访问的逻辑。",
          "similarity": 0.5307013988494873
        },
        {
          "chunk_id": 0,
          "file_path": "mm/list_lru.c",
          "start_line": 1,
          "end_line": 21,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2013 Red Hat, Inc. and Parallels Inc. All rights reserved.",
            " * Authors: David Chinner and Glauber Costa",
            " *",
            " * Generic LRU infrastructure",
            " */",
            "#include <linux/kernel.h>",
            "#include <linux/module.h>",
            "#include <linux/mm.h>",
            "#include <linux/list_lru.h>",
            "#include <linux/slab.h>",
            "#include <linux/mutex.h>",
            "#include <linux/memcontrol.h>",
            "#include \"slab.h\"",
            "#include \"internal.h\"",
            "",
            "#ifdef CONFIG_MEMCG",
            "static LIST_HEAD(memcg_list_lrus);",
            "static DEFINE_MUTEX(list_lrus_mutex);",
            ""
          ],
          "function_name": null,
          "description": "定义了支持内存控制组（MemCG）的LRU基础设施，声明了全局链表头memcg_list_lrus和互斥锁list_lrus_mutex，用于管理MemCG环境下的LRU列表注册与注销操作。",
          "similarity": 0.4977855682373047
        }
      ]
    },
    {
      "source_file": "mm/damon/reclaim.c",
      "md_summary": "> 自动生成时间: 2025-12-07 15:50:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `damon\\reclaim.c`\n\n---\n\n# `damon/reclaim.c` 技术文档\n\n## 1. 文件概述\n\n`damon/reclaim.c` 是 Linux 内核中基于 **DAMON（Data Access MONitor）** 框架实现的**自动内存回收模块**。该模块通过监控物理内存区域的访问模式，识别长时间未被访问的“冷”内存页，并主动将其回收（page-out），从而释放系统内存资源。其核心目标是在不影响系统性能的前提下，智能地回收低价值内存，提升内存利用率。\n\n该模块以可加载内核模块（LKM）形式存在，通过一组可调参数控制其行为，并支持基于水位线（watermarks）的条件激活机制，避免在内存充足时进行不必要的回收操作。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enabled`**: 全局开关，控制 DAMON_RECLAIM 功能是否启用。\n- **`commit_inputs`**: 触发参数重载的标志位，用于运行时动态更新配置（除 `enabled` 外）。\n- **`min_age`**: 冷内存判定阈值（微秒），默认 120 秒。\n- **`damon_reclaim_quota`**: 回收配额控制结构，限制单位时间内的最大回收量（默认每秒最多 128 MiB）和 CPU 时间开销（默认最多 10 ms）。\n- **`damon_reclaim_wmarks`**: 水位线配置，基于空闲内存比率决定是否激活回收（高/中/低水位分别为 50%/40%/20%）。\n- **`damon_reclaim_mon_attrs`**: DAMON 监控属性，定义采样间隔（5ms）、聚合间隔（100ms）等。\n- **`monitor_region_start/end`**: 目标监控内存区域的物理地址范围，默认为系统最大连续 RAM 区域。\n- **`skip_anon`**: 布尔标志，若为真则跳过匿名页（anonymous pages）的回收。\n- **`kdamond_pid`**: DAMON 工作线程的 PID，未启用时为 -1。\n- **`damon_reclaim_stat`**: 统计信息结构，记录尝试回收区域数、成功回收区域数及配额超限次数。\n\n### 主要函数\n\n- **`damon_reclaim_new_scheme()`**: 创建 DAMOS（DAMON Operation Scheme）策略，定义“冷内存”模式（大小 ≥ PAGE_SIZE、访问次数为 0、年龄 ≥ `min_age`）并指定操作为 `DAMOS_PAGEOUT`。\n- **`damon_reclaim_apply_parameters()`**: 应用所有用户配置参数到 DAMON 上下文（`ctx`），包括监控属性、回收策略、过滤器（如 `skip_anon`）和监控区域。\n- **`damon_reclaim_turn()`**: 启动或停止 DAMON_RECLAIM 的核心监控与回收逻辑。\n- **`damon_reclaim_enabled_store()`**: `enabled` 参数的 setter 回调，处理启用/禁用逻辑。\n- **`damon_reclaim_handle_commit_inputs()`**: 处理 `commit_inputs` 标志，触发运行时参数重载。\n- **`damon_reclaim_after_aggregation()` / `damon_reclaim_after_wmarks_check()`**: DAMON 回调函数，在聚合后和水位检查后更新统计信息并处理参数提交。\n- **`damon_reclaim_init()`**: 模块初始化函数，创建 DAMON 上下文和目标，注册回调，并根据初始 `enabled` 状态决定是否启动。\n\n## 3. 关键实现\n\n### 冷内存识别与回收策略\n- 通过 `damon_reclaim_new_scheme()` 定义 DAMOS 策略：\n  - **访问模式匹配**：区域大小 ≥ `PAGE_SIZE`、访问次数 = 0、年龄 ≥ `min_age / aggr_interval`（转换为聚合周期单位）。\n  - **操作类型**：`DAMOS_PAGEOUT`，即对匹配区域执行页面回收。\n  - **配额控制**：使用 `damon_reclaim_quota` 限制回收速度和 CPU 开销，确保系统稳定性。\n  - **水位激活**：仅当空闲内存比率低于 `high` 水位（50%）时激活策略，高于 `low` 水位（20%）时停用。\n\n### 动态参数更新机制\n- 用户可通过写入 `commit_inputs=Y` 触发运行时参数重载（`min_age`、配额、水位、监控区域等）。\n- `damon_reclaim_handle_commit_inputs()` 在 DAMON 的聚合后或水位检查后回调中执行重载，确保线程安全。\n- 重载时保留旧策略的配额状态（如已消耗的配额），避免统计中断。\n\n### 匿名页过滤\n- 若 `skip_anon=Y`，通过 `DAMOS_FILTER_TYPE_ANON` 过滤器排除匿名页（如进程堆栈、堆内存），仅回收文件缓存等页面。\n\n### 监控区域自动配置\n- 默认使用 `damon_set_region_biggest_system_ram_default()` 自动选择系统中最大的连续物理 RAM 区域作为监控目标，用户也可通过 `monitor_region_start/end` 手动指定。\n\n## 4. 依赖关系\n\n- **DAMON 核心框架** (`<linux/damon.h>`): 依赖 DAMON 提供的内存访问监控、策略引擎（DAMOS）、配额管理、水位控制等基础设施。\n- **内核模块通用接口** (`modules-common.h`): 使用 `DEFINE_DAMON_MODULES_*` 宏简化参数声明和统计暴露。\n- **内存管理子系统**: 通过 `DAMOS_PAGEOUT` 操作与 MM 子系统交互，实际执行页面回收。\n- **参数解析工具** (`<linux/kstrtox.h>`): 用于解析用户输入的布尔值和数值参数。\n\n## 5. 使用场景\n\n- **内存压力缓解**: 在内存紧张但尚未触发传统 LRU 回收或 OOM Killer 之前，提前回收长期未使用的冷内存，延缓内存压力。\n- **容器/虚拟机内存优化**: 在容器或 VM 中部署，自动回收应用未使用的缓存内存，提高宿主机内存密度。\n- **大内存系统调优**: 在 TB 级内存服务器上，减少因缓存膨胀导致的内存浪费，提升整体内存效率。\n- **低延迟敏感场景**: 通过配额限制（`ms=10`）确保回收操作不会显著影响关键任务的延迟。\n- **调试与监控**: 通过 `kdamond_pid` 和统计参数（`reclaim_tried_regions` 等）监控 DAMON_RECLAIM 的运行状态和效果。",
      "similarity": 0.61045241355896,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 153,
          "end_line": 254,
          "content": [
            "static void damon_reclaim_copy_quota_status(struct damos_quota *dst,",
            "\t\tstruct damos_quota *src)",
            "{",
            "\tdst->total_charged_sz = src->total_charged_sz;",
            "\tdst->total_charged_ns = src->total_charged_ns;",
            "\tdst->charged_sz = src->charged_sz;",
            "\tdst->charged_from = src->charged_from;",
            "\tdst->charge_target_from = src->charge_target_from;",
            "\tdst->charge_addr_from = src->charge_addr_from;",
            "}",
            "static int damon_reclaim_apply_parameters(void)",
            "{",
            "\tstruct damos *scheme, *old_scheme;",
            "\tstruct damos_filter *filter;",
            "\tint err = 0;",
            "",
            "\terr = damon_set_attrs(ctx, &damon_reclaim_mon_attrs);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\t/* Will be freed by next 'damon_set_schemes()' below */",
            "\tscheme = damon_reclaim_new_scheme();",
            "\tif (!scheme)",
            "\t\treturn -ENOMEM;",
            "\tif (!list_empty(&ctx->schemes)) {",
            "\t\tdamon_for_each_scheme(old_scheme, ctx)",
            "\t\t\tdamon_reclaim_copy_quota_status(&scheme->quota,",
            "\t\t\t\t\t&old_scheme->quota);",
            "\t}",
            "\tif (skip_anon) {",
            "\t\tfilter = damos_new_filter(DAMOS_FILTER_TYPE_ANON, true);",
            "\t\tif (!filter) {",
            "\t\t\t/* Will be freed by next 'damon_set_schemes()' below */",
            "\t\t\tdamon_destroy_scheme(scheme);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t\tdamos_add_filter(scheme, filter);",
            "\t}",
            "\tdamon_set_schemes(ctx, &scheme, 1);",
            "",
            "\treturn damon_set_region_biggest_system_ram_default(target,",
            "\t\t\t\t\t&monitor_region_start,",
            "\t\t\t\t\t&monitor_region_end);",
            "}",
            "static int damon_reclaim_turn(bool on)",
            "{",
            "\tint err;",
            "",
            "\tif (!on) {",
            "\t\terr = damon_stop(&ctx, 1);",
            "\t\tif (!err)",
            "\t\t\tkdamond_pid = -1;",
            "\t\treturn err;",
            "\t}",
            "",
            "\terr = damon_reclaim_apply_parameters();",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\terr = damon_start(&ctx, 1, true);",
            "\tif (err)",
            "\t\treturn err;",
            "\tkdamond_pid = ctx->kdamond->pid;",
            "\treturn 0;",
            "}",
            "static int damon_reclaim_enabled_store(const char *val,",
            "\t\tconst struct kernel_param *kp)",
            "{",
            "\tbool is_enabled = enabled;",
            "\tbool enable;",
            "\tint err;",
            "",
            "\terr = kstrtobool(val, &enable);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (is_enabled == enable)",
            "\t\treturn 0;",
            "",
            "\t/* Called before init function.  The function will handle this. */",
            "\tif (!ctx)",
            "\t\tgoto set_param_out;",
            "",
            "\terr = damon_reclaim_turn(enable);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "set_param_out:",
            "\tenabled = enable;",
            "\treturn err;",
            "}",
            "static int damon_reclaim_handle_commit_inputs(void)",
            "{",
            "\tint err;",
            "",
            "\tif (!commit_inputs)",
            "\t\treturn 0;",
            "",
            "\terr = damon_reclaim_apply_parameters();",
            "\tcommit_inputs = false;",
            "\treturn err;",
            "}"
          ],
          "function_name": "damon_reclaim_copy_quota_status, damon_reclaim_apply_parameters, damon_reclaim_turn, damon_reclaim_enabled_store, damon_reclaim_handle_commit_inputs",
          "description": "实现DAMON_RECLAIM参数动态应用、启停切换及配额状态复制逻辑，通过回调机制协调监控上下文与回收策略，支持运行时参数更新和资源回收操作。",
          "similarity": 0.5142766833305359
        },
        {
          "chunk_id": 0,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 1,
          "end_line": 152,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * DAMON-based page reclamation",
            " *",
            " * Author: SeongJae Park <sj@kernel.org>",
            " */",
            "",
            "#define pr_fmt(fmt) \"damon-reclaim: \" fmt",
            "",
            "#include <linux/damon.h>",
            "#include <linux/kstrtox.h>",
            "#include <linux/module.h>",
            "",
            "#include \"modules-common.h\"",
            "",
            "#ifdef MODULE_PARAM_PREFIX",
            "#undef MODULE_PARAM_PREFIX",
            "#endif",
            "#define MODULE_PARAM_PREFIX \"damon_reclaim.\"",
            "",
            "/*",
            " * Enable or disable DAMON_RECLAIM.",
            " *",
            " * You can enable DAMON_RCLAIM by setting the value of this parameter as ``Y``.",
            " * Setting it as ``N`` disables DAMON_RECLAIM.  Note that DAMON_RECLAIM could",
            " * do no real monitoring and reclamation due to the watermarks-based activation",
            " * condition.  Refer to below descriptions for the watermarks parameter for",
            " * this.",
            " */",
            "static bool enabled __read_mostly;",
            "",
            "/*",
            " * Make DAMON_RECLAIM reads the input parameters again, except ``enabled``.",
            " *",
            " * Input parameters that updated while DAMON_RECLAIM is running are not applied",
            " * by default.  Once this parameter is set as ``Y``, DAMON_RECLAIM reads values",
            " * of parametrs except ``enabled`` again.  Once the re-reading is done, this",
            " * parameter is set as ``N``.  If invalid parameters are found while the",
            " * re-reading, DAMON_RECLAIM will be disabled.",
            " */",
            "static bool commit_inputs __read_mostly;",
            "module_param(commit_inputs, bool, 0600);",
            "",
            "/*",
            " * Time threshold for cold memory regions identification in microseconds.",
            " *",
            " * If a memory region is not accessed for this or longer time, DAMON_RECLAIM",
            " * identifies the region as cold, and reclaims.  120 seconds by default.",
            " */",
            "static unsigned long min_age __read_mostly = 120000000;",
            "module_param(min_age, ulong, 0600);",
            "",
            "static struct damos_quota damon_reclaim_quota = {",
            "\t/* use up to 10 ms time, reclaim up to 128 MiB per 1 sec by default */",
            "\t.ms = 10,",
            "\t.sz = 128 * 1024 * 1024,",
            "\t.reset_interval = 1000,",
            "\t/* Within the quota, page out older regions first. */",
            "\t.weight_sz = 0,",
            "\t.weight_nr_accesses = 0,",
            "\t.weight_age = 1",
            "};",
            "DEFINE_DAMON_MODULES_DAMOS_QUOTAS(damon_reclaim_quota);",
            "",
            "static struct damos_watermarks damon_reclaim_wmarks = {",
            "\t.metric = DAMOS_WMARK_FREE_MEM_RATE,",
            "\t.interval = 5000000,\t/* 5 seconds */",
            "\t.high = 500,\t\t/* 50 percent */",
            "\t.mid = 400,\t\t/* 40 percent */",
            "\t.low = 200,\t\t/* 20 percent */",
            "};",
            "DEFINE_DAMON_MODULES_WMARKS_PARAMS(damon_reclaim_wmarks);",
            "",
            "static struct damon_attrs damon_reclaim_mon_attrs = {",
            "\t.sample_interval = 5000,\t/* 5 ms */",
            "\t.aggr_interval = 100000,\t/* 100 ms */",
            "\t.ops_update_interval = 0,",
            "\t.min_nr_regions = 10,",
            "\t.max_nr_regions = 1000,",
            "};",
            "DEFINE_DAMON_MODULES_MON_ATTRS_PARAMS(damon_reclaim_mon_attrs);",
            "",
            "/*",
            " * Start of the target memory region in physical address.",
            " *",
            " * The start physical address of memory region that DAMON_RECLAIM will do work",
            " * against.  By default, biggest System RAM is used as the region.",
            " */",
            "static unsigned long monitor_region_start __read_mostly;",
            "module_param(monitor_region_start, ulong, 0600);",
            "",
            "/*",
            " * End of the target memory region in physical address.",
            " *",
            " * The end physical address of memory region that DAMON_RECLAIM will do work",
            " * against.  By default, biggest System RAM is used as the region.",
            " */",
            "static unsigned long monitor_region_end __read_mostly;",
            "module_param(monitor_region_end, ulong, 0600);",
            "",
            "/*",
            " * Skip anonymous pages reclamation.",
            " *",
            " * If this parameter is set as ``Y``, DAMON_RECLAIM does not reclaim anonymous",
            " * pages.  By default, ``N``.",
            " */",
            "static bool skip_anon __read_mostly;",
            "module_param(skip_anon, bool, 0600);",
            "",
            "/*",
            " * PID of the DAMON thread",
            " *",
            " * If DAMON_RECLAIM is enabled, this becomes the PID of the worker thread.",
            " * Else, -1.",
            " */",
            "static int kdamond_pid __read_mostly = -1;",
            "module_param(kdamond_pid, int, 0400);",
            "",
            "static struct damos_stat damon_reclaim_stat;",
            "DEFINE_DAMON_MODULES_DAMOS_STATS_PARAMS(damon_reclaim_stat,",
            "\t\treclaim_tried_regions, reclaimed_regions, quota_exceeds);",
            "",
            "static struct damon_ctx *ctx;",
            "static struct damon_target *target;",
            "",
            "static struct damos *damon_reclaim_new_scheme(void)",
            "{",
            "\tstruct damos_access_pattern pattern = {",
            "\t\t/* Find regions having PAGE_SIZE or larger size */",
            "\t\t.min_sz_region = PAGE_SIZE,",
            "\t\t.max_sz_region = ULONG_MAX,",
            "\t\t/* and not accessed at all */",
            "\t\t.min_nr_accesses = 0,",
            "\t\t.max_nr_accesses = 0,",
            "\t\t/* for min_age or more micro-seconds */",
            "\t\t.min_age_region = min_age /",
            "\t\t\tdamon_reclaim_mon_attrs.aggr_interval,",
            "\t\t.max_age_region = UINT_MAX,",
            "\t};",
            "",
            "\treturn damon_new_scheme(",
            "\t\t\t&pattern,",
            "\t\t\t/* page out those, as soon as found */",
            "\t\t\tDAMOS_PAGEOUT,",
            "\t\t\t/* for each aggregation interval */",
            "\t\t\t0,",
            "\t\t\t/* under the quota. */",
            "\t\t\t&damon_reclaim_quota,",
            "\t\t\t/* (De)activate this according to the watermarks. */",
            "\t\t\t&damon_reclaim_wmarks);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义DAMON_RECLAIM模块的全局参数和配置，包括启用状态、冷内存识别时间阈值、配额限制、水印条件及监控属性，用于控制基于DAMON的页面回收行为。",
          "similarity": 0.47797900438308716
        },
        {
          "chunk_id": 2,
          "file_path": "mm/damon/reclaim.c",
          "start_line": 269,
          "end_line": 298,
          "content": [
            "static int damon_reclaim_after_aggregation(struct damon_ctx *c)",
            "{",
            "\tstruct damos *s;",
            "",
            "\t/* update the stats parameter */",
            "\tdamon_for_each_scheme(s, c)",
            "\t\tdamon_reclaim_stat = s->stat;",
            "",
            "\treturn damon_reclaim_handle_commit_inputs();",
            "}",
            "static int damon_reclaim_after_wmarks_check(struct damon_ctx *c)",
            "{",
            "\treturn damon_reclaim_handle_commit_inputs();",
            "}",
            "static int __init damon_reclaim_init(void)",
            "{",
            "\tint err = damon_modules_new_paddr_ctx_target(&ctx, &target);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tctx->callback.after_wmarks_check = damon_reclaim_after_wmarks_check;",
            "\tctx->callback.after_aggregation = damon_reclaim_after_aggregation;",
            "",
            "\t/* 'enabled' has set before this function, probably via command line */",
            "\tif (enabled)",
            "\t\terr = damon_reclaim_turn(true);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "damon_reclaim_after_aggregation, damon_reclaim_after_wmarks_check, damon_reclaim_init",
          "description": "注册DAMON框架的回调函数以实现回收策略的动态调整，初始化阶段绑定自定义回调至上下文，确保在监控周期关键节点触发参数重载和回收策略更新。",
          "similarity": 0.45758602023124695
        }
      ]
    }
  ]
}