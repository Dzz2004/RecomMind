{
  "query": "模块记录中断时间间隔 CPU调度策略",
  "timestamp": "2025-12-25 23:36:53",
  "retrieved_files": [
    {
      "source_file": "kernel/sched/rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:14:51\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\rt.c`\n\n---\n\n# `sched/rt.c` 技术文档\n\n## 1. 文件概述\n\n`sched/rt.c` 是 Linux 内核调度子系统中实现实时（Real-Time, RT）调度类的核心文件，主要支持 `SCHED_FIFO` 和 `SCHED_RR` 两种 POSIX 实时调度策略。该文件负责管理实时任务的运行队列、优先级调度、时间片分配、带宽限制（RT throttling）以及在多核系统（SMP）下的负载均衡机制。此外，它还提供了对实时任务组调度（RT Group Scheduling）的支持，允许通过 cgroups 对实时任务的 CPU 使用进行资源控制。\n\n## 2. 核心功能\n\n### 全局变量\n- `sched_rr_timeslice`：定义 `SCHED_RR` 策略的默认时间片长度（单位：调度 tick）。\n- `max_rt_runtime`：实时任务在单个周期内可使用的最大运行时间上限（通常为 4 小时以上）。\n- `sysctl_sched_rt_period`：实时带宽控制的周期，默认为 1,000,000 微秒（1 秒）。\n- `sysctl_sched_rt_runtime`：每个周期内允许实时任务运行的时间，默认为 950,000 微秒（0.95 秒）。\n\n### sysctl 接口（`CONFIG_SYSCTL` 启用时）\n- `/proc/sys/kernel/sched_rt_period_us`：设置 RT 带宽控制周期。\n- `/proc/sys/kernel/sched_rt_runtime_us`：设置 RT 带宽控制运行时间（可设为 -1 表示无限制）。\n- `/proc/sys/kernel/sched_rr_timeslice_ms`：设置 `SCHED_RR` 时间片（毫秒）。\n\n### 主要函数\n- `init_rt_rq(struct rt_rq *rt_rq)`：初始化实时运行队列（`rt_rq`），包括优先级位图、链表、SMP 相关字段及带宽控制状态。\n- `init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)`：初始化 RT 带宽控制结构，配置高精度定时器。\n- `sched_rt_period_timer(struct hrtimer *timer)`：高精度定时器回调函数，用于周期性重置 RT 运行时间配额。\n- `start_rt_bandwidth(struct rt_bandwidth *rt_b)`：启动 RT 带宽控制定时器。\n- `alloc_rt_sched_group / free_rt_sched_group / unregister_rt_sched_group`：管理实时任务组（task group）的资源分配与释放。\n- `init_tg_rt_entry`：初始化任务组在指定 CPU 上的 RT 调度实体和运行队列。\n- `rt_task_of / rq_of_rt_rq / rt_rq_of_se / rq_of_rt_se`：辅助函数，用于在调度实体、任务、运行队列和 CPU 队列之间相互转换。\n\n### SMP 支持函数（`CONFIG_SMP` 启用时）\n- `need_pull_rt_task`：判断是否需要从其他 CPU 拉取高优先级 RT 任务。\n- `rt_overloaded` / `rt_set_overload`：用于跟踪系统中是否存在过载的 RT 运行队列，支持 RT 任务迁移。\n\n## 3. 关键实现\n\n### 实时运行队列（`rt_rq`）管理\n- 使用 `rt_prio_array` 结构维护 0 到 `MAX_RT_PRIO-1`（通常为 99）共 100 个优先级的双向链表。\n- 通过位图（`bitmap`）快速查找最高优先级的可运行任务，`__set_bit(MAX_RT_PRIO, bitmap)` 作为位图搜索的终止标记。\n- `rt_queued` 标志表示是否有 RT 任务入队；`highest_prio.curr/next` 跟踪当前和下一个最高优先级（SMP 专用）。\n\n### RT 带宽控制（Throttling）\n- 通过 `rt_bandwidth` 结构限制 RT 任务在每个 `rt_period` 内最多使用 `rt_runtime` 的 CPU 时间。\n- 使用高精度定时器（`hrtimer`）实现周期性重置：每经过 `rt_period`，将 `rt_time` 清零并解除 throttling。\n- 若 `rt_runtime == RUNTIME_INF`（即 -1），则禁用带宽限制。\n- 定时器回调 `sched_rt_period_timer` 支持处理定时器 overrun（跳过多个周期），确保带宽控制的准确性。\n\n### RT 任务组调度（`CONFIG_RT_GROUP_SCHED`）\n- 每个 `task_group` 拥有 per-CPU 的 `rt_rq` 和 `sched_rt_entity`。\n- 根叶节点（普通任务）的 `rt_se` 直接链接到 CPU 的全局 `rt_rq`；非叶节点（cgroup）的 `rt_se` 链接到父组的 `rt_rq`，形成调度树。\n- `rt_entity_is_task()` 用于区分调度实体是任务还是任务组。\n\n### SMP 负载均衡\n- 当某 CPU 上运行的 RT 任务优先级降低（如被抢占或阻塞），若其当前最高优先级高于刚被替换的任务，则触发 `need_pull_rt_task`，尝试从其他 CPU 拉取更高优先级的 RT 任务。\n- `overloaded` 标志和 `pushable_tasks` 链表用于支持 RT 任务的主动推送（push）和拉取（pull）机制，确保高优先级任务尽快运行。\n\n## 4. 依赖关系\n\n- **调度核心**：依赖 `kernel/sched/core.c` 提供的通用调度框架、运行队列（`rq`）结构和调度类注册机制。\n- **高精度定时器**：使用 `kernel/time/hrtimer.c` 实现 RT 带宽控制的周期性重置。\n- **SMP 调度**：与 `kernel/sched/topology.c` 和 `kernel/sched/fair.c` 协同实现跨 CPU 的 RT 任务迁移。\n- **cgroups**：当启用 `CONFIG_RT_GROUP_SCHED` 时，与 `kernel/cgroup/` 子系统集成，支持基于 cgroup v1/v2 的 RT 带宽分配。\n- **sysctl**：通过 `kernel/sysctl.c` 暴露运行时可调参数。\n\n## 5. 使用场景\n\n- **实时应用调度**：为音视频处理、工业控制、机器人等需要确定性延迟的应用提供 `SCHED_FIFO`/`SCHED_RR` 调度支持。\n- **系统资源保护**：通过 `sched_rt_runtime_us` 限制 RT 任务的 CPU 占用率（默认 95%），防止其独占 CPU 导致系统僵死。\n- **多租户 RT 资源隔离**：在容器或虚拟化环境中，利用 RT 任务组调度为不同租户分配独立的 RT 带宽配额。\n- **SMP 实时性能优化**：在多核系统中，通过 RT 任务迁移机制减少高优先级任务的调度延迟，提升实时响应能力。",
      "similarity": 0.6525018215179443,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/rt.c",
          "start_line": 529,
          "end_line": 633,
          "content": [
            "static void sched_rt_rq_enqueue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct task_struct *curr = rq_of_rt_rq(rt_rq)->curr;",
            "\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\tstruct sched_rt_entity *rt_se;",
            "",
            "\tint cpu = cpu_of(rq);",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (rt_rq->rt_nr_running) {",
            "\t\tif (!rt_se)",
            "\t\t\tenqueue_top_rt_rq(rt_rq);",
            "\t\telse if (!on_rt_rq(rt_se))",
            "\t\t\tenqueue_rt_entity(rt_se, 0);",
            "",
            "\t\tif (rt_rq->highest_prio.curr < curr->prio)",
            "\t\t\tresched_curr(rq);",
            "\t}",
            "}",
            "static void sched_rt_rq_dequeue(struct rt_rq *rt_rq)",
            "{",
            "\tstruct sched_rt_entity *rt_se;",
            "\tint cpu = cpu_of(rq_of_rt_rq(rt_rq));",
            "",
            "\trt_se = rt_rq->tg->rt_se[cpu];",
            "",
            "\tif (!rt_se) {",
            "\t\tdequeue_top_rt_rq(rt_rq, rt_rq->rt_nr_running);",
            "\t\t/* Kick cpufreq (see the comment in kernel/sched/sched.h). */",
            "\t\tcpufreq_update_util(rq_of_rt_rq(rt_rq), 0);",
            "\t}",
            "\telse if (on_rt_rq(rt_se))",
            "\t\tdequeue_rt_entity(rt_se, 0);",
            "}",
            "static inline int rt_rq_throttled(struct rt_rq *rt_rq)",
            "{",
            "\treturn rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;",
            "}",
            "static int rt_se_boosted(struct sched_rt_entity *rt_se)",
            "{",
            "\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "\tstruct task_struct *p;",
            "",
            "\tif (rt_rq)",
            "\t\treturn !!rt_rq->rt_nr_boosted;",
            "",
            "\tp = rt_task_of(rt_se);",
            "\treturn p->prio != p->normal_prio;",
            "}",
            "bool sched_rt_bandwidth_account(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\treturn (hrtimer_active(&rt_b->rt_period_timer) ||",
            "\t\trt_rq->rt_time < rt_b->rt_runtime);",
            "}",
            "static void do_balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "\tstruct root_domain *rd = rq_of_rt_rq(rt_rq)->rd;",
            "\tint i, weight;",
            "\tu64 rt_period;",
            "",
            "\tweight = cpumask_weight(rd->span);",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\trt_period = ktime_to_ns(rt_b->rt_period);",
            "\tfor_each_cpu(i, rd->span) {",
            "\t\tstruct rt_rq *iter = sched_rt_period_rt_rq(rt_b, i);",
            "\t\ts64 diff;",
            "",
            "\t\tif (iter == rt_rq)",
            "\t\t\tcontinue;",
            "",
            "\t\traw_spin_lock(&iter->rt_runtime_lock);",
            "\t\t/*",
            "\t\t * Either all rqs have inf runtime and there's nothing to steal",
            "\t\t * or __disable_runtime() below sets a specific rq to inf to",
            "\t\t * indicate its been disabled and disallow stealing.",
            "\t\t */",
            "\t\tif (iter->rt_runtime == RUNTIME_INF)",
            "\t\t\tgoto next;",
            "",
            "\t\t/*",
            "\t\t * From runqueues with spare time, take 1/n part of their",
            "\t\t * spare time, but no more than our period.",
            "\t\t */",
            "\t\tdiff = iter->rt_runtime - iter->rt_time;",
            "\t\tif (diff > 0) {",
            "\t\t\tdiff = div_u64((u64)diff, weight);",
            "\t\t\tif (rt_rq->rt_runtime + diff > rt_period)",
            "\t\t\t\tdiff = rt_period - rt_rq->rt_runtime;",
            "\t\t\titer->rt_runtime -= diff;",
            "\t\t\trt_rq->rt_runtime += diff;",
            "\t\t\tif (rt_rq->rt_runtime == rt_period) {",
            "\t\t\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "next:",
            "\t\traw_spin_unlock(&iter->rt_runtime_lock);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}"
          ],
          "function_name": "sched_rt_rq_enqueue, sched_rt_rq_dequeue, rt_rq_throttled, rt_se_boosted, sched_rt_bandwidth_account, do_balance_runtime",
          "description": "实现实时任务队列的插入/移除逻辑，跟踪运行时间消耗，通过跨CPU运行时间平衡算法实现带宽公平分配。",
          "similarity": 0.6428112387657166
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/sched/rt.c",
          "start_line": 1449,
          "end_line": 1589,
          "content": [
            "static void dequeue_rt_entity(struct sched_rt_entity *rt_se, unsigned int flags)",
            "{",
            "\tstruct rq *rq = rq_of_rt_se(rt_se);",
            "",
            "\tupdate_stats_dequeue_rt(rt_rq_of_se(rt_se), rt_se, flags);",
            "",
            "\tdequeue_rt_stack(rt_se, flags);",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\tstruct rt_rq *rt_rq = group_rt_rq(rt_se);",
            "",
            "\t\tif (rt_rq && rt_rq->rt_nr_running)",
            "\t\t\t__enqueue_rt_entity(rt_se, flags);",
            "\t}",
            "\tenqueue_top_rt_rq(&rq->rt);",
            "}",
            "static void",
            "enqueue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tif (flags & ENQUEUE_WAKEUP)",
            "\t\trt_se->timeout = 0;",
            "",
            "\tcheck_schedstat_required();",
            "\tupdate_stats_wait_start_rt(rt_rq_of_se(rt_se), rt_se);",
            "",
            "\tenqueue_rt_entity(rt_se, flags);",
            "",
            "\tif (!task_current(rq, p) && p->nr_cpus_allowed > 1)",
            "\t\tenqueue_pushable_task(rq, p);",
            "}",
            "static bool dequeue_task_rt(struct rq *rq, struct task_struct *p, int flags)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "",
            "\tupdate_curr_rt(rq);",
            "\tdequeue_rt_entity(rt_se, flags);",
            "",
            "\tdequeue_pushable_task(rq, p);",
            "",
            "\treturn true;",
            "}",
            "static void",
            "requeue_rt_entity(struct rt_rq *rt_rq, struct sched_rt_entity *rt_se, int head)",
            "{",
            "\tif (on_rt_rq(rt_se)) {",
            "\t\tstruct rt_prio_array *array = &rt_rq->active;",
            "\t\tstruct list_head *queue = array->queue + rt_se_prio(rt_se);",
            "",
            "\t\tif (head)",
            "\t\t\tlist_move(&rt_se->run_list, queue);",
            "\t\telse",
            "\t\t\tlist_move_tail(&rt_se->run_list, queue);",
            "\t}",
            "}",
            "static void requeue_task_rt(struct rq *rq, struct task_struct *p, int head)",
            "{",
            "\tstruct sched_rt_entity *rt_se = &p->rt;",
            "\tstruct rt_rq *rt_rq;",
            "",
            "\tfor_each_sched_rt_entity(rt_se) {",
            "\t\trt_rq = rt_rq_of_se(rt_se);",
            "\t\trequeue_rt_entity(rt_rq, rt_se, head);",
            "\t}",
            "}",
            "static void yield_task_rt(struct rq *rq)",
            "{",
            "\trequeue_task_rt(rq, rq->curr, 0);",
            "}",
            "static int",
            "select_task_rq_rt(struct task_struct *p, int cpu, int flags)",
            "{",
            "\tstruct task_struct *curr;",
            "\tstruct rq *rq;",
            "\tbool test;",
            "",
            "\t/* For anything but wake ups, just return the task_cpu */",
            "\tif (!(flags & (WF_TTWU | WF_FORK)))",
            "\t\tgoto out;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\trcu_read_lock();",
            "\tcurr = READ_ONCE(rq->curr); /* unlocked access */",
            "",
            "\t/*",
            "\t * If the current task on @p's runqueue is an RT task, then",
            "\t * try to see if we can wake this RT task up on another",
            "\t * runqueue. Otherwise simply start this RT task",
            "\t * on its current runqueue.",
            "\t *",
            "\t * We want to avoid overloading runqueues. If the woken",
            "\t * task is a higher priority, then it will stay on this CPU",
            "\t * and the lower prio task should be moved to another CPU.",
            "\t * Even though this will probably make the lower prio task",
            "\t * lose its cache, we do not want to bounce a higher task",
            "\t * around just because it gave up its CPU, perhaps for a",
            "\t * lock?",
            "\t *",
            "\t * For equal prio tasks, we just let the scheduler sort it out.",
            "\t *",
            "\t * Otherwise, just let it ride on the affined RQ and the",
            "\t * post-schedule router will push the preempted task away",
            "\t *",
            "\t * This test is optimistic, if we get it wrong the load-balancer",
            "\t * will have to sort it out.",
            "\t *",
            "\t * We take into account the capacity of the CPU to ensure it fits the",
            "\t * requirement of the task - which is only important on heterogeneous",
            "\t * systems like big.LITTLE.",
            "\t */",
            "\ttest = curr &&",
            "\t       unlikely(rt_task(curr)) &&",
            "\t       (curr->nr_cpus_allowed < 2 || curr->prio <= p->prio);",
            "",
            "\tif (test || !rt_task_fits_capacity(p, cpu)) {",
            "\t\tint target = find_lowest_rq(p);",
            "",
            "\t\t/*",
            "\t\t * Bail out if we were forcing a migration to find a better",
            "\t\t * fitting CPU but our search failed.",
            "\t\t */",
            "\t\tif (!test && target != -1 && !rt_task_fits_capacity(p, target))",
            "\t\t\tgoto out_unlock;",
            "",
            "\t\t/*",
            "\t\t * Don't bother moving it if the destination CPU is",
            "\t\t * not running a lower priority task.",
            "\t\t */",
            "\t\tif (target != -1 &&",
            "\t\t    p->prio < cpu_rq(target)->rt.highest_prio.curr)",
            "\t\t\tcpu = target;",
            "\t}",
            "",
            "out_unlock:",
            "\trcu_read_unlock();",
            "",
            "out:",
            "\treturn cpu;",
            "}"
          ],
          "function_name": "dequeue_rt_entity, enqueue_task_rt, dequeue_task_rt, requeue_rt_entity, requeue_task_rt, yield_task_rt, select_task_rq_rt",
          "description": "实现实时任务的出队逻辑、唤醒和迁移策略，提供CPU亲和性选择及负载均衡支持，维护优先级队列的动态调整。",
          "similarity": 0.627980649471283
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/sched/rt.c",
          "start_line": 2102,
          "end_line": 2228,
          "content": [
            "static void push_rt_tasks(struct rq *rq)",
            "{",
            "\t/* push_rt_task will return true if it moved an RT */",
            "\twhile (push_rt_task(rq, false))",
            "\t\t;",
            "}",
            "static int rto_next_cpu(struct root_domain *rd)",
            "{",
            "\tint next;",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * When starting the IPI RT pushing, the rto_cpu is set to -1,",
            "\t * rt_next_cpu() will simply return the first CPU found in",
            "\t * the rto_mask.",
            "\t *",
            "\t * If rto_next_cpu() is called with rto_cpu is a valid CPU, it",
            "\t * will return the next CPU found in the rto_mask.",
            "\t *",
            "\t * If there are no more CPUs left in the rto_mask, then a check is made",
            "\t * against rto_loop and rto_loop_next. rto_loop is only updated with",
            "\t * the rto_lock held, but any CPU may increment the rto_loop_next",
            "\t * without any locking.",
            "\t */",
            "\tfor (;;) {",
            "",
            "\t\t/* When rto_cpu is -1 this acts like cpumask_first() */",
            "\t\tcpu = cpumask_next(rd->rto_cpu, rd->rto_mask);",
            "",
            "\t\trd->rto_cpu = cpu;",
            "",
            "\t\tif (cpu < nr_cpu_ids)",
            "\t\t\treturn cpu;",
            "",
            "\t\trd->rto_cpu = -1;",
            "",
            "\t\t/*",
            "\t\t * ACQUIRE ensures we see the @rto_mask changes",
            "\t\t * made prior to the @next value observed.",
            "\t\t *",
            "\t\t * Matches WMB in rt_set_overload().",
            "\t\t */",
            "\t\tnext = atomic_read_acquire(&rd->rto_loop_next);",
            "",
            "\t\tif (rd->rto_loop == next)",
            "\t\t\tbreak;",
            "",
            "\t\trd->rto_loop = next;",
            "\t}",
            "",
            "\treturn -1;",
            "}",
            "static inline bool rto_start_trylock(atomic_t *v)",
            "{",
            "\treturn !atomic_cmpxchg_acquire(v, 0, 1);",
            "}",
            "static inline void rto_start_unlock(atomic_t *v)",
            "{",
            "\tatomic_set_release(v, 0);",
            "}",
            "static void tell_cpu_to_push(struct rq *rq)",
            "{",
            "\tint cpu = -1;",
            "",
            "\t/* Keep the loop going if the IPI is currently active */",
            "\tatomic_inc(&rq->rd->rto_loop_next);",
            "",
            "\t/* Only one CPU can initiate a loop at a time */",
            "\tif (!rto_start_trylock(&rq->rd->rto_loop_start))",
            "\t\treturn;",
            "",
            "\traw_spin_lock(&rq->rd->rto_lock);",
            "",
            "\t/*",
            "\t * The rto_cpu is updated under the lock, if it has a valid CPU",
            "\t * then the IPI is still running and will continue due to the",
            "\t * update to loop_next, and nothing needs to be done here.",
            "\t * Otherwise it is finishing up and an ipi needs to be sent.",
            "\t */",
            "\tif (rq->rd->rto_cpu < 0)",
            "\t\tcpu = rto_next_cpu(rq->rd);",
            "",
            "\traw_spin_unlock(&rq->rd->rto_lock);",
            "",
            "\trto_start_unlock(&rq->rd->rto_loop_start);",
            "",
            "\tif (cpu >= 0) {",
            "\t\t/* Make sure the rd does not get freed while pushing */",
            "\t\tsched_get_rd(rq->rd);",
            "\t\tirq_work_queue_on(&rq->rd->rto_push_work, cpu);",
            "\t}",
            "}",
            "void rto_push_irq_work_func(struct irq_work *work)",
            "{",
            "\tstruct root_domain *rd =",
            "\t\tcontainer_of(work, struct root_domain, rto_push_work);",
            "\tstruct rq *rq;",
            "\tint cpu;",
            "",
            "\trq = this_rq();",
            "",
            "\t/*",
            "\t * We do not need to grab the lock to check for has_pushable_tasks.",
            "\t * When it gets updated, a check is made if a push is possible.",
            "\t */",
            "\tif (has_pushable_tasks(rq)) {",
            "\t\traw_spin_rq_lock(rq);",
            "\t\twhile (push_rt_task(rq, true))",
            "\t\t\t;",
            "\t\traw_spin_rq_unlock(rq);",
            "\t}",
            "",
            "\traw_spin_lock(&rd->rto_lock);",
            "",
            "\t/* Pass the IPI to the next rt overloaded queue */",
            "\tcpu = rto_next_cpu(rd);",
            "",
            "\traw_spin_unlock(&rd->rto_lock);",
            "",
            "\tif (cpu < 0) {",
            "\t\tsched_put_rd(rd);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Try the next RT overloaded CPU */",
            "\tirq_work_queue_on(&rd->rto_push_work, cpu);",
            "}"
          ],
          "function_name": "push_rt_tasks, rto_next_cpu, rto_start_trylock, rto_start_unlock, tell_cpu_to_push, rto_push_irq_work_func",
          "description": "实现实时任务批量迁移、冗余CPU迭代选择、锁竞争控制及中断工作队列驱动的跨CPU负载均衡机制。",
          "similarity": 0.616247296333313
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/rt.c",
          "start_line": 57,
          "end_line": 159,
          "content": [
            "static int __init sched_rt_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", sched_rt_sysctls);",
            "\treturn 0;",
            "}",
            "void init_rt_rq(struct rt_rq *rt_rq)",
            "{",
            "\tstruct rt_prio_array *array;",
            "\tint i;",
            "",
            "\tarray = &rt_rq->active;",
            "\tfor (i = 0; i < MAX_RT_PRIO; i++) {",
            "\t\tINIT_LIST_HEAD(array->queue + i);",
            "\t\t__clear_bit(i, array->bitmap);",
            "\t}",
            "\t/* delimiter for bitsearch: */",
            "\t__set_bit(MAX_RT_PRIO, array->bitmap);",
            "",
            "#if defined CONFIG_SMP",
            "\trt_rq->highest_prio.curr = MAX_RT_PRIO-1;",
            "\trt_rq->highest_prio.next = MAX_RT_PRIO-1;",
            "\trt_rq->overloaded = 0;",
            "\tplist_head_init(&rt_rq->pushable_tasks);",
            "#endif /* CONFIG_SMP */",
            "\t/* We start is dequeued state, because no RT tasks are queued */",
            "\trt_rq->rt_queued = 0;",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\trt_rq->rt_time = 0;",
            "\trt_rq->rt_throttled = 0;",
            "\trt_rq->rt_runtime = 0;",
            "\traw_spin_lock_init(&rt_rq->rt_runtime_lock);",
            "#endif",
            "}",
            "static enum hrtimer_restart sched_rt_period_timer(struct hrtimer *timer)",
            "{",
            "\tstruct rt_bandwidth *rt_b =",
            "\t\tcontainer_of(timer, struct rt_bandwidth, rt_period_timer);",
            "\tint idle = 0;",
            "\tint overrun;",
            "",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tfor (;;) {",
            "\t\toverrun = hrtimer_forward_now(timer, rt_b->rt_period);",
            "\t\tif (!overrun)",
            "\t\t\tbreak;",
            "",
            "\t\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "\t\tidle = do_sched_rt_period_timer(rt_b, overrun);",
            "\t\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\t}",
            "\tif (idle)",
            "\t\trt_b->rt_period_active = 0;",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "",
            "\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART;",
            "}",
            "void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime)",
            "{",
            "\trt_b->rt_period = ns_to_ktime(period);",
            "\trt_b->rt_runtime = runtime;",
            "",
            "\traw_spin_lock_init(&rt_b->rt_runtime_lock);",
            "",
            "\thrtimer_init(&rt_b->rt_period_timer, CLOCK_MONOTONIC,",
            "\t\t     HRTIMER_MODE_REL_HARD);",
            "\trt_b->rt_period_timer.function = sched_rt_period_timer;",
            "}",
            "static inline void do_start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\traw_spin_lock(&rt_b->rt_runtime_lock);",
            "\tif (!rt_b->rt_period_active) {",
            "\t\trt_b->rt_period_active = 1;",
            "\t\t/*",
            "\t\t * SCHED_DEADLINE updates the bandwidth, as a run away",
            "\t\t * RT task with a DL task could hog a CPU. But DL does",
            "\t\t * not reset the period. If a deadline task was running",
            "\t\t * without an RT task running, it can cause RT tasks to",
            "\t\t * throttle when they start up. Kick the timer right away",
            "\t\t * to update the period.",
            "\t\t */",
            "\t\thrtimer_forward_now(&rt_b->rt_period_timer, ns_to_ktime(0));",
            "\t\thrtimer_start_expires(&rt_b->rt_period_timer,",
            "\t\t\t\t      HRTIMER_MODE_ABS_PINNED_HARD);",
            "\t}",
            "\traw_spin_unlock(&rt_b->rt_runtime_lock);",
            "}",
            "static void start_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\tif (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF)",
            "\t\treturn;",
            "",
            "\tdo_start_rt_bandwidth(rt_b);",
            "}",
            "static void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)",
            "{",
            "\thrtimer_cancel(&rt_b->rt_period_timer);",
            "}",
            "void unregister_rt_sched_group(struct task_group *tg)",
            "{",
            "\tif (tg->rt_se)",
            "\t\tdestroy_rt_bandwidth(&tg->rt_bandwidth);",
            "}"
          ],
          "function_name": "sched_rt_sysctl_init, init_rt_rq, sched_rt_period_timer, init_rt_bandwidth, do_start_rt_bandwidth, start_rt_bandwidth, destroy_rt_bandwidth, unregister_rt_sched_group",
          "description": "初始化实时调度相关数据结构，管理实时任务周期定时器，控制实时带宽分配与回收，实现基于时间片轮转的调度策略。",
          "similarity": 0.6066047549247742
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/rt.c",
          "start_line": 776,
          "end_line": 913,
          "content": [
            "static void balance_runtime(struct rt_rq *rt_rq)",
            "{",
            "\tif (!sched_feat(RT_RUNTIME_SHARE))",
            "\t\treturn;",
            "",
            "\tif (rt_rq->rt_time > rt_rq->rt_runtime) {",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tdo_balance_runtime(rt_rq);",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t}",
            "}",
            "static inline void balance_runtime(struct rt_rq *rt_rq) {}",
            "static int do_sched_rt_period_timer(struct rt_bandwidth *rt_b, int overrun)",
            "{",
            "\tint i, idle = 1, throttled = 0;",
            "\tconst struct cpumask *span;",
            "",
            "\tspan = sched_rt_period_mask();",
            "",
            "\t/*",
            "\t * FIXME: isolated CPUs should really leave the root task group,",
            "\t * whether they are isolcpus or were isolated via cpusets, lest",
            "\t * the timer run on a CPU which does not service all runqueues,",
            "\t * potentially leaving other CPUs indefinitely throttled.  If",
            "\t * isolation is really required, the user will turn the throttle",
            "\t * off to kill the perturbations it causes anyway.  Meanwhile,",
            "\t * this maintains functionality for boot and/or troubleshooting.",
            "\t */",
            "\tif (rt_b == &root_task_group.rt_bandwidth)",
            "\t\tspan = cpu_online_mask;",
            "",
            "\tfor_each_cpu(i, span) {",
            "\t\tint enqueue = 0;",
            "\t\tstruct rt_rq *rt_rq = sched_rt_period_rt_rq(rt_b, i);",
            "\t\tstruct rq *rq = rq_of_rt_rq(rt_rq);",
            "\t\tstruct rq_flags rf;",
            "\t\tint skip;",
            "",
            "\t\t/*",
            "\t\t * When span == cpu_online_mask, taking each rq->lock",
            "\t\t * can be time-consuming. Try to avoid it when possible.",
            "\t\t */",
            "\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\tif (!sched_feat(RT_RUNTIME_SHARE) && rt_rq->rt_runtime != RUNTIME_INF)",
            "\t\t\trt_rq->rt_runtime = rt_b->rt_runtime;",
            "\t\tskip = !rt_rq->rt_time && !rt_rq->rt_nr_running;",
            "\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\tif (skip)",
            "\t\t\tcontinue;",
            "",
            "\t\trq_lock(rq, &rf);",
            "\t\tupdate_rq_clock(rq);",
            "",
            "\t\tif (rt_rq->rt_time) {",
            "\t\t\tu64 runtime;",
            "",
            "\t\t\traw_spin_lock(&rt_rq->rt_runtime_lock);",
            "\t\t\tif (rt_rq->rt_throttled)",
            "\t\t\t\tbalance_runtime(rt_rq);",
            "\t\t\truntime = rt_rq->rt_runtime;",
            "\t\t\trt_rq->rt_time -= min(rt_rq->rt_time, overrun*runtime);",
            "\t\t\tif (rt_rq->rt_throttled && rt_rq->rt_time < runtime) {",
            "\t\t\t\trt_rq->rt_throttled = 0;",
            "\t\t\t\tenqueue = 1;",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * When we're idle and a woken (rt) task is",
            "\t\t\t\t * throttled wakeup_preempt() will set",
            "\t\t\t\t * skip_update and the time between the wakeup",
            "\t\t\t\t * and this unthrottle will get accounted as",
            "\t\t\t\t * 'runtime'.",
            "\t\t\t\t */",
            "\t\t\t\tif (rt_rq->rt_nr_running && rq->curr == rq->idle)",
            "\t\t\t\t\trq_clock_cancel_skipupdate(rq);",
            "\t\t\t}",
            "\t\t\tif (rt_rq->rt_time || rt_rq->rt_nr_running)",
            "\t\t\t\tidle = 0;",
            "\t\t\traw_spin_unlock(&rt_rq->rt_runtime_lock);",
            "\t\t} else if (rt_rq->rt_nr_running) {",
            "\t\t\tidle = 0;",
            "\t\t\tif (!rt_rq_throttled(rt_rq))",
            "\t\t\t\tenqueue = 1;",
            "\t\t}",
            "\t\tif (rt_rq->rt_throttled)",
            "\t\t\tthrottled = 1;",
            "",
            "\t\tif (enqueue)",
            "\t\t\tsched_rt_rq_enqueue(rt_rq);",
            "\t\trq_unlock(rq, &rf);",
            "\t}",
            "",
            "\tif (!throttled && (!rt_bandwidth_enabled() || rt_b->rt_runtime == RUNTIME_INF))",
            "\t\treturn 1;",
            "",
            "\treturn idle;",
            "}",
            "static int sched_rt_runtime_exceeded(struct rt_rq *rt_rq)",
            "{",
            "\tu64 runtime = sched_rt_runtime(rt_rq);",
            "",
            "\tif (rt_rq->rt_throttled)",
            "\t\treturn rt_rq_throttled(rt_rq);",
            "",
            "\tif (runtime >= sched_rt_period(rt_rq))",
            "\t\treturn 0;",
            "",
            "\tbalance_runtime(rt_rq);",
            "\truntime = sched_rt_runtime(rt_rq);",
            "\tif (runtime == RUNTIME_INF)",
            "\t\treturn 0;",
            "",
            "\tif (rt_rq->rt_time > runtime) {",
            "\t\tstruct rt_bandwidth *rt_b = sched_rt_bandwidth(rt_rq);",
            "",
            "\t\t/*",
            "\t\t * Don't actually throttle groups that have no runtime assigned",
            "\t\t * but accrue some time due to boosting.",
            "\t\t */",
            "\t\tif (likely(rt_b->rt_runtime)) {",
            "\t\t\trt_rq->rt_throttled = 1;",
            "\t\t\tprintk_deferred_once(\"sched: RT throttling activated\\n\");",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * In case we did anyway, make it go away,",
            "\t\t\t * replenishment is a joke, since it will replenish us",
            "\t\t\t * with exactly 0 ns.",
            "\t\t\t */",
            "\t\t\trt_rq->rt_time = 0;",
            "\t\t}",
            "",
            "\t\tif (rt_rq_throttled(rt_rq)) {",
            "\t\t\tsched_rt_rq_dequeue(rt_rq);",
            "\t\t\treturn 1;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "balance_runtime, balance_runtime, do_sched_rt_period_timer, sched_rt_runtime_exceeded",
          "description": "`balance_runtime`在超时时触发重新平衡，`do_sched_rt_period_timer`周期性调整运行时并检查节流状态，`sched_rt_runtime_exceeded`判断是否超出运行时限制并标记节流",
          "similarity": 0.598139762878418
        }
      ]
    },
    {
      "source_file": "kernel/irq/timings.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:10:35\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\timings.c`\n\n---\n\n# irq/timings.c 技术文档\n\n## 1. 文件概述\n\n`irq/timings.c` 是 Linux 内核中用于中断时间预测的核心模块。该文件实现了基于历史中断时间戳的预测算法，旨在通过分析中断发生的周期性模式，预测下一次中断可能发生的时间。此功能主要用于低功耗场景（如 CPU 空闲状态管理），帮助调度器或电源管理子系统更精确地设置唤醒时间，从而在保证响应性的同时减少不必要的唤醒开销。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `struct irq_timings`：每个 CPU 私有的中断时间记录结构，包含一个循环缓冲区，用于存储 `<中断号, 时间戳>` 元组。\n- `irqt_stats`：全局 IDR（整数到指针映射）结构，用于按中断号索引中断统计信息。\n- `irq_timing_enabled`：静态分支键（`static_key`），用于在运行时动态启用/禁用中断时间跟踪功能，避免性能开销。\n\n### 主要函数\n\n- `irq_timings_enable(void)`：启用中断时间跟踪功能，激活静态分支。\n- `irq_timings_disable(void)`：禁用中断时间跟踪功能，关闭静态分支。\n- （注：实际的预测算法逻辑虽未在代码片段中完整展示，但文档详细描述了其实现原理）\n\n## 3. 关键实现\n\n### 中断时间预测算法\n\n该模块采用三阶段算法预测中断周期：\n\n#### 1) 后缀数组（Suffix Array）模式识别\n- 将中断间隔（经 `ilog2` 映射后的索引值）序列视为字符串。\n- 构建长度为 2 到 5 的后缀（受限于实际设备周期经验）。\n- 在最近 `3 × max_period`（即 15）个索引中搜索后缀的重复出现。\n- 若某后缀连续出现 3 次，则认为发现有效周期模式，其长度即为预测周期。\n\n#### 2) 对数间隔桶（Log Interval Bucketing）\n- 使用 `ilog2(interval)` 将原始时间间隔映射到 0~63 的桶索引（因 `u64` 最大为 2^64）。\n- 该方法将大范围的时间值压缩到小数组中，例如值 1123 映射到索引 10（因 2^10 = 1024 ≤ 1123 < 2048 = 2^11）。\n\n#### 3) 指数移动平均（Exponential Moving Average, EMA）\n- 每个桶维护一个 EMA 值，用于平滑同一数量级间隔的波动。\n- EMA 公式使平均值对新数据具有可调的响应速度（通过 alpha 参数隐式控制）。\n- 预测时，根据识别出的周期模式中的桶索引，返回对应桶的 EMA 值作为预测间隔。\n\n### 工作流程\n1. 中断发生时，若 `irq_timing_enabled` 为真，则将 `<irq, timestamp>` 记录到 per-CPU 的循环缓冲区。\n2. 当需要预测某中断的下次发生时间时：\n   - 清空并处理循环缓冲区，将间隔数据分发到各中断的统计结构中。\n   - 对每个中断的间隔序列执行上述三阶段算法。\n   - 若找到重复模式，则用 EMA 值计算预测时间；否则返回未预测。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/percpu.h>`：实现 per-CPU 变量 `irq_timings`。\n  - `<linux/static_key.h>`：提供静态分支优化，避免未启用时的条件判断开销。\n  - `<linux/math64.h>` 和 `<linux/log2.h>`：用于 `ilog2` 等数学运算。\n  - `<trace/events/irq.h>`：可能用于跟踪事件（虽未在片段中调用）。\n  - `\"internals.h\"`：内核中断子系统内部头文件。\n- **子系统依赖**：\n  - 通用中断子系统（`<linux/irq.h>`, `<linux/interrupt.h>`）。\n  - 内存管理（`<linux/slab.h>`）用于动态分配统计结构。\n  - IDR 机制（`<linux/idr.h>`）用于中断号到统计结构的映射。\n\n## 5. 使用场景\n\n- **CPU 空闲状态管理（cpuidle）**：在进入深度 C-state 前，预测下一次中断时间以设置精确的唤醒定时器，避免过早或过晚唤醒。\n- **实时系统调度**：辅助调度器预判周期性中断（如 tickless 系统中的高精度定时器），优化任务调度时机。\n- **电源管理**：结合设备驱动的中断模式，动态调整设备或 CPU 的电源状态。\n- **性能分析**：通过跟踪中断时间模式，诊断中断风暴或异常周期行为。\n\n该功能默认关闭，仅在需要时通过 `irq_timings_enable()` 动态启用，确保对常规系统性能无影响。",
      "similarity": 0.6464900374412537,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/timings.c",
          "start_line": 1,
          "end_line": 25,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "// Copyright (C) 2016, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>",
            "#define pr_fmt(fmt) \"irq_timings: \" fmt",
            "",
            "#include <linux/kernel.h>",
            "#include <linux/percpu.h>",
            "#include <linux/slab.h>",
            "#include <linux/static_key.h>",
            "#include <linux/init.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/idr.h>",
            "#include <linux/irq.h>",
            "#include <linux/math64.h>",
            "#include <linux/log2.h>",
            "",
            "#include <trace/events/irq.h>",
            "",
            "#include \"internals.h\"",
            "",
            "DEFINE_STATIC_KEY_FALSE(irq_timing_enabled);",
            "",
            "DEFINE_PER_CPU(struct irq_timings, irq_timings);",
            "",
            "static DEFINE_IDR(irqt_stats);",
            ""
          ],
          "function_name": null,
          "description": "定义中断计时模块的静态键开关和PER-CPU结构体，声明IDR管理器用于动态分配中断统计实例",
          "similarity": 0.6732043623924255
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/timings.c",
          "start_line": 26,
          "end_line": 165,
          "content": [
            "void irq_timings_enable(void)",
            "{",
            "\tstatic_branch_enable(&irq_timing_enabled);",
            "}",
            "void irq_timings_disable(void)",
            "{",
            "\tstatic_branch_disable(&irq_timing_enabled);",
            "}",
            "static u64 irq_timings_ema_new(u64 value, u64 ema_old)",
            "{",
            "\ts64 diff;",
            "",
            "\tif (unlikely(!ema_old))",
            "\t\treturn value;",
            "",
            "\tdiff = (value - ema_old) * EMA_ALPHA_VAL;",
            "\t/*",
            "\t * We can use a s64 type variable to be added with the u64",
            "\t * ema_old variable as this one will never have its topmost",
            "\t * bit set, it will be always smaller than 2^63 nanosec",
            "\t * interrupt interval (292 years).",
            "\t */",
            "\treturn ema_old + (diff >> EMA_ALPHA_SHIFT);",
            "}",
            "static int irq_timings_next_event_index(int *buffer, size_t len, int period_max)",
            "{",
            "\tint period;",
            "",
            "\t/*",
            "\t * Move the beginning pointer to the end minus the max period x 3.",
            "\t * We are at the point we can begin searching the pattern",
            "\t */",
            "\tbuffer = &buffer[len - (period_max * 3)];",
            "",
            "\t/* Adjust the length to the maximum allowed period x 3 */",
            "\tlen = period_max * 3;",
            "",
            "\t/*",
            "\t * The buffer contains the suite of intervals, in a ilog2",
            "\t * basis, we are looking for a repetition. We point the",
            "\t * beginning of the search three times the length of the",
            "\t * period beginning at the end of the buffer. We do that for",
            "\t * each suffix.",
            "\t */",
            "\tfor (period = period_max; period >= PREDICTION_PERIOD_MIN; period--) {",
            "",
            "\t\t/*",
            "\t\t * The first comparison always succeed because the",
            "\t\t * suffix is deduced from the first n-period bytes of",
            "\t\t * the buffer and we compare the initial suffix with",
            "\t\t * itself, so we can skip the first iteration.",
            "\t\t */",
            "\t\tint idx = period;",
            "\t\tsize_t size = period;",
            "",
            "\t\t/*",
            "\t\t * We look if the suite with period 'i' repeat",
            "\t\t * itself. If it is truncated at the end, as it",
            "\t\t * repeats we can use the period to find out the next",
            "\t\t * element with the modulo.",
            "\t\t */",
            "\t\twhile (!memcmp(buffer, &buffer[idx], size * sizeof(int))) {",
            "",
            "\t\t\t/*",
            "\t\t\t * Move the index in a period basis",
            "\t\t\t */",
            "\t\t\tidx += size;",
            "",
            "\t\t\t/*",
            "\t\t\t * If this condition is reached, all previous",
            "\t\t\t * memcmp were successful, so the period is",
            "\t\t\t * found.",
            "\t\t\t */",
            "\t\t\tif (idx == len)",
            "\t\t\t\treturn buffer[len % period];",
            "",
            "\t\t\t/*",
            "\t\t\t * If the remaining elements to compare are",
            "\t\t\t * smaller than the period, readjust the size",
            "\t\t\t * of the comparison for the last iteration.",
            "\t\t\t */",
            "\t\t\tif (len - idx < period)",
            "\t\t\t\tsize = len - idx;",
            "\t\t}",
            "\t}",
            "",
            "\treturn -1;",
            "}",
            "static u64 __irq_timings_next_event(struct irqt_stat *irqs, int irq, u64 now)",
            "{",
            "\tint index, i, period_max, count, start, min = INT_MAX;",
            "",
            "\tif ((now - irqs->last_ts) >= NSEC_PER_SEC) {",
            "\t\tirqs->count = irqs->last_ts = 0;",
            "\t\treturn U64_MAX;",
            "\t}",
            "",
            "\t/*",
            "\t * As we want to find three times the repetition, we need a",
            "\t * number of intervals greater or equal to three times the",
            "\t * maximum period, otherwise we truncate the max period.",
            "\t */",
            "\tperiod_max = irqs->count > (3 * PREDICTION_PERIOD_MAX) ?",
            "\t\tPREDICTION_PERIOD_MAX : irqs->count / 3;",
            "",
            "\t/*",
            "\t * If we don't have enough irq timings for this prediction,",
            "\t * just bail out.",
            "\t */",
            "\tif (period_max <= PREDICTION_PERIOD_MIN)",
            "\t\treturn U64_MAX;",
            "",
            "\t/*",
            "\t * 'count' will depends if the circular buffer wrapped or not",
            "\t */",
            "\tcount = irqs->count < IRQ_TIMINGS_SIZE ?",
            "\t\tirqs->count : IRQ_TIMINGS_SIZE;",
            "",
            "\tstart = irqs->count < IRQ_TIMINGS_SIZE ?",
            "\t\t0 : (irqs->count & IRQ_TIMINGS_MASK);",
            "",
            "\t/*",
            "\t * Copy the content of the circular buffer into another buffer",
            "\t * in order to linearize the buffer instead of dealing with",
            "\t * wrapping indexes and shifted array which will be prone to",
            "\t * error and extremely difficult to debug.",
            "\t */",
            "\tfor (i = 0; i < count; i++) {",
            "\t\tint index = (start + i) & IRQ_TIMINGS_MASK;",
            "",
            "\t\tirqs->timings[i] = irqs->circ_timings[index];",
            "\t\tmin = min_t(int, irqs->timings[i], min);",
            "\t}",
            "",
            "\tindex = irq_timings_next_event_index(irqs->timings, count, period_max);",
            "\tif (index < 0)",
            "\t\treturn irqs->last_ts + irqs->ema_time[min];",
            "",
            "\treturn irqs->last_ts + irqs->ema_time[index];",
            "}"
          ],
          "function_name": "irq_timings_enable, irq_timings_disable, irq_timings_ema_new, irq_timings_next_event_index, __irq_timings_next_event",
          "description": "实现中断计时功能的启用/禁用控制，EMA计算算法及基于模式匹配的周期预测核心逻辑",
          "similarity": 0.6405832171440125
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/irq/timings.c",
          "start_line": 821,
          "end_line": 923,
          "content": [
            "static int __init irq_timings_irqs_selftest(void)",
            "{",
            "\tint i, ret;",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(tis); i++) {",
            "\t\tpr_info(\"---> Injecting intervals number #%d (count=%zd)\\n\",",
            "\t\t\ti, tis[i].count);",
            "\t\tret = irq_timings_test_irqs(&tis[i]);",
            "\t\tif (ret)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "static int __init irq_timings_test_irqts(struct irq_timings *irqts,",
            "\t\t\t\t\t unsigned count)",
            "{",
            "\tint start = count >= IRQ_TIMINGS_SIZE ? count - IRQ_TIMINGS_SIZE : 0;",
            "\tint i, irq, oirq = 0xBEEF;",
            "\tu64 ots = 0xDEAD, ts;",
            "",
            "\t/*",
            "\t * Fill the circular buffer by using the dedicated function.",
            "\t */",
            "\tfor (i = 0; i < count; i++) {",
            "\t\tpr_debug(\"%d: index=%d, ts=%llX irq=%X\\n\",",
            "\t\t\t i, i & IRQ_TIMINGS_MASK, ots + i, oirq + i);",
            "",
            "\t\tirq_timings_push(ots + i, oirq + i);",
            "\t}",
            "",
            "\t/*",
            "\t * Compute the first elements values after the index wrapped",
            "\t * up or not.",
            "\t */",
            "\tots += start;",
            "\toirq += start;",
            "",
            "\t/*",
            "\t * Test the circular buffer count is correct.",
            "\t */",
            "\tpr_debug(\"---> Checking timings array count (%d) is right\\n\", count);",
            "\tif (WARN_ON(irqts->count != count))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Test the macro allowing to browse all the irqts.",
            "\t */",
            "\tpr_debug(\"---> Checking the for_each_irqts() macro\\n\");",
            "\tfor_each_irqts(i, irqts) {",
            "",
            "\t\tirq = irq_timing_decode(irqts->values[i], &ts);",
            "",
            "\t\tpr_debug(\"index=%d, ts=%llX / %llX, irq=%X / %X\\n\",",
            "\t\t\t i, ts, ots, irq, oirq);",
            "",
            "\t\tif (WARN_ON(ts != ots || irq != oirq))",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\tots++; oirq++;",
            "\t}",
            "",
            "\t/*",
            "\t * The circular buffer should have be flushed when browsed",
            "\t * with for_each_irqts",
            "\t */",
            "\tpr_debug(\"---> Checking timings array is empty after browsing it\\n\");",
            "\tif (WARN_ON(irqts->count))",
            "\t\treturn -EINVAL;",
            "",
            "\treturn 0;",
            "}",
            "static int __init irq_timings_irqts_selftest(void)",
            "{",
            "\tstruct irq_timings *irqts = this_cpu_ptr(&irq_timings);",
            "\tint i, ret;",
            "",
            "\t/*",
            "\t * Test the circular buffer with different number of",
            "\t * elements. The purpose is to test at the limits (empty, half",
            "\t * full, full, wrapped with the cursor at the boundaries,",
            "\t * wrapped several times, etc ...",
            "\t */",
            "\tint count[] = { 0,",
            "\t\t\tIRQ_TIMINGS_SIZE >> 1,",
            "\t\t\tIRQ_TIMINGS_SIZE,",
            "\t\t\tIRQ_TIMINGS_SIZE + (IRQ_TIMINGS_SIZE >> 1),",
            "\t\t\t2 * IRQ_TIMINGS_SIZE,",
            "\t\t\t(2 * IRQ_TIMINGS_SIZE) + 3,",
            "\t};",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(count); i++) {",
            "",
            "\t\tpr_info(\"---> Checking the timings with %d/%d values\\n\",",
            "\t\t\tcount[i], IRQ_TIMINGS_SIZE);",
            "",
            "\t\tret = irq_timings_test_irqts(irqts, count[i]);",
            "\t\tif (ret)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "irq_timings_irqs_selftest, irq_timings_test_irqts, irq_timings_irqts_selftest",
          "description": "包含完整的中断计时模块自检框架，验证环形缓冲区操作和for_each_irqts宏的正确性",
          "similarity": 0.5796962976455688
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/timings.c",
          "start_line": 435,
          "end_line": 563,
          "content": [
            "static __always_inline int irq_timings_interval_index(u64 interval)",
            "{",
            "\t/*",
            "\t * The PREDICTION_FACTOR increase the interval size for the",
            "\t * array of exponential average.",
            "\t */",
            "\tu64 interval_us = (interval >> 10) / PREDICTION_FACTOR;",
            "",
            "\treturn likely(interval_us) ? ilog2(interval_us) : 0;",
            "}",
            "static __always_inline void __irq_timings_store(int irq, struct irqt_stat *irqs,",
            "\t\t\t\t\t\tu64 interval)",
            "{",
            "\tint index;",
            "",
            "\t/*",
            "\t * Get the index in the ema table for this interrupt.",
            "\t */",
            "\tindex = irq_timings_interval_index(interval);",
            "",
            "\tif (index > PREDICTION_BUFFER_SIZE - 1) {",
            "\t\tirqs->count = 0;",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Store the index as an element of the pattern in another",
            "\t * circular array.",
            "\t */",
            "\tirqs->circ_timings[irqs->count & IRQ_TIMINGS_MASK] = index;",
            "",
            "\tirqs->ema_time[index] = irq_timings_ema_new(interval,",
            "\t\t\t\t\t\t    irqs->ema_time[index]);",
            "",
            "\tirqs->count++;",
            "}",
            "static inline void irq_timings_store(int irq, struct irqt_stat *irqs, u64 ts)",
            "{",
            "\tu64 old_ts = irqs->last_ts;",
            "\tu64 interval;",
            "",
            "\t/*",
            "\t * The timestamps are absolute time values, we need to compute",
            "\t * the timing interval between two interrupts.",
            "\t */",
            "\tirqs->last_ts = ts;",
            "",
            "\t/*",
            "\t * The interval type is u64 in order to deal with the same",
            "\t * type in our computation, that prevent mindfuck issues with",
            "\t * overflow, sign and division.",
            "\t */",
            "\tinterval = ts - old_ts;",
            "",
            "\t/*",
            "\t * The interrupt triggered more than one second apart, that",
            "\t * ends the sequence as predictable for our purpose. In this",
            "\t * case, assume we have the beginning of a sequence and the",
            "\t * timestamp is the first value. As it is impossible to",
            "\t * predict anything at this point, return.",
            "\t *",
            "\t * Note the first timestamp of the sequence will always fall",
            "\t * in this test because the old_ts is zero. That is what we",
            "\t * want as we need another timestamp to compute an interval.",
            "\t */",
            "\tif (interval >= NSEC_PER_SEC) {",
            "\t\tirqs->count = 0;",
            "\t\treturn;",
            "\t}",
            "",
            "\t__irq_timings_store(irq, irqs, interval);",
            "}",
            "u64 irq_timings_next_event(u64 now)",
            "{",
            "\tstruct irq_timings *irqts = this_cpu_ptr(&irq_timings);",
            "\tstruct irqt_stat *irqs;",
            "\tstruct irqt_stat __percpu *s;",
            "\tu64 ts, next_evt = U64_MAX;",
            "\tint i, irq = 0;",
            "",
            "\t/*",
            "\t * This function must be called with the local irq disabled in",
            "\t * order to prevent the timings circular buffer to be updated",
            "\t * while we are reading it.",
            "\t */",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tif (!irqts->count)",
            "\t\treturn next_evt;",
            "",
            "\t/*",
            "\t * Number of elements in the circular buffer: If it happens it",
            "\t * was flushed before, then the number of elements could be",
            "\t * smaller than IRQ_TIMINGS_SIZE, so the count is used,",
            "\t * otherwise the array size is used as we wrapped. The index",
            "\t * begins from zero when we did not wrap. That could be done",
            "\t * in a nicer way with the proper circular array structure",
            "\t * type but with the cost of extra computation in the",
            "\t * interrupt handler hot path. We choose efficiency.",
            "\t *",
            "\t * Inject measured irq/timestamp to the pattern prediction",
            "\t * model while decrementing the counter because we consume the",
            "\t * data from our circular buffer.",
            "\t */",
            "\tfor_each_irqts(i, irqts) {",
            "\t\tirq = irq_timing_decode(irqts->values[i], &ts);",
            "\t\ts = idr_find(&irqt_stats, irq);",
            "\t\tif (s)",
            "\t\t\tirq_timings_store(irq, this_cpu_ptr(s), ts);",
            "\t}",
            "",
            "\t/*",
            "\t * Look in the list of interrupts' statistics, the earliest",
            "\t * next event.",
            "\t */",
            "\tidr_for_each_entry(&irqt_stats, s, i) {",
            "",
            "\t\tirqs = this_cpu_ptr(s);",
            "",
            "\t\tts = __irq_timings_next_event(irqs, i, now);",
            "\t\tif (ts <= now)",
            "\t\t\treturn now;",
            "",
            "\t\tif (ts < next_evt)",
            "\t\t\tnext_evt = ts;",
            "\t}",
            "",
            "\treturn next_evt;",
            "}"
          ],
          "function_name": "irq_timings_interval_index, __irq_timings_store, irq_timings_store, irq_timings_next_event",
          "description": "提供时间间隔转索引的映射函数，实现环形缓冲区数据存储及主事件预测函数",
          "similarity": 0.5561109185218811
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq/timings.c",
          "start_line": 594,
          "end_line": 739,
          "content": [
            "void irq_timings_free(int irq)",
            "{",
            "\tstruct irqt_stat __percpu *s;",
            "",
            "\ts = idr_find(&irqt_stats, irq);",
            "\tif (s) {",
            "\t\tfree_percpu(s);",
            "\t\tidr_remove(&irqt_stats, irq);",
            "\t}",
            "}",
            "int irq_timings_alloc(int irq)",
            "{",
            "\tstruct irqt_stat __percpu *s;",
            "\tint id;",
            "",
            "\t/*",
            "\t * Some platforms can have the same private interrupt per cpu,",
            "\t * so this function may be called several times with the",
            "\t * same interrupt number. Just bail out in case the per cpu",
            "\t * stat structure is already allocated.",
            "\t */",
            "\ts = idr_find(&irqt_stats, irq);",
            "\tif (s)",
            "\t\treturn 0;",
            "",
            "\ts = alloc_percpu(*s);",
            "\tif (!s)",
            "\t\treturn -ENOMEM;",
            "",
            "\tidr_preload(GFP_KERNEL);",
            "\tid = idr_alloc(&irqt_stats, s, irq, irq + 1, GFP_NOWAIT);",
            "\tidr_preload_end();",
            "",
            "\tif (id < 0) {",
            "\t\tfree_percpu(s);",
            "\t\treturn id;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int __init irq_timings_test_next_index(struct timings_intervals *ti)",
            "{",
            "\tint _buffer[IRQ_TIMINGS_SIZE];",
            "\tint buffer[IRQ_TIMINGS_SIZE];",
            "\tint index, start, i, count, period_max;",
            "",
            "\tcount = ti->count - 1;",
            "",
            "\tperiod_max = count > (3 * PREDICTION_PERIOD_MAX) ?",
            "\t\tPREDICTION_PERIOD_MAX : count / 3;",
            "",
            "\t/*",
            "\t * Inject all values except the last one which will be used",
            "\t * to compare with the next index result.",
            "\t */",
            "\tpr_debug(\"index suite: \");",
            "",
            "\tfor (i = 0; i < count; i++) {",
            "\t\tindex = irq_timings_interval_index(ti->intervals[i]);",
            "\t\t_buffer[i & IRQ_TIMINGS_MASK] = index;",
            "\t\tpr_cont(\"%d \", index);",
            "\t}",
            "",
            "\tstart = count < IRQ_TIMINGS_SIZE ? 0 :",
            "\t\tcount & IRQ_TIMINGS_MASK;",
            "",
            "\tcount = min_t(int, count, IRQ_TIMINGS_SIZE);",
            "",
            "\tfor (i = 0; i < count; i++) {",
            "\t\tint index = (start + i) & IRQ_TIMINGS_MASK;",
            "\t\tbuffer[i] = _buffer[index];",
            "\t}",
            "",
            "\tindex = irq_timings_next_event_index(buffer, count, period_max);",
            "\ti = irq_timings_interval_index(ti->intervals[ti->count - 1]);",
            "",
            "\tif (index != i) {",
            "\t\tpr_err(\"Expected (%d) and computed (%d) next indexes differ\\n\",",
            "\t\t       i, index);",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int __init irq_timings_next_index_selftest(void)",
            "{",
            "\tint i, ret;",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(tis); i++) {",
            "",
            "\t\tpr_info(\"---> Injecting intervals number #%d (count=%zd)\\n\",",
            "\t\t\ti, tis[i].count);",
            "",
            "\t\tret = irq_timings_test_next_index(&tis[i]);",
            "\t\tif (ret)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "static int __init irq_timings_test_irqs(struct timings_intervals *ti)",
            "{",
            "\tstruct irqt_stat __percpu *s;",
            "\tstruct irqt_stat *irqs;",
            "\tint i, index, ret, irq = 0xACE5;",
            "",
            "\tret = irq_timings_alloc(irq);",
            "\tif (ret) {",
            "\t\tpr_err(\"Failed to allocate irq timings\\n\");",
            "\t\treturn ret;",
            "\t}",
            "",
            "\ts = idr_find(&irqt_stats, irq);",
            "\tif (!s) {",
            "\t\tret = -EIDRM;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tirqs = this_cpu_ptr(s);",
            "",
            "\tfor (i = 0; i < ti->count; i++) {",
            "",
            "\t\tindex = irq_timings_interval_index(ti->intervals[i]);",
            "\t\tpr_debug(\"%d: interval=%llu ema_index=%d\\n\",",
            "\t\t\t i, ti->intervals[i], index);",
            "",
            "\t\t__irq_timings_store(irq, irqs, ti->intervals[i]);",
            "\t\tif (irqs->circ_timings[i & IRQ_TIMINGS_MASK] != index) {",
            "\t\t\tret = -EBADSLT;",
            "\t\t\tpr_err(\"Failed to store in the circular buffer\\n\");",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\tif (irqs->count != ti->count) {",
            "\t\tret = -ERANGE;",
            "\t\tpr_err(\"Count differs\\n\");",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tret = 0;",
            "out:",
            "\tirq_timings_free(irq);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "irq_timings_free, irq_timings_alloc, irq_timings_test_next_index, irq_timings_next_index_selftest, irq_timings_test_irqs",
          "description": "实现中断统计结构的动态分配/释放机制，包含预测算法的自检测试函数",
          "similarity": 0.5536180734634399
        }
      ]
    },
    {
      "source_file": "kernel/sched/cputime.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:05:48\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\cputime.c`\n\n---\n\n# `sched/cputime.c` 技术文档\n\n## 1. 文件概述\n\n`sched/cputime.c` 是 Linux 内核调度子系统中负责 **CPU 时间统计与会计（accounting）** 的核心实现文件。其主要功能包括：\n\n- 对用户态、内核态、中断（硬中断/软中断）、虚拟机（guest）、steal、idle、iowait 等各类 CPU 时间进行精确统计；\n- 支持将 CPU 时间按进程、线程组（thread group）和 cgroup 层级进行聚合；\n- 在启用 `CONFIG_IRQ_TIME_ACCOUNTING` 时，提供基于高精度调度时钟（`sched_clock`）的中断时间细粒度追踪；\n- 为 `/proc/stat`、`/proc/[pid]/stat`、cgroup v1/v2 的 CPU 统计接口提供底层数据支持。\n\n该文件是内核 CPU 使用率监控、资源控制和性能分析的基础组件。\n\n---\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `struct irqtime`（仅当 `CONFIG_IRQ_TIME_ACCOUNTING` 启用）  \n  每 CPU 变量，用于记录当前 CPU 上硬中断和软中断的累计时间及同步状态。\n- `cpu_irqtime`  \n  `DEFINE_PER_CPU(struct irqtime, cpu_irqtime)`，每个 CPU 的中断时间统计结构。\n\n### 主要函数\n\n| 函数 | 功能说明 |\n|------|--------|\n| `account_user_time()` | 统计进程在用户态消耗的 CPU 时间 |\n| `account_guest_time()` | 统计进程作为虚拟机 guest 消耗的 CPU 时间 |\n| `account_system_time()` | 统计进程在内核态（含中断上下文）消耗的 CPU 时间 |\n| `account_system_index_time()` | 按指定类型（IRQ/SOFTIRQ/SYSTEM）统计系统态时间 |\n| `account_steal_time()` | 统计因虚拟化导致的“被偷走”的 CPU 时间 |\n| `account_idle_time()` | 统计 CPU 空闲时间（区分 iowait 与普通 idle） |\n| `irqtime_account_irq()` | 在中断进入/退出时更新中断时间统计 |\n| `thread_group_cputime()` | 聚合线程组内所有任务的累计 CPU 时间（未完整显示） |\n| `account_other_time()` | 综合统计 steal、IRQ、softirq 等“非任务执行”时间 |\n| `read_sum_exec_runtime()` | 安全读取任务的累计执行时间（32 位平台加锁） |\n\n### 控制接口\n\n- `enable_sched_clock_irqtime()` / `disable_sched_clock_irqtime()`  \n  动态启停基于 `sched_clock` 的中断时间会计功能。\n\n---\n\n## 3. 关键实现\n\n### 中断时间会计（`CONFIG_IRQ_TIME_ACCOUNTING`）\n\n- 使用每 CPU 的 `cpu_irqtime` 结构，通过 `u64_stats` 同步机制保证读写一致性；\n- 在 `{soft,}irq_enter/exit` 路径中调用 `irqtime_account_irq()`，利用 `sched_clock_cpu()` 计算中断持续时间；\n- **无锁设计**：写操作仅在本 CPU 中断关闭时进行，读操作（如 `update_rq_clock()`）可能读到稍旧值，但避免了中断路径加锁开销；\n- 特殊处理 `ksoftirqd`：其软中断时间仍计入该内核线程自身，避免调度器误判。\n\n### 时间分类与统计\n\n- **用户时间**：根据 `task_nice(p) > 0` 区分 `CPUTIME_USER` 与 `CPUTIME_NICE`；\n- **Guest 时间**：同时计入用户时间与 `CPUTIME_GUEST`/`GUEST_NICE`；\n- **系统时间**：根据中断上下文动态判断为 `IRQ`、`SOFTIRQ` 或 `SYSTEM`；\n- **Idle 时间**：通过 `rq->nr_iowait` 判断是否为 I/O 等待状态；\n- **Steal 时间**：通过 `paravirt_steal_clock()` 获取虚拟化层报告的被抢占时间。\n\n### cgroup 集成\n\n- 所有时间统计均通过 `task_group_account_field()` 同时更新：\n  - 全局 `kernel_cpustat`（用于 `/proc/stat`）；\n  - 当前任务所属 cgroup 的 CPU 统计（通过 `cgroup_account_cputime_field()`）。\n\n### 32 位平台兼容性\n\n- 在 32 位系统上，`sum_exec_runtime` 的读取需加 `rq` 锁以避免 64 位值撕裂（tearing）；\n- 64 位平台可直接原子读取。\n\n---\n\n## 4. 依赖关系\n\n- **架构依赖**：\n  - `CONFIG_VIRT_CPU_ACCOUNTING_NATIVE`：包含 `<asm/cputime.h>`，用于架构特定的 CPU 时间处理；\n- **配置选项**：\n  - `CONFIG_IRQ_TIME_ACCOUNTING`：启用高精度中断时间追踪；\n  - `CONFIG_PARAVIRT`：支持虚拟化 steal time 统计；\n  - `CONFIG_SCHED_CORE`：支持 core scheduling 的 forceidle 时间统计；\n  - `CONFIG_CGROUPS`：通过 `cgroup_account_cputime_field()` 集成 cgroup CPU 统计；\n- **内核模块**：\n  - 调度器核心（`kernel/sched/core.c`）：调用时间会计函数；\n  - 进程管理（`kernel/fork.c`, `kernel/exit.c`）：使用 `thread_group_cputime()`；\n  - 虚拟化子系统（`arch/*/kernel/paravirt.c`）：提供 `paravirt_steal_clock()`；\n  - procfs（`fs/proc/stat.c`）：读取 `kcpustat_this_cpu` 生成 `/proc/stat`。\n\n---\n\n## 5. 使用场景\n\n1. **系统监控工具**  \n   `top`、`htop`、`vmstat`、`iostat` 等通过 `/proc/stat` 获取全局 CPU 使用分布（user/nice/system/irq/steal 等）。\n\n2. **进程资源统计**  \n   `/proc/[pid]/stat` 中的 `utime`/`stime` 字段由 `account_user_time()` 和 `account_system_time()` 更新。\n\n3. **cgroup 资源控制**  \n   cgroup v1 的 `cpuacct` 子系统和 cgroup v2 的 `cpu.stat` 依赖 `task_group_account_field()` 聚合计费数据。\n\n4. **虚拟化性能分析**  \n   在 KVM/Xen 等虚拟机中，`steal time` 反映宿主机对 vCPU 的调度延迟，用于诊断性能瓶颈。\n\n5. **调度器决策**  \n   CFS 调度器使用 `sum_exec_runtime` 进行公平调度；`ksoftirqd` 的特殊处理确保软中断负载被正确感知。\n\n6. **功耗与能效分析**  \n   精确的 idle/iowait/irq 时间统计为 CPU 频率调节（如 `intel_pstate`）和电源管理提供依据。",
      "similarity": 0.6443284153938293,
      "chunks": [
        {
          "chunk_id": 7,
          "file_path": "kernel/sched/cputime.c",
          "start_line": 896,
          "end_line": 1014,
          "content": [
            "static int vtime_state_fetch(struct vtime *vtime, int cpu)",
            "{",
            "\tint state = READ_ONCE(vtime->state);",
            "",
            "\t/*",
            "\t * We raced against a context switch, fetch the",
            "\t * kcpustat task again.",
            "\t */",
            "\tif (vtime->cpu != cpu && vtime->cpu != -1)",
            "\t\treturn -EAGAIN;",
            "",
            "\t/*",
            "\t * Two possible things here:",
            "\t * 1) We are seeing the scheduling out task (prev) or any past one.",
            "\t * 2) We are seeing the scheduling in task (next) but it hasn't",
            "\t *    passed though vtime_task_switch() yet so the pending",
            "\t *    cputime of the prev task may not be flushed yet.",
            "\t *",
            "\t * Case 1) is ok but 2) is not. So wait for a safe VTIME state.",
            "\t */",
            "\tif (state == VTIME_INACTIVE)",
            "\t\treturn -EAGAIN;",
            "",
            "\treturn state;",
            "}",
            "static u64 kcpustat_user_vtime(struct vtime *vtime)",
            "{",
            "\tif (vtime->state == VTIME_USER)",
            "\t\treturn vtime->utime + vtime_delta(vtime);",
            "\telse if (vtime->state == VTIME_GUEST)",
            "\t\treturn vtime->gtime + vtime_delta(vtime);",
            "\treturn 0;",
            "}",
            "static int kcpustat_field_vtime(u64 *cpustat,",
            "\t\t\t\tstruct task_struct *tsk,",
            "\t\t\t\tenum cpu_usage_stat usage,",
            "\t\t\t\tint cpu, u64 *val)",
            "{",
            "\tstruct vtime *vtime = &tsk->vtime;",
            "\tunsigned int seq;",
            "",
            "\tdo {",
            "\t\tint state;",
            "",
            "\t\tseq = read_seqcount_begin(&vtime->seqcount);",
            "",
            "\t\tstate = vtime_state_fetch(vtime, cpu);",
            "\t\tif (state < 0)",
            "\t\t\treturn state;",
            "",
            "\t\t*val = cpustat[usage];",
            "",
            "\t\t/*",
            "\t\t * Nice VS unnice cputime accounting may be inaccurate if",
            "\t\t * the nice value has changed since the last vtime update.",
            "\t\t * But proper fix would involve interrupting target on nice",
            "\t\t * updates which is a no go on nohz_full (although the scheduler",
            "\t\t * may still interrupt the target if rescheduling is needed...)",
            "\t\t */",
            "\t\tswitch (usage) {",
            "\t\tcase CPUTIME_SYSTEM:",
            "\t\t\tif (state == VTIME_SYS)",
            "\t\t\t\t*val += vtime->stime + vtime_delta(vtime);",
            "\t\t\tbreak;",
            "\t\tcase CPUTIME_USER:",
            "\t\t\tif (task_nice(tsk) <= 0)",
            "\t\t\t\t*val += kcpustat_user_vtime(vtime);",
            "\t\t\tbreak;",
            "\t\tcase CPUTIME_NICE:",
            "\t\t\tif (task_nice(tsk) > 0)",
            "\t\t\t\t*val += kcpustat_user_vtime(vtime);",
            "\t\t\tbreak;",
            "\t\tcase CPUTIME_GUEST:",
            "\t\t\tif (state == VTIME_GUEST && task_nice(tsk) <= 0)",
            "\t\t\t\t*val += vtime->gtime + vtime_delta(vtime);",
            "\t\t\tbreak;",
            "\t\tcase CPUTIME_GUEST_NICE:",
            "\t\t\tif (state == VTIME_GUEST && task_nice(tsk) > 0)",
            "\t\t\t\t*val += vtime->gtime + vtime_delta(vtime);",
            "\t\t\tbreak;",
            "\t\tdefault:",
            "\t\t\tbreak;",
            "\t\t}",
            "\t} while (read_seqcount_retry(&vtime->seqcount, seq));",
            "",
            "\treturn 0;",
            "}",
            "u64 kcpustat_field(struct kernel_cpustat *kcpustat,",
            "\t\t   enum cpu_usage_stat usage, int cpu)",
            "{",
            "\tu64 *cpustat = kcpustat->cpustat;",
            "\tu64 val = cpustat[usage];",
            "\tstruct rq *rq;",
            "\tint err;",
            "",
            "\tif (!vtime_accounting_enabled_cpu(cpu))",
            "\t\treturn val;",
            "",
            "\trq = cpu_rq(cpu);",
            "",
            "\tfor (;;) {",
            "\t\tstruct task_struct *curr;",
            "",
            "\t\trcu_read_lock();",
            "\t\tcurr = rcu_dereference(rq->curr);",
            "\t\tif (WARN_ON_ONCE(!curr)) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\treturn cpustat[usage];",
            "\t\t}",
            "",
            "\t\terr = kcpustat_field_vtime(cpustat, curr, usage, cpu, &val);",
            "\t\trcu_read_unlock();",
            "",
            "\t\tif (!err)",
            "\t\t\treturn val;",
            "",
            "\t\tcpu_relax();",
            "\t}",
            "}"
          ],
          "function_name": "vtime_state_fetch, kcpustat_user_vtime, kcpustat_field_vtime, kcpustat_field",
          "description": "实现CPU统计字段的虚拟时间修正逻辑，根据任务状态区分系统/用户/Guest时间，并处理优先级调整导致的统计偏差",
          "similarity": 0.6224492192268372
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/sched/cputime.c",
          "start_line": 772,
          "end_line": 883,
          "content": [
            "void vtime_guest_exit(struct task_struct *tsk)",
            "{",
            "\tstruct vtime *vtime = &tsk->vtime;",
            "",
            "\twrite_seqcount_begin(&vtime->seqcount);",
            "\tvtime_account_guest(tsk, vtime);",
            "\ttsk->flags &= ~PF_VCPU;",
            "\tvtime->state = VTIME_SYS;",
            "\twrite_seqcount_end(&vtime->seqcount);",
            "}",
            "void vtime_account_idle(struct task_struct *tsk)",
            "{",
            "\taccount_idle_time(get_vtime_delta(&tsk->vtime));",
            "}",
            "void vtime_task_switch_generic(struct task_struct *prev)",
            "{",
            "\tstruct vtime *vtime = &prev->vtime;",
            "",
            "\twrite_seqcount_begin(&vtime->seqcount);",
            "\tif (vtime->state == VTIME_IDLE)",
            "\t\tvtime_account_idle(prev);",
            "\telse",
            "\t\t__vtime_account_kernel(prev, vtime);",
            "\tvtime->state = VTIME_INACTIVE;",
            "\tvtime->cpu = -1;",
            "\twrite_seqcount_end(&vtime->seqcount);",
            "",
            "\tvtime = &current->vtime;",
            "",
            "\twrite_seqcount_begin(&vtime->seqcount);",
            "\tif (is_idle_task(current))",
            "\t\tvtime->state = VTIME_IDLE;",
            "\telse if (current->flags & PF_VCPU)",
            "\t\tvtime->state = VTIME_GUEST;",
            "\telse",
            "\t\tvtime->state = VTIME_SYS;",
            "\tvtime->starttime = sched_clock();",
            "\tvtime->cpu = smp_processor_id();",
            "\twrite_seqcount_end(&vtime->seqcount);",
            "}",
            "void vtime_init_idle(struct task_struct *t, int cpu)",
            "{",
            "\tstruct vtime *vtime = &t->vtime;",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\twrite_seqcount_begin(&vtime->seqcount);",
            "\tvtime->state = VTIME_IDLE;",
            "\tvtime->starttime = sched_clock();",
            "\tvtime->cpu = cpu;",
            "\twrite_seqcount_end(&vtime->seqcount);",
            "\tlocal_irq_restore(flags);",
            "}",
            "u64 task_gtime(struct task_struct *t)",
            "{",
            "\tstruct vtime *vtime = &t->vtime;",
            "\tunsigned int seq;",
            "\tu64 gtime;",
            "",
            "\tif (!vtime_accounting_enabled())",
            "\t\treturn t->gtime;",
            "",
            "\tdo {",
            "\t\tseq = read_seqcount_begin(&vtime->seqcount);",
            "",
            "\t\tgtime = t->gtime;",
            "\t\tif (vtime->state == VTIME_GUEST)",
            "\t\t\tgtime += vtime->gtime + vtime_delta(vtime);",
            "",
            "\t} while (read_seqcount_retry(&vtime->seqcount, seq));",
            "",
            "\treturn gtime;",
            "}",
            "bool task_cputime(struct task_struct *t, u64 *utime, u64 *stime)",
            "{",
            "\tstruct vtime *vtime = &t->vtime;",
            "\tunsigned int seq;",
            "\tu64 delta;",
            "\tint ret;",
            "",
            "\tif (!vtime_accounting_enabled()) {",
            "\t\t*utime = t->utime;",
            "\t\t*stime = t->stime;",
            "\t\treturn false;",
            "\t}",
            "",
            "\tdo {",
            "\t\tret = false;",
            "\t\tseq = read_seqcount_begin(&vtime->seqcount);",
            "",
            "\t\t*utime = t->utime;",
            "\t\t*stime = t->stime;",
            "",
            "\t\t/* Task is sleeping or idle, nothing to add */",
            "\t\tif (vtime->state < VTIME_SYS)",
            "\t\t\tcontinue;",
            "",
            "\t\tret = true;",
            "\t\tdelta = vtime_delta(vtime);",
            "",
            "\t\t/*",
            "\t\t * Task runs either in user (including guest) or kernel space,",
            "\t\t * add pending nohz time to the right place.",
            "\t\t */",
            "\t\tif (vtime->state == VTIME_SYS)",
            "\t\t\t*stime += vtime->stime + delta;",
            "\t\telse",
            "\t\t\t*utime += vtime->utime + delta;",
            "\t} while (read_seqcount_retry(&vtime->seqcount, seq));",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "vtime_guest_exit, vtime_account_idle, vtime_task_switch_generic, vtime_init_idle, task_gtime, task_cputime",
          "description": "管理任务切换时的空闲时间会计，初始化空闲任务时间戳，提供任务实际运行时间查询及CPU时间统计接口",
          "similarity": 0.6197007894515991
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/cputime.c",
          "start_line": 1,
          "end_line": 26,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Simple CPU accounting cgroup controller",
            " */",
            "",
            "#ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE",
            " #include <asm/cputime.h>",
            "#endif",
            "",
            "#ifdef CONFIG_IRQ_TIME_ACCOUNTING",
            "",
            "/*",
            " * There are no locks covering percpu hardirq/softirq time.",
            " * They are only modified in vtime_account, on corresponding CPU",
            " * with interrupts disabled. So, writes are safe.",
            " * They are read and saved off onto struct rq in update_rq_clock().",
            " * This may result in other CPU reading this CPU's irq time and can",
            " * race with irq/vtime_account on this CPU. We would either get old",
            " * or new value with a side effect of accounting a slice of irq time to wrong",
            " * task when irq is in progress while we read rq->clock. That is a worthy",
            " * compromise in place of having locks on each irq in account_system_time.",
            " */",
            "DEFINE_PER_CPU(struct irqtime, cpu_irqtime);",
            "",
            "static int sched_clock_irqtime;",
            ""
          ],
          "function_name": null,
          "description": "定义并初始化与中断时间统计相关的PER_CPU结构体cpu_irqtime及全局标志位sched_clock_irqtime，用于追踪硬中断和软中断时间，注释表明其在禁用中断时安全地修改，并通过rq->clock保存读取值以支持后续统计",
          "similarity": 0.6195666193962097
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/sched/cputime.c",
          "start_line": 519,
          "end_line": 623,
          "content": [
            "void account_idle_ticks(unsigned long ticks)",
            "{",
            "\tu64 cputime, steal;",
            "",
            "\tif (sched_clock_irqtime) {",
            "\t\tirqtime_account_idle_ticks(ticks);",
            "\t\treturn;",
            "\t}",
            "",
            "\tcputime = ticks * TICK_NSEC;",
            "\tsteal = steal_account_process_time(ULONG_MAX);",
            "",
            "\tif (steal >= cputime)",
            "\t\treturn;",
            "",
            "\tcputime -= steal;",
            "\taccount_idle_time(cputime);",
            "}",
            "void cputime_adjust(struct task_cputime *curr, struct prev_cputime *prev,",
            "\t\t    u64 *ut, u64 *st)",
            "{",
            "\tu64 rtime, stime, utime;",
            "\tunsigned long flags;",
            "",
            "\t/* Serialize concurrent callers such that we can honour our guarantees */",
            "\traw_spin_lock_irqsave(&prev->lock, flags);",
            "\trtime = curr->sum_exec_runtime;",
            "",
            "\t/*",
            "\t * This is possible under two circumstances:",
            "\t *  - rtime isn't monotonic after all (a bug);",
            "\t *  - we got reordered by the lock.",
            "\t *",
            "\t * In both cases this acts as a filter such that the rest of the code",
            "\t * can assume it is monotonic regardless of anything else.",
            "\t */",
            "\tif (prev->stime + prev->utime >= rtime)",
            "\t\tgoto out;",
            "",
            "\tstime = curr->stime;",
            "\tutime = curr->utime;",
            "",
            "\t/*",
            "\t * If either stime or utime are 0, assume all runtime is userspace.",
            "\t * Once a task gets some ticks, the monotonicity code at 'update:'",
            "\t * will ensure things converge to the observed ratio.",
            "\t */",
            "\tif (stime == 0) {",
            "\t\tutime = rtime;",
            "\t\tgoto update;",
            "\t}",
            "",
            "\tif (utime == 0) {",
            "\t\tstime = rtime;",
            "\t\tgoto update;",
            "\t}",
            "",
            "\tstime = mul_u64_u64_div_u64(stime, rtime, stime + utime);",
            "\t/*",
            "\t * Because mul_u64_u64_div_u64() can approximate on some",
            "\t * achitectures; enforce the constraint that: a*b/(b+c) <= a.",
            "\t */",
            "\tif (unlikely(stime > rtime))",
            "\t\tstime = rtime;",
            "",
            "update:",
            "\t/*",
            "\t * Make sure stime doesn't go backwards; this preserves monotonicity",
            "\t * for utime because rtime is monotonic.",
            "\t *",
            "\t *  utime_i+1 = rtime_i+1 - stime_i",
            "\t *            = rtime_i+1 - (rtime_i - utime_i)",
            "\t *            = (rtime_i+1 - rtime_i) + utime_i",
            "\t *            >= utime_i",
            "\t */",
            "\tif (stime < prev->stime)",
            "\t\tstime = prev->stime;",
            "\tutime = rtime - stime;",
            "",
            "\t/*",
            "\t * Make sure utime doesn't go backwards; this still preserves",
            "\t * monotonicity for stime, analogous argument to above.",
            "\t */",
            "\tif (utime < prev->utime) {",
            "\t\tutime = prev->utime;",
            "\t\tstime = rtime - utime;",
            "\t}",
            "",
            "\tprev->stime = stime;",
            "\tprev->utime = utime;",
            "out:",
            "\t*ut = prev->utime;",
            "\t*st = prev->stime;",
            "\traw_spin_unlock_irqrestore(&prev->lock, flags);",
            "}",
            "void task_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st)",
            "{",
            "\tstruct task_cputime cputime = {",
            "\t\t.sum_exec_runtime = p->se.sum_exec_runtime,",
            "\t};",
            "",
            "\tif (task_cputime(p, &cputime.utime, &cputime.stime))",
            "\t\tcputime.sum_exec_runtime = task_sched_runtime(p);",
            "\tcputime_adjust(&cputime, &p->prev_cputime, ut, st);",
            "}"
          ],
          "function_name": "account_idle_ticks, cputime_adjust, task_cputime_adjusted",
          "description": "account_idle_ticks统计空闲时间并扣除偷取时间，cputime_adjust通过锁保护保证统计值单调性，task_cputime_adjusted将任务CPU时间调整为基于运行时间的比例分布",
          "similarity": 0.6179898381233215
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/sched/cputime.c",
          "start_line": 648,
          "end_line": 758,
          "content": [
            "void thread_group_cputime_adjusted(struct task_struct *p, u64 *ut, u64 *st)",
            "{",
            "\tstruct task_cputime cputime;",
            "",
            "\tthread_group_cputime(p, &cputime);",
            "\tcputime_adjust(&cputime, &p->signal->prev_cputime, ut, st);",
            "}",
            "static u64 vtime_delta(struct vtime *vtime)",
            "{",
            "\tunsigned long long clock;",
            "",
            "\tclock = sched_clock();",
            "\tif (clock < vtime->starttime)",
            "\t\treturn 0;",
            "",
            "\treturn clock - vtime->starttime;",
            "}",
            "static u64 get_vtime_delta(struct vtime *vtime)",
            "{",
            "\tu64 delta = vtime_delta(vtime);",
            "\tu64 other;",
            "",
            "\t/*",
            "\t * Unlike tick based timing, vtime based timing never has lost",
            "\t * ticks, and no need for steal time accounting to make up for",
            "\t * lost ticks. Vtime accounts a rounded version of actual",
            "\t * elapsed time. Limit account_other_time to prevent rounding",
            "\t * errors from causing elapsed vtime to go negative.",
            "\t */",
            "\tother = account_other_time(delta);",
            "\tWARN_ON_ONCE(vtime->state == VTIME_INACTIVE);",
            "\tvtime->starttime += delta;",
            "",
            "\treturn delta - other;",
            "}",
            "static void vtime_account_system(struct task_struct *tsk,",
            "\t\t\t\t struct vtime *vtime)",
            "{",
            "\tvtime->stime += get_vtime_delta(vtime);",
            "\tif (vtime->stime >= TICK_NSEC) {",
            "\t\taccount_system_time(tsk, irq_count(), vtime->stime);",
            "\t\tvtime->stime = 0;",
            "\t}",
            "}",
            "static void vtime_account_guest(struct task_struct *tsk,",
            "\t\t\t\tstruct vtime *vtime)",
            "{",
            "\tvtime->gtime += get_vtime_delta(vtime);",
            "\tif (vtime->gtime >= TICK_NSEC) {",
            "\t\taccount_guest_time(tsk, vtime->gtime);",
            "\t\tvtime->gtime = 0;",
            "\t}",
            "}",
            "static void __vtime_account_kernel(struct task_struct *tsk,",
            "\t\t\t\t   struct vtime *vtime)",
            "{",
            "\t/* We might have scheduled out from guest path */",
            "\tif (vtime->state == VTIME_GUEST)",
            "\t\tvtime_account_guest(tsk, vtime);",
            "\telse",
            "\t\tvtime_account_system(tsk, vtime);",
            "}",
            "void vtime_account_kernel(struct task_struct *tsk)",
            "{",
            "\tstruct vtime *vtime = &tsk->vtime;",
            "",
            "\tif (!vtime_delta(vtime))",
            "\t\treturn;",
            "",
            "\twrite_seqcount_begin(&vtime->seqcount);",
            "\t__vtime_account_kernel(tsk, vtime);",
            "\twrite_seqcount_end(&vtime->seqcount);",
            "}",
            "void vtime_user_enter(struct task_struct *tsk)",
            "{",
            "\tstruct vtime *vtime = &tsk->vtime;",
            "",
            "\twrite_seqcount_begin(&vtime->seqcount);",
            "\tvtime_account_system(tsk, vtime);",
            "\tvtime->state = VTIME_USER;",
            "\twrite_seqcount_end(&vtime->seqcount);",
            "}",
            "void vtime_user_exit(struct task_struct *tsk)",
            "{",
            "\tstruct vtime *vtime = &tsk->vtime;",
            "",
            "\twrite_seqcount_begin(&vtime->seqcount);",
            "\tvtime->utime += get_vtime_delta(vtime);",
            "\tif (vtime->utime >= TICK_NSEC) {",
            "\t\taccount_user_time(tsk, vtime->utime);",
            "\t\tvtime->utime = 0;",
            "\t}",
            "\tvtime->state = VTIME_SYS;",
            "\twrite_seqcount_end(&vtime->seqcount);",
            "}",
            "void vtime_guest_enter(struct task_struct *tsk)",
            "{",
            "\tstruct vtime *vtime = &tsk->vtime;",
            "\t/*",
            "\t * The flags must be updated under the lock with",
            "\t * the vtime_starttime flush and update.",
            "\t * That enforces a right ordering and update sequence",
            "\t * synchronization against the reader (task_gtime())",
            "\t * that can thus safely catch up with a tickless delta.",
            "\t */",
            "\twrite_seqcount_begin(&vtime->seqcount);",
            "\tvtime_account_system(tsk, vtime);",
            "\ttsk->flags |= PF_VCPU;",
            "\tvtime->state = VTIME_GUEST;",
            "\twrite_seqcount_end(&vtime->seqcount);",
            "}"
          ],
          "function_name": "thread_group_cputime_adjusted, vtime_delta, get_vtime_delta, vtime_account_system, vtime_account_guest, __vtime_account_kernel, vtime_account_kernel, vtime_user_enter, vtime_user_exit, vtime_guest_enter",
          "description": "处理线程组累计CPU时间调整，计算虚拟时间差值并更新系统/用户/客体时间，通过状态机管理任务时间统计",
          "similarity": 0.613732099533081
        }
      ]
    }
  ]
}