{
  "query": "共享内存函数shm_open和mmap的关联",
  "timestamp": "2025-12-26 01:11:40",
  "retrieved_files": [
    {
      "source_file": "mm/mmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:51:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mmap.c`\n\n---\n\n# mmap.c 技术文档\n\n## 1. 文件概述\n\n`mm/mmap.c` 是 Linux 内核内存管理子系统的核心源文件之一，主要负责虚拟内存区域（VMA, Virtual Memory Area）的创建、修改、删除以及与用户空间 `mmap()` 和 `brk()` 系统调用相关的逻辑实现。该文件实现了进程地址空间的动态扩展（如堆的 `brk` 调整）、文件映射、匿名映射、VMA 结构的生命周期管理、页表保护属性更新等关键功能，并为内核其他子系统（如安全模块、性能分析、内存压缩等）提供底层支持。\n\n## 2. 核心功能\n\n### 主要函数\n- `vma_set_page_prot()`：根据 VMA 的标志位（`vm_flags`）更新其页表保护属性（`vm_page_prot`），并处理写时通知（writenotify）逻辑。\n- `unlink_file_vma()`：从文件的地址空间映射树（`i_mmap`）中移除一个基于文件的 VMA，用于在释放前隐藏 VMA。\n- `unlink_file_vma_batch_*()` 系列函数：批量处理多个 VMA 从同一文件映射树中的移除操作，提升性能。\n- `remove_vma()`：关闭并释放一个 VMA 结构，包括调用 `vma_close()`、释放关联文件引用和内存策略。\n- `check_brk_limits()`：检查 `brk` 扩展请求是否满足地址空间分配和内存锁定限制。\n- `SYSCALL_DEFINE1(brk, ...)`：实现 `brk()` 系统调用，用于调整进程数据段（堆）的结束地址。\n- `do_brk_flags()`（声明）：实际执行 `brk` 扩展逻辑的内部函数（定义在其他位置）。\n\n### 关键数据结构\n- `struct vm_area_struct`（VMA）：表示进程地址空间中的一段连续虚拟内存区域，包含起始/结束地址、访问权限、映射文件、操作函数指针等。\n- `struct unlink_vma_file_batch`：用于批量处理文件 VMA 解链操作的临时结构。\n- `struct vma_iterator`：用于高效遍历 VMA 树的迭代器（基于 Maple Tree）。\n\n### 全局变量\n- `mmap_rnd_bits` / `mmap_rnd_compat_bits`：控制 ASLR（地址空间布局随机化）中 mmap 基址随机化位数的可调参数。\n- `ignore_rlimit_data`：内核启动参数，用于忽略 `RLIMIT_DATA` 资源限制（调试用途）。\n\n## 3. 关键实现\n\n### VMA 页表保护属性更新\n`vma_set_page_prot()` 函数通过 `vm_pgprot_modify()` 将 VMA 的标志位（如 `VM_READ`、`VM_WRITE`、`VM_EXEC`、`VM_SHARED`）转换为底层架构相关的页表项保护位（`pgprot_t`）。特别地，当 VMA 需要写时通知（例如用于 COW 或跟踪）时，会临时清除 `VM_SHARED` 标志以生成非共享的写保护页表项，确保写操作能触发缺页异常。\n\n### 文件 VMA 批量解链优化\n为避免频繁加锁/解锁文件地址空间的 `i_mmap_rwsem`，内核引入了批量解链机制。`unlink_file_vma_batch_add()` 将待处理的 VMA 缓存到批次结构中，仅当遇到不同文件或批次满时才批量处理，显著减少锁竞争开销。\n\n### `brk()` 系统调用实现\n`brk()` 系统调用处理进程堆的扩展或收缩：\n- **收缩**：直接调用 `do_vma_munmap()` 释放多余内存区域。\n- **扩展**：\n  1. 检查是否超出 `RLIMIT_DATA` 限制；\n  2. 验证新堆顶与栈之间保留足够的安全间隙（`stack_guard_gap`）；\n  3. 调用 `do_brk_flags()` 创建新的匿名 VMA；\n  4. 更新 `mm->brk` 指针。\n- 支持 `CONFIG_COMPAT_BRK` 选项以兼容旧版 ABI 的堆起始地址行为。\n\n### 地址空间随机化（ASLR）\n通过 `mmap_rnd_bits` 等全局变量，内核允许动态调整 mmap 区域基址的随机化熵值，增强系统安全性。这些值受架构配置（`CONFIG_ARCH_MMAP_RND_BITS*`）约束，并可通过 `/proc/sys/kernel/` 接口运行时调整。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/vmalloc.h>`、`\"internal.h\"` 等提供的 VMA 操作、页表管理、内存分配接口。\n- **文件系统**：通过 `struct file` 和 `address_space` 与 VFS 层交互，处理文件映射（`shmem_fs.h` 用于 tmpfs/shm）。\n- **安全子系统**：集成 LSM（`<linux/security.h>`）钩子，支持安全策略检查。\n- **硬件架构**：依赖 `<asm/mmu_context.h>`、`<asm/tlb.h>` 等架构相关头文件处理 TLB 刷新和页表格式。\n- **其他子系统**：\n  - 用户态缺页处理（`userfaultfd_k.h`）\n  - 内存压缩（`ksm.h`）\n  - 大页支持（`hugetlb.h`）\n  - 性能事件（`perf_event.h`）\n  - OOM Killer（`oom.h`）\n\n## 5. 使用场景\n\n- **用户程序调用 `mmap()`/`munmap()`**：创建/销毁内存映射（文件映射、匿名映射、共享内存等）。\n- **动态内存分配**：`malloc()` 等库函数通过 `brk()` 或 `mmap()` 向内核申请堆内存。\n- **进程加载**：ELF 加载器使用 `mmap()` 映射可执行文件段和共享库。\n- **IPC 通信**：POSIX 共享内存（`shm_open` + `mmap`）和 System V 共享内存依赖此模块。\n- **内核子系统协作**：\n  - KSM（Kernel Samepage Merging）扫描 VMA 进行内存去重；\n  - userfaultfd 监控 VMA 的缺页事件；\n  - perf 工具通过 VMA 信息关联性能采样到代码位置；\n  - 安全模块（如 SELinux）在映射时实施访问控制。",
      "similarity": 0.6149886846542358,
      "chunks": [
        {
          "chunk_id": 19,
          "file_path": "mm/mmap.c",
          "start_line": 4054,
          "end_line": 4060,
          "content": [
            "static int __meminit init_reserve_notifier(void)",
            "{",
            "\tif (hotplug_memory_notifier(reserve_mem_notifier, DEFAULT_CALLBACK_PRI))",
            "\t\tpr_err(\"Failed registering memory add/remove notifier for admin reserve\\n\");",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "init_reserve_notifier",
          "description": "该代码段定义于`mm/mmap.c`，核心功能是注册内存预留相关通知回调。函数通过`hotplug_memory_notifier`注册`reserve_mem_notifier`回调至内存热插拔事件链表，用于跟踪内存预留状态变化。由于`reserve_mem_notifier`结构体定义缺失，上下文不完整。",
          "similarity": 0.6326766014099121
        },
        {
          "chunk_id": 18,
          "file_path": "mm/mmap.c",
          "start_line": 3884,
          "end_line": 3999,
          "content": [
            "static void vm_unlock_anon_vma(struct anon_vma *anon_vma)",
            "{",
            "\tif (test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {",
            "\t\t/*",
            "\t\t * The LSB of head.next can't change to 0 from under",
            "\t\t * us because we hold the mm_all_locks_mutex.",
            "\t\t *",
            "\t\t * We must however clear the bitflag before unlocking",
            "\t\t * the vma so the users using the anon_vma->rb_root will",
            "\t\t * never see our bitflag.",
            "\t\t *",
            "\t\t * No need of atomic instructions here, head.next",
            "\t\t * can't change from under us until we release the",
            "\t\t * anon_vma->root->rwsem.",
            "\t\t */",
            "\t\tif (!__test_and_clear_bit(0, (unsigned long *)",
            "\t\t\t\t\t  &anon_vma->root->rb_root.rb_root.rb_node))",
            "\t\t\tBUG();",
            "\t\tanon_vma_unlock_write(anon_vma);",
            "\t}",
            "}",
            "static void vm_unlock_mapping(struct address_space *mapping)",
            "{",
            "\tif (test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {",
            "\t\t/*",
            "\t\t * AS_MM_ALL_LOCKS can't change to 0 from under us",
            "\t\t * because we hold the mm_all_locks_mutex.",
            "\t\t */",
            "\t\ti_mmap_unlock_write(mapping);",
            "\t\tif (!test_and_clear_bit(AS_MM_ALL_LOCKS,",
            "\t\t\t\t\t&mapping->flags))",
            "\t\t\tBUG();",
            "\t}",
            "}",
            "void mm_drop_all_locks(struct mm_struct *mm)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "\tstruct anon_vma_chain *avc;",
            "\tMA_STATE(mas, &mm->mm_mt, 0, 0);",
            "",
            "\tmmap_assert_write_locked(mm);",
            "\tBUG_ON(!mutex_is_locked(&mm_all_locks_mutex));",
            "",
            "\tmas_for_each(&mas, vma, ULONG_MAX) {",
            "\t\tif (vma->anon_vma)",
            "\t\t\tlist_for_each_entry(avc, &vma->anon_vma_chain, same_vma)",
            "\t\t\t\tvm_unlock_anon_vma(avc->anon_vma);",
            "\t\tif (vma->vm_file && vma->vm_file->f_mapping)",
            "\t\t\tvm_unlock_mapping(vma->vm_file->f_mapping);",
            "\t}",
            "",
            "\tmutex_unlock(&mm_all_locks_mutex);",
            "}",
            "void __init mmap_init(void)",
            "{",
            "\tint ret;",
            "",
            "\tret = percpu_counter_init(&vm_committed_as, 0, GFP_KERNEL);",
            "\tVM_BUG_ON(ret);",
            "}",
            "static int init_user_reserve(void)",
            "{",
            "\tunsigned long free_kbytes;",
            "",
            "\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));",
            "",
            "\tsysctl_user_reserve_kbytes = min(free_kbytes / 32, 1UL << 17);",
            "\treturn 0;",
            "}",
            "static int init_admin_reserve(void)",
            "{",
            "\tunsigned long free_kbytes;",
            "",
            "\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));",
            "",
            "\tsysctl_admin_reserve_kbytes = min(free_kbytes / 32, 1UL << 13);",
            "\treturn 0;",
            "}",
            "static int reserve_mem_notifier(struct notifier_block *nb,",
            "\t\t\t     unsigned long action, void *data)",
            "{",
            "\tunsigned long tmp, free_kbytes;",
            "",
            "\tswitch (action) {",
            "\tcase MEM_ONLINE:",
            "\t\t/* Default max is 128MB. Leave alone if modified by operator. */",
            "\t\ttmp = sysctl_user_reserve_kbytes;",
            "\t\tif (0 < tmp && tmp < (1UL << 17))",
            "\t\t\tinit_user_reserve();",
            "",
            "\t\t/* Default max is 8MB.  Leave alone if modified by operator. */",
            "\t\ttmp = sysctl_admin_reserve_kbytes;",
            "\t\tif (0 < tmp && tmp < (1UL << 13))",
            "\t\t\tinit_admin_reserve();",
            "",
            "\t\tbreak;",
            "\tcase MEM_OFFLINE:",
            "\t\tfree_kbytes = K(global_zone_page_state(NR_FREE_PAGES));",
            "",
            "\t\tif (sysctl_user_reserve_kbytes > free_kbytes) {",
            "\t\t\tinit_user_reserve();",
            "\t\t\tpr_info(\"vm.user_reserve_kbytes reset to %lu\\n\",",
            "\t\t\t\tsysctl_user_reserve_kbytes);",
            "\t\t}",
            "",
            "\t\tif (sysctl_admin_reserve_kbytes > free_kbytes) {",
            "\t\t\tinit_admin_reserve();",
            "\t\t\tpr_info(\"vm.admin_reserve_kbytes reset to %lu\\n\",",
            "\t\t\t\tsysctl_admin_reserve_kbytes);",
            "\t\t}",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "\treturn NOTIFY_OK;",
            "}"
          ],
          "function_name": "vm_unlock_anon_vma, vm_unlock_mapping, mm_drop_all_locks, mmap_init, init_user_reserve, init_admin_reserve, reserve_mem_notifier",
          "description": "该代码段核心功能是管理内存锁定释放与保留内存配置。vm_unlock_* 系列函数通过清除标志位并释放写锁，实现对匿名VMA和文件映射的解锁；mm_drop_all_locks 遍历VMA链表执行全局锁释放；init_*_reserve 和 reserve_mem_notifier 动态配置用户/管理员内存保留策略，确保系统内存分配合理。",
          "similarity": 0.5554094314575195
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mmap.c",
          "start_line": 1193,
          "end_line": 1424,
          "content": [
            "static inline unsigned long round_hint_to_min(unsigned long hint)",
            "{",
            "\thint &= PAGE_MASK;",
            "\tif (((void *)hint != NULL) &&",
            "\t    (hint < mmap_min_addr))",
            "\t\treturn PAGE_ALIGN(mmap_min_addr);",
            "\treturn hint;",
            "}",
            "bool mlock_future_ok(struct mm_struct *mm, unsigned long flags,",
            "\t\t\tunsigned long bytes)",
            "{",
            "\tunsigned long locked_pages, limit_pages;",
            "",
            "\tif (!(flags & VM_LOCKED) || capable(CAP_IPC_LOCK))",
            "\t\treturn true;",
            "",
            "\tlocked_pages = bytes >> PAGE_SHIFT;",
            "\tlocked_pages += mm->locked_vm;",
            "",
            "\tlimit_pages = rlimit(RLIMIT_MEMLOCK);",
            "\tlimit_pages >>= PAGE_SHIFT;",
            "",
            "\treturn locked_pages <= limit_pages;",
            "}",
            "static inline u64 file_mmap_size_max(struct file *file, struct inode *inode)",
            "{",
            "\tif (S_ISREG(inode->i_mode))",
            "\t\treturn MAX_LFS_FILESIZE;",
            "",
            "\tif (S_ISBLK(inode->i_mode))",
            "\t\treturn MAX_LFS_FILESIZE;",
            "",
            "\tif (S_ISSOCK(inode->i_mode))",
            "\t\treturn MAX_LFS_FILESIZE;",
            "",
            "\t/* Special \"we do even unsigned file positions\" case */",
            "\tif (file->f_mode & FMODE_UNSIGNED_OFFSET)",
            "\t\treturn 0;",
            "",
            "\t/* Yes, random drivers might want more. But I'm tired of buggy drivers */",
            "\treturn ULONG_MAX;",
            "}",
            "static inline bool file_mmap_ok(struct file *file, struct inode *inode,",
            "\t\t\t\tunsigned long pgoff, unsigned long len)",
            "{",
            "\tu64 maxsize = file_mmap_size_max(file, inode);",
            "",
            "\tif (maxsize && len > maxsize)",
            "\t\treturn false;",
            "\tmaxsize -= len;",
            "\tif (pgoff > maxsize >> PAGE_SHIFT)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "unsigned long do_mmap(struct file *file, unsigned long addr,",
            "\t\t\tunsigned long len, unsigned long prot,",
            "\t\t\tunsigned long flags, vm_flags_t vm_flags,",
            "\t\t\tunsigned long pgoff, unsigned long *populate,",
            "\t\t\tstruct list_head *uf)",
            "{",
            "\tstruct mm_struct *mm = current->mm;",
            "\tint pkey = 0;",
            "",
            "\t*populate = 0;",
            "",
            "\tif (!len)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Does the application expect PROT_READ to imply PROT_EXEC?",
            "\t *",
            "\t * (the exception is when the underlying filesystem is noexec",
            "\t *  mounted, in which case we dont add PROT_EXEC.)",
            "\t */",
            "\tif ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))",
            "\t\tif (!(file && path_noexec(&file->f_path)))",
            "\t\t\tprot |= PROT_EXEC;",
            "",
            "\t/* force arch specific MAP_FIXED handling in get_unmapped_area */",
            "\tif (flags & MAP_FIXED_NOREPLACE)",
            "\t\tflags |= MAP_FIXED;",
            "",
            "\tif (!(flags & MAP_FIXED))",
            "\t\taddr = round_hint_to_min(addr);",
            "",
            "\t/* Careful about overflows.. */",
            "\tlen = PAGE_ALIGN(len);",
            "\tif (!len)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/* offset overflow? */",
            "\tif ((pgoff + (len >> PAGE_SHIFT)) < pgoff)",
            "\t\treturn -EOVERFLOW;",
            "",
            "\t/* Too many mappings? */",
            "\tif (mm->map_count > sysctl_max_map_count)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/* Obtain the address to map to. we verify (or select) it and ensure",
            "\t * that it represents a valid section of the address space.",
            "\t */",
            "\taddr = get_unmapped_area(file, addr, len, pgoff, flags);",
            "\tif (IS_ERR_VALUE(addr))",
            "\t\treturn addr;",
            "",
            "\tif (flags & MAP_FIXED_NOREPLACE) {",
            "\t\tif (find_vma_intersection(mm, addr, addr + len))",
            "\t\t\treturn -EEXIST;",
            "\t}",
            "",
            "\tif (prot == PROT_EXEC) {",
            "\t\tpkey = execute_only_pkey(mm);",
            "\t\tif (pkey < 0)",
            "\t\t\tpkey = 0;",
            "\t}",
            "",
            "\t/* Do simple checking here so the lower-level routines won't have",
            "\t * to. we assume access permissions have been handled by the open",
            "\t * of the memory object, so we don't do any here.",
            "\t */",
            "\tvm_flags |= calc_vm_prot_bits(prot, pkey) | calc_vm_flag_bits(file, flags) |",
            "\t\t\tmm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;",
            "",
            "\tif (flags & MAP_LOCKED)",
            "\t\tif (!can_do_mlock())",
            "\t\t\treturn -EPERM;",
            "",
            "\tif (!mlock_future_ok(mm, vm_flags, len))",
            "\t\treturn -EAGAIN;",
            "",
            "\tif (file) {",
            "\t\tstruct inode *inode = file_inode(file);",
            "\t\tunsigned long flags_mask;",
            "",
            "\t\tif (!file_mmap_ok(file, inode, pgoff, len))",
            "\t\t\treturn -EOVERFLOW;",
            "",
            "\t\tflags_mask = LEGACY_MAP_MASK | file->f_op->mmap_supported_flags;",
            "",
            "\t\tswitch (flags & MAP_TYPE) {",
            "\t\tcase MAP_SHARED:",
            "\t\t\t/*",
            "\t\t\t * Force use of MAP_SHARED_VALIDATE with non-legacy",
            "\t\t\t * flags. E.g. MAP_SYNC is dangerous to use with",
            "\t\t\t * MAP_SHARED as you don't know which consistency model",
            "\t\t\t * you will get. We silently ignore unsupported flags",
            "\t\t\t * with MAP_SHARED to preserve backward compatibility.",
            "\t\t\t */",
            "\t\t\tflags &= LEGACY_MAP_MASK;",
            "\t\t\tfallthrough;",
            "\t\tcase MAP_SHARED_VALIDATE:",
            "\t\t\tif (flags & ~flags_mask)",
            "\t\t\t\treturn -EOPNOTSUPP;",
            "\t\t\tif (prot & PROT_WRITE) {",
            "\t\t\t\tif (!(file->f_mode & FMODE_WRITE))",
            "\t\t\t\t\treturn -EACCES;",
            "\t\t\t\tif (IS_SWAPFILE(file->f_mapping->host))",
            "\t\t\t\t\treturn -ETXTBSY;",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * Make sure we don't allow writing to an append-only",
            "\t\t\t * file..",
            "\t\t\t */",
            "\t\t\tif (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))",
            "\t\t\t\treturn -EACCES;",
            "",
            "\t\t\tvm_flags |= VM_SHARED | VM_MAYSHARE;",
            "\t\t\tif (!(file->f_mode & FMODE_WRITE))",
            "\t\t\t\tvm_flags &= ~(VM_MAYWRITE | VM_SHARED);",
            "\t\t\tfallthrough;",
            "\t\tcase MAP_PRIVATE:",
            "\t\t\tif (!(file->f_mode & FMODE_READ))",
            "\t\t\t\treturn -EACCES;",
            "\t\t\tif (path_noexec(&file->f_path)) {",
            "\t\t\t\tif (vm_flags & VM_EXEC)",
            "\t\t\t\t\treturn -EPERM;",
            "\t\t\t\tvm_flags &= ~VM_MAYEXEC;",
            "\t\t\t}",
            "",
            "\t\t\tif (!file->f_op->mmap)",
            "\t\t\t\treturn -ENODEV;",
            "\t\t\tif (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))",
            "\t\t\t\treturn -EINVAL;",
            "\t\t\tbreak;",
            "",
            "\t\tdefault:",
            "\t\t\treturn -EINVAL;",
            "\t\t}",
            "\t} else {",
            "\t\tswitch (flags & MAP_TYPE) {",
            "\t\tcase MAP_SHARED:",
            "\t\t\tif (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))",
            "\t\t\t\treturn -EINVAL;",
            "\t\t\t/*",
            "\t\t\t * Ignore pgoff.",
            "\t\t\t */",
            "\t\t\tpgoff = 0;",
            "\t\t\tvm_flags |= VM_SHARED | VM_MAYSHARE;",
            "\t\t\tbreak;",
            "\t\tcase MAP_PRIVATE:",
            "\t\t\t/*",
            "\t\t\t * Set pgoff according to addr for anon_vma.",
            "\t\t\t */",
            "\t\t\tpgoff = addr >> PAGE_SHIFT;",
            "\t\t\tbreak;",
            "\t\tdefault:",
            "\t\t\treturn -EINVAL;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Set 'VM_NORESERVE' if we should not account for the",
            "\t * memory use of this mapping.",
            "\t */",
            "\tif (flags & MAP_NORESERVE) {",
            "\t\t/* We honor MAP_NORESERVE if allowed to overcommit */",
            "\t\tif (sysctl_overcommit_memory != OVERCOMMIT_NEVER)",
            "\t\t\tvm_flags |= VM_NORESERVE;",
            "",
            "\t\t/* hugetlb applies strict overcommit unless MAP_NORESERVE */",
            "\t\tif (file && is_file_hugepages(file))",
            "\t\t\tvm_flags |= VM_NORESERVE;",
            "\t}",
            "",
            "\taddr = mmap_region(file, addr, len, vm_flags, pgoff, uf);",
            "\tif (!IS_ERR_VALUE(addr) &&",
            "\t    ((vm_flags & VM_LOCKED) ||",
            "\t     (flags & (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))",
            "\t\t*populate = len;",
            "\treturn addr;",
            "}"
          ],
          "function_name": "round_hint_to_min, mlock_future_ok, file_mmap_size_max, file_mmap_ok, do_mmap",
          "description": "round_hint_to_min调整地址提示到最小值；mlock_future_ok检查进程能否锁定新页；file_mmap_size_max确定文件映射最大尺寸；file_mmap_ok验证映射合法性；do_mmap执行实际内存映射，处理地址选择、权限设置及资源检查。",
          "similarity": 0.5530147552490234
        },
        {
          "chunk_id": 14,
          "file_path": "mm/mmap.c",
          "start_line": 3029,
          "end_line": 3138,
          "content": [
            "static int __vm_munmap(unsigned long start, size_t len, bool unlock)",
            "{",
            "\tint ret;",
            "\tstruct mm_struct *mm = current->mm;",
            "\tLIST_HEAD(uf);",
            "\tVMA_ITERATOR(vmi, mm, start);",
            "",
            "\tif (mmap_write_lock_killable(mm))",
            "\t\treturn -EINTR;",
            "",
            "\tret = do_vmi_munmap(&vmi, mm, start, len, &uf, unlock);",
            "\tif (ret || !unlock)",
            "\t\tmmap_write_unlock(mm);",
            "",
            "\tuserfaultfd_unmap_complete(mm, &uf);",
            "\treturn ret;",
            "}",
            "int vm_munmap(unsigned long start, size_t len)",
            "{",
            "\treturn __vm_munmap(start, len, false);",
            "}",
            "int do_vma_munmap(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tunsigned long start, unsigned long end, struct list_head *uf,",
            "\t\tbool unlock)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "",
            "\tarch_unmap(mm, start, end);",
            "\treturn do_vmi_align_munmap(vmi, vma, mm, start, end, uf, unlock);",
            "}",
            "static int do_brk_flags(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tunsigned long addr, unsigned long len, unsigned long flags)",
            "{",
            "\tstruct mm_struct *mm = current->mm;",
            "\tstruct vma_prepare vp;",
            "",
            "\t/*",
            "\t * Check against address space limits by the changed size",
            "\t * Note: This happens *after* clearing old mappings in some code paths.",
            "\t */",
            "\tflags |= VM_DATA_DEFAULT_FLAGS | VM_ACCOUNT | mm->def_flags;",
            "\tif (!may_expand_vm(mm, flags, len >> PAGE_SHIFT))",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (mm->map_count > sysctl_max_map_count)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (security_vm_enough_memory_mm(mm, len >> PAGE_SHIFT))",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * Expand the existing vma if possible; Note that singular lists do not",
            "\t * occur after forking, so the expand will only happen on new VMAs.",
            "\t */",
            "\tif (vma && vma->vm_end == addr && !vma_policy(vma) &&",
            "\t    can_vma_merge_after(vma, flags, NULL, NULL,",
            "\t\t\t\taddr >> PAGE_SHIFT, NULL_VM_UFFD_CTX, NULL)) {",
            "\t\tvma_iter_config(vmi, vma->vm_start, addr + len);",
            "\t\tif (vma_iter_prealloc(vmi, vma))",
            "\t\t\tgoto unacct_fail;",
            "",
            "\t\tvma_start_write(vma);",
            "",
            "\t\tinit_vma_prep(&vp, vma);",
            "\t\tvma_prepare(&vp);",
            "\t\tvma_adjust_trans_huge(vma, vma->vm_start, addr + len, 0);",
            "\t\tvma->vm_end = addr + len;",
            "\t\tvm_flags_set(vma, VM_SOFTDIRTY);",
            "\t\tvma_iter_store(vmi, vma);",
            "",
            "\t\tvma_complete(&vp, vmi, mm);",
            "\t\tkhugepaged_enter_vma(vma, flags);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tif (vma)",
            "\t\tvma_iter_next_range(vmi);",
            "\t/* create a vma struct for an anonymous mapping */",
            "\tvma = vm_area_alloc(mm);",
            "\tif (!vma)",
            "\t\tgoto unacct_fail;",
            "",
            "\tvma_set_anonymous(vma);",
            "\tvma->vm_start = addr;",
            "\tvma->vm_end = addr + len;",
            "\tvma->vm_pgoff = addr >> PAGE_SHIFT;",
            "\tvm_flags_init(vma, flags);",
            "\tvma->vm_page_prot = vm_get_page_prot(flags);",
            "\tvma_start_write(vma);",
            "\tif (vma_iter_store_gfp(vmi, vma, GFP_KERNEL))",
            "\t\tgoto mas_store_fail;",
            "",
            "\tmm->map_count++;",
            "\tvalidate_mm(mm);",
            "\tksm_add_vma(vma);",
            "out:",
            "\tperf_event_mmap(vma);",
            "\tmm->total_vm += len >> PAGE_SHIFT;",
            "\tmm->data_vm += len >> PAGE_SHIFT;",
            "\tif (flags & VM_LOCKED)",
            "\t\tmm->locked_vm += (len >> PAGE_SHIFT);",
            "\tvm_flags_set(vma, VM_SOFTDIRTY);",
            "\treturn 0;",
            "",
            "mas_store_fail:",
            "\tvm_area_free(vma);",
            "unacct_fail:",
            "\tvm_unacct_memory(len >> PAGE_SHIFT);",
            "\treturn -ENOMEM;",
            "}"
          ],
          "function_name": "__vm_munmap, vm_munmap, do_vma_munmap, do_brk_flags",
          "description": "__vm_munmap通过持有mmap锁执行解除映射操作，do_brk_flags用于调整堆大小，尝试扩展现有VMA或新建匿名区域，包含地址空间限制检查、VMA合并尝试以及失败时的资源回滚逻辑",
          "similarity": 0.5489453673362732
        },
        {
          "chunk_id": 6,
          "file_path": "mm/mmap.c",
          "start_line": 1433,
          "end_line": 1540,
          "content": [
            "unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,",
            "\t\t\t      unsigned long prot, unsigned long flags,",
            "\t\t\t      unsigned long fd, unsigned long pgoff)",
            "{",
            "\tstruct file *file = NULL;",
            "\tunsigned long retval;",
            "",
            "\tif (!(flags & MAP_ANONYMOUS)) {",
            "\t\taudit_mmap_fd(fd, flags);",
            "\t\tfile = fget(fd);",
            "\t\tif (!file)",
            "\t\t\treturn -EBADF;",
            "\t\tif (is_file_hugepages(file)) {",
            "\t\t\tlen = ALIGN(len, huge_page_size(hstate_file(file)));",
            "\t\t} else if (unlikely(flags & MAP_HUGETLB)) {",
            "\t\t\tretval = -EINVAL;",
            "\t\t\tgoto out_fput;",
            "\t\t}",
            "\t} else if (flags & MAP_HUGETLB) {",
            "\t\tstruct hstate *hs;",
            "",
            "\t\ths = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);",
            "\t\tif (!hs)",
            "\t\t\treturn -EINVAL;",
            "",
            "\t\tlen = ALIGN(len, huge_page_size(hs));",
            "\t\t/*",
            "\t\t * VM_NORESERVE is used because the reservations will be",
            "\t\t * taken when vm_ops->mmap() is called",
            "\t\t */",
            "\t\tfile = hugetlb_file_setup(HUGETLB_ANON_FILE, len,",
            "\t\t\t\tVM_NORESERVE,",
            "\t\t\t\tHUGETLB_ANONHUGE_INODE,",
            "\t\t\t\t(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);",
            "\t\tif (IS_ERR(file))",
            "\t\t\treturn PTR_ERR(file);",
            "\t}",
            "",
            "\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);",
            "out_fput:",
            "\tif (file)",
            "\t\tfput(file);",
            "\treturn retval;",
            "}",
            "static bool vm_ops_needs_writenotify(const struct vm_operations_struct *vm_ops)",
            "{",
            "\treturn vm_ops && (vm_ops->page_mkwrite || vm_ops->pfn_mkwrite);",
            "}",
            "static bool vma_is_shared_writable(struct vm_area_struct *vma)",
            "{",
            "\treturn (vma->vm_flags & (VM_WRITE | VM_SHARED)) ==",
            "\t\t(VM_WRITE | VM_SHARED);",
            "}",
            "static bool vma_fs_can_writeback(struct vm_area_struct *vma)",
            "{",
            "\t/* No managed pages to writeback. */",
            "\tif (vma->vm_flags & VM_PFNMAP)",
            "\t\treturn false;",
            "",
            "\treturn vma->vm_file && vma->vm_file->f_mapping &&",
            "\t\tmapping_can_writeback(vma->vm_file->f_mapping);",
            "}",
            "bool vma_needs_dirty_tracking(struct vm_area_struct *vma)",
            "{",
            "\t/* Only shared, writable VMAs require dirty tracking. */",
            "\tif (!vma_is_shared_writable(vma))",
            "\t\treturn false;",
            "",
            "\t/* Does the filesystem need to be notified? */",
            "\tif (vm_ops_needs_writenotify(vma->vm_ops))",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Even if the filesystem doesn't indicate a need for writenotify, if it",
            "\t * can writeback, dirty tracking is still required.",
            "\t */",
            "\treturn vma_fs_can_writeback(vma);",
            "}",
            "bool vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)",
            "{",
            "\t/* If it was private or non-writable, the write bit is already clear */",
            "\tif (!vma_is_shared_writable(vma))",
            "\t\treturn false;",
            "",
            "\t/* The backer wishes to know when pages are first written to? */",
            "\tif (vm_ops_needs_writenotify(vma->vm_ops))",
            "\t\treturn true;",
            "",
            "\t/* The open routine did something to the protections that pgprot_modify",
            "\t * won't preserve? */",
            "\tif (pgprot_val(vm_page_prot) !=",
            "\t    pgprot_val(vm_pgprot_modify(vm_page_prot, vma->vm_flags)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Do we need to track softdirty? hugetlb does not support softdirty",
            "\t * tracking yet.",
            "\t */",
            "\tif (vma_soft_dirty_enabled(vma) && !is_vm_hugetlb_page(vma))",
            "\t\treturn true;",
            "",
            "\t/* Do we need write faults for uffd-wp tracking? */",
            "\tif (userfaultfd_wp(vma))",
            "\t\treturn true;",
            "",
            "\t/* Can the mapping track the dirty pages? */",
            "\treturn vma_fs_can_writeback(vma);",
            "}"
          ],
          "function_name": "ksys_mmap_pgoff, vm_ops_needs_writenotify, vma_is_shared_writable, vma_fs_can_writeback, vma_needs_dirty_tracking, vma_wants_writenotify",
          "description": "ksys_mmap_pgoff创建huge pages映射并调用vm_mmap_pgoff；vm_ops_needs_writenotify检测是否需要写通知；vma_is_shared_writable判断共享可写VMA；vma_needs_dirty_tracking决定是否需要脏页跟踪；vma_wants_writenotify综合判断是否触发写通知。",
          "similarity": 0.5464677810668945
        }
      ]
    },
    {
      "source_file": "mm/shmem.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:17:29\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `shmem.c`\n\n---\n\n# shmem.c 技术文档\n\n## 1. 文件概述\n\n`shmem.c` 实现了 Linux 内核中的 **共享内存虚拟文件系统（tmpfs）**，它基于 `ramfs` 扩展而来，支持使用交换空间（swap）并遵守资源限制，从而成为一个完全可用的内存文件系统。该文件系统用于实现 POSIX 共享内存、匿名映射（如 `/dev/zero`）、`memfd_create()` 创建的内存文件以及 tmpfs 挂载点（如 `/tmp` 或 `/dev/shm`）。其核心特点是：数据存储在内存中，可被换出到 swap，支持稀疏文件，并受内存和 inode 配额限制。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct shmem_falloc`：用于 `fallocate` 操作与缺页处理之间的通信，记录预分配范围、已分配页数等。\n- `struct shmem_options`：解析挂载选项（如 size、nr_inodes、huge、uid/gid 等）时使用的临时结构。\n- `struct shmem_sb_info`：超级块私有信息，包含块/ inode 配额、内存策略、配额计数器等。\n- `struct shmem_inode_info`：inode 私有信息，扩展标准 inode 以支持共享内存特性。\n\n### 关键函数\n- `shmem_acct_size()` / `shmem_unacct_size()`：对固定大小 VM 对象进行内存预占（如共享内存映射）。\n- `shmem_acct_blocks()` / `shmem_unacct_blocks()`：对 tmpfs 稀疏文件按实际分配页进行内存核算。\n- `shmem_inode_acct_blocks()` / `shmem_inode_unacct_blocks()`：结合文件系统配额（`max_blocks`）和磁盘配额（`dquot`）进行块分配/释放。\n- `shmem_swapin_folio()`：从 swap 中换入指定页。\n- `vma_is_anon_shmem()`：判断 VMA 是否为匿名共享内存映射。\n- `SHMEM_SB()`：宏，快速获取超级块的 `shmem_sb_info`。\n\n### 全局操作结构体\n- `shmem_ops`：超级块操作（如 `statfs`、`put_super`）。\n- `shmem_aops`：地址空间操作（如 `readpage`、`writepage`、`set_page_dirty`）。\n- `shmem_file_operations`：文件操作（如 `read`、`write`、`mmap`）。\n- `shmem_inode_operations`：普通文件 inode 操作。\n- `shmem_dir_inode_operations`：目录 inode 操作。\n- `shmem_special_inode_operations`：特殊文件（设备、socket）inode 操作。\n- `shmem_vm_ops` / `shmem_anon_vm_ops`：VMA 操作结构体，分别用于 tmpfs 文件映射和匿名共享内存映射。\n\n## 3. 关键实现\n\n### 内存核算机制\n- **预占模式（Pre-accounting）**：用于 `shmem_file_setup()` 创建的固定大小对象（如 POSIX 共享内存），在创建时即核算全部内存（通过 `shmem_acct_size`），避免运行时 OOM。\n- **增量核算（Incremental accounting）**：用于 tmpfs 文件，仅在实际分配页面时核算（通过 `shmem_acct_blocks`），支持大稀疏文件。失败返回 `-ENOSPC` 而非 `-ENOMEM`，使用户态收到 `SIGBUS` 而非触发 OOM killer。\n\n### 配额管理\n- 使用 `percpu_counter` 高效跟踪已用块数（`used_blocks`），并与挂载时指定的 `max_blocks` 限制比较。\n- 集成内核通用配额子系统（`dquot_alloc_block_nodirty` / `dquot_free_block_nodirty`），支持用户/组配额。\n\n### 大页（Huge Page）支持\n- 通过 `huge` 挂载选项和 `madvise(MADV_HUGEPAGE)` 控制透明大页（THP）行为。\n- 维护多个位图（`huge_shmem_orders_*`）记录不同场景下允许的大页阶数。\n\n### fallocate 与缺页协同\n- `shmem_falloc` 结构通过 `inode->i_private` 在 `fallocate` 和 `shmem_fault`/`shmem_writepage` 之间传递状态。\n- 使用等待队列（`waitq`）确保在 punch hole 操作期间，访问空洞的缺页请求会等待操作完成。\n\n### 匿名 vs 命名共享内存\n- **匿名共享内存**：由 `shmem_zero_setup()` 创建（如 `/dev/zero` 映射），使用 `shmem_anon_vm_ops`。\n- **命名共享内存**：通过 tmpfs 文件系统接口创建（如 `shm_open()`），使用 `shmem_vm_ops`。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `mm/` 中的页分配、swap、rmap、mempolicy、hugetlb 等机制。\n- **VFS 层**：实现标准文件系统接口（`super_operations`, `inode_operations` 等）。\n- **安全模块**：调用 LSM 钩子（`security_vm_enough_memory_mm`）进行内存安全检查。\n- **配额子系统**：通过 `dquot_*` 函数集成磁盘配额功能。\n- **swap 子系统**：通过 `swap.h` 和 `swapops.h` 实现页面换入换出。\n- **其他**：依赖 `ramfs` 基础结构、`xattr`、`posix_acl`、`splice`、`falloc` 等通用内核组件。\n\n## 5. 使用场景\n\n- **POSIX 共享内存**：`shm_open()` / `shm_unlink()` 创建的共享内存对象。\n- **System V 共享内存**：`shmget()` / `shmat()` 使用的底层存储。\n- **匿名映射**：`mmap()` 映射 `/dev/zero` 或 `MAP_ANONYMOUS | MAP_SHARED` 创建的共享内存区域。\n- **tmpfs 文件系统**：挂载 tmpfs（如 `/dev/shm`）后创建的文件和目录。\n- **memfd 文件**：通过 `memfd_create()` 系统调用创建的匿名内存文件，支持密封（sealing）和共享。\n- **内核内部用途**：作为某些需要临时可换出内存缓冲区的子系统的后端存储。",
      "similarity": 0.6035561561584473,
      "chunks": [
        {
          "chunk_id": 16,
          "file_path": "mm/shmem.c",
          "start_line": 2837,
          "end_line": 3022,
          "content": [
            "static int shmem_set_policy(struct vm_area_struct *vma, struct mempolicy *mpol)",
            "{",
            "\tstruct inode *inode = file_inode(vma->vm_file);",
            "\treturn mpol_set_shared_policy(&SHMEM_I(inode)->policy, vma, mpol);",
            "}",
            "int shmem_lock(struct file *file, int lock, struct ucounts *ucounts)",
            "{",
            "\tstruct inode *inode = file_inode(file);",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tint retval = -ENOMEM;",
            "",
            "\t/*",
            "\t * What serializes the accesses to info->flags?",
            "\t * ipc_lock_object() when called from shmctl_do_lock(),",
            "\t * no serialization needed when called from shm_destroy().",
            "\t */",
            "\tif (lock && !(info->flags & VM_LOCKED)) {",
            "\t\tif (!user_shm_lock(inode->i_size, ucounts))",
            "\t\t\tgoto out_nomem;",
            "\t\tinfo->flags |= VM_LOCKED;",
            "\t\tmapping_set_unevictable(file->f_mapping);",
            "\t}",
            "\tif (!lock && (info->flags & VM_LOCKED) && ucounts) {",
            "\t\tuser_shm_unlock(inode->i_size, ucounts);",
            "\t\tinfo->flags &= ~VM_LOCKED;",
            "\t\tmapping_clear_unevictable(file->f_mapping);",
            "\t}",
            "\tretval = 0;",
            "",
            "out_nomem:",
            "\treturn retval;",
            "}",
            "static int shmem_mmap(struct file *file, struct vm_area_struct *vma)",
            "{",
            "\tstruct inode *inode = file_inode(file);",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tint ret;",
            "",
            "\tret = seal_check_write(info->seals, vma);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tfile_accessed(file);",
            "\t/* This is anonymous shared memory if it is unlinked at the time of mmap */",
            "\tif (inode->i_nlink)",
            "\t\tvma->vm_ops = &shmem_vm_ops;",
            "\telse",
            "\t\tvma->vm_ops = &shmem_anon_vm_ops;",
            "\treturn 0;",
            "}",
            "static int shmem_file_open(struct inode *inode, struct file *file)",
            "{",
            "\tfile->f_mode |= FMODE_CAN_ODIRECT;",
            "\treturn generic_file_open(inode, file);",
            "}",
            "static void shmem_set_inode_flags(struct inode *inode, unsigned int fsflags)",
            "{",
            "\tunsigned int i_flags = 0;",
            "",
            "\tif (fsflags & FS_NOATIME_FL)",
            "\t\ti_flags |= S_NOATIME;",
            "\tif (fsflags & FS_APPEND_FL)",
            "\t\ti_flags |= S_APPEND;",
            "\tif (fsflags & FS_IMMUTABLE_FL)",
            "\t\ti_flags |= S_IMMUTABLE;",
            "\t/*",
            "\t * But FS_NODUMP_FL does not require any action in i_flags.",
            "\t */",
            "\tinode_set_flags(inode, i_flags, S_NOATIME | S_APPEND | S_IMMUTABLE);",
            "}",
            "static void shmem_set_inode_flags(struct inode *inode, unsigned int fsflags)",
            "{",
            "}",
            "int shmem_mfill_atomic_pte(pmd_t *dst_pmd,",
            "\t\t\t   struct vm_area_struct *dst_vma,",
            "\t\t\t   unsigned long dst_addr,",
            "\t\t\t   unsigned long src_addr,",
            "\t\t\t   uffd_flags_t flags,",
            "\t\t\t   struct folio **foliop)",
            "{",
            "\tstruct inode *inode = file_inode(dst_vma->vm_file);",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tstruct address_space *mapping = inode->i_mapping;",
            "\tgfp_t gfp = mapping_gfp_mask(mapping);",
            "\tpgoff_t pgoff = linear_page_index(dst_vma, dst_addr);",
            "\tvoid *page_kaddr;",
            "\tstruct folio *folio;",
            "\tint ret;",
            "\tpgoff_t max_off;",
            "",
            "\tif (shmem_inode_acct_blocks(inode, 1)) {",
            "\t\t/*",
            "\t\t * We may have got a page, returned -ENOENT triggering a retry,",
            "\t\t * and now we find ourselves with -ENOMEM. Release the page, to",
            "\t\t * avoid a BUG_ON in our caller.",
            "\t\t */",
            "\t\tif (unlikely(*foliop)) {",
            "\t\t\tfolio_put(*foliop);",
            "\t\t\t*foliop = NULL;",
            "\t\t}",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\tif (!*foliop) {",
            "\t\tret = -ENOMEM;",
            "\t\tfolio = shmem_alloc_folio(gfp, 0, info, pgoff);",
            "\t\tif (!folio)",
            "\t\t\tgoto out_unacct_blocks;",
            "",
            "\t\tif (uffd_flags_mode_is(flags, MFILL_ATOMIC_COPY)) {",
            "\t\t\tpage_kaddr = kmap_local_folio(folio, 0);",
            "\t\t\t/*",
            "\t\t\t * The read mmap_lock is held here.  Despite the",
            "\t\t\t * mmap_lock being read recursive a deadlock is still",
            "\t\t\t * possible if a writer has taken a lock.  For example:",
            "\t\t\t *",
            "\t\t\t * process A thread 1 takes read lock on own mmap_lock",
            "\t\t\t * process A thread 2 calls mmap, blocks taking write lock",
            "\t\t\t * process B thread 1 takes page fault, read lock on own mmap lock",
            "\t\t\t * process B thread 2 calls mmap, blocks taking write lock",
            "\t\t\t * process A thread 1 blocks taking read lock on process B",
            "\t\t\t * process B thread 1 blocks taking read lock on process A",
            "\t\t\t *",
            "\t\t\t * Disable page faults to prevent potential deadlock",
            "\t\t\t * and retry the copy outside the mmap_lock.",
            "\t\t\t */",
            "\t\t\tpagefault_disable();",
            "\t\t\tret = copy_from_user(page_kaddr,",
            "\t\t\t\t\t     (const void __user *)src_addr,",
            "\t\t\t\t\t     PAGE_SIZE);",
            "\t\t\tpagefault_enable();",
            "\t\t\tkunmap_local(page_kaddr);",
            "",
            "\t\t\t/* fallback to copy_from_user outside mmap_lock */",
            "\t\t\tif (unlikely(ret)) {",
            "\t\t\t\t*foliop = folio;",
            "\t\t\t\tret = -ENOENT;",
            "\t\t\t\t/* don't free the page */",
            "\t\t\t\tgoto out_unacct_blocks;",
            "\t\t\t}",
            "",
            "\t\t\tflush_dcache_folio(folio);",
            "\t\t} else {\t\t/* ZEROPAGE */",
            "\t\t\tclear_user_highpage(&folio->page, dst_addr);",
            "\t\t}",
            "\t} else {",
            "\t\tfolio = *foliop;",
            "\t\tVM_BUG_ON_FOLIO(folio_test_large(folio), folio);",
            "\t\t*foliop = NULL;",
            "\t}",
            "",
            "\tVM_BUG_ON(folio_test_locked(folio));",
            "\tVM_BUG_ON(folio_test_swapbacked(folio));",
            "\t__folio_set_locked(folio);",
            "\t__folio_set_swapbacked(folio);",
            "\t__folio_mark_uptodate(folio);",
            "",
            "\tret = -EFAULT;",
            "\tmax_off = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);",
            "\tif (unlikely(pgoff >= max_off))",
            "\t\tgoto out_release;",
            "",
            "\tret = mem_cgroup_charge(folio, dst_vma->vm_mm, gfp);",
            "\tif (ret)",
            "\t\tgoto out_release;",
            "\tret = shmem_add_to_page_cache(folio, mapping, pgoff, NULL, gfp);",
            "\tif (ret)",
            "\t\tgoto out_release;",
            "",
            "\tret = mfill_atomic_install_pte(dst_pmd, dst_vma, dst_addr,",
            "\t\t\t\t       &folio->page, true, flags);",
            "\tif (ret)",
            "\t\tgoto out_delete_from_cache;",
            "",
            "\tshmem_recalc_inode(inode, 1, 0);",
            "\tfolio_unlock(folio);",
            "\treturn 0;",
            "out_delete_from_cache:",
            "\tfilemap_remove_folio(folio);",
            "out_release:",
            "\tfolio_unlock(folio);",
            "\tfolio_put(folio);",
            "out_unacct_blocks:",
            "\tshmem_inode_unacct_blocks(inode, 1);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "shmem_set_policy, shmem_lock, shmem_mmap, shmem_file_open, shmem_set_inode_flags, shmem_set_inode_flags, shmem_mfill_atomic_pte",
          "description": "提供共享内存策略设置、锁操作、mmap映射配置、文件打开及原子页填充功能，包含shmem_set_policy设置内存策略，shmem_lock管理内存锁定，shmem_mmap配置虚拟内存区域，shmem_mfill_atomic_pte执行原子页填充",
          "similarity": 0.6697585582733154
        },
        {
          "chunk_id": 10,
          "file_path": "mm/shmem.c",
          "start_line": 1656,
          "end_line": 1760,
          "content": [
            "static void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)",
            "{",
            "\tchar buffer[64];",
            "",
            "\tif (!mpol || mpol->mode == MPOL_DEFAULT)",
            "\t\treturn;\t\t/* show nothing */",
            "",
            "\tmpol_to_str(buffer, sizeof(buffer), mpol);",
            "",
            "\tseq_printf(seq, \",mpol=%s\", buffer);",
            "}",
            "static inline void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)",
            "{",
            "}",
            "static gfp_t limit_gfp_mask(gfp_t huge_gfp, gfp_t limit_gfp)",
            "{",
            "\tgfp_t allowflags = __GFP_IO | __GFP_FS | __GFP_RECLAIM;",
            "\tgfp_t denyflags = __GFP_NOWARN | __GFP_NORETRY;",
            "\tgfp_t zoneflags = limit_gfp & GFP_ZONEMASK;",
            "\tgfp_t result = huge_gfp & ~(allowflags | GFP_ZONEMASK);",
            "",
            "\t/* Allow allocations only from the originally specified zones. */",
            "\tresult |= zoneflags;",
            "",
            "\t/*",
            "\t * Minimize the result gfp by taking the union with the deny flags,",
            "\t * and the intersection of the allow flags.",
            "\t */",
            "\tresult |= (limit_gfp & denyflags);",
            "\tresult |= (huge_gfp & limit_gfp) & allowflags;",
            "",
            "\treturn result;",
            "}",
            "bool shmem_hpage_pmd_enabled(void)",
            "{",
            "\tif (shmem_huge == SHMEM_HUGE_DENY)",
            "\t\treturn false;",
            "\tif (test_bit(HPAGE_PMD_ORDER, &huge_shmem_orders_always))",
            "\t\treturn true;",
            "\tif (test_bit(HPAGE_PMD_ORDER, &huge_shmem_orders_madvise))",
            "\t\treturn true;",
            "\tif (test_bit(HPAGE_PMD_ORDER, &huge_shmem_orders_within_size))",
            "\t\treturn true;",
            "\tif (test_bit(HPAGE_PMD_ORDER, &huge_shmem_orders_inherit) &&",
            "\t    shmem_huge != SHMEM_HUGE_NEVER)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "unsigned long shmem_allowable_huge_orders(struct inode *inode,",
            "\t\t\t\tstruct vm_area_struct *vma, pgoff_t index,",
            "\t\t\t\tloff_t write_end, bool shmem_huge_force)",
            "{",
            "\tunsigned long mask = READ_ONCE(huge_shmem_orders_always);",
            "\tunsigned long within_size_orders = READ_ONCE(huge_shmem_orders_within_size);",
            "\tunsigned long vm_flags = vma ? vma->vm_flags : 0;",
            "\tpgoff_t aligned_index;",
            "\tunsigned int global_orders;",
            "\tloff_t i_size;",
            "\tint order;",
            "",
            "\tif (thp_disabled_by_hw() || (vma && vma_thp_disabled(vma, vm_flags)))",
            "\t\treturn 0;",
            "",
            "\tglobal_orders = shmem_huge_global_enabled(inode, index, write_end,",
            "\t\t\t\t\t\t  shmem_huge_force, vma, vm_flags);",
            "\t/* Tmpfs huge pages allocation */",
            "\tif (!vma || !vma_is_anon_shmem(vma))",
            "\t\treturn global_orders;",
            "",
            "\t/*",
            "\t * Following the 'deny' semantics of the top level, force the huge",
            "\t * option off from all mounts.",
            "\t */",
            "\tif (shmem_huge == SHMEM_HUGE_DENY)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Only allow inherit orders if the top-level value is 'force', which",
            "\t * means non-PMD sized THP can not override 'huge' mount option now.",
            "\t */",
            "\tif (shmem_huge == SHMEM_HUGE_FORCE)",
            "\t\treturn READ_ONCE(huge_shmem_orders_inherit);",
            "",
            "\t/* Allow mTHP that will be fully within i_size. */",
            "\torder = highest_order(within_size_orders);",
            "\twhile (within_size_orders) {",
            "\t\taligned_index = round_up(index + 1, 1 << order);",
            "\t\ti_size = round_up(i_size_read(inode), PAGE_SIZE);",
            "\t\tif (i_size >> PAGE_SHIFT >= aligned_index) {",
            "\t\t\tmask |= within_size_orders;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\torder = next_order(&within_size_orders, order);",
            "\t}",
            "",
            "\tif (vm_flags & VM_HUGEPAGE)",
            "\t\tmask |= READ_ONCE(huge_shmem_orders_madvise);",
            "",
            "\tif (global_orders > 0)",
            "\t\tmask |= READ_ONCE(huge_shmem_orders_inherit);",
            "",
            "\treturn THP_ORDERS_ALL_FILE_DEFAULT & mask;",
            "}"
          ],
          "function_name": "shmem_show_mpol, shmem_show_mpol, limit_gfp_mask, shmem_hpage_pmd_enabled, shmem_allowable_huge_orders",
          "description": "shmem_show_mpol函数用于在序列化输出中显示内存策略信息，若非默认策略则转换为字符串附加至seq_file。同名空函数可能为占位符。limit_gfp_mask计算受限的内存分配标志，结合huge页面需求与限制标志。shmem_hpage_pmd_enabled检查是否启用HPAGE_PM D支持。shmem_allowable_huge_orders确定允许的huge页面订单，综合考虑挂载选项、文件大小及VMA属性。",
          "similarity": 0.614834189414978
        },
        {
          "chunk_id": 17,
          "file_path": "mm/shmem.c",
          "start_line": 3202,
          "end_line": 3379,
          "content": [
            "static int",
            "shmem_write_begin(struct file *file, struct address_space *mapping,",
            "\t\t\tloff_t pos, unsigned len,",
            "\t\t\tstruct page **pagep, void **fsdata)",
            "{",
            "\tstruct inode *inode = mapping->host;",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tpgoff_t index = pos >> PAGE_SHIFT;",
            "\tstruct folio *folio;",
            "\tint ret = 0;",
            "",
            "\t/* i_rwsem is held by caller */",
            "\tif (unlikely(info->seals & (F_SEAL_GROW |",
            "\t\t\t\t   F_SEAL_WRITE | F_SEAL_FUTURE_WRITE))) {",
            "\t\tif (info->seals & (F_SEAL_WRITE | F_SEAL_FUTURE_WRITE))",
            "\t\t\treturn -EPERM;",
            "\t\tif ((info->seals & F_SEAL_GROW) && pos + len > inode->i_size)",
            "\t\t\treturn -EPERM;",
            "\t}",
            "",
            "\tret = shmem_get_folio(inode, index, pos + len, &folio, SGP_WRITE);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\t*pagep = folio_file_page(folio, index);",
            "\tif (PageHWPoison(*pagep)) {",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_put(folio);",
            "\t\t*pagep = NULL;",
            "\t\treturn -EIO;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int",
            "shmem_write_end(struct file *file, struct address_space *mapping,",
            "\t\t\tloff_t pos, unsigned len, unsigned copied,",
            "\t\t\tstruct page *page, void *fsdata)",
            "{",
            "\tstruct folio *folio = page_folio(page);",
            "\tstruct inode *inode = mapping->host;",
            "",
            "\tif (pos + copied > inode->i_size)",
            "\t\ti_size_write(inode, pos + copied);",
            "",
            "\tif (!folio_test_uptodate(folio)) {",
            "\t\tif (copied < folio_size(folio)) {",
            "\t\t\tsize_t from = offset_in_folio(folio, pos);",
            "\t\t\tfolio_zero_segments(folio, 0, from,",
            "\t\t\t\t\tfrom + copied, folio_size(folio));",
            "\t\t}",
            "\t\tfolio_mark_uptodate(folio);",
            "\t}",
            "\tfolio_mark_dirty(folio);",
            "\tfolio_unlock(folio);",
            "\tfolio_put(folio);",
            "",
            "\treturn copied;",
            "}",
            "static ssize_t shmem_file_read_iter(struct kiocb *iocb, struct iov_iter *to)",
            "{",
            "\tstruct file *file = iocb->ki_filp;",
            "\tstruct inode *inode = file_inode(file);",
            "\tstruct address_space *mapping = inode->i_mapping;",
            "\tpgoff_t index;",
            "\tunsigned long offset;",
            "\tint error = 0;",
            "\tssize_t retval = 0;",
            "",
            "\tfor (;;) {",
            "\t\tstruct folio *folio = NULL;",
            "\t\tstruct page *page = NULL;",
            "\t\tunsigned long nr, ret;",
            "\t\tloff_t end_offset, i_size = i_size_read(inode);",
            "\t\tbool fallback_page_copy = false;",
            "\t\tsize_t fsize;",
            "",
            "\t\tif (unlikely(iocb->ki_pos >= i_size))",
            "\t\t\tbreak;",
            "",
            "\t\tindex = iocb->ki_pos >> PAGE_SHIFT;",
            "\t\terror = shmem_get_folio(inode, index, 0, &folio, SGP_READ);",
            "\t\tif (error) {",
            "\t\t\tif (error == -EINVAL)",
            "\t\t\t\terror = 0;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (folio) {",
            "\t\t\tfolio_unlock(folio);",
            "",
            "\t\t\tpage = folio_file_page(folio, index);",
            "\t\t\tif (PageHWPoison(page)) {",
            "\t\t\t\tfolio_put(folio);",
            "\t\t\t\terror = -EIO;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "",
            "\t\t\tif (folio_test_large(folio) &&",
            "\t\t\t    folio_test_has_hwpoisoned(folio))",
            "\t\t\t\tfallback_page_copy = true;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We must evaluate after, since reads (unlike writes)",
            "\t\t * are called without i_rwsem protection against truncate",
            "\t\t */",
            "\t\ti_size = i_size_read(inode);",
            "\t\tif (unlikely(iocb->ki_pos >= i_size)) {",
            "\t\t\tif (folio)",
            "\t\t\t\tfolio_put(folio);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tend_offset = min_t(loff_t, i_size, iocb->ki_pos + to->count);",
            "\t\tif (folio && likely(!fallback_page_copy))",
            "\t\t\tfsize = folio_size(folio);",
            "\t\telse",
            "\t\t\tfsize = PAGE_SIZE;",
            "\t\toffset = iocb->ki_pos & (fsize - 1);",
            "\t\tnr = min_t(loff_t, end_offset - iocb->ki_pos, fsize - offset);",
            "",
            "\t\tif (folio) {",
            "\t\t\t/*",
            "\t\t\t * If users can be writing to this page using arbitrary",
            "\t\t\t * virtual addresses, take care about potential aliasing",
            "\t\t\t * before reading the page on the kernel side.",
            "\t\t\t */",
            "\t\t\tif (mapping_writably_mapped(mapping)) {",
            "\t\t\t\tif (likely(!fallback_page_copy))",
            "\t\t\t\t\tflush_dcache_folio(folio);",
            "\t\t\t\telse",
            "\t\t\t\t\tflush_dcache_page(page);",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * Mark the folio accessed if we read the beginning.",
            "\t\t\t */",
            "\t\t\tif (!offset)",
            "\t\t\t\tfolio_mark_accessed(folio);",
            "\t\t\t/*",
            "\t\t\t * Ok, we have the page, and it's up-to-date, so",
            "\t\t\t * now we can copy it to user space...",
            "\t\t\t */",
            "\t\t\tif (likely(!fallback_page_copy))",
            "\t\t\t\tret = copy_folio_to_iter(folio, offset, nr, to);",
            "\t\t\telse",
            "\t\t\t\tret = copy_page_to_iter(page, offset, nr, to);",
            "\t\t\tfolio_put(folio);",
            "\t\t} else if (user_backed_iter(to)) {",
            "\t\t\t/*",
            "\t\t\t * Copy to user tends to be so well optimized, but",
            "\t\t\t * clear_user() not so much, that it is noticeably",
            "\t\t\t * faster to copy the zero page instead of clearing.",
            "\t\t\t */",
            "\t\t\tret = copy_page_to_iter(ZERO_PAGE(0), offset, nr, to);",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * But submitting the same page twice in a row to",
            "\t\t\t * splice() - or others? - can result in confusion:",
            "\t\t\t * so don't attempt that optimization on pipes etc.",
            "\t\t\t */",
            "\t\t\tret = iov_iter_zero(nr, to);",
            "\t\t}",
            "",
            "\t\tretval += ret;",
            "\t\tiocb->ki_pos += ret;",
            "",
            "\t\tif (!iov_iter_count(to))",
            "\t\t\tbreak;",
            "\t\tif (ret < nr) {",
            "\t\t\terror = -EFAULT;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\tfile_accessed(file);",
            "\treturn retval ? retval : error;",
            "}"
          ],
          "function_name": "shmem_write_begin, shmem_write_end, shmem_file_read_iter",
          "description": "实现共享内存读写操作，包含shmem_write_begin准备写入页面，shmem_write_end完成写入并更新文件大小，shmem_file_read_iter执行文件读取迭代操作，处理页面有效性检查和数据复制",
          "similarity": 0.6125278472900391
        },
        {
          "chunk_id": 15,
          "file_path": "mm/shmem.c",
          "start_line": 2617,
          "end_line": 2808,
          "content": [
            "int shmem_get_folio(struct inode *inode, pgoff_t index, loff_t write_end,",
            "\t\t    struct folio **foliop, enum sgp_type sgp)",
            "{",
            "\treturn shmem_get_folio_gfp(inode, index, write_end, foliop, sgp,",
            "\t\t\tmapping_gfp_mask(inode->i_mapping), NULL, NULL);",
            "}",
            "static int synchronous_wake_function(wait_queue_entry_t *wait,",
            "\t\t\tunsigned int mode, int sync, void *key)",
            "{",
            "\tint ret = default_wake_function(wait, mode, sync, key);",
            "\tlist_del_init(&wait->entry);",
            "\treturn ret;",
            "}",
            "static vm_fault_t shmem_falloc_wait(struct vm_fault *vmf, struct inode *inode)",
            "{",
            "\tstruct shmem_falloc *shmem_falloc;",
            "\tstruct file *fpin = NULL;",
            "\tvm_fault_t ret = 0;",
            "",
            "\tspin_lock(&inode->i_lock);",
            "\tshmem_falloc = inode->i_private;",
            "\tif (shmem_falloc &&",
            "\t    shmem_falloc->waitq &&",
            "\t    vmf->pgoff >= shmem_falloc->start &&",
            "\t    vmf->pgoff < shmem_falloc->next) {",
            "\t\twait_queue_head_t *shmem_falloc_waitq;",
            "\t\tDEFINE_WAIT_FUNC(shmem_fault_wait, synchronous_wake_function);",
            "",
            "\t\tret = VM_FAULT_NOPAGE;",
            "\t\tfpin = maybe_unlock_mmap_for_io(vmf, NULL);",
            "\t\tshmem_falloc_waitq = shmem_falloc->waitq;",
            "\t\tprepare_to_wait(shmem_falloc_waitq, &shmem_fault_wait,",
            "\t\t\t\tTASK_UNINTERRUPTIBLE);",
            "\t\tspin_unlock(&inode->i_lock);",
            "\t\tschedule();",
            "",
            "\t\t/*",
            "\t\t * shmem_falloc_waitq points into the shmem_fallocate()",
            "\t\t * stack of the hole-punching task: shmem_falloc_waitq",
            "\t\t * is usually invalid by the time we reach here, but",
            "\t\t * finish_wait() does not dereference it in that case;",
            "\t\t * though i_lock needed lest racing with wake_up_all().",
            "\t\t */",
            "\t\tspin_lock(&inode->i_lock);",
            "\t\tfinish_wait(shmem_falloc_waitq, &shmem_fault_wait);",
            "\t}",
            "\tspin_unlock(&inode->i_lock);",
            "\tif (fpin) {",
            "\t\tfput(fpin);",
            "\t\tret = VM_FAULT_RETRY;",
            "\t}",
            "\treturn ret;",
            "}",
            "static vm_fault_t shmem_fault(struct vm_fault *vmf)",
            "{",
            "\tstruct inode *inode = file_inode(vmf->vma->vm_file);",
            "\tgfp_t gfp = mapping_gfp_mask(inode->i_mapping);",
            "\tstruct folio *folio = NULL;",
            "\tvm_fault_t ret = 0;",
            "\tint err;",
            "",
            "\t/*",
            "\t * Trinity finds that probing a hole which tmpfs is punching can",
            "\t * prevent the hole-punch from ever completing: noted in i_private.",
            "\t */",
            "\tif (unlikely(inode->i_private)) {",
            "\t\tret = shmem_falloc_wait(vmf, inode);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\tWARN_ON_ONCE(vmf->page != NULL);",
            "\terr = shmem_get_folio_gfp(inode, vmf->pgoff, 0, &folio, SGP_CACHE,",
            "\t\t\t\t  gfp, vmf, &ret);",
            "\tif (err)",
            "\t\treturn vmf_error(err);",
            "\tif (folio) {",
            "\t\tvmf->page = folio_file_page(folio, vmf->pgoff);",
            "\t\tret |= VM_FAULT_LOCKED;",
            "\t}",
            "\treturn ret;",
            "}",
            "unsigned long shmem_get_unmapped_area(struct file *file,",
            "\t\t\t\t      unsigned long uaddr, unsigned long len,",
            "\t\t\t\t      unsigned long pgoff, unsigned long flags)",
            "{",
            "\tunsigned long (*get_area)(struct file *,",
            "\t\tunsigned long, unsigned long, unsigned long, unsigned long);",
            "\tunsigned long addr;",
            "\tunsigned long offset;",
            "\tunsigned long inflated_len;",
            "\tunsigned long inflated_addr;",
            "\tunsigned long inflated_offset;",
            "\tunsigned long hpage_size;",
            "",
            "\tif (len > TASK_SIZE)",
            "\t\treturn -ENOMEM;",
            "",
            "\tget_area = current->mm->get_unmapped_area;",
            "\taddr = get_area(file, uaddr, len, pgoff, flags);",
            "",
            "\tif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE))",
            "\t\treturn addr;",
            "\tif (IS_ERR_VALUE(addr))",
            "\t\treturn addr;",
            "\tif (addr & ~PAGE_MASK)",
            "\t\treturn addr;",
            "\tif (addr > TASK_SIZE - len)",
            "\t\treturn addr;",
            "",
            "\tif (shmem_huge == SHMEM_HUGE_DENY)",
            "\t\treturn addr;",
            "\tif (flags & MAP_FIXED)",
            "\t\treturn addr;",
            "\t/*",
            "\t * Our priority is to support MAP_SHARED mapped hugely;",
            "\t * and support MAP_PRIVATE mapped hugely too, until it is COWed.",
            "\t * But if caller specified an address hint and we allocated area there",
            "\t * successfully, respect that as before.",
            "\t */",
            "\tif (uaddr == addr)",
            "\t\treturn addr;",
            "",
            "\thpage_size = HPAGE_PMD_SIZE;",
            "\tif (shmem_huge != SHMEM_HUGE_FORCE) {",
            "\t\tstruct super_block *sb;",
            "\t\tunsigned long __maybe_unused hpage_orders;",
            "\t\tint order = 0;",
            "",
            "\t\tif (file) {",
            "\t\t\tVM_BUG_ON(file->f_op != &shmem_file_operations);",
            "\t\t\tsb = file_inode(file)->i_sb;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * Called directly from mm/mmap.c, or drivers/char/mem.c",
            "\t\t\t * for \"/dev/zero\", to create a shared anonymous object.",
            "\t\t\t */",
            "\t\t\tif (IS_ERR(shm_mnt))",
            "\t\t\t\treturn addr;",
            "\t\t\tsb = shm_mnt->mnt_sb;",
            "",
            "\t\t\t/*",
            "\t\t\t * Find the highest mTHP order used for anonymous shmem to",
            "\t\t\t * provide a suitable alignment address.",
            "\t\t\t */",
            "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
            "\t\t\thpage_orders = READ_ONCE(huge_shmem_orders_always);",
            "\t\t\thpage_orders |= READ_ONCE(huge_shmem_orders_within_size);",
            "\t\t\thpage_orders |= READ_ONCE(huge_shmem_orders_madvise);",
            "\t\t\tif (SHMEM_SB(sb)->huge != SHMEM_HUGE_NEVER)",
            "\t\t\t\thpage_orders |= READ_ONCE(huge_shmem_orders_inherit);",
            "",
            "\t\t\tif (hpage_orders > 0) {",
            "\t\t\t\torder = highest_order(hpage_orders);",
            "\t\t\t\thpage_size = PAGE_SIZE << order;",
            "\t\t\t}",
            "#endif",
            "\t\t}",
            "\t\tif (SHMEM_SB(sb)->huge == SHMEM_HUGE_NEVER && !order)",
            "\t\t\treturn addr;",
            "\t}",
            "",
            "\tif (len < hpage_size)",
            "\t\treturn addr;",
            "",
            "\toffset = (pgoff << PAGE_SHIFT) & (hpage_size - 1);",
            "\tif (offset && offset + len < 2 * hpage_size)",
            "\t\treturn addr;",
            "\tif ((addr & (hpage_size - 1)) == offset)",
            "\t\treturn addr;",
            "",
            "\tinflated_len = len + hpage_size - PAGE_SIZE;",
            "\tif (inflated_len > TASK_SIZE)",
            "\t\treturn addr;",
            "\tif (inflated_len < len)",
            "\t\treturn addr;",
            "",
            "\tinflated_addr = get_area(NULL, uaddr, inflated_len, 0, flags);",
            "\tif (IS_ERR_VALUE(inflated_addr))",
            "\t\treturn addr;",
            "\tif (inflated_addr & ~PAGE_MASK)",
            "\t\treturn addr;",
            "",
            "\tinflated_offset = inflated_addr & (hpage_size - 1);",
            "\tinflated_addr += offset - inflated_offset;",
            "\tif (inflated_offset > offset)",
            "\t\tinflated_addr += hpage_size;",
            "",
            "\tif (inflated_addr > TASK_SIZE - len)",
            "\t\treturn addr;",
            "\treturn inflated_addr;",
            "}"
          ],
          "function_name": "shmem_get_folio, synchronous_wake_function, shmem_falloc_wait, shmem_fault, shmem_get_unmapped_area",
          "description": "实现共享内存页面分配与地址映射，包含shmem_get_folio获取页面，synchronous_wake_function同步唤醒等待队列，shmem_falloc_wait处理文件分配等待逻辑，shmem_fault处理页面故障，shmem_get_unmapped_area查找未映射区域并支持透明大页",
          "similarity": 0.6006531715393066
        },
        {
          "chunk_id": 1,
          "file_path": "mm/shmem.c",
          "start_line": 143,
          "end_line": 264,
          "content": [
            "static unsigned long shmem_default_max_blocks(void)",
            "{",
            "\treturn totalram_pages() / 2;",
            "}",
            "static unsigned long shmem_default_max_inodes(void)",
            "{",
            "\tunsigned long nr_pages = totalram_pages();",
            "",
            "\treturn min3(nr_pages - totalhigh_pages(), nr_pages / 2,",
            "\t\t\tULONG_MAX / BOGO_INODE_SIZE);",
            "}",
            "static inline int shmem_acct_size(unsigned long flags, loff_t size)",
            "{",
            "\treturn (flags & VM_NORESERVE) ?",
            "\t\t0 : security_vm_enough_memory_mm(current->mm, VM_ACCT(size));",
            "}",
            "static inline void shmem_unacct_size(unsigned long flags, loff_t size)",
            "{",
            "\tif (!(flags & VM_NORESERVE))",
            "\t\tvm_unacct_memory(VM_ACCT(size));",
            "}",
            "static inline int shmem_reacct_size(unsigned long flags,",
            "\t\tloff_t oldsize, loff_t newsize)",
            "{",
            "\tif (!(flags & VM_NORESERVE)) {",
            "\t\tif (VM_ACCT(newsize) > VM_ACCT(oldsize))",
            "\t\t\treturn security_vm_enough_memory_mm(current->mm,",
            "\t\t\t\t\tVM_ACCT(newsize) - VM_ACCT(oldsize));",
            "\t\telse if (VM_ACCT(newsize) < VM_ACCT(oldsize))",
            "\t\t\tvm_unacct_memory(VM_ACCT(oldsize) - VM_ACCT(newsize));",
            "\t}",
            "\treturn 0;",
            "}",
            "static inline int shmem_acct_blocks(unsigned long flags, long pages)",
            "{",
            "\tif (!(flags & VM_NORESERVE))",
            "\t\treturn 0;",
            "",
            "\treturn security_vm_enough_memory_mm(current->mm,",
            "\t\t\tpages * VM_ACCT(PAGE_SIZE));",
            "}",
            "static inline void shmem_unacct_blocks(unsigned long flags, long pages)",
            "{",
            "\tif (flags & VM_NORESERVE)",
            "\t\tvm_unacct_memory(pages * VM_ACCT(PAGE_SIZE));",
            "}",
            "static int shmem_inode_acct_blocks(struct inode *inode, long pages)",
            "{",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);",
            "\tint err = -ENOSPC;",
            "",
            "\tif (shmem_acct_blocks(info->flags, pages))",
            "\t\treturn err;",
            "",
            "\tmight_sleep();\t/* when quotas */",
            "\tif (sbinfo->max_blocks) {",
            "\t\tif (!percpu_counter_limited_add(&sbinfo->used_blocks,",
            "\t\t\t\t\t\tsbinfo->max_blocks, pages))",
            "\t\t\tgoto unacct;",
            "",
            "\t\terr = dquot_alloc_block_nodirty(inode, pages);",
            "\t\tif (err) {",
            "\t\t\tpercpu_counter_sub(&sbinfo->used_blocks, pages);",
            "\t\t\tgoto unacct;",
            "\t\t}",
            "\t} else {",
            "\t\terr = dquot_alloc_block_nodirty(inode, pages);",
            "\t\tif (err)",
            "\t\t\tgoto unacct;",
            "\t}",
            "",
            "\treturn 0;",
            "",
            "unacct:",
            "\tshmem_unacct_blocks(info->flags, pages);",
            "\treturn err;",
            "}",
            "static void shmem_inode_unacct_blocks(struct inode *inode, long pages)",
            "{",
            "\tstruct shmem_inode_info *info = SHMEM_I(inode);",
            "\tstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);",
            "",
            "\tmight_sleep();\t/* when quotas */",
            "\tdquot_free_block_nodirty(inode, pages);",
            "",
            "\tif (sbinfo->max_blocks)",
            "\t\tpercpu_counter_sub(&sbinfo->used_blocks, pages);",
            "\tshmem_unacct_blocks(info->flags, pages);",
            "}",
            "bool vma_is_anon_shmem(struct vm_area_struct *vma)",
            "{",
            "\treturn vma->vm_ops == &shmem_anon_vm_ops;",
            "}",
            "bool vma_is_shmem(struct vm_area_struct *vma)",
            "{",
            "\treturn vma_is_anon_shmem(vma) || vma->vm_ops == &shmem_vm_ops;",
            "}",
            "static int shmem_enable_quotas(struct super_block *sb,",
            "\t\t\t       unsigned short quota_types)",
            "{",
            "\tint type, err = 0;",
            "",
            "\tsb_dqopt(sb)->flags |= DQUOT_QUOTA_SYS_FILE | DQUOT_NOLIST_DIRTY;",
            "\tfor (type = 0; type < SHMEM_MAXQUOTAS; type++) {",
            "\t\tif (!(quota_types & (1 << type)))",
            "\t\t\tcontinue;",
            "\t\terr = dquot_load_quota_sb(sb, type, QFMT_SHMEM,",
            "\t\t\t\t\t  DQUOT_USAGE_ENABLED |",
            "\t\t\t\t\t  DQUOT_LIMITS_ENABLED);",
            "\t\tif (err)",
            "\t\t\tgoto out_err;",
            "\t}",
            "\treturn 0;",
            "",
            "out_err:",
            "\tpr_warn(\"tmpfs: failed to enable quota tracking (type=%d, err=%d)\\n\",",
            "\t\ttype, err);",
            "\tfor (type--; type >= 0; type--)",
            "\t\tdquot_quota_off(sb, type);",
            "\treturn err;",
            "}"
          ],
          "function_name": "shmem_default_max_blocks, shmem_default_max_inodes, shmem_acct_size, shmem_unacct_size, shmem_reacct_size, shmem_acct_blocks, shmem_unacct_blocks, shmem_inode_acct_blocks, shmem_inode_unacct_blocks, vma_is_anon_shmem, vma_is_shmem, shmem_enable_quotas",
          "description": "实现shmem文件系统的资源会计逻辑（如shmem_acct_size、shmem_inode_acct_blocks）、VMA类型判断（vma_is_anon_shmem）、配额启用（shmem_enable_quotas）等功能，用于控制内存和配额限制。",
          "similarity": 0.5795838832855225
        }
      ]
    },
    {
      "source_file": "mm/memory_hotplug.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:43:14\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memory_hotplug.c`\n\n---\n\n# memory_hotplug.c 技术文档\n\n## 1. 文件概述\n\n`memory_hotplug.c` 是 Linux 内核中实现内存热插拔（Memory Hotplug）功能的核心源文件，位于 `mm/` 子系统目录下。该文件提供了在系统运行时动态添加或移除物理内存区域的能力，包括内存资源注册、页表映射管理、内存上线策略控制、以及与 NUMA 架构的协同支持。它主要处理热添加内存时的初始化、内存块（memory block）管理、vmemmap 映射优化、以及在线策略配置等关键逻辑。\n\n## 2. 核心功能\n\n### 主要全局变量与参数\n- `memmap_mode`：控制是否启用“内存上的 memmap”（memmap on memory）特性，支持 `disable`、`enable` 和 `force` 三种模式。\n- `online_policy`：定义内存上线时的默认区域分配策略，可选 `contig-zones`（保持区域连续）或 `auto-movable`（自动分配到 ZONE_MOVABLE）。\n- `auto_movable_ratio`：在 `auto-movable` 策略下，系统允许的 MOVABLE 与 KERNEL 内存的最大百分比比例（默认 301%，即约 3:1）。\n- `auto_movable_numa_aware`（仅 CONFIG_NUMA）：是否在 `auto-movable` 策略中考虑 NUMA 节点级别的内存统计。\n- `mhp_default_online_type`：内存热插拔时的默认上线类型（如 `MMOP_ONLINE_KERNEL`、`MMOP_ONLINE_MOVABLE` 等）。\n- `movable_node_enabled`：标志是否启用了可移动节点（movable node）功能。\n- `max_mem_size`：系统允许的最大内存大小上限（默认为 `U64_MAX`）。\n\n### 主要函数与接口\n- `get_online_mems()` / `put_online_mems()`：获取/释放内存热插拔读锁，用于保护内存上线/下线操作。\n- `mem_hotplug_begin()` / `mem_hotplug_done()`：执行内存热插拔写操作前后的同步原语，同时持有 CPU 热插拔读锁和内存热插拔写锁。\n- `mhp_get_default_online_type()` / `mhp_set_default_online_type()`：获取或设置内存热插拔的默认上线类型。\n- `register_memory_resource()`：将新添加的内存区域注册为 I/O 资源（`System RAM` 类型），并检查是否超出 `max_mem_size` 限制。\n- `mhp_memmap_on_memory()`：判断当前是否启用了 memmap on memory 特性。\n- `memory_block_memmap_on_memory_pages()`：计算在 memmap on memory 模式下，每个内存块所需的额外页数（可能因对齐而浪费内存）。\n\n### 回调机制\n- `online_page_callback`：指向当前用于上线单个页面的回调函数，默认为 `generic_online_page`。\n- `set_online_page_callback()` / `restore_online_page_callback()`（声明未在片段中，但有注释说明）：用于动态替换或恢复页面上线回调。\n\n### 内核参数（module_param）\n- `memmap_on_memory`：启用 memmap on memory 功能（Y/N/force）。\n- `online_policy`：设置默认上线策略。\n- `auto_movable_ratio`：设置 MOVABLE/KERNEL 内存比例上限。\n- `auto_movable_numa_aware`：是否在 NUMA 感知下应用 auto-movable 策略。\n- 启动参数 `memhp_default_state=`：通过内核命令行设置默认上线状态。\n\n## 3. 关键实现\n\n### Memmap on Memory 机制\n当启用 `CONFIG_MHP_MEMMAP_ON_MEMORY` 时，内核尝试将描述物理页的 `struct page` 数组（即 vmemmap）直接放置在待热插拔的内存区域内，而非依赖预先保留的虚拟地址空间。这减少了对固定 vmemmap 区域的依赖，提升灵活性：\n- **ENABLE 模式**：仅当 vmemmap 大小能被页块（pageblock）整除时才启用。\n- **FORCE 模式**：强制对齐到页块边界，即使造成内存浪费（通过 `pageblock_align()` 实现），确保总能使用该内存区域存放 memmap。\n\n### 内存上线策略\n- **contig-zones（默认）**：将新内存添加到现有内存区域末尾，保持 ZONE_NORMAL 等区域的物理连续性。\n- **auto-movable**：根据全局（及 NUMA 节点）的 KERNEL 与 MOVABLE 内存比例，智能决定是否将新内存加入 ZONE_MOVABLE，以提高内存可迁移性和碎片整理效率。比例由 `auto_movable_ratio` 控制。\n\n### 并发控制\n使用 `percpu_rwsem mem_hotplug_lock` 作为内存热插拔操作的主同步机制：\n- 读操作（如内存访问路径）调用 `get/put_online_mems()` 获取读锁。\n- 写操作（如 add_memory）调用 `mem_hotplug_begin/done()` 获取写锁，并同时持有 `cpus_read_lock()` 防止 CPU 热插拔干扰。\n\n### 资源与大小限制\n- 通过 `mhp_range_allowed()` 检查待添加内存是否超出 `max_mem_size`。\n- 使用 `register_memory_resource()` 将内存注册为 `IORESOURCE_SYSTEM_RAM` 资源，若资源名非 \"System RAM\" 则标记为驱动管理（`IORESOURCE_SYSRAM_DRIVER_MANAGED`）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `mm.h`、`page-isolation.h`、`migrate.h`、`compaction.h` 等，用于页面分配、隔离、迁移和压缩。\n- **体系结构相关**：包含 `asm/tlbflush.h` 用于 TLB 刷新；依赖 `pfn.h`、`memblock.h` 处理物理页帧和启动内存布局。\n- **设备模型与 sysfs**：通过 `memory.h` 与用户空间交互（如 `/sys/devices/system/memory/`）。\n- **NUMA 支持**：在 `CONFIG_NUMA` 下使用节点感知策略。\n- **虚拟内存**：依赖 `vmalloc.h` 和 `memremap.h` 管理 vmemmap 映射。\n- **电源管理**：包含 `suspend.h`，可能与休眠/唤醒流程协调。\n- **固件接口**：使用 `firmware-map.h` 与平台固件交互内存布局信息。\n\n## 5. 使用场景\n\n- **物理内存热添加**：在支持内存热插拔的服务器（如 IBM Power、x86 ACPI 系统）上，动态增加 DIMM 或内存模块后，内核通过此文件完成内存初始化和上线。\n- **虚拟化环境**：KVM、Xen 等 hypervisor 向客户机热添加内存时，客户机内核调用此模块处理新增内存。\n- **内存故障恢复**：在某些 RAS（Reliability, Availability, Serviceability）场景中，隔离坏页后重新上线备用内存。\n- **测试与开发**：通过 sysfs 接口（如 `echo online > /sys/devices/system/memory/memoryX/state`）手动上线内存块，配合 `online_policy` 和 `memmap_on_memory` 参数进行功能验证。\n- **容器与云平台**：支持弹性内存扩展，按需分配物理内存资源。",
      "similarity": 0.581162691116333,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 52,
          "end_line": 153,
          "content": [
            "static inline unsigned long memory_block_memmap_size(void)",
            "{",
            "\treturn PHYS_PFN(memory_block_size_bytes()) * sizeof(struct page);",
            "}",
            "static inline unsigned long memory_block_memmap_on_memory_pages(void)",
            "{",
            "\tunsigned long nr_pages = PFN_UP(memory_block_memmap_size());",
            "",
            "\t/*",
            "\t * In \"forced\" memmap_on_memory mode, we add extra pages to align the",
            "\t * vmemmap size to cover full pageblocks. That way, we can add memory",
            "\t * even if the vmemmap size is not properly aligned, however, we might waste",
            "\t * memory.",
            "\t */",
            "\tif (memmap_mode == MEMMAP_ON_MEMORY_FORCE)",
            "\t\treturn pageblock_align(nr_pages);",
            "\treturn nr_pages;",
            "}",
            "static int set_memmap_mode(const char *val, const struct kernel_param *kp)",
            "{",
            "\tint ret, mode;",
            "\tbool enabled;",
            "",
            "\tif (sysfs_streq(val, \"force\") ||  sysfs_streq(val, \"FORCE\")) {",
            "\t\tmode = MEMMAP_ON_MEMORY_FORCE;",
            "\t} else {",
            "\t\tret = kstrtobool(val, &enabled);",
            "\t\tif (ret < 0)",
            "\t\t\treturn ret;",
            "\t\tif (enabled)",
            "\t\t\tmode = MEMMAP_ON_MEMORY_ENABLE;",
            "\t\telse",
            "\t\t\tmode = MEMMAP_ON_MEMORY_DISABLE;",
            "\t}",
            "\t*((int *)kp->arg) = mode;",
            "\tif (mode == MEMMAP_ON_MEMORY_FORCE) {",
            "\t\tunsigned long memmap_pages = memory_block_memmap_on_memory_pages();",
            "",
            "\t\tpr_info_once(\"Memory hotplug will waste %ld pages in each memory block\\n\",",
            "\t\t\t     memmap_pages - PFN_UP(memory_block_memmap_size()));",
            "\t}",
            "\treturn 0;",
            "}",
            "static int get_memmap_mode(char *buffer, const struct kernel_param *kp)",
            "{",
            "\tint mode = *((int *)kp->arg);",
            "",
            "\tif (mode == MEMMAP_ON_MEMORY_FORCE)",
            "\t\treturn sprintf(buffer, \"force\\n\");",
            "\treturn sprintf(buffer, \"%c\\n\", mode ? 'Y' : 'N');",
            "}",
            "static inline bool mhp_memmap_on_memory(void)",
            "{",
            "\treturn memmap_mode != MEMMAP_ON_MEMORY_DISABLE;",
            "}",
            "static inline bool mhp_memmap_on_memory(void)",
            "{",
            "\treturn false;",
            "}",
            "static int set_online_policy(const char *val, const struct kernel_param *kp)",
            "{",
            "\tint ret = sysfs_match_string(online_policy_to_str, val);",
            "",
            "\tif (ret < 0)",
            "\t\treturn ret;",
            "\t*((int *)kp->arg) = ret;",
            "\treturn 0;",
            "}",
            "static int get_online_policy(char *buffer, const struct kernel_param *kp)",
            "{",
            "\treturn sprintf(buffer, \"%s\\n\", online_policy_to_str[*((int *)kp->arg)]);",
            "}",
            "void get_online_mems(void)",
            "{",
            "\tpercpu_down_read(&mem_hotplug_lock);",
            "}",
            "void put_online_mems(void)",
            "{",
            "\tpercpu_up_read(&mem_hotplug_lock);",
            "}",
            "int mhp_get_default_online_type(void)",
            "{",
            "\tif (mhp_default_online_type >= 0)",
            "\t\treturn mhp_default_online_type;",
            "",
            "\tif (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_OFFLINE))",
            "\t\tmhp_default_online_type = MMOP_OFFLINE;",
            "\telse if (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_ONLINE_AUTO))",
            "\t\tmhp_default_online_type = MMOP_ONLINE;",
            "\telse if (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_ONLINE_KERNEL))",
            "\t\tmhp_default_online_type = MMOP_ONLINE_KERNEL;",
            "\telse if (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_ONLINE_MOVABLE))",
            "\t\tmhp_default_online_type = MMOP_ONLINE_MOVABLE;",
            "\telse",
            "\t\tmhp_default_online_type = MMOP_OFFLINE;",
            "",
            "\treturn mhp_default_online_type;",
            "}",
            "void mhp_set_default_online_type(int online_type)",
            "{",
            "\tmhp_default_online_type = online_type;",
            "}"
          ],
          "function_name": "memory_block_memmap_size, memory_block_memmap_on_memory_pages, set_memmap_mode, get_memmap_mode, mhp_memmap_on_memory, mhp_memmap_on_memory, set_online_policy, get_online_policy, get_online_mems, put_online_mems, mhp_get_default_online_type, mhp_set_default_online_type",
          "description": "提供内存映射模式配置接口(set/get)，实现memmap_on_memory策略判断逻辑，包含在线策略设置与获取函数及默认类型处理逻辑。",
          "similarity": 0.5682441592216492
        },
        {
          "chunk_id": 0,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 1,
          "end_line": 51,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " *  linux/mm/memory_hotplug.c",
            " *",
            " *  Copyright (C)",
            " */",
            "",
            "#include <linux/stddef.h>",
            "#include <linux/mm.h>",
            "#include <linux/sched/signal.h>",
            "#include <linux/swap.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/compiler.h>",
            "#include <linux/export.h>",
            "#include <linux/writeback.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/cpu.h>",
            "#include <linux/memory.h>",
            "#include <linux/memremap.h>",
            "#include <linux/memory_hotplug.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/ioport.h>",
            "#include <linux/delay.h>",
            "#include <linux/migrate.h>",
            "#include <linux/page-isolation.h>",
            "#include <linux/pfn.h>",
            "#include <linux/suspend.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/firmware-map.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/memblock.h>",
            "#include <linux/compaction.h>",
            "#include <linux/rmap.h>",
            "#include <linux/module.h>",
            "",
            "#include <asm/tlbflush.h>",
            "",
            "#include \"internal.h\"",
            "#include \"shuffle.h\"",
            "",
            "enum {",
            "\tMEMMAP_ON_MEMORY_DISABLE = 0,",
            "\tMEMMAP_ON_MEMORY_ENABLE,",
            "\tMEMMAP_ON_MEMORY_FORCE,",
            "};",
            "",
            "static int memmap_mode __read_mostly = MEMMAP_ON_MEMORY_DISABLE;",
            ""
          ],
          "function_name": null,
          "description": "声明内存热插拔相关枚举常量和全局变量，定义memmap_mode控制内存映射策略，包含必要的内核头文件和模块化实现依赖项。",
          "similarity": 0.5678389668464661
        },
        {
          "chunk_id": 6,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 855,
          "end_line": 970,
          "content": [
            "static bool auto_movable_can_online_movable(int nid, struct memory_group *group,",
            "\t\t\t\t\t    unsigned long nr_pages)",
            "{",
            "\tunsigned long kernel_early_pages, movable_pages;",
            "\tstruct auto_movable_group_stats group_stats = {};",
            "\tstruct auto_movable_stats stats = {};",
            "\tpg_data_t *pgdat = NODE_DATA(nid);",
            "\tstruct zone *zone;",
            "\tint i;",
            "",
            "\t/* Walk all relevant zones and collect MOVABLE vs. KERNEL stats. */",
            "\tif (nid == NUMA_NO_NODE) {",
            "\t\t/* TODO: cache values */",
            "\t\tfor_each_populated_zone(zone)",
            "\t\t\tauto_movable_stats_account_zone(&stats, zone);",
            "\t} else {",
            "\t\tfor (i = 0; i < MAX_NR_ZONES; i++) {",
            "\t\t\tzone = pgdat->node_zones + i;",
            "\t\t\tif (populated_zone(zone))",
            "\t\t\t\tauto_movable_stats_account_zone(&stats, zone);",
            "\t\t}",
            "\t}",
            "",
            "\tkernel_early_pages = stats.kernel_early_pages;",
            "\tmovable_pages = stats.movable_pages;",
            "",
            "\t/*",
            "\t * Kernel memory inside dynamic memory group allows for more MOVABLE",
            "\t * memory within the same group. Remove the effect of all but the",
            "\t * current group from the stats.",
            "\t */",
            "\twalk_dynamic_memory_groups(nid, auto_movable_stats_account_group,",
            "\t\t\t\t   group, &group_stats);",
            "\tif (kernel_early_pages <= group_stats.req_kernel_early_pages)",
            "\t\treturn false;",
            "\tkernel_early_pages -= group_stats.req_kernel_early_pages;",
            "\tmovable_pages -= group_stats.movable_pages;",
            "",
            "\tif (group && group->is_dynamic)",
            "\t\tkernel_early_pages += group->present_kernel_pages;",
            "",
            "\t/*",
            "\t * Test if we could online the given number of pages to ZONE_MOVABLE",
            "\t * and still stay in the configured ratio.",
            "\t */",
            "\tmovable_pages += nr_pages;",
            "\treturn movable_pages <= (auto_movable_ratio * kernel_early_pages) / 100;",
            "}",
            "void adjust_present_page_count(struct page *page, struct memory_group *group,",
            "\t\t\t       long nr_pages)",
            "{",
            "\tstruct zone *zone = page_zone(page);",
            "\tconst bool movable = zone_idx(zone) == ZONE_MOVABLE;",
            "",
            "\t/*",
            "\t * We only support onlining/offlining/adding/removing of complete",
            "\t * memory blocks; therefore, either all is either early or hotplugged.",
            "\t */",
            "\tif (early_section(__pfn_to_section(page_to_pfn(page))))",
            "\t\tzone->present_early_pages += nr_pages;",
            "\tzone->present_pages += nr_pages;",
            "\tzone->zone_pgdat->node_present_pages += nr_pages;",
            "",
            "\tif (group && movable)",
            "\t\tgroup->present_movable_pages += nr_pages;",
            "\telse if (group && !movable)",
            "\t\tgroup->present_kernel_pages += nr_pages;",
            "}",
            "int mhp_init_memmap_on_memory(unsigned long pfn, unsigned long nr_pages,",
            "\t\t\t      struct zone *zone)",
            "{",
            "\tunsigned long end_pfn = pfn + nr_pages;",
            "\tint ret, i;",
            "",
            "\tret = kasan_add_zero_shadow(__va(PFN_PHYS(pfn)), PFN_PHYS(nr_pages));",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tmove_pfn_range_to_zone(zone, pfn, nr_pages, NULL, MIGRATE_UNMOVABLE);",
            "",
            "\tfor (i = 0; i < nr_pages; i++) {",
            "\t\tstruct page *page = pfn_to_page(pfn + i);",
            "",
            "\t\t__ClearPageOffline(page);",
            "\t\tSetPageVmemmapSelfHosted(page);",
            "\t}",
            "",
            "\t/*",
            "\t * It might be that the vmemmap_pages fully span sections. If that is",
            "\t * the case, mark those sections online here as otherwise they will be",
            "\t * left offline.",
            "\t */",
            "\tif (nr_pages >= PAGES_PER_SECTION)",
            "\t        online_mem_sections(pfn, ALIGN_DOWN(end_pfn, PAGES_PER_SECTION));",
            "",
            "\treturn ret;",
            "}",
            "void mhp_deinit_memmap_on_memory(unsigned long pfn, unsigned long nr_pages)",
            "{",
            "\tunsigned long end_pfn = pfn + nr_pages;",
            "",
            "\t/*",
            "\t * It might be that the vmemmap_pages fully span sections. If that is",
            "\t * the case, mark those sections offline here as otherwise they will be",
            "\t * left online.",
            "\t */",
            "\tif (nr_pages >= PAGES_PER_SECTION)",
            "\t\toffline_mem_sections(pfn, ALIGN_DOWN(end_pfn, PAGES_PER_SECTION));",
            "",
            "        /*",
            "\t * The pages associated with this vmemmap have been offlined, so",
            "\t * we can reset its state here.",
            "\t */",
            "\tremove_pfn_range_from_zone(page_zone(pfn_to_page(pfn)), pfn, nr_pages);",
            "\tkasan_remove_zero_shadow(__va(PFN_PHYS(pfn)), PFN_PHYS(nr_pages));",
            "}"
          ],
          "function_name": "auto_movable_can_online_movable, adjust_present_page_count, mhp_init_memmap_on_memory, mhp_deinit_memmap_on_memory",
          "description": "auto_movable_can_online_movable 判断是否可添加可移动页面；adjust_present_page_count 更新zone的present_pages等统计；mhp_init_memmap_on_memory 初始化内存映射并标记为在线；mhp_deinit_memmap_on_memory 清理内存映射并标记为离线",
          "similarity": 0.5616291761398315
        },
        {
          "chunk_id": 9,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 1529,
          "end_line": 1660,
          "content": [
            "int __ref __add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags)",
            "{",
            "\tstruct resource *res;",
            "\tint ret;",
            "",
            "\tres = register_memory_resource(start, size, \"System RAM\");",
            "\tif (IS_ERR(res))",
            "\t\treturn PTR_ERR(res);",
            "",
            "\tret = add_memory_resource(nid, res, mhp_flags);",
            "\tif (ret < 0)",
            "\t\trelease_memory_resource(res);",
            "\treturn ret;",
            "}",
            "int add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags)",
            "{",
            "\tint rc;",
            "",
            "\tlock_device_hotplug();",
            "\trc = __add_memory(nid, start, size, mhp_flags);",
            "\tunlock_device_hotplug();",
            "",
            "\treturn rc;",
            "}",
            "int add_memory_driver_managed(int nid, u64 start, u64 size,",
            "\t\t\t      const char *resource_name, mhp_t mhp_flags)",
            "{",
            "\tstruct resource *res;",
            "\tint rc;",
            "",
            "\tif (!resource_name ||",
            "\t    strstr(resource_name, \"System RAM (\") != resource_name ||",
            "\t    resource_name[strlen(resource_name) - 1] != ')')",
            "\t\treturn -EINVAL;",
            "",
            "\tlock_device_hotplug();",
            "",
            "\tres = register_memory_resource(start, size, resource_name);",
            "\tif (IS_ERR(res)) {",
            "\t\trc = PTR_ERR(res);",
            "\t\tgoto out_unlock;",
            "\t}",
            "",
            "\trc = add_memory_resource(nid, res, mhp_flags);",
            "\tif (rc < 0)",
            "\t\trelease_memory_resource(res);",
            "",
            "out_unlock:",
            "\tunlock_device_hotplug();",
            "\treturn rc;",
            "}",
            "struct range __weak arch_get_mappable_range(void)",
            "{",
            "\tstruct range mhp_range = {",
            "\t\t.start = 0UL,",
            "\t\t.end = -1ULL,",
            "\t};",
            "\treturn mhp_range;",
            "}",
            "struct range mhp_get_pluggable_range(bool need_mapping)",
            "{",
            "\tconst u64 max_phys = PHYSMEM_END;",
            "\tstruct range mhp_range;",
            "",
            "\tif (need_mapping) {",
            "\t\tmhp_range = arch_get_mappable_range();",
            "\t\tif (mhp_range.start > max_phys) {",
            "\t\t\tmhp_range.start = 0;",
            "\t\t\tmhp_range.end = 0;",
            "\t\t}",
            "\t\tmhp_range.end = min_t(u64, mhp_range.end, max_phys);",
            "\t} else {",
            "\t\tmhp_range.start = 0;",
            "\t\tmhp_range.end = max_phys;",
            "\t}",
            "\treturn mhp_range;",
            "}",
            "bool mhp_range_allowed(u64 start, u64 size, bool need_mapping)",
            "{",
            "\tstruct range mhp_range = mhp_get_pluggable_range(need_mapping);",
            "\tu64 end = start + size;",
            "",
            "\tif (start < end && start >= mhp_range.start && (end - 1) <= mhp_range.end)",
            "\t\treturn true;",
            "",
            "\tpr_warn(\"Hotplug memory [%#llx-%#llx] exceeds maximum addressable range [%#llx-%#llx]\\n\",",
            "\t\tstart, end, mhp_range.start, mhp_range.end);",
            "\treturn false;",
            "}",
            "static int scan_movable_pages(unsigned long start, unsigned long end,",
            "\t\t\t      unsigned long *movable_pfn)",
            "{",
            "\tunsigned long pfn;",
            "",
            "\tfor_each_valid_pfn(pfn, start, end) {",
            "\t\tstruct page *page;",
            "\t\tstruct folio *folio;",
            "",
            "\t\tpage = pfn_to_page(pfn);",
            "\t\tif (PageLRU(page))",
            "\t\t\tgoto found;",
            "\t\tif (__PageMovable(page))",
            "\t\t\tgoto found;",
            "",
            "\t\t/*",
            "\t\t * PageOffline() pages that are not marked __PageMovable() and",
            "\t\t * have a reference count > 0 (after MEM_GOING_OFFLINE) are",
            "\t\t * definitely unmovable. If their reference count would be 0,",
            "\t\t * they could at least be skipped when offlining memory.",
            "\t\t */",
            "\t\tif (PageOffline(page) && page_count(page))",
            "\t\t\treturn -EBUSY;",
            "",
            "\t\tif (!PageHuge(page))",
            "\t\t\tcontinue;",
            "\t\tfolio = page_folio(page);",
            "\t\t/*",
            "\t\t * This test is racy as we hold no reference or lock.  The",
            "\t\t * hugetlb page could have been free'ed and head is no longer",
            "\t\t * a hugetlb page before the following check.  In such unlikely",
            "\t\t * cases false positives and negatives are possible.  Calling",
            "\t\t * code must deal with these scenarios.",
            "\t\t */",
            "\t\tif (folio_test_hugetlb_migratable(folio))",
            "\t\t\tgoto found;",
            "\t\tpfn |= folio_nr_pages(folio) - 1;",
            "\t}",
            "\treturn -ENOENT;",
            "found:",
            "\t*movable_pfn = pfn;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "__add_memory, add_memory, add_memory_driver_managed, arch_get_mappable_range, mhp_get_pluggable_range, mhp_range_allowed, scan_movable_pages",
          "description": "__add_memory 和 add_memory 添加内存资源；add_memory_driver_managed 处理驱动管理内存资源；arch_get_mappable_range 获取可映射范围；mhp_get_pluggable_range 确定可插入内存范围；mhp_range_allowed 检查地址有效性；scan_movable_pages 扫描可移动页面",
          "similarity": 0.5580405592918396
        },
        {
          "chunk_id": 8,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 1306,
          "end_line": 1513,
          "content": [
            "int try_online_node(int nid)",
            "{",
            "\tint ret;",
            "",
            "\tmem_hotplug_begin();",
            "\tret =  __try_online_node(nid, true);",
            "\tmem_hotplug_done();",
            "\treturn ret;",
            "}",
            "static int check_hotplug_memory_range(u64 start, u64 size)",
            "{",
            "\t/* memory range must be block size aligned */",
            "\tif (!size || !IS_ALIGNED(start, memory_block_size_bytes()) ||",
            "\t    !IS_ALIGNED(size, memory_block_size_bytes())) {",
            "\t\tpr_err(\"Block size [%#lx] unaligned hotplug range: start %#llx, size %#llx\",",
            "\t\t       memory_block_size_bytes(), start, size);",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int online_memory_block(struct memory_block *mem, void *arg)",
            "{",
            "\tmem->online_type = mhp_get_default_online_type();",
            "\treturn device_online(&mem->dev);",
            "}",
            "static inline bool arch_supports_memmap_on_memory(unsigned long vmemmap_size)",
            "{",
            "\t/*",
            "\t * As default, we want the vmemmap to span a complete PMD such that we",
            "\t * can map the vmemmap using a single PMD if supported by the",
            "\t * architecture.",
            "\t */",
            "\treturn IS_ALIGNED(vmemmap_size, PMD_SIZE);",
            "}",
            "static bool mhp_supports_memmap_on_memory(unsigned long size)",
            "{",
            "\tunsigned long vmemmap_size = memory_block_memmap_size();",
            "\tunsigned long memmap_pages = memory_block_memmap_on_memory_pages();",
            "",
            "\t/*",
            "\t * Besides having arch support and the feature enabled at runtime, we",
            "\t * need a few more assumptions to hold true:",
            "\t *",
            "\t * a) We span a single memory block: memory onlining/offlinin;g happens",
            "\t *    in memory block granularity. We don't want the vmemmap of online",
            "\t *    memory blocks to reside on offline memory blocks. In the future,",
            "\t *    we might want to support variable-sized memory blocks to make the",
            "\t *    feature more versatile.",
            "\t *",
            "\t * b) The vmemmap pages span complete PMDs: We don't want vmemmap code",
            "\t *    to populate memory from the altmap for unrelated parts (i.e.,",
            "\t *    other memory blocks)",
            "\t *",
            "\t * c) The vmemmap pages (and thereby the pages that will be exposed to",
            "\t *    the buddy) have to cover full pageblocks: memory onlining/offlining",
            "\t *    code requires applicable ranges to be page-aligned, for example, to",
            "\t *    set the migratetypes properly.",
            "\t *",
            "\t * TODO: Although we have a check here to make sure that vmemmap pages",
            "\t *       fully populate a PMD, it is not the right place to check for",
            "\t *       this. A much better solution involves improving vmemmap code",
            "\t *       to fallback to base pages when trying to populate vmemmap using",
            "\t *       altmap as an alternative source of memory, and we do not exactly",
            "\t *       populate a single PMD.",
            "\t */",
            "\tif (!mhp_memmap_on_memory() || size != memory_block_size_bytes())",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Make sure the vmemmap allocation is fully contained",
            "\t * so that we always allocate vmemmap memory from altmap area.",
            "\t */",
            "\tif (!IS_ALIGNED(vmemmap_size, PAGE_SIZE))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * start pfn should be pageblock_nr_pages aligned for correctly",
            "\t * setting migrate types",
            "\t */",
            "\tif (!pageblock_aligned(memmap_pages))",
            "\t\treturn false;",
            "",
            "\tif (memmap_pages == PHYS_PFN(memory_block_size_bytes()))",
            "\t\t/* No effective hotplugged memory doesn't make sense. */",
            "\t\treturn false;",
            "",
            "\treturn arch_supports_memmap_on_memory(vmemmap_size);",
            "}",
            "int __ref add_memory_resource(int nid, struct resource *res, mhp_t mhp_flags)",
            "{",
            "\tstruct mhp_params params = { .pgprot = pgprot_mhp(PAGE_KERNEL) };",
            "\tenum memblock_flags memblock_flags = MEMBLOCK_NONE;",
            "\tstruct vmem_altmap mhp_altmap = {",
            "\t\t.base_pfn =  PHYS_PFN(res->start),",
            "\t\t.end_pfn  =  PHYS_PFN(res->end),",
            "\t};",
            "\tstruct memory_group *group = NULL;",
            "\tu64 start, size;",
            "\tbool new_node = false;",
            "\tint ret;",
            "",
            "\tstart = res->start;",
            "\tsize = resource_size(res);",
            "",
            "\tret = check_hotplug_memory_range(start, size);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tif (mhp_flags & MHP_NID_IS_MGID) {",
            "\t\tgroup = memory_group_find_by_id(nid);",
            "\t\tif (!group)",
            "\t\t\treturn -EINVAL;",
            "\t\tnid = group->nid;",
            "\t}",
            "",
            "\tif (!node_possible(nid)) {",
            "\t\tWARN(1, \"node %d was absent from the node_possible_map\\n\", nid);",
            "\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tmem_hotplug_begin();",
            "",
            "\tif (IS_ENABLED(CONFIG_ARCH_KEEP_MEMBLOCK)) {",
            "\t\tif (res->flags & IORESOURCE_SYSRAM_DRIVER_MANAGED)",
            "\t\t\tmemblock_flags = MEMBLOCK_DRIVER_MANAGED;",
            "\t\tret = memblock_add_node(start, size, nid, memblock_flags);",
            "\t\tif (ret)",
            "\t\t\tgoto error_mem_hotplug_end;",
            "\t}",
            "",
            "\tret = __try_online_node(nid, false);",
            "\tif (ret < 0)",
            "\t\tgoto error;",
            "\tnew_node = ret;",
            "",
            "\t/*",
            "\t * Self hosted memmap array",
            "\t */",
            "\tif (mhp_flags & MHP_MEMMAP_ON_MEMORY) {",
            "\t\tif (mhp_supports_memmap_on_memory(size)) {",
            "\t\t\tmhp_altmap.free = memory_block_memmap_on_memory_pages();",
            "\t\t\tparams.altmap = kmalloc(sizeof(struct vmem_altmap), GFP_KERNEL);",
            "\t\t\tif (!params.altmap) {",
            "\t\t\t\tret = -ENOMEM;",
            "\t\t\t\tgoto error;",
            "\t\t\t}",
            "",
            "\t\t\tmemcpy(params.altmap, &mhp_altmap, sizeof(mhp_altmap));",
            "\t\t}",
            "\t\t/* fallback to not using altmap  */",
            "\t}",
            "",
            "\t/* call arch's memory hotadd */",
            "\tret = arch_add_memory(nid, start, size, &params);",
            "\tif (ret < 0)",
            "\t\tgoto error_free;",
            "",
            "\t/* create memory block devices after memory was added */",
            "\tret = create_memory_block_devices(start, size, params.altmap, group);",
            "\tif (ret) {",
            "\t\tarch_remove_memory(start, size, params.altmap);",
            "\t\tgoto error_free;",
            "\t}",
            "",
            "\tif (new_node) {",
            "\t\t/* If sysfs file of new node can't be created, cpu on the node",
            "\t\t * can't be hot-added. There is no rollback way now.",
            "\t\t * So, check by BUG_ON() to catch it reluctantly..",
            "\t\t * We online node here. We can't roll back from here.",
            "\t\t */",
            "\t\tnode_set_online(nid);",
            "\t\tret = __register_one_node(nid);",
            "\t\tBUG_ON(ret);",
            "\t}",
            "",
            "\tregister_memory_blocks_under_node(nid, PFN_DOWN(start),",
            "\t\t\t\t\t  PFN_UP(start + size - 1),",
            "\t\t\t\t\t  MEMINIT_HOTPLUG);",
            "",
            "\t/* create new memmap entry */",
            "\tif (!strcmp(res->name, \"System RAM\"))",
            "\t\tfirmware_map_add_hotplug(start, start + size, \"System RAM\");",
            "",
            "\t/* device_online() will take the lock when calling online_pages() */",
            "\tmem_hotplug_done();",
            "",
            "\t/*",
            "\t * In case we're allowed to merge the resource, flag it and trigger",
            "\t * merging now that adding succeeded.",
            "\t */",
            "\tif (mhp_flags & MHP_MERGE_RESOURCE)",
            "\t\tmerge_system_ram_resource(res);",
            "",
            "\t/* online pages if requested */",
            "\tif (mhp_get_default_online_type() != MMOP_OFFLINE)",
            "\t\twalk_memory_blocks(start, size, NULL, online_memory_block);",
            "",
            "\treturn ret;",
            "error_free:",
            "\tkfree(params.altmap);",
            "error:",
            "\tif (IS_ENABLED(CONFIG_ARCH_KEEP_MEMBLOCK))",
            "\t\tmemblock_remove(start, size);",
            "error_mem_hotplug_end:",
            "\tmem_hotplug_done();",
            "\treturn ret;",
            "}"
          ],
          "function_name": "try_online_node, check_hotplug_memory_range, online_memory_block, arch_supports_memmap_on_memory, mhp_supports_memmap_on_memory, add_memory_resource",
          "description": "try_online_node 尝试上线指定节点；check_hotplug_memory_range 验证内存范围对齐；online_memory_block 设备上线回调；arch_supports_memmap_on_memory 和 mhp_supports_memmap_on_memory 检查架构是否支持内存映射；add_memory_resource 添加内存资源并处理设备注册",
          "similarity": 0.5037276744842529
        }
      ]
    }
  ]
}