{
  "query": "基于插件架构的系统可扩展性实现方案",
  "timestamp": "2025-12-26 02:01:53",
  "retrieved_files": [
    {
      "source_file": "mm/page_ext.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:01:26\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_ext.c`\n\n---\n\n# page_ext.c 技术文档\n\n## 1. 文件概述\n\n`page_ext.c` 实现了 Linux 内核中的 **页面扩展（page extension）** 机制，用于在不修改 `struct page` 结构体的前提下，为每个物理页附加额外的元数据。该机制解决了传统方式中因直接扩增 `struct page` 而导致的内核重建成本高、第三方模块兼容性差以及潜在系统行为变更等问题。\n\n页面扩展内存按需分配：仅当至少一个功能模块声明需要扩展数据时，才在启动阶段分配大块连续内存；否则完全跳过分配，避免内存浪费。该机制支持平坦内存（FLATMEM）和稀疏内存（SPARSEMEM）两种内存模型，并提供统一的访问接口。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct page_ext_operations`：客户端模块注册的回调操作集，包含：\n  - `need()`：判断是否需要为本模块分配扩展内存（必选）\n  - `init()`：扩展内存分配完成后执行的初始化函数（可选）\n  - `size`：所需额外内存大小（字节）\n  - `offset`：分配后返回的偏移量（由核心填充）\n  - `need_shared_flags`：是否需要共享基础 `struct page_ext` 结构\n\n### 主要全局变量\n- `page_ext_size`：每个页面对应的扩展数据总大小（含基础结构和各模块私有数据）\n- `total_usage`：已分配的页面扩展内存总量（字节）\n- `early_page_ext`：是否强制在早期（早于常规内存初始化）启用 page_ext（通过内核参数控制）\n\n### 主要函数\n- `invoke_need_callbacks()`：遍历所有注册的 `page_ext_ops`，调用其 `need()` 回调，决定是否分配内存并计算总大小\n- `invoke_init_callbacks()`：在 page_ext 内存分配完成后，调用各模块的 `init()` 回调进行初始化\n- `lookup_page_ext(const struct page *page)`：根据 `page` 指针查找对应的 `page_ext` 扩展数据结构（区分 FLATMEM/SPARSEMEM 实现）\n- `alloc_node_page_ext(int nid)`（FLATMEM）：为指定 NUMA 节点分配 page_ext 内存表\n- `page_ext_init_flatmem()` / `page_ext_init_flatmem_late()`（FLATMEM）：平坦内存模型下的初始化入口\n- `setup_early_page_ext()`：解析内核启动参数 `early_page_ext`\n\n## 3. 关键实现\n\n### 按需内存分配机制\n- 启动时调用 `invoke_need_callbacks()` 遍历所有注册的扩展模块。\n- 若任一模块的 `need()` 返回 `true`，则触发内存分配；否则完全跳过，零开销。\n- 对于声明 `need_shared_flags = true` 的模块（如 32 位下的 `PAGE_IDLE`），强制使用基础 `struct page_ext` 结构，确保标志位共享。\n\n### 内存布局与索引\n- 每个物理页对应一个 `page_ext` 条目，大小为 `page_ext_size`。\n- 条目按 PFN（Page Frame Number）顺序线性排列。\n- 在 FLATMEM 模型中，以节点起始 PFN 对齐到 `MAX_ORDER_NR_PAGES` 为基址，支持 buddy allocator 跨边界访问。\n- 在 SPARSEMEM 模型中，每个内存 section 独立维护 `page_ext` 数组，通过 `__pfn_to_section(pfn)` 定位。\n\n### 稀疏内存特殊处理\n- 定义 `PAGE_EXT_INVALID = 0x1` 标志位，用于标记未初始化的 section。\n- `lookup_page_ext()` 中通过低位掩码检查有效性，避免访问未分配内存。\n- 支持内存热插拔场景下动态分配 section 的 page_ext。\n\n### 早期初始化支持\n- 通过 `early_param(\"early_page_ext\", ...)` 支持内核参数强制提前初始化。\n- `CONFIG_MEM_ALLOC_PROFILING_DEBUG` 下默认启用，确保分配标签（如 task stack）不丢失。\n\n### 安全访问保障\n- `lookup_page_ext()` 中包含 `WARN_ON_ONCE(!rcu_read_lock_held())`，要求调用者持有 RCU 读锁，防止并发释放期间访问悬空指针。\n- 处理内存子系统早期初始化阶段（如 boot-time page free）可能触发的空指针访问，安全返回 `NULL`。\n\n## 4. 依赖关系\n\n### 编译依赖（Kconfig）\n- `CONFIG_PAGE_OWNER`：页面归属跟踪\n- `CONFIG_PAGE_IDLE_FLAG`：页面空闲标记（32 位架构）\n- `CONFIG_MEM_ALLOC_PROFILING`：内存分配打标（PGALLOC_TAG）\n- `CONFIG_PAGE_TABLE_CHECK`：页表一致性检查\n- `CONFIG_SPARSEMEM`：稀疏内存模型支持\n\n### 头文件依赖\n- `<linux/mm.h>` / `<linux/mmzone.h>`：内存管理核心结构\n- `<linux/memblock.h>`：启动阶段内存分配\n- `<linux/vmalloc.h>`：备用分配路径（未在当前片段体现）\n- `<linux/rcupdate.h>`：RCU 锁定语义\n- 各功能模块头文件（如 `page_owner.h`, `page_idle.h` 等）\n\n### 运行时依赖\n- 内存初始化流程（`memmap` 建立之后）\n- NUMA 节点信息（`NODE_DATA(nid)`）\n- 稀疏内存 section 管理（`__pfn_to_section`）\n\n## 5. 使用场景\n\n1. **调试与追踪**：\n   - `PAGE_OWNER`：记录每页的分配/释放调用栈，用于内存泄漏检测\n   - `PAGE_TABLE_CHECK`：验证页表映射一致性\n\n2. **性能分析**：\n   - `PAGE_IDLE`：标记长时间未访问的页面，供内存回收或迁移策略使用（32 位架构因地址空间限制需外部存储）\n\n3. **内存分配剖析**：\n   - `MEM_ALLOC_PROFILING`（PGALLOC_TAG）：为页面打上分配上下文标签，用于内存使用归因分析\n\n4. **内核自检**：\n   - 在 buddy allocator 释放路径中校验页面状态，需访问扩展数据\n\n5. **早期内存分配保障**：\n   - 当需要在常规内存子系统完全初始化前（如 early initcall 阶段）使用带标签的页面时，通过 `early_page_ext` 参数确保 page_ext 提前就绪",
      "similarity": 0.5605758428573608,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/page_ext.c",
          "start_line": 69,
          "end_line": 170,
          "content": [
            "static bool need_page_idle(void)",
            "{",
            "\treturn true;",
            "}",
            "static int __init setup_early_page_ext(char *str)",
            "{",
            "\tearly_page_ext = true;",
            "\treturn 0;",
            "}",
            "static bool __init invoke_need_callbacks(void)",
            "{",
            "\tint i;",
            "\tint entries = ARRAY_SIZE(page_ext_ops);",
            "\tbool need = false;",
            "",
            "\tfor (i = 0; i < entries; i++) {",
            "\t\tif (page_ext_ops[i]->need()) {",
            "\t\t\tif (page_ext_ops[i]->need_shared_flags) {",
            "\t\t\t\tpage_ext_size = sizeof(struct page_ext);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\tfor (i = 0; i < entries; i++) {",
            "\t\tif (page_ext_ops[i]->need()) {",
            "\t\t\tpage_ext_ops[i]->offset = page_ext_size;",
            "\t\t\tpage_ext_size += page_ext_ops[i]->size;",
            "\t\t\tneed = true;",
            "\t\t}",
            "\t}",
            "",
            "\treturn need;",
            "}",
            "static void __init invoke_init_callbacks(void)",
            "{",
            "\tint i;",
            "\tint entries = ARRAY_SIZE(page_ext_ops);",
            "",
            "\tfor (i = 0; i < entries; i++) {",
            "\t\tif (page_ext_ops[i]->init)",
            "\t\t\tpage_ext_ops[i]->init();",
            "\t}",
            "}",
            "void __init page_ext_init_flatmem_late(void)",
            "{",
            "\tinvoke_init_callbacks();",
            "}",
            "void __meminit pgdat_page_ext_init(struct pglist_data *pgdat)",
            "{",
            "\tpgdat->node_page_ext = NULL;",
            "}",
            "static int __init alloc_node_page_ext(int nid)",
            "{",
            "\tstruct page_ext *base;",
            "\tunsigned long table_size;",
            "\tunsigned long nr_pages;",
            "",
            "\tnr_pages = NODE_DATA(nid)->node_spanned_pages;",
            "\tif (!nr_pages)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * Need extra space if node range is not aligned with",
            "\t * MAX_ORDER_NR_PAGES. When page allocator's buddy algorithm",
            "\t * checks buddy's status, range could be out of exact node range.",
            "\t */",
            "\tif (!IS_ALIGNED(node_start_pfn(nid), MAX_ORDER_NR_PAGES) ||",
            "\t\t!IS_ALIGNED(node_end_pfn(nid), MAX_ORDER_NR_PAGES))",
            "\t\tnr_pages += MAX_ORDER_NR_PAGES;",
            "",
            "\ttable_size = page_ext_size * nr_pages;",
            "",
            "\tbase = memblock_alloc_try_nid(",
            "\t\t\ttable_size, PAGE_SIZE, __pa(MAX_DMA_ADDRESS),",
            "\t\t\tMEMBLOCK_ALLOC_ACCESSIBLE, nid);",
            "\tif (!base)",
            "\t\treturn -ENOMEM;",
            "\tNODE_DATA(nid)->node_page_ext = base;",
            "\ttotal_usage += table_size;",
            "\treturn 0;",
            "}",
            "void __init page_ext_init_flatmem(void)",
            "{",
            "",
            "\tint nid, fail;",
            "",
            "\tif (!invoke_need_callbacks())",
            "\t\treturn;",
            "",
            "\tfor_each_online_node(nid)  {",
            "\t\tfail = alloc_node_page_ext(nid);",
            "\t\tif (fail)",
            "\t\t\tgoto fail;",
            "\t}",
            "\tpr_info(\"allocated %ld bytes of page_ext\\n\", total_usage);",
            "\treturn;",
            "",
            "fail:",
            "\tpr_crit(\"allocation of page_ext failed.\\n\");",
            "\tpanic(\"Out of memory\");",
            "}"
          ],
          "function_name": "need_page_idle, setup_early_page_ext, invoke_need_callbacks, invoke_init_callbacks, page_ext_init_flatmem_late, pgdat_page_ext_init, alloc_node_page_ext, page_ext_init_flatmem",
          "description": "实现页面扩展内存分配逻辑，包含need回调检查、早期扩展启用设置、初始化回调触发、节点级扩展内存分配及失败处理，通过遍历所有节点完成扩展内存表的创建与错误恢复。",
          "similarity": 0.6048189997673035
        },
        {
          "chunk_id": 0,
          "file_path": "mm/page_ext.c",
          "start_line": 1,
          "end_line": 68,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/page_ext.h>",
            "#include <linux/memory.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/kmemleak.h>",
            "#include <linux/page_owner.h>",
            "#include <linux/page_idle.h>",
            "#include <linux/page_table_check.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/pgalloc_tag.h>",
            "",
            "/*",
            " * struct page extension",
            " *",
            " * This is the feature to manage memory for extended data per page.",
            " *",
            " * Until now, we must modify struct page itself to store extra data per page.",
            " * This requires rebuilding the kernel and it is really time consuming process.",
            " * And, sometimes, rebuild is impossible due to third party module dependency.",
            " * At last, enlarging struct page could cause un-wanted system behaviour change.",
            " *",
            " * This feature is intended to overcome above mentioned problems. This feature",
            " * allocates memory for extended data per page in certain place rather than",
            " * the struct page itself. This memory can be accessed by the accessor",
            " * functions provided by this code. During the boot process, it checks whether",
            " * allocation of huge chunk of memory is needed or not. If not, it avoids",
            " * allocating memory at all. With this advantage, we can include this feature",
            " * into the kernel in default and can avoid rebuild and solve related problems.",
            " *",
            " * To help these things to work well, there are two callbacks for clients. One",
            " * is the need callback which is mandatory if user wants to avoid useless",
            " * memory allocation at boot-time. The other is optional, init callback, which",
            " * is used to do proper initialization after memory is allocated.",
            " *",
            " * The need callback is used to decide whether extended memory allocation is",
            " * needed or not. Sometimes users want to deactivate some features in this",
            " * boot and extra memory would be unnecessary. In this case, to avoid",
            " * allocating huge chunk of memory, each clients represent their need of",
            " * extra memory through the need callback. If one of the need callbacks",
            " * returns true, it means that someone needs extra memory so that",
            " * page extension core should allocates memory for page extension. If",
            " * none of need callbacks return true, memory isn't needed at all in this boot",
            " * and page extension core can skip to allocate memory. As result,",
            " * none of memory is wasted.",
            " *",
            " * When need callback returns true, page_ext checks if there is a request for",
            " * extra memory through size in struct page_ext_operations. If it is non-zero,",
            " * extra space is allocated for each page_ext entry and offset is returned to",
            " * user through offset in struct page_ext_operations.",
            " *",
            " * The init callback is used to do proper initialization after page extension",
            " * is completely initialized. In sparse memory system, extra memory is",
            " * allocated some time later than memmap is allocated. In other words, lifetime",
            " * of memory for page extension isn't same with memmap for struct page.",
            " * Therefore, clients can't store extra data until page extension is",
            " * initialized, even if pages are allocated and used freely. This could",
            " * cause inadequate state of extra data per page, so, to prevent it, client",
            " * can utilize this callback to initialize the state of it correctly.",
            " */",
            "",
            "#ifdef CONFIG_SPARSEMEM",
            "#define PAGE_EXT_INVALID       (0x1)",
            "#endif",
            "",
            "#if defined(CONFIG_PAGE_IDLE_FLAG) && !defined(CONFIG_64BIT)"
          ],
          "function_name": null,
          "description": "定义了page extension机制，通过外部内存分配方式管理页扩展数据，提供need回调判断是否需要分配及init回调进行初始化，避免直接修改struct page结构体，支持动态内存分配决策以减少冗余开销。",
          "similarity": 0.5834248065948486
        },
        {
          "chunk_id": 2,
          "file_path": "mm/page_ext.c",
          "start_line": 242,
          "end_line": 359,
          "content": [
            "static bool page_ext_invalid(struct page_ext *page_ext)",
            "{",
            "\treturn !page_ext || (((unsigned long)page_ext & PAGE_EXT_INVALID) == PAGE_EXT_INVALID);",
            "}",
            "static int __meminit init_section_page_ext(unsigned long pfn, int nid)",
            "{",
            "\tstruct mem_section *section;",
            "\tstruct page_ext *base;",
            "\tunsigned long table_size;",
            "",
            "\tsection = __pfn_to_section(pfn);",
            "",
            "\tif (section->page_ext)",
            "\t\treturn 0;",
            "",
            "\ttable_size = page_ext_size * PAGES_PER_SECTION;",
            "\tbase = alloc_page_ext(table_size, nid);",
            "",
            "\t/*",
            "\t * The value stored in section->page_ext is (base - pfn)",
            "\t * and it does not point to the memory block allocated above,",
            "\t * causing kmemleak false positives.",
            "\t */",
            "\tkmemleak_not_leak(base);",
            "",
            "\tif (!base) {",
            "\t\tpr_err(\"page ext allocation failure\\n\");",
            "\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\t/*",
            "\t * The passed \"pfn\" may not be aligned to SECTION.  For the calculation",
            "\t * we need to apply a mask.",
            "\t */",
            "\tpfn &= PAGE_SECTION_MASK;",
            "\tsection->page_ext = (void *)base - page_ext_size * pfn;",
            "\ttotal_usage += table_size;",
            "\treturn 0;",
            "}",
            "static void free_page_ext(void *addr)",
            "{",
            "\tif (is_vmalloc_addr(addr)) {",
            "\t\tvfree(addr);",
            "\t} else {",
            "\t\tstruct page *page = virt_to_page(addr);",
            "\t\tsize_t table_size;",
            "",
            "\t\ttable_size = page_ext_size * PAGES_PER_SECTION;",
            "",
            "\t\tBUG_ON(PageReserved(page));",
            "\t\tkmemleak_free(addr);",
            "\t\tfree_pages_exact(addr, table_size);",
            "\t}",
            "}",
            "static void __free_page_ext(unsigned long pfn)",
            "{",
            "\tstruct mem_section *ms;",
            "\tstruct page_ext *base;",
            "",
            "\tms = __pfn_to_section(pfn);",
            "\tif (!ms || !ms->page_ext)",
            "\t\treturn;",
            "",
            "\tbase = READ_ONCE(ms->page_ext);",
            "\t/*",
            "\t * page_ext here can be valid while doing the roll back",
            "\t * operation in online_page_ext().",
            "\t */",
            "\tif (page_ext_invalid(base))",
            "\t\tbase = (void *)base - PAGE_EXT_INVALID;",
            "\tWRITE_ONCE(ms->page_ext, NULL);",
            "",
            "\tbase = get_entry(base, pfn);",
            "\tfree_page_ext(base);",
            "}",
            "static void __invalidate_page_ext(unsigned long pfn)",
            "{",
            "\tstruct mem_section *ms;",
            "\tvoid *val;",
            "",
            "\tms = __pfn_to_section(pfn);",
            "\tif (!ms || !ms->page_ext)",
            "\t\treturn;",
            "\tval = (void *)ms->page_ext + PAGE_EXT_INVALID;",
            "\tWRITE_ONCE(ms->page_ext, val);",
            "}",
            "static int __meminit online_page_ext(unsigned long start_pfn,",
            "\t\t\t\tunsigned long nr_pages,",
            "\t\t\t\tint nid)",
            "{",
            "\tunsigned long start, end, pfn;",
            "\tint fail = 0;",
            "",
            "\tstart = SECTION_ALIGN_DOWN(start_pfn);",
            "\tend = SECTION_ALIGN_UP(start_pfn + nr_pages);",
            "",
            "\tif (nid == NUMA_NO_NODE) {",
            "\t\t/*",
            "\t\t * In this case, \"nid\" already exists and contains valid memory.",
            "\t\t * \"start_pfn\" passed to us is a pfn which is an arg for",
            "\t\t * online__pages(), and start_pfn should exist.",
            "\t\t */",
            "\t\tnid = pfn_to_nid(start_pfn);",
            "\t\tVM_BUG_ON(!node_online(nid));",
            "\t}",
            "",
            "\tfor (pfn = start; !fail && pfn < end; pfn += PAGES_PER_SECTION)",
            "\t\tfail = init_section_page_ext(pfn, nid);",
            "\tif (!fail)",
            "\t\treturn 0;",
            "",
            "\t/* rollback */",
            "\tend = pfn - PAGES_PER_SECTION;",
            "\tfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION)",
            "\t\t__free_page_ext(pfn);",
            "",
            "\treturn -ENOMEM;",
            "}"
          ],
          "function_name": "page_ext_invalid, init_section_page_ext, free_page_ext, __free_page_ext, __invalidate_page_ext, online_page_ext",
          "description": "实现页面扩展内存的初始化、释放与无效化操作，包含针对内存段的扩展内存分配、基于虚拟地址的释放处理、无效化标记机制及分段式扩展内存的回收流程。",
          "similarity": 0.5673772096633911
        },
        {
          "chunk_id": 3,
          "file_path": "mm/page_ext.c",
          "start_line": 400,
          "end_line": 506,
          "content": [
            "static void __meminit offline_page_ext(unsigned long start_pfn,",
            "\t\t\t\tunsigned long nr_pages)",
            "{",
            "\tunsigned long start, end, pfn;",
            "",
            "\tstart = SECTION_ALIGN_DOWN(start_pfn);",
            "\tend = SECTION_ALIGN_UP(start_pfn + nr_pages);",
            "",
            "\t/*",
            "\t * Freeing of page_ext is done in 3 steps to avoid",
            "\t * use-after-free of it:",
            "\t * 1) Traverse all the sections and mark their page_ext",
            "\t *    as invalid.",
            "\t * 2) Wait for all the existing users of page_ext who",
            "\t *    started before invalidation to finish.",
            "\t * 3) Free the page_ext.",
            "\t */",
            "\tfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION)",
            "\t\t__invalidate_page_ext(pfn);",
            "",
            "\tsynchronize_rcu();",
            "",
            "\tfor (pfn = start; pfn < end; pfn += PAGES_PER_SECTION)",
            "\t\t__free_page_ext(pfn);",
            "}",
            "static int __meminit page_ext_callback(struct notifier_block *self,",
            "\t\t\t       unsigned long action, void *arg)",
            "{",
            "\tstruct memory_notify *mn = arg;",
            "\tint ret = 0;",
            "",
            "\tswitch (action) {",
            "\tcase MEM_GOING_ONLINE:",
            "\t\tret = online_page_ext(mn->start_pfn,",
            "\t\t\t\t   mn->nr_pages, mn->status_change_nid);",
            "\t\tbreak;",
            "\tcase MEM_OFFLINE:",
            "\t\toffline_page_ext(mn->start_pfn,",
            "\t\t\t\tmn->nr_pages);",
            "\t\tbreak;",
            "\tcase MEM_CANCEL_ONLINE:",
            "\t\toffline_page_ext(mn->start_pfn,",
            "\t\t\t\tmn->nr_pages);",
            "\t\tbreak;",
            "\tcase MEM_GOING_OFFLINE:",
            "\t\tbreak;",
            "\tcase MEM_ONLINE:",
            "\tcase MEM_CANCEL_OFFLINE:",
            "\t\tbreak;",
            "\t}",
            "",
            "\treturn notifier_from_errno(ret);",
            "}",
            "void __init page_ext_init(void)",
            "{",
            "\tunsigned long pfn;",
            "\tint nid;",
            "",
            "\tif (!invoke_need_callbacks())",
            "\t\treturn;",
            "",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\tunsigned long start_pfn, end_pfn;",
            "",
            "\t\tstart_pfn = node_start_pfn(nid);",
            "\t\tend_pfn = node_end_pfn(nid);",
            "\t\t/*",
            "\t\t * start_pfn and end_pfn may not be aligned to SECTION and the",
            "\t\t * page->flags of out of node pages are not initialized.  So we",
            "\t\t * scan [start_pfn, the biggest section's pfn < end_pfn) here.",
            "\t\t */",
            "\t\tfor (pfn = start_pfn; pfn < end_pfn;",
            "\t\t\tpfn = ALIGN(pfn + 1, PAGES_PER_SECTION)) {",
            "",
            "\t\t\tif (!pfn_valid(pfn))",
            "\t\t\t\tcontinue;",
            "\t\t\t/*",
            "\t\t\t * Nodes's pfns can be overlapping.",
            "\t\t\t * We know some arch can have a nodes layout such as",
            "\t\t\t * -------------pfn-------------->",
            "\t\t\t * N0 | N1 | N2 | N0 | N1 | N2|....",
            "\t\t\t */",
            "\t\t\tif (pfn_to_nid(pfn) != nid)",
            "\t\t\t\tcontinue;",
            "\t\t\tif (init_section_page_ext(pfn, nid))",
            "\t\t\t\tgoto oom;",
            "\t\t\tcond_resched();",
            "\t\t}",
            "\t}",
            "\thotplug_memory_notifier(page_ext_callback, DEFAULT_CALLBACK_PRI);",
            "\tpr_info(\"allocated %ld bytes of page_ext\\n\", total_usage);",
            "\tinvoke_init_callbacks();",
            "\treturn;",
            "",
            "oom:",
            "\tpanic(\"Out of memory\");",
            "}",
            "void __meminit pgdat_page_ext_init(struct pglist_data *pgdat)",
            "{",
            "}",
            "void page_ext_put(struct page_ext *page_ext)",
            "{",
            "\tif (unlikely(!page_ext))",
            "\t\treturn;",
            "",
            "\trcu_read_unlock();",
            "}"
          ],
          "function_name": "offline_page_ext, page_ext_callback, page_ext_init, pgdat_page_ext_init, page_ext_put",
          "description": "实现内存状态变更回调处理及全局初始化，包含在线/离线事件响应、扩展内存的动态分配回收、内存热插拔通知注册及初始化回调执行，通过RCU机制保证并发安全。",
          "similarity": 0.5062692761421204
        }
      ]
    },
    {
      "source_file": "kernel/sched/ext.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:08:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\ext.c`\n\n---\n\n# `sched/ext.c` 技术文档\n\n## 文件概述\n\n`sched/ext.c` 是 Linux 内核中 **BPF 可扩展调度器（sched_ext）** 的核心实现文件之一，定义了调度器与 BPF 程序交互所需的数据结构、常量和操作接口。该文件为用户空间通过 BPF 实现自定义调度策略提供了内核侧的框架支持，允许将任务调度逻辑完全委托给加载的 BPF 程序，同时保留与内核调度子系统的安全集成。\n\n## 核心功能\n\n### 主要数据结构\n\n- **`struct sched_ext_ops`**  \n  BPF 调度器的操作函数表，包含调度器必须或可选实现的回调函数，如 `select_cpu`、`enqueue`、`dequeue`、`dispatch` 等，用于控制任务的 CPU 选择、入队、出队和分发逻辑。\n\n- **`struct scx_exit_info`**  \n  描述 BPF 调度器退出原因的结构体，包含退出类型（`kind`）、退出码（`exit_code`）、错误信息（`reason`、`msg`）、回溯栈（`bt`）和调试转储（`dump`）。\n\n- **`struct scx_init_task_args` / `scx_exit_task_args`**  \n  分别用于 `ops.init_task()` 和 `ops.exit_task()` 回调的参数容器，传递任务初始化/退出上下文（如是否由 fork 触发、所属 cgroup 等）。\n\n- **`struct scx_cpu_acquire_args` / `scx_cpu_release_args`**  \n  用于 CPU 获取/释放回调的参数结构，其中 `cpu_release` 包含抢占原因（如 RT/DL 任务抢占）和即将运行的任务。\n\n- **`struct scx_dump_ctx`**  \n  为调度器转储（dump）操作提供上下文信息，包括退出类型、时间戳等。\n\n### 关键枚举与常量\n\n- **`enum scx_exit_kind`**  \n  定义调度器退出的类别，如正常退出（`SCX_EXIT_DONE`）、用户/BPF/内核主动注销（`SCX_EXIT_UNREG*`）、系统请求（`SCX_EXIT_SYSRQ`）或运行时错误（`SCX_EXIT_ERROR*`）。\n\n- **`enum scx_exit_code`**  \n  定义 64 位退出码的位域格式，支持系统原因（如 `SCX_ECODE_RSN_HOTPLUG`）和系统动作（如 `SCX_ECODE_ACT_RESTART`），允许用户自定义退出上下文。\n\n- **`enum scx_ops_flags`**  \n  调度器操作标志，控制调度行为：\n  - `SCX_OPS_KEEP_BUILTIN_IDLE`：保留内建空闲跟踪\n  - `SCX_OPS_ENQ_LAST`：切片到期后仍无任务时重新入队\n  - `SCX_OPS_ENQ_EXITING`：由 BPF 处理退出中任务\n  - `SCX_OPS_SWITCH_PARTIAL`：仅调度 `SCHED_EXT` 策略任务\n  - `SCX_OPS_HAS_CGROUP_WEIGHT`：支持 cgroup cpu.weight\n\n- **调度器常量**  \n  如 `SCX_DSP_DFL_MAX_BATCH`（默认分发批大小）、`SCX_WATCHDOG_MAX_TIMEOUT`（看门狗超时）、`SCX_OPS_TASK_ITER_BATCH`（任务迭代锁释放批次）等，用于控制调度器内部行为。\n\n## 关键实现\n\n- **BPF 调度器生命周期管理**  \n  通过 `scx_exit_info` 和退出码机制，支持多种退出路径（用户、BPF、内核、SysRq、错误），并提供详细的诊断信息（回溯、消息、转储）。\n\n- **任务入队优化**  \n  在 `select_cpu` 中允许直接插入 DSQ（如本地 DSQ），跳过后续 `enqueue` 调用，减少调度开销；同时通过 `SCX_OPS_ENQ_EXITING` 标志处理退出中任务的调度问题，避免 RCU 停顿。\n\n- **CPU 抢占通知**  \n  通过 `scx_cpu_release_args` 向 BPF 调度器传递 CPU 被高优先级调度类（RT/DL/Stop）抢占的原因，便于调度器做出相应调整。\n\n- **cgroup 集成**  \n  支持 cgroup 调度（`CONFIG_EXT_GROUP_SCHED`），在任务加入 cgroup 时传递权重信息（`scx_cgroup_init_args`），并通过 `SCX_OPS_HAS_CGROUP_WEIGHT` 标志启用。\n\n- **安全与鲁棒性**  \n  内核侧跟踪 BPF 是否拥有任务，可忽略无效分发；任务迭代时定期释放锁（`SCX_OPS_TASK_ITER_BATCH`），防止 RCU/CSD 停顿；看门狗机制（`SCX_WATCHDOG_MAX_TIMEOUT`）检测任务卡死。\n\n## 依赖关系\n\n- **BPF 子系统**：通过 `#include <linux/bpf.h>` 依赖 BPF 基础设施，用于加载和验证调度器 BPF 程序。\n- **调度核心**：与 `kernel/sched/` 下的核心调度代码（如 `core.c`、`rt.c`、`dl.c`）交互，处理任务入队、CPU 选择和抢占。\n- **cgroup 子系统**：当启用 `CONFIG_EXT_GROUP_SCHED` 时，依赖 cgroup CPU 控制器获取任务权重和层级信息。\n- **RCU 与锁机制**：使用 `scx_tasks_lock` 保护任务迭代，需与 RCU 同步机制协调。\n\n## 使用场景\n\n- **自定义调度策略开发**：用户通过 BPF 实现特定工作负载的调度逻辑（如延迟敏感型、批处理优化、NUMA 感知等），并注册到 `sched_ext`。\n- **系统调试与监控**：利用 `ops.dump()` 和退出信息结构体，在调度器异常退出时收集诊断数据。\n- **混合调度部署**：通过 `SCX_OPS_SWITCH_PARTIAL` 标志，仅对部分任务（`SCHED_EXT`）启用 BPF 调度，其余任务仍由 CFS 处理。\n- **资源隔离与 QoS**：结合 cgroup 支持，为不同 cgroup 配置不同的调度行为和资源权重。\n- **内核调度实验平台**：作为安全的沙箱环境，测试新型调度算法而无需修改核心调度代码。",
      "similarity": 0.5458017587661743,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/ext.c",
          "start_line": 1,
          "end_line": 1034,
          "content": [
            "/* SPDX-License-Identifier: GPL-2.0 */",
            "/*",
            " * BPF extensible scheduler class: Documentation/scheduler/sched-ext.rst",
            " *",
            " * Copyright (c) 2022 Meta Platforms, Inc. and affiliates.",
            " * Copyright (c) 2022 Tejun Heo <tj@kernel.org>",
            " * Copyright (c) 2022 David Vernet <dvernet@meta.com>",
            " */",
            "#include <linux/bpf.h>",
            "",
            "#define SCX_OP_IDX(op)\t\t(offsetof(struct sched_ext_ops, op) / sizeof(void (*)(void)))",
            "",
            "enum scx_consts {",
            "\tSCX_DSP_DFL_MAX_BATCH\t\t= 32,",
            "\tSCX_DSP_MAX_LOOPS\t\t= 32,",
            "\tSCX_WATCHDOG_MAX_TIMEOUT\t= 30 * HZ,",
            "",
            "\tSCX_EXIT_BT_LEN\t\t\t= 64,",
            "\tSCX_EXIT_MSG_LEN\t\t= 1024,",
            "\tSCX_EXIT_DUMP_DFL_LEN\t\t= 32768,",
            "",
            "\tSCX_CPUPERF_ONE\t\t\t= SCHED_CAPACITY_SCALE,",
            "",
            "\t/*",
            "\t * Iterating all tasks may take a while. Periodically drop",
            "\t * scx_tasks_lock to avoid causing e.g. CSD and RCU stalls.",
            "\t */",
            "\tSCX_OPS_TASK_ITER_BATCH\t\t= 32,",
            "};",
            "",
            "enum scx_exit_kind {",
            "\tSCX_EXIT_NONE,",
            "\tSCX_EXIT_DONE,",
            "",
            "\tSCX_EXIT_UNREG = 64,\t/* user-space initiated unregistration */",
            "\tSCX_EXIT_UNREG_BPF,\t/* BPF-initiated unregistration */",
            "\tSCX_EXIT_UNREG_KERN,\t/* kernel-initiated unregistration */",
            "\tSCX_EXIT_SYSRQ,\t\t/* requested by 'S' sysrq */",
            "",
            "\tSCX_EXIT_ERROR = 1024,\t/* runtime error, error msg contains details */",
            "\tSCX_EXIT_ERROR_BPF,\t/* ERROR but triggered through scx_bpf_error() */",
            "\tSCX_EXIT_ERROR_STALL,\t/* watchdog detected stalled runnable tasks */",
            "};",
            "",
            "/*",
            " * An exit code can be specified when exiting with scx_bpf_exit() or",
            " * scx_ops_exit(), corresponding to exit_kind UNREG_BPF and UNREG_KERN",
            " * respectively. The codes are 64bit of the format:",
            " *",
            " *   Bits: [63  ..  48 47   ..  32 31 .. 0]",
            " *         [ SYS ACT ] [ SYS RSN ] [ USR  ]",
            " *",
            " *   SYS ACT: System-defined exit actions",
            " *   SYS RSN: System-defined exit reasons",
            " *   USR    : User-defined exit codes and reasons",
            " *",
            " * Using the above, users may communicate intention and context by ORing system",
            " * actions and/or system reasons with a user-defined exit code.",
            " */",
            "enum scx_exit_code {",
            "\t/* Reasons */",
            "\tSCX_ECODE_RSN_HOTPLUG\t= 1LLU << 32,",
            "",
            "\t/* Actions */",
            "\tSCX_ECODE_ACT_RESTART\t= 1LLU << 48,",
            "};",
            "",
            "/*",
            " * scx_exit_info is passed to ops.exit() to describe why the BPF scheduler is",
            " * being disabled.",
            " */",
            "struct scx_exit_info {",
            "\t/* %SCX_EXIT_* - broad category of the exit reason */",
            "\tenum scx_exit_kind\tkind;",
            "",
            "\t/* exit code if gracefully exiting */",
            "\ts64\t\t\texit_code;",
            "",
            "\t/* textual representation of the above */",
            "\tconst char\t\t*reason;",
            "",
            "\t/* backtrace if exiting due to an error */",
            "\tunsigned long\t\t*bt;",
            "\tu32\t\t\tbt_len;",
            "",
            "\t/* informational message */",
            "\tchar\t\t\t*msg;",
            "",
            "\t/* debug dump */",
            "\tchar\t\t\t*dump;",
            "};",
            "",
            "/* sched_ext_ops.flags */",
            "enum scx_ops_flags {",
            "\t/*",
            "\t * Keep built-in idle tracking even if ops.update_idle() is implemented.",
            "\t */",
            "\tSCX_OPS_KEEP_BUILTIN_IDLE = 1LLU << 0,",
            "",
            "\t/*",
            "\t * By default, if there are no other task to run on the CPU, ext core",
            "\t * keeps running the current task even after its slice expires. If this",
            "\t * flag is specified, such tasks are passed to ops.enqueue() with",
            "\t * %SCX_ENQ_LAST. See the comment above %SCX_ENQ_LAST for more info.",
            "\t */",
            "\tSCX_OPS_ENQ_LAST\t= 1LLU << 1,",
            "",
            "\t/*",
            "\t * An exiting task may schedule after PF_EXITING is set. In such cases,",
            "\t * bpf_task_from_pid() may not be able to find the task and if the BPF",
            "\t * scheduler depends on pid lookup for dispatching, the task will be",
            "\t * lost leading to various issues including RCU grace period stalls.",
            "\t *",
            "\t * To mask this problem, by default, unhashed tasks are automatically",
            "\t * dispatched to the local DSQ on enqueue. If the BPF scheduler doesn't",
            "\t * depend on pid lookups and wants to handle these tasks directly, the",
            "\t * following flag can be used.",
            "\t */",
            "\tSCX_OPS_ENQ_EXITING\t= 1LLU << 2,",
            "",
            "\t/*",
            "\t * If set, only tasks with policy set to SCHED_EXT are attached to",
            "\t * sched_ext. If clear, SCHED_NORMAL tasks are also included.",
            "\t */",
            "\tSCX_OPS_SWITCH_PARTIAL\t= 1LLU << 3,",
            "",
            "\t/*",
            "\t * CPU cgroup support flags",
            "\t */",
            "\tSCX_OPS_HAS_CGROUP_WEIGHT = 1LLU << 16,\t/* cpu.weight */",
            "",
            "\tSCX_OPS_ALL_FLAGS\t= SCX_OPS_KEEP_BUILTIN_IDLE |",
            "\t\t\t\t  SCX_OPS_ENQ_LAST |",
            "\t\t\t\t  SCX_OPS_ENQ_EXITING |",
            "\t\t\t\t  SCX_OPS_SWITCH_PARTIAL |",
            "\t\t\t\t  SCX_OPS_HAS_CGROUP_WEIGHT,",
            "};",
            "",
            "/* argument container for ops.init_task() */",
            "struct scx_init_task_args {",
            "\t/*",
            "\t * Set if ops.init_task() is being invoked on the fork path, as opposed",
            "\t * to the scheduler transition path.",
            "\t */",
            "\tbool\t\t\tfork;",
            "#ifdef CONFIG_EXT_GROUP_SCHED",
            "\t/* the cgroup the task is joining */",
            "\tstruct cgroup\t\t*cgroup;",
            "#endif",
            "};",
            "",
            "/* argument container for ops.exit_task() */",
            "struct scx_exit_task_args {",
            "\t/* Whether the task exited before running on sched_ext. */",
            "\tbool cancelled;",
            "};",
            "",
            "/* argument container for ops->cgroup_init() */",
            "struct scx_cgroup_init_args {",
            "\t/* the weight of the cgroup [1..10000] */",
            "\tu32\t\t\tweight;",
            "};",
            "",
            "enum scx_cpu_preempt_reason {",
            "\t/* next task is being scheduled by &sched_class_rt */",
            "\tSCX_CPU_PREEMPT_RT,",
            "\t/* next task is being scheduled by &sched_class_dl */",
            "\tSCX_CPU_PREEMPT_DL,",
            "\t/* next task is being scheduled by &sched_class_stop */",
            "\tSCX_CPU_PREEMPT_STOP,",
            "\t/* unknown reason for SCX being preempted */",
            "\tSCX_CPU_PREEMPT_UNKNOWN,",
            "};",
            "",
            "/*",
            " * Argument container for ops->cpu_acquire(). Currently empty, but may be",
            " * expanded in the future.",
            " */",
            "struct scx_cpu_acquire_args {};",
            "",
            "/* argument container for ops->cpu_release() */",
            "struct scx_cpu_release_args {",
            "\t/* the reason the CPU was preempted */",
            "\tenum scx_cpu_preempt_reason reason;",
            "",
            "\t/* the task that's going to be scheduled on the CPU */",
            "\tstruct task_struct\t*task;",
            "};",
            "",
            "/*",
            " * Informational context provided to dump operations.",
            " */",
            "struct scx_dump_ctx {",
            "\tenum scx_exit_kind\tkind;",
            "\ts64\t\t\texit_code;",
            "\tconst char\t\t*reason;",
            "\tu64\t\t\tat_ns;",
            "\tu64\t\t\tat_jiffies;",
            "};",
            "",
            "/**",
            " * struct sched_ext_ops - Operation table for BPF scheduler implementation",
            " *",
            " * Userland can implement an arbitrary scheduling policy by implementing and",
            " * loading operations in this table.",
            " */",
            "struct sched_ext_ops {",
            "\t/**",
            "\t * select_cpu - Pick the target CPU for a task which is being woken up",
            "\t * @p: task being woken up",
            "\t * @prev_cpu: the cpu @p was on before sleeping",
            "\t * @wake_flags: SCX_WAKE_*",
            "\t *",
            "\t * Decision made here isn't final. @p may be moved to any CPU while it",
            "\t * is getting dispatched for execution later. However, as @p is not on",
            "\t * the rq at this point, getting the eventual execution CPU right here",
            "\t * saves a small bit of overhead down the line.",
            "\t *",
            "\t * If an idle CPU is returned, the CPU is kicked and will try to",
            "\t * dispatch. While an explicit custom mechanism can be added,",
            "\t * select_cpu() serves as the default way to wake up idle CPUs.",
            "\t *",
            "\t * @p may be inserted into a DSQ directly by calling",
            "\t * scx_bpf_dsq_insert(). If so, the ops.enqueue() will be skipped.",
            "\t * Directly inserting into %SCX_DSQ_LOCAL will put @p in the local DSQ",
            "\t * of the CPU returned by this operation.",
            "\t */",
            "\ts32 (*select_cpu)(struct task_struct *p, s32 prev_cpu, u64 wake_flags);",
            "",
            "\t/**",
            "\t * enqueue - Enqueue a task on the BPF scheduler",
            "\t * @p: task being enqueued",
            "\t * @enq_flags: %SCX_ENQ_*",
            "\t *",
            "\t * @p is ready to run. Insert directly into a DSQ by calling",
            "\t * scx_bpf_dsq_insert() or enqueue on the BPF scheduler. If not directly",
            "\t * inserted, the bpf scheduler owns @p and if it fails to dispatch @p,",
            "\t * the task will stall.",
            "\t *",
            "\t * If @p was inserted into a DSQ from ops.select_cpu(), this callback is",
            "\t * skipped.",
            "\t */",
            "\tvoid (*enqueue)(struct task_struct *p, u64 enq_flags);",
            "",
            "\t/**",
            "\t * dequeue - Remove a task from the BPF scheduler",
            "\t * @p: task being dequeued",
            "\t * @deq_flags: %SCX_DEQ_*",
            "\t *",
            "\t * Remove @p from the BPF scheduler. This is usually called to isolate",
            "\t * the task while updating its scheduling properties (e.g. priority).",
            "\t *",
            "\t * The ext core keeps track of whether the BPF side owns a given task or",
            "\t * not and can gracefully ignore spurious dispatches from BPF side,",
            "\t * which makes it safe to not implement this method. However, depending",
            "\t * on the scheduling logic, this can lead to confusing behaviors - e.g.",
            "\t * scheduling position not being updated across a priority change.",
            "\t */",
            "\tvoid (*dequeue)(struct task_struct *p, u64 deq_flags);",
            "",
            "\t/**",
            "\t * dispatch - Dispatch tasks from the BPF scheduler and/or user DSQs",
            "\t * @cpu: CPU to dispatch tasks for",
            "\t * @prev: previous task being switched out",
            "\t *",
            "\t * Called when a CPU's local dsq is empty. The operation should dispatch",
            "\t * one or more tasks from the BPF scheduler into the DSQs using",
            "\t * scx_bpf_dsq_insert() and/or move from user DSQs into the local DSQ",
            "\t * using scx_bpf_dsq_move_to_local().",
            "\t *",
            "\t * The maximum number of times scx_bpf_dsq_insert() can be called",
            "\t * without an intervening scx_bpf_dsq_move_to_local() is specified by",
            "\t * ops.dispatch_max_batch. See the comments on top of the two functions",
            "\t * for more details.",
            "\t *",
            "\t * When not %NULL, @prev is an SCX task with its slice depleted. If",
            "\t * @prev is still runnable as indicated by set %SCX_TASK_QUEUED in",
            "\t * @prev->scx.flags, it is not enqueued yet and will be enqueued after",
            "\t * ops.dispatch() returns. To keep executing @prev, return without",
            "\t * dispatching or moving any tasks. Also see %SCX_OPS_ENQ_LAST.",
            "\t */",
            "\tvoid (*dispatch)(s32 cpu, struct task_struct *prev);",
            "",
            "\t/**",
            "\t * tick - Periodic tick",
            "\t * @p: task running currently",
            "\t *",
            "\t * This operation is called every 1/HZ seconds on CPUs which are",
            "\t * executing an SCX task. Setting @p->scx.slice to 0 will trigger an",
            "\t * immediate dispatch cycle on the CPU.",
            "\t */",
            "\tvoid (*tick)(struct task_struct *p);",
            "",
            "\t/**",
            "\t * runnable - A task is becoming runnable on its associated CPU",
            "\t * @p: task becoming runnable",
            "\t * @enq_flags: %SCX_ENQ_*",
            "\t *",
            "\t * This and the following three functions can be used to track a task's",
            "\t * execution state transitions. A task becomes ->runnable() on a CPU,",
            "\t * and then goes through one or more ->running() and ->stopping() pairs",
            "\t * as it runs on the CPU, and eventually becomes ->quiescent() when it's",
            "\t * done running on the CPU.",
            "\t *",
            "\t * @p is becoming runnable on the CPU because it's",
            "\t *",
            "\t * - waking up (%SCX_ENQ_WAKEUP)",
            "\t * - being moved from another CPU",
            "\t * - being restored after temporarily taken off the queue for an",
            "\t *   attribute change.",
            "\t *",
            "\t * This and ->enqueue() are related but not coupled. This operation",
            "\t * notifies @p's state transition and may not be followed by ->enqueue()",
            "\t * e.g. when @p is being dispatched to a remote CPU, or when @p is",
            "\t * being enqueued on a CPU experiencing a hotplug event. Likewise, a",
            "\t * task may be ->enqueue()'d without being preceded by this operation",
            "\t * e.g. after exhausting its slice.",
            "\t */",
            "\tvoid (*runnable)(struct task_struct *p, u64 enq_flags);",
            "",
            "\t/**",
            "\t * running - A task is starting to run on its associated CPU",
            "\t * @p: task starting to run",
            "\t *",
            "\t * See ->runnable() for explanation on the task state notifiers.",
            "\t */",
            "\tvoid (*running)(struct task_struct *p);",
            "",
            "\t/**",
            "\t * stopping - A task is stopping execution",
            "\t * @p: task stopping to run",
            "\t * @runnable: is task @p still runnable?",
            "\t *",
            "\t * See ->runnable() for explanation on the task state notifiers. If",
            "\t * !@runnable, ->quiescent() will be invoked after this operation",
            "\t * returns.",
            "\t */",
            "\tvoid (*stopping)(struct task_struct *p, bool runnable);",
            "",
            "\t/**",
            "\t * quiescent - A task is becoming not runnable on its associated CPU",
            "\t * @p: task becoming not runnable",
            "\t * @deq_flags: %SCX_DEQ_*",
            "\t *",
            "\t * See ->runnable() for explanation on the task state notifiers.",
            "\t *",
            "\t * @p is becoming quiescent on the CPU because it's",
            "\t *",
            "\t * - sleeping (%SCX_DEQ_SLEEP)",
            "\t * - being moved to another CPU",
            "\t * - being temporarily taken off the queue for an attribute change",
            "\t *   (%SCX_DEQ_SAVE)",
            "\t *",
            "\t * This and ->dequeue() are related but not coupled. This operation",
            "\t * notifies @p's state transition and may not be preceded by ->dequeue()",
            "\t * e.g. when @p is being dispatched to a remote CPU.",
            "\t */",
            "\tvoid (*quiescent)(struct task_struct *p, u64 deq_flags);",
            "",
            "\t/**",
            "\t * yield - Yield CPU",
            "\t * @from: yielding task",
            "\t * @to: optional yield target task",
            "\t *",
            "\t * If @to is NULL, @from is yielding the CPU to other runnable tasks.",
            "\t * The BPF scheduler should ensure that other available tasks are",
            "\t * dispatched before the yielding task. Return value is ignored in this",
            "\t * case.",
            "\t *",
            "\t * If @to is not-NULL, @from wants to yield the CPU to @to. If the bpf",
            "\t * scheduler can implement the request, return %true; otherwise, %false.",
            "\t */",
            "\tbool (*yield)(struct task_struct *from, struct task_struct *to);",
            "",
            "\t/**",
            "\t * core_sched_before - Task ordering for core-sched",
            "\t * @a: task A",
            "\t * @b: task B",
            "\t *",
            "\t * Used by core-sched to determine the ordering between two tasks. See",
            "\t * Documentation/admin-guide/hw-vuln/core-scheduling.rst for details on",
            "\t * core-sched.",
            "\t *",
            "\t * Both @a and @b are runnable and may or may not currently be queued on",
            "\t * the BPF scheduler. Should return %true if @a should run before @b.",
            "\t * %false if there's no required ordering or @b should run before @a.",
            "\t *",
            "\t * If not specified, the default is ordering them according to when they",
            "\t * became runnable.",
            "\t */",
            "\tbool (*core_sched_before)(struct task_struct *a, struct task_struct *b);",
            "",
            "\t/**",
            "\t * set_weight - Set task weight",
            "\t * @p: task to set weight for",
            "\t * @weight: new weight [1..10000]",
            "\t *",
            "\t * Update @p's weight to @weight.",
            "\t */",
            "\tvoid (*set_weight)(struct task_struct *p, u32 weight);",
            "",
            "\t/**",
            "\t * set_cpumask - Set CPU affinity",
            "\t * @p: task to set CPU affinity for",
            "\t * @cpumask: cpumask of cpus that @p can run on",
            "\t *",
            "\t * Update @p's CPU affinity to @cpumask.",
            "\t */",
            "\tvoid (*set_cpumask)(struct task_struct *p,",
            "\t\t\t    const struct cpumask *cpumask);",
            "",
            "\t/**",
            "\t * update_idle - Update the idle state of a CPU",
            "\t * @cpu: CPU to udpate the idle state for",
            "\t * @idle: whether entering or exiting the idle state",
            "\t *",
            "\t * This operation is called when @rq's CPU goes or leaves the idle",
            "\t * state. By default, implementing this operation disables the built-in",
            "\t * idle CPU tracking and the following helpers become unavailable:",
            "\t *",
            "\t * - scx_bpf_select_cpu_dfl()",
            "\t * - scx_bpf_test_and_clear_cpu_idle()",
            "\t * - scx_bpf_pick_idle_cpu()",
            "\t *",
            "\t * The user also must implement ops.select_cpu() as the default",
            "\t * implementation relies on scx_bpf_select_cpu_dfl().",
            "\t *",
            "\t * Specify the %SCX_OPS_KEEP_BUILTIN_IDLE flag to keep the built-in idle",
            "\t * tracking.",
            "\t */",
            "\tvoid (*update_idle)(s32 cpu, bool idle);",
            "",
            "\t/**",
            "\t * cpu_acquire - A CPU is becoming available to the BPF scheduler",
            "\t * @cpu: The CPU being acquired by the BPF scheduler.",
            "\t * @args: Acquire arguments, see the struct definition.",
            "\t *",
            "\t * A CPU that was previously released from the BPF scheduler is now once",
            "\t * again under its control.",
            "\t */",
            "\tvoid (*cpu_acquire)(s32 cpu, struct scx_cpu_acquire_args *args);",
            "",
            "\t/**",
            "\t * cpu_release - A CPU is taken away from the BPF scheduler",
            "\t * @cpu: The CPU being released by the BPF scheduler.",
            "\t * @args: Release arguments, see the struct definition.",
            "\t *",
            "\t * The specified CPU is no longer under the control of the BPF",
            "\t * scheduler. This could be because it was preempted by a higher",
            "\t * priority sched_class, though there may be other reasons as well. The",
            "\t * caller should consult @args->reason to determine the cause.",
            "\t */",
            "\tvoid (*cpu_release)(s32 cpu, struct scx_cpu_release_args *args);",
            "",
            "\t/**",
            "\t * init_task - Initialize a task to run in a BPF scheduler",
            "\t * @p: task to initialize for BPF scheduling",
            "\t * @args: init arguments, see the struct definition",
            "\t *",
            "\t * Either we're loading a BPF scheduler or a new task is being forked.",
            "\t * Initialize @p for BPF scheduling. This operation may block and can",
            "\t * be used for allocations, and is called exactly once for a task.",
            "\t *",
            "\t * Return 0 for success, -errno for failure. An error return while",
            "\t * loading will abort loading of the BPF scheduler. During a fork, it",
            "\t * will abort that specific fork.",
            "\t */",
            "\ts32 (*init_task)(struct task_struct *p, struct scx_init_task_args *args);",
            "",
            "\t/**",
            "\t * exit_task - Exit a previously-running task from the system",
            "\t * @p: task to exit",
            "\t *",
            "\t * @p is exiting or the BPF scheduler is being unloaded. Perform any",
            "\t * necessary cleanup for @p.",
            "\t */",
            "\tvoid (*exit_task)(struct task_struct *p, struct scx_exit_task_args *args);",
            "",
            "\t/**",
            "\t * enable - Enable BPF scheduling for a task",
            "\t * @p: task to enable BPF scheduling for",
            "\t *",
            "\t * Enable @p for BPF scheduling. enable() is called on @p any time it",
            "\t * enters SCX, and is always paired with a matching disable().",
            "\t */",
            "\tvoid (*enable)(struct task_struct *p);",
            "",
            "\t/**",
            "\t * disable - Disable BPF scheduling for a task",
            "\t * @p: task to disable BPF scheduling for",
            "\t *",
            "\t * @p is exiting, leaving SCX or the BPF scheduler is being unloaded.",
            "\t * Disable BPF scheduling for @p. A disable() call is always matched",
            "\t * with a prior enable() call.",
            "\t */",
            "\tvoid (*disable)(struct task_struct *p);",
            "",
            "\t/**",
            "\t * dump - Dump BPF scheduler state on error",
            "\t * @ctx: debug dump context",
            "\t *",
            "\t * Use scx_bpf_dump() to generate BPF scheduler specific debug dump.",
            "\t */",
            "\tvoid (*dump)(struct scx_dump_ctx *ctx);",
            "",
            "\t/**",
            "\t * dump_cpu - Dump BPF scheduler state for a CPU on error",
            "\t * @ctx: debug dump context",
            "\t * @cpu: CPU to generate debug dump for",
            "\t * @idle: @cpu is currently idle without any runnable tasks",
            "\t *",
            "\t * Use scx_bpf_dump() to generate BPF scheduler specific debug dump for",
            "\t * @cpu. If @idle is %true and this operation doesn't produce any",
            "\t * output, @cpu is skipped for dump.",
            "\t */",
            "\tvoid (*dump_cpu)(struct scx_dump_ctx *ctx, s32 cpu, bool idle);",
            "",
            "\t/**",
            "\t * dump_task - Dump BPF scheduler state for a runnable task on error",
            "\t * @ctx: debug dump context",
            "\t * @p: runnable task to generate debug dump for",
            "\t *",
            "\t * Use scx_bpf_dump() to generate BPF scheduler specific debug dump for",
            "\t * @p.",
            "\t */",
            "\tvoid (*dump_task)(struct scx_dump_ctx *ctx, struct task_struct *p);",
            "",
            "#ifdef CONFIG_EXT_GROUP_SCHED",
            "\t/**",
            "\t * cgroup_init - Initialize a cgroup",
            "\t * @cgrp: cgroup being initialized",
            "\t * @args: init arguments, see the struct definition",
            "\t *",
            "\t * Either the BPF scheduler is being loaded or @cgrp created, initialize",
            "\t * @cgrp for sched_ext. This operation may block.",
            "\t *",
            "\t * Return 0 for success, -errno for failure. An error return while",
            "\t * loading will abort loading of the BPF scheduler. During cgroup",
            "\t * creation, it will abort the specific cgroup creation.",
            "\t */",
            "\ts32 (*cgroup_init)(struct cgroup *cgrp,",
            "\t\t\t   struct scx_cgroup_init_args *args);",
            "",
            "\t/**",
            "\t * cgroup_exit - Exit a cgroup",
            "\t * @cgrp: cgroup being exited",
            "\t *",
            "\t * Either the BPF scheduler is being unloaded or @cgrp destroyed, exit",
            "\t * @cgrp for sched_ext. This operation my block.",
            "\t */",
            "\tvoid (*cgroup_exit)(struct cgroup *cgrp);",
            "",
            "\t/**",
            "\t * cgroup_prep_move - Prepare a task to be moved to a different cgroup",
            "\t * @p: task being moved",
            "\t * @from: cgroup @p is being moved from",
            "\t * @to: cgroup @p is being moved to",
            "\t *",
            "\t * Prepare @p for move from cgroup @from to @to. This operation may",
            "\t * block and can be used for allocations.",
            "\t *",
            "\t * Return 0 for success, -errno for failure. An error return aborts the",
            "\t * migration.",
            "\t */",
            "\ts32 (*cgroup_prep_move)(struct task_struct *p,",
            "\t\t\t\tstruct cgroup *from, struct cgroup *to);",
            "",
            "\t/**",
            "\t * cgroup_move - Commit cgroup move",
            "\t * @p: task being moved",
            "\t * @from: cgroup @p is being moved from",
            "\t * @to: cgroup @p is being moved to",
            "\t *",
            "\t * Commit the move. @p is dequeued during this operation.",
            "\t */",
            "\tvoid (*cgroup_move)(struct task_struct *p,",
            "\t\t\t    struct cgroup *from, struct cgroup *to);",
            "",
            "\t/**",
            "\t * cgroup_cancel_move - Cancel cgroup move",
            "\t * @p: task whose cgroup move is being canceled",
            "\t * @from: cgroup @p was being moved from",
            "\t * @to: cgroup @p was being moved to",
            "\t *",
            "\t * @p was cgroup_prep_move()'d but failed before reaching cgroup_move().",
            "\t * Undo the preparation.",
            "\t */",
            "\tvoid (*cgroup_cancel_move)(struct task_struct *p,",
            "\t\t\t\t   struct cgroup *from, struct cgroup *to);",
            "",
            "\t/**",
            "\t * cgroup_set_weight - A cgroup's weight is being changed",
            "\t * @cgrp: cgroup whose weight is being updated",
            "\t * @weight: new weight [1..10000]",
            "\t *",
            "\t * Update @tg's weight to @weight.",
            "\t */",
            "\tvoid (*cgroup_set_weight)(struct cgroup *cgrp, u32 weight);",
            "#endif\t/* CONFIG_CGROUPS */",
            "",
            "\t/*",
            "\t * All online ops must come before ops.cpu_online().",
            "\t */",
            "",
            "\t/**",
            "\t * cpu_online - A CPU became online",
            "\t * @cpu: CPU which just came up",
            "\t *",
            "\t * @cpu just came online. @cpu will not call ops.enqueue() or",
            "\t * ops.dispatch(), nor run tasks associated with other CPUs beforehand.",
            "\t */",
            "\tvoid (*cpu_online)(s32 cpu);",
            "",
            "\t/**",
            "\t * cpu_offline - A CPU is going offline",
            "\t * @cpu: CPU which is going offline",
            "\t *",
            "\t * @cpu is going offline. @cpu will not call ops.enqueue() or",
            "\t * ops.dispatch(), nor run tasks associated with other CPUs afterwards.",
            "\t */",
            "\tvoid (*cpu_offline)(s32 cpu);",
            "",
            "\t/*",
            "\t * All CPU hotplug ops must come before ops.init().",
            "\t */",
            "",
            "\t/**",
            "\t * init - Initialize the BPF scheduler",
            "\t */",
            "\ts32 (*init)(void);",
            "",
            "\t/**",
            "\t * exit - Clean up after the BPF scheduler",
            "\t * @info: Exit info",
            "\t *",
            "\t * ops.exit() is also called on ops.init() failure, which is a bit",
            "\t * unusual. This is to allow rich reporting through @info on how",
            "\t * ops.init() failed.",
            "\t */",
            "\tvoid (*exit)(struct scx_exit_info *info);",
            "",
            "\t/**",
            "\t * dispatch_max_batch - Max nr of tasks that dispatch() can dispatch",
            "\t */",
            "\tu32 dispatch_max_batch;",
            "",
            "\t/**",
            "\t * flags - %SCX_OPS_* flags",
            "\t */",
            "\tu64 flags;",
            "",
            "\t/**",
            "\t * timeout_ms - The maximum amount of time, in milliseconds, that a",
            "\t * runnable task should be able to wait before being scheduled. The",
            "\t * maximum timeout may not exceed the default timeout of 30 seconds.",
            "\t *",
            "\t * Defaults to the maximum allowed timeout value of 30 seconds.",
            "\t */",
            "\tu32 timeout_ms;",
            "",
            "\t/**",
            "\t * exit_dump_len - scx_exit_info.dump buffer length. If 0, the default",
            "\t * value of 32768 is used.",
            "\t */",
            "\tu32 exit_dump_len;",
            "",
            "\t/**",
            "\t * hotplug_seq - A sequence number that may be set by the scheduler to",
            "\t * detect when a hotplug event has occurred during the loading process.",
            "\t * If 0, no detection occurs. Otherwise, the scheduler will fail to",
            "\t * load if the sequence number does not match @scx_hotplug_seq on the",
            "\t * enable path.",
            "\t */",
            "\tu64 hotplug_seq;",
            "",
            "\t/**",
            "\t * name - BPF scheduler's name",
            "\t *",
            "\t * Must be a non-zero valid BPF object name including only isalnum(),",
            "\t * '_' and '.' chars. Shows up in kernel.sched_ext_ops sysctl while the",
            "\t * BPF scheduler is enabled.",
            "\t */",
            "\tchar name[SCX_OPS_NAME_LEN];",
            "};",
            "",
            "enum scx_opi {",
            "\tSCX_OPI_BEGIN\t\t\t= 0,",
            "\tSCX_OPI_NORMAL_BEGIN\t\t= 0,",
            "\tSCX_OPI_NORMAL_END\t\t= SCX_OP_IDX(cpu_online),",
            "\tSCX_OPI_CPU_HOTPLUG_BEGIN\t= SCX_OP_IDX(cpu_online),",
            "\tSCX_OPI_CPU_HOTPLUG_END\t\t= SCX_OP_IDX(init),",
            "\tSCX_OPI_END\t\t\t= SCX_OP_IDX(init),",
            "};",
            "",
            "enum scx_wake_flags {",
            "\t/* expose select WF_* flags as enums */",
            "\tSCX_WAKE_FORK\t\t= WF_FORK,",
            "\tSCX_WAKE_TTWU\t\t= WF_TTWU,",
            "\tSCX_WAKE_SYNC\t\t= WF_SYNC,",
            "};",
            "",
            "enum scx_enq_flags {",
            "\t/* expose select ENQUEUE_* flags as enums */",
            "\tSCX_ENQ_WAKEUP\t\t= ENQUEUE_WAKEUP,",
            "\tSCX_ENQ_HEAD\t\t= ENQUEUE_HEAD,",
            "\tSCX_ENQ_CPU_SELECTED\t= ENQUEUE_RQ_SELECTED,",
            "",
            "\t/* high 32bits are SCX specific */",
            "",
            "\t/*",
            "\t * Set the following to trigger preemption when calling",
            "\t * scx_bpf_dsq_insert() with a local dsq as the target. The slice of the",
            "\t * current task is cleared to zero and the CPU is kicked into the",
            "\t * scheduling path. Implies %SCX_ENQ_HEAD.",
            "\t */",
            "\tSCX_ENQ_PREEMPT\t\t= 1LLU << 32,",
            "",
            "\t/*",
            "\t * The task being enqueued was previously enqueued on the current CPU's",
            "\t * %SCX_DSQ_LOCAL, but was removed from it in a call to the",
            "\t * bpf_scx_reenqueue_local() kfunc. If bpf_scx_reenqueue_local() was",
            "\t * invoked in a ->cpu_release() callback, and the task is again",
            "\t * dispatched back to %SCX_LOCAL_DSQ by this current ->enqueue(), the",
            "\t * task will not be scheduled on the CPU until at least the next invocation",
            "\t * of the ->cpu_acquire() callback.",
            "\t */",
            "\tSCX_ENQ_REENQ\t\t= 1LLU << 40,",
            "",
            "\t/*",
            "\t * The task being enqueued is the only task available for the cpu. By",
            "\t * default, ext core keeps executing such tasks but when",
            "\t * %SCX_OPS_ENQ_LAST is specified, they're ops.enqueue()'d with the",
            "\t * %SCX_ENQ_LAST flag set.",
            "\t *",
            "\t * The BPF scheduler is responsible for triggering a follow-up",
            "\t * scheduling event. Otherwise, Execution may stall.",
            "\t */",
            "\tSCX_ENQ_LAST\t\t= 1LLU << 41,",
            "",
            "\t/* high 8 bits are internal */",
            "\t__SCX_ENQ_INTERNAL_MASK\t= 0xffLLU << 56,",
            "",
            "\tSCX_ENQ_CLEAR_OPSS\t= 1LLU << 56,",
            "\tSCX_ENQ_DSQ_PRIQ\t= 1LLU << 57,",
            "};",
            "",
            "enum scx_deq_flags {",
            "\t/* expose select DEQUEUE_* flags as enums */",
            "\tSCX_DEQ_SLEEP\t\t= DEQUEUE_SLEEP,",
            "",
            "\t/* high 32bits are SCX specific */",
            "",
            "\t/*",
            "\t * The generic core-sched layer decided to execute the task even though",
            "\t * it hasn't been dispatched yet. Dequeue from the BPF side.",
            "\t */",
            "\tSCX_DEQ_CORE_SCHED_EXEC\t= 1LLU << 32,",
            "};",
            "",
            "enum scx_pick_idle_cpu_flags {",
            "\tSCX_PICK_IDLE_CORE\t= 1LLU << 0,\t/* pick a CPU whose SMT siblings are also idle */",
            "};",
            "",
            "enum scx_kick_flags {",
            "\t/*",
            "\t * Kick the target CPU if idle. Guarantees that the target CPU goes",
            "\t * through at least one full scheduling cycle before going idle. If the",
            "\t * target CPU can be determined to be currently not idle and going to go",
            "\t * through a scheduling cycle before going idle, noop.",
            "\t */",
            "\tSCX_KICK_IDLE\t\t= 1LLU << 0,",
            "",
            "\t/*",
            "\t * Preempt the current task and execute the dispatch path. If the",
            "\t * current task of the target CPU is an SCX task, its ->scx.slice is",
            "\t * cleared to zero before the scheduling path is invoked so that the",
            "\t * task expires and the dispatch path is invoked.",
            "\t */",
            "\tSCX_KICK_PREEMPT\t= 1LLU << 1,",
            "",
            "\t/*",
            "\t * Wait for the CPU to be rescheduled. The scx_bpf_kick_cpu() call will",
            "\t * return after the target CPU finishes picking the next task.",
            "\t */",
            "\tSCX_KICK_WAIT\t\t= 1LLU << 2,",
            "};",
            "",
            "enum scx_tg_flags {",
            "\tSCX_TG_ONLINE\t\t= 1U << 0,",
            "\tSCX_TG_INITED\t\t= 1U << 1,",
            "};",
            "",
            "enum scx_ops_enable_state {",
            "\tSCX_OPS_ENABLING,",
            "\tSCX_OPS_ENABLED,",
            "\tSCX_OPS_DISABLING,",
            "\tSCX_OPS_DISABLED,",
            "};",
            "",
            "static const char *scx_ops_enable_state_str[] = {",
            "\t[SCX_OPS_ENABLING]\t= \"enabling\",",
            "\t[SCX_OPS_ENABLED]\t= \"enabled\",",
            "\t[SCX_OPS_DISABLING]\t= \"disabling\",",
            "\t[SCX_OPS_DISABLED]\t= \"disabled\",",
            "};",
            "",
            "/*",
            " * sched_ext_entity->ops_state",
            " *",
            " * Used to track the task ownership between the SCX core and the BPF scheduler.",
            " * State transitions look as follows:",
            " *",
            " * NONE -> QUEUEING -> QUEUED -> DISPATCHING",
            " *   ^              |                 |",
            " *   |              v                 v",
            " *   \\-------------------------------/",
            " *",
            " * QUEUEING and DISPATCHING states can be waited upon. See wait_ops_state() call",
            " * sites for explanations on the conditions being waited upon and why they are",
            " * safe. Transitions out of them into NONE or QUEUED must store_release and the",
            " * waiters should load_acquire.",
            " *",
            " * Tracking scx_ops_state enables sched_ext core to reliably determine whether",
            " * any given task can be dispatched by the BPF scheduler at all times and thus",
            " * relaxes the requirements on the BPF scheduler. This allows the BPF scheduler",
            " * to try to dispatch any task anytime regardless of its state as the SCX core",
            " * can safely reject invalid dispatches.",
            " */",
            "enum scx_ops_state {",
            "\tSCX_OPSS_NONE,\t\t/* owned by the SCX core */",
            "\tSCX_OPSS_QUEUEING,\t/* in transit to the BPF scheduler */",
            "\tSCX_OPSS_QUEUED,\t/* owned by the BPF scheduler */",
            "\tSCX_OPSS_DISPATCHING,\t/* in transit back to the SCX core */",
            "",
            "\t/*",
            "\t * QSEQ brands each QUEUED instance so that, when dispatch races",
            "\t * dequeue/requeue, the dispatcher can tell whether it still has a claim",
            "\t * on the task being dispatched.",
            "\t *",
            "\t * As some 32bit archs can't do 64bit store_release/load_acquire,",
            "\t * p->scx.ops_state is atomic_long_t which leaves 30 bits for QSEQ on",
            "\t * 32bit machines. The dispatch race window QSEQ protects is very narrow",
            "\t * and runs with IRQ disabled. 30 bits should be sufficient.",
            "\t */",
            "\tSCX_OPSS_QSEQ_SHIFT\t= 2,",
            "};",
            "",
            "/* Use macros to ensure that the type is unsigned long for the masks */",
            "#define SCX_OPSS_STATE_MASK\t((1LU << SCX_OPSS_QSEQ_SHIFT) - 1)",
            "#define SCX_OPSS_QSEQ_MASK\t(~SCX_OPSS_STATE_MASK)",
            "",
            "/*",
            " * During exit, a task may schedule after losing its PIDs. When disabling the",
            " * BPF scheduler, we need to be able to iterate tasks in every state to",
            " * guarantee system safety. Maintain a dedicated task list which contains every",
            " * task between its fork and eventual free.",
            " */",
            "static DEFINE_SPINLOCK(scx_tasks_lock);",
            "static LIST_HEAD(scx_tasks);",
            "",
            "/* ops enable/disable */",
            "static struct kthread_worker *scx_ops_helper;",
            "static DEFINE_MUTEX(scx_ops_enable_mutex);",
            "DEFINE_STATIC_KEY_FALSE(__scx_ops_enabled);",
            "DEFINE_STATIC_PERCPU_RWSEM(scx_fork_rwsem);",
            "static atomic_t scx_ops_enable_state_var = ATOMIC_INIT(SCX_OPS_DISABLED);",
            "static int scx_ops_bypass_depth;",
            "static DEFINE_RAW_SPINLOCK(__scx_ops_bypass_lock);",
            "static bool scx_ops_init_task_enabled;",
            "static bool scx_switching_all;",
            "DEFINE_STATIC_KEY_FALSE(__scx_switched_all);",
            "",
            "static struct sched_ext_ops scx_ops;",
            "static bool scx_warned_zero_slice;",
            "",
            "static DEFINE_STATIC_KEY_FALSE(scx_ops_enq_last);",
            "static DEFINE_STATIC_KEY_FALSE(scx_ops_enq_exiting);",
            "static DEFINE_STATIC_KEY_FALSE(scx_ops_cpu_preempt);",
            "static DEFINE_STATIC_KEY_FALSE(scx_builtin_idle_enabled);",
            "",
            "static struct static_key_false scx_has_op[SCX_OPI_END] =",
            "\t{ [0 ... SCX_OPI_END-1] = STATIC_KEY_FALSE_INIT };",
            "",
            "static atomic_t scx_exit_kind = ATOMIC_INIT(SCX_EXIT_DONE);",
            "static struct scx_exit_info *scx_exit_info;",
            "",
            "static atomic_long_t scx_nr_rejected = ATOMIC_LONG_INIT(0);",
            "static atomic_long_t scx_hotplug_seq = ATOMIC_LONG_INIT(0);",
            "",
            "/*",
            " * A monotically increasing sequence number that is incremented every time a",
            " * scheduler is enabled. This can be used by to check if any custom sched_ext",
            " * scheduler has ever been used in the system.",
            " */",
            "static atomic_long_t scx_enable_seq = ATOMIC_LONG_INIT(0);",
            "",
            "/*",
            " * The maximum amount of time in jiffies that a task may be runnable without",
            " * being scheduled on a CPU. If this timeout is exceeded, it will trigger",
            " * scx_ops_error().",
            " */",
            "static unsigned long scx_watchdog_timeout;",
            "",
            "/*",
            " * The last time the delayed work was run. This delayed work relies on",
            " * ksoftirqd being able to run to service timer interrupts, so it's possible",
            " * that this work itself could get wedged. To account for this, we check that",
            " * it's not stalled in the timer tick, and trigger an error if it is.",
            " */",
            "static unsigned long scx_watchdog_timestamp = INITIAL_JIFFIES;",
            "",
            "static struct delayed_work scx_watchdog_work;",
            "",
            "/* idle tracking */",
            "#ifdef CONFIG_SMP",
            "#ifdef CONFIG_CPUMASK_OFFSTACK",
            "#define CL_ALIGNED_IF_ONSTACK",
            "#else",
            "#define CL_ALIGNED_IF_ONSTACK __cacheline_aligned_in_smp",
            "#endif",
            "",
            "static struct {",
            "\tcpumask_var_t cpu;",
            "\tcpumask_var_t smt;",
            "} idle_masks CL_ALIGNED_IF_ONSTACK;",
            "",
            "#endif\t/* CONFIG_SMP */",
            "",
            "/* for %SCX_KICK_WAIT */",
            "static unsigned long __percpu *scx_kick_cpus_pnt_seqs;",
            "",
            "/*",
            " * Direct dispatch marker.",
            " *",
            " * Non-NULL values are used for direct dispatch from enqueue path. A valid",
            " * pointer points to the task currently being enqueued. An ERR_PTR value is used",
            " * to indicate that direct dispatch has already happened.",
            " */",
            "static DEFINE_PER_CPU(struct task_struct *, direct_dispatch_task);",
            "",
            "/*",
            " * Dispatch queues.",
            " *",
            " * The global DSQ (%SCX_DSQ_GLOBAL) is split per-node for scalability. This is",
            " * to avoid live-locking in bypass mode where all tasks are dispatched to",
            " * %SCX_DSQ_GLOBAL and all CPUs consume from it. If per-node split isn't",
            " * sufficient, it can be further split.",
            " */",
            "static struct scx_dispatch_q **global_dsqs;",
            "",
            "static const struct rhashtable_params dsq_hash_params = {",
            "\t.key_len\t\t= 8,",
            "\t.key_offset\t\t= offsetof(struct scx_dispatch_q, id),",
            "\t.head_offset\t\t= offsetof(struct scx_dispatch_q, hash_node),",
            "};",
            "",
            "static struct rhashtable dsq_hash;",
            "static LLIST_HEAD(dsqs_to_free);",
            "",
            "/* dispatch buf */",
            "struct scx_dsp_buf_ent {",
            "\tstruct task_struct\t*task;",
            "\tunsigned long\t\tqseq;",
            "\tu64\t\t\tdsq_id;",
            "\tu64\t\t\tenq_flags;",
            "};",
            "",
            "static u32 scx_dsp_max_batch;",
            "",
            "struct scx_dsp_ctx {",
            "\tstruct rq\t\t*rq;",
            "\tu32\t\t\tcursor;",
            "\tu32\t\t\tnr_tasks;",
            "\tstruct scx_dsp_buf_ent\tbuf[];",
            "};",
            "",
            "static struct scx_dsp_ctx __percpu *scx_dsp_ctx;",
            "",
            "/* string formatting from BPF */",
            "struct scx_bstr_buf {",
            "\tu64\t\t\tdata[MAX_BPRINTF_VARARGS];",
            "\tchar\t\t\tline[SCX_EXIT_MSG_LEN];",
            "};",
            "",
            "static DEFINE_RAW_SPINLOCK(scx_exit_bstr_buf_lock);",
            "static struct scx_bstr_buf scx_exit_bstr_buf;",
            "",
            "/* ops debug dump */",
            "struct scx_dump_data {",
            "\ts32\t\t\tcpu;",
            "\tbool\t\t\tfirst;",
            "\ts32\t\t\tcursor;",
            "\tstruct seq_buf\t\t*s;",
            "\tconst char\t\t*prefix;",
            "\tstruct scx_bstr_buf\tbuf;",
            "};",
            "",
            "static struct scx_dump_data scx_dump_data = {",
            "\t.cpu\t\t\t= -1,",
            "};",
            "",
            "/* /sys/kernel/sched_ext interface */",
            "static struct kset *scx_kset;",
            "static struct kobject *scx_root_kobj;",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/sched_ext.h>",
            "",
            "static void process_ddsp_deferred_locals(struct rq *rq);",
            "static void scx_bpf_kick_cpu(s32 cpu, u64 flags);",
            "static __printf(3, 4) void scx_ops_exit_kind(enum scx_exit_kind kind,",
            "\t\t\t\t\t     s64 exit_code,",
            "\t\t\t\t\t     const char *fmt, ...);",
            "",
            "#define scx_ops_error_kind(err, fmt, args...)\t\t\t\t\t\\",
            "\tscx_ops_exit_kind((err), 0, fmt, ##args)",
            "",
            "#define scx_ops_exit(code, fmt, args...)\t\t\t\t\t\\",
            "\tscx_ops_exit_kind(SCX_EXIT_UNREG_KERN, (code), fmt, ##args)",
            "",
            "#define scx_ops_error(fmt, args...)\t\t\t\t\t\t\\",
            "\tscx_ops_error_kind(SCX_EXIT_ERROR, fmt, ##args)",
            "",
            "#define SCX_HAS_OP(op)\tstatic_branch_likely(&scx_has_op[SCX_OP_IDX(op)])",
            "",
            "static long jiffies_delta_msecs(unsigned long at, unsigned long now)",
            "{",
            "\tif (time_after(at, now))",
            "\t\treturn jiffies_to_msecs(at - now);",
            "\telse",
            "\t\treturn -(long)jiffies_to_msecs(now - at);",
            "}",
            "",
            "/* if the highest set bit is N, return a mask with bits [N+1, 31] set */"
          ],
          "function_name": null,
          "description": "定义了BPF可扩展调度器的核心数据结构和常量，包括调度操作表（sched_ext_ops）、状态标志、错误码及辅助宏，为BPF调度策略实现提供基础框架。",
          "similarity": 0.5811399221420288
        },
        {
          "chunk_id": 30,
          "file_path": "kernel/sched/ext.c",
          "start_line": 5824,
          "end_line": 5938,
          "content": [
            "void __init init_sched_ext_class(void)",
            "{",
            "\ts32 cpu, v;",
            "",
            "\t/*",
            "\t * The following is to prevent the compiler from optimizing out the enum",
            "\t * definitions so that BPF scheduler implementations can use them",
            "\t * through the generated vmlinux.h.",
            "\t */",
            "\tWRITE_ONCE(v, SCX_ENQ_WAKEUP | SCX_DEQ_SLEEP | SCX_KICK_PREEMPT |",
            "\t\t   SCX_TG_ONLINE);",
            "",
            "\tBUG_ON(rhashtable_init(&dsq_hash, &dsq_hash_params));",
            "#ifdef CONFIG_SMP",
            "\tBUG_ON(!alloc_cpumask_var(&idle_masks.cpu, GFP_KERNEL));",
            "\tBUG_ON(!alloc_cpumask_var(&idle_masks.smt, GFP_KERNEL));",
            "#endif",
            "\tscx_kick_cpus_pnt_seqs =",
            "\t\t__alloc_percpu(sizeof(scx_kick_cpus_pnt_seqs[0]) * nr_cpu_ids,",
            "\t\t\t       __alignof__(scx_kick_cpus_pnt_seqs[0]));",
            "\tBUG_ON(!scx_kick_cpus_pnt_seqs);",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct rq *rq = cpu_rq(cpu);",
            "",
            "\t\tinit_dsq(&rq->scx.local_dsq, SCX_DSQ_LOCAL);",
            "\t\tINIT_LIST_HEAD(&rq->scx.runnable_list);",
            "\t\tINIT_LIST_HEAD(&rq->scx.ddsp_deferred_locals);",
            "",
            "\t\tBUG_ON(!zalloc_cpumask_var(&rq->scx.cpus_to_kick, GFP_KERNEL));",
            "\t\tBUG_ON(!zalloc_cpumask_var(&rq->scx.cpus_to_kick_if_idle, GFP_KERNEL));",
            "\t\tBUG_ON(!zalloc_cpumask_var(&rq->scx.cpus_to_preempt, GFP_KERNEL));",
            "\t\tBUG_ON(!zalloc_cpumask_var(&rq->scx.cpus_to_wait, GFP_KERNEL));",
            "\t\tinit_irq_work(&rq->scx.deferred_irq_work, deferred_irq_workfn);",
            "\t\tinit_irq_work(&rq->scx.kick_cpus_irq_work, kick_cpus_irq_workfn);",
            "",
            "\t\tif (cpu_online(cpu))",
            "\t\t\tcpu_rq(cpu)->scx.flags |= SCX_RQ_ONLINE;",
            "\t}",
            "",
            "\tregister_sysrq_key('S', &sysrq_sched_ext_reset_op);",
            "\tregister_sysrq_key('D', &sysrq_sched_ext_dump_op);",
            "\tINIT_DELAYED_WORK(&scx_watchdog_work, scx_watchdog_workfn);",
            "}",
            "static bool check_builtin_idle_enabled(void)",
            "{",
            "\tif (static_branch_likely(&scx_builtin_idle_enabled))",
            "\t\treturn true;",
            "",
            "\tscx_ops_error(\"built-in idle tracking is disabled\");",
            "\treturn false;",
            "}",
            "__bpf_kfunc s32 scx_bpf_select_cpu_dfl(struct task_struct *p, s32 prev_cpu,",
            "\t\t\t\t       u64 wake_flags, bool *is_idle)",
            "{",
            "\tif (!ops_cpu_valid(prev_cpu, NULL))",
            "\t\tgoto prev_cpu;",
            "",
            "\tif (!check_builtin_idle_enabled())",
            "\t\tgoto prev_cpu;",
            "",
            "\tif (!scx_kf_allowed(SCX_KF_SELECT_CPU))",
            "\t\tgoto prev_cpu;",
            "",
            "#ifdef CONFIG_SMP",
            "\treturn scx_select_cpu_dfl(p, prev_cpu, wake_flags, is_idle);",
            "#endif",
            "",
            "prev_cpu:",
            "\t*is_idle = false;",
            "\treturn prev_cpu;",
            "}",
            "static bool scx_dsq_insert_preamble(struct task_struct *p, u64 enq_flags)",
            "{",
            "\tif (!scx_kf_allowed(SCX_KF_ENQUEUE | SCX_KF_DISPATCH))",
            "\t\treturn false;",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\tif (unlikely(!p)) {",
            "\t\tscx_ops_error(\"called with NULL task\");",
            "\t\treturn false;",
            "\t}",
            "",
            "\tif (unlikely(enq_flags & __SCX_ENQ_INTERNAL_MASK)) {",
            "\t\tscx_ops_error(\"invalid enq_flags 0x%llx\", enq_flags);",
            "\t\treturn false;",
            "\t}",
            "",
            "\treturn true;",
            "}",
            "static void scx_dsq_insert_commit(struct task_struct *p, u64 dsq_id,",
            "\t\t\t\t  u64 enq_flags)",
            "{",
            "\tstruct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);",
            "\tstruct task_struct *ddsp_task;",
            "",
            "\tddsp_task = __this_cpu_read(direct_dispatch_task);",
            "\tif (ddsp_task) {",
            "\t\tmark_direct_dispatch(ddsp_task, p, dsq_id, enq_flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (unlikely(dspc->cursor >= scx_dsp_max_batch)) {",
            "\t\tscx_ops_error(\"dispatch buffer overflow\");",
            "\t\treturn;",
            "\t}",
            "",
            "\tdspc->buf[dspc->cursor++] = (struct scx_dsp_buf_ent){",
            "\t\t.task = p,",
            "\t\t.qseq = atomic_long_read(&p->scx.ops_state) & SCX_OPSS_QSEQ_MASK,",
            "\t\t.dsq_id = dsq_id,",
            "\t\t.enq_flags = enq_flags,",
            "\t};",
            "}"
          ],
          "function_name": "init_sched_ext_class, check_builtin_idle_enabled, scx_bpf_select_cpu_dfl, scx_dsq_insert_preamble, scx_dsq_insert_commit",
          "description": "该代码段主要初始化扩展调度器（SCX）的核心数据结构及资源，包括DSQ哈希表、CPU掩码分配和PER-CPU数组，并注册系统请求接口。其中`init_sched_ext_class`负责全局初始化，`check_builtin_idle_enabled`检查内置空闲跟踪是否启用，`scx_bpf_select_cpu_dfl`实现BPF调度策略的CPU选择逻辑，`scx_dsq_insert_preamble`/`scx_dsq_insert_commit`分别处理任务入队前的校验与提交操作。",
          "similarity": 0.5568411350250244
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/sched/ext.c",
          "start_line": 3141,
          "end_line": 3247,
          "content": [
            "static int select_task_rq_scx(struct task_struct *p, int prev_cpu, int wake_flags)",
            "{",
            "\t/*",
            "\t * sched_exec() calls with %WF_EXEC when @p is about to exec(2) as it",
            "\t * can be a good migration opportunity with low cache and memory",
            "\t * footprint. Returning a CPU different than @prev_cpu triggers",
            "\t * immediate rq migration. However, for SCX, as the current rq",
            "\t * association doesn't dictate where the task is going to run, this",
            "\t * doesn't fit well. If necessary, we can later add a dedicated method",
            "\t * which can decide to preempt self to force it through the regular",
            "\t * scheduling path.",
            "\t */",
            "\tif (unlikely(wake_flags & WF_EXEC))",
            "\t\treturn prev_cpu;",
            "",
            "\tif (SCX_HAS_OP(select_cpu) && !scx_rq_bypassing(task_rq(p))) {",
            "\t\ts32 cpu;",
            "\t\tstruct task_struct **ddsp_taskp;",
            "",
            "\t\tddsp_taskp = this_cpu_ptr(&direct_dispatch_task);",
            "\t\tWARN_ON_ONCE(*ddsp_taskp);",
            "\t\t*ddsp_taskp = p;",
            "",
            "\t\tcpu = SCX_CALL_OP_TASK_RET(SCX_KF_ENQUEUE | SCX_KF_SELECT_CPU,",
            "\t\t\t\t\t   select_cpu, p, prev_cpu, wake_flags);",
            "\t\t*ddsp_taskp = NULL;",
            "\t\tif (ops_cpu_valid(cpu, \"from ops.select_cpu()\"))",
            "\t\t\treturn cpu;",
            "\t\telse",
            "\t\t\treturn prev_cpu;",
            "\t} else {",
            "\t\tbool found;",
            "\t\ts32 cpu;",
            "",
            "\t\tcpu = scx_select_cpu_dfl(p, prev_cpu, wake_flags, &found);",
            "\t\tif (found) {",
            "\t\t\tp->scx.slice = SCX_SLICE_DFL;",
            "\t\t\tp->scx.ddsp_dsq_id = SCX_DSQ_LOCAL;",
            "\t\t}",
            "\t\treturn cpu;",
            "\t}",
            "}",
            "static void task_woken_scx(struct rq *rq, struct task_struct *p)",
            "{",
            "\trun_deferred(rq);",
            "}",
            "static void set_cpus_allowed_scx(struct task_struct *p,",
            "\t\t\t\t struct affinity_context *ac)",
            "{",
            "\tset_cpus_allowed_common(p, ac);",
            "",
            "\t/*",
            "\t * The effective cpumask is stored in @p->cpus_ptr which may temporarily",
            "\t * differ from the configured one in @p->cpus_mask. Always tell the bpf",
            "\t * scheduler the effective one.",
            "\t *",
            "\t * Fine-grained memory write control is enforced by BPF making the const",
            "\t * designation pointless. Cast it away when calling the operation.",
            "\t */",
            "\tif (SCX_HAS_OP(set_cpumask))",
            "\t\tSCX_CALL_OP_TASK(SCX_KF_REST, set_cpumask, p,",
            "\t\t\t\t (struct cpumask *)p->cpus_ptr);",
            "}",
            "static void reset_idle_masks(void)",
            "{",
            "\t/*",
            "\t * Consider all online cpus idle. Should converge to the actual state",
            "\t * quickly.",
            "\t */",
            "\tcpumask_copy(idle_masks.cpu, cpu_online_mask);",
            "\tcpumask_copy(idle_masks.smt, cpu_online_mask);",
            "}",
            "void __scx_update_idle(struct rq *rq, bool idle)",
            "{",
            "\tint cpu = cpu_of(rq);",
            "",
            "\tif (SCX_HAS_OP(update_idle) && !scx_rq_bypassing(rq)) {",
            "\t\tSCX_CALL_OP(SCX_KF_REST, update_idle, cpu_of(rq), idle);",
            "\t\tif (!static_branch_unlikely(&scx_builtin_idle_enabled))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\tif (idle)",
            "\t\tcpumask_set_cpu(cpu, idle_masks.cpu);",
            "\telse",
            "\t\tcpumask_clear_cpu(cpu, idle_masks.cpu);",
            "",
            "#ifdef CONFIG_SCHED_SMT",
            "\tif (sched_smt_active()) {",
            "\t\tconst struct cpumask *smt = cpu_smt_mask(cpu);",
            "",
            "\t\tif (idle) {",
            "\t\t\t/*",
            "\t\t\t * idle_masks.smt handling is racy but that's fine as",
            "\t\t\t * it's only for optimization and self-correcting.",
            "\t\t\t */",
            "\t\t\tfor_each_cpu(cpu, smt) {",
            "\t\t\t\tif (!cpumask_test_cpu(cpu, idle_masks.cpu))",
            "\t\t\t\t\treturn;",
            "\t\t\t}",
            "\t\t\tcpumask_or(idle_masks.smt, idle_masks.smt, smt);",
            "\t\t} else {",
            "\t\t\tcpumask_andnot(idle_masks.smt, idle_masks.smt, smt);",
            "\t\t}",
            "\t}",
            "#endif",
            "}"
          ],
          "function_name": "select_task_rq_scx, task_woken_scx, set_cpus_allowed_scx, reset_idle_masks, __scx_update_idle",
          "description": "该代码块实现了SCX（Scalable Cores eXtension）调度扩展的辅助逻辑，主要包含任务CPU选择、唤醒后处理、CPU亲和性设置及空闲状态管理等功能。  \n`select_task_rq_scx` 根据SCX操作是否存在决定任务迁移目标CPU，`set_cpus_allowed_scx` 更新BPF调度器的CPU掩码，`reset_idle_masks` 和 `__scx_update_idle` 维护全局空闲CPU掩码以优化负载均衡。  \n由于缺少`run_deferred`等关键函数定义及SCX操作的具体实现，上下文存在不完整性。",
          "similarity": 0.5492422580718994
        },
        {
          "chunk_id": 31,
          "file_path": "kernel/sched/ext.c",
          "start_line": 6015,
          "end_line": 6165,
          "content": [
            "__bpf_kfunc void scx_bpf_dsq_insert(struct task_struct *p, u64 dsq_id, u64 slice,",
            "\t\t\t\t    u64 enq_flags)",
            "{",
            "\tif (!scx_dsq_insert_preamble(p, enq_flags))",
            "\t\treturn;",
            "",
            "\tif (slice)",
            "\t\tp->scx.slice = slice;",
            "\telse",
            "\t\tp->scx.slice = p->scx.slice ?: 1;",
            "",
            "\tscx_dsq_insert_commit(p, dsq_id, enq_flags);",
            "}",
            "__bpf_kfunc void scx_bpf_dispatch(struct task_struct *p, u64 dsq_id, u64 slice,",
            "\t\t\t\t  u64 enq_flags)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_dispatch() renamed to scx_bpf_dsq_insert()\");",
            "\tscx_bpf_dsq_insert(p, dsq_id, slice, enq_flags);",
            "}",
            "__bpf_kfunc void scx_bpf_dsq_insert_vtime(struct task_struct *p, u64 dsq_id,",
            "\t\t\t\t\t  u64 slice, u64 vtime, u64 enq_flags)",
            "{",
            "\tif (!scx_dsq_insert_preamble(p, enq_flags))",
            "\t\treturn;",
            "",
            "\tif (slice)",
            "\t\tp->scx.slice = slice;",
            "\telse",
            "\t\tp->scx.slice = p->scx.slice ?: 1;",
            "",
            "\tp->scx.dsq_vtime = vtime;",
            "",
            "\tscx_dsq_insert_commit(p, dsq_id, enq_flags | SCX_ENQ_DSQ_PRIQ);",
            "}",
            "__bpf_kfunc void scx_bpf_dispatch_vtime(struct task_struct *p, u64 dsq_id,",
            "\t\t\t\t\tu64 slice, u64 vtime, u64 enq_flags)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_dispatch_vtime() renamed to scx_bpf_dsq_insert_vtime()\");",
            "\tscx_bpf_dsq_insert_vtime(p, dsq_id, slice, vtime, enq_flags);",
            "}",
            "static bool scx_dsq_move(struct bpf_iter_scx_dsq_kern *kit,",
            "\t\t\t struct task_struct *p, u64 dsq_id, u64 enq_flags)",
            "{",
            "",
            "\tstruct scx_dispatch_q *src_dsq = kit->dsq, *dst_dsq;",
            "\tstruct rq *this_rq, *src_rq, *dst_rq, *locked_rq;",
            "\tbool dispatched = false;",
            "\tbool in_balance;",
            "\tunsigned long flags;",
            "",
            "\tif (!scx_kf_allowed_if_unlocked() && !scx_kf_allowed(SCX_KF_DISPATCH))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Can be called from either ops.dispatch() locking this_rq() or any",
            "\t * context where no rq lock is held. If latter, lock @p's task_rq which",
            "\t * we'll likely need anyway.",
            "\t */",
            "\tsrc_rq = task_rq(p);",
            "",
            "\tlocal_irq_save(flags);",
            "\tthis_rq = this_rq();",
            "\tin_balance = this_rq->scx.flags & SCX_RQ_IN_BALANCE;",
            "",
            "\tif (in_balance) {",
            "\t\tif (this_rq != src_rq) {",
            "\t\t\traw_spin_rq_unlock(this_rq);",
            "\t\t\traw_spin_rq_lock(src_rq);",
            "\t\t}",
            "\t} else {",
            "\t\traw_spin_rq_lock(src_rq);",
            "\t}",
            "",
            "\tlocked_rq = src_rq;",
            "\traw_spin_lock(&src_dsq->lock);",
            "",
            "\t/*",
            "\t * Did someone else get to it? @p could have already left $src_dsq, got",
            "\t * re-enqueud, or be in the process of being consumed by someone else.",
            "\t */",
            "\tif (unlikely(p->scx.dsq != src_dsq ||",
            "\t\t     u32_before(kit->cursor.priv, p->scx.dsq_seq) ||",
            "\t\t     p->scx.holding_cpu >= 0) ||",
            "\t    WARN_ON_ONCE(src_rq != task_rq(p))) {",
            "\t\traw_spin_unlock(&src_dsq->lock);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* @p is still on $src_dsq and stable, determine the destination */",
            "\tdst_dsq = find_dsq_for_dispatch(this_rq, dsq_id, p);",
            "",
            "\tif (dst_dsq->id == SCX_DSQ_LOCAL) {",
            "\t\tdst_rq = container_of(dst_dsq, struct rq, scx.local_dsq);",
            "\t\tif (!task_can_run_on_remote_rq(p, dst_rq, true)) {",
            "\t\t\tdst_dsq = find_global_dsq(p);",
            "\t\t\tdst_rq = src_rq;",
            "\t\t}",
            "\t} else {",
            "\t\t/* no need to migrate if destination is a non-local DSQ */",
            "\t\tdst_rq = src_rq;",
            "\t}",
            "",
            "\t/*",
            "\t * Move @p into $dst_dsq. If $dst_dsq is the local DSQ of a different",
            "\t * CPU, @p will be migrated.",
            "\t */",
            "\tif (dst_dsq->id == SCX_DSQ_LOCAL) {",
            "\t\t/* @p is going from a non-local DSQ to a local DSQ */",
            "\t\tif (src_rq == dst_rq) {",
            "\t\t\ttask_unlink_from_dsq(p, src_dsq);",
            "\t\t\tmove_local_task_to_local_dsq(p, enq_flags,",
            "\t\t\t\t\t\t     src_dsq, dst_rq);",
            "\t\t\traw_spin_unlock(&src_dsq->lock);",
            "\t\t} else {",
            "\t\t\traw_spin_unlock(&src_dsq->lock);",
            "\t\t\tmove_remote_task_to_local_dsq(p, enq_flags,",
            "\t\t\t\t\t\t      src_rq, dst_rq);",
            "\t\t\tlocked_rq = dst_rq;",
            "\t\t}",
            "\t} else {",
            "\t\t/*",
            "\t\t * @p is going from a non-local DSQ to a non-local DSQ. As",
            "\t\t * $src_dsq is already locked, do an abbreviated dequeue.",
            "\t\t */",
            "\t\ttask_unlink_from_dsq(p, src_dsq);",
            "\t\tp->scx.dsq = NULL;",
            "\t\traw_spin_unlock(&src_dsq->lock);",
            "",
            "\t\tif (kit->cursor.flags & __SCX_DSQ_ITER_HAS_VTIME)",
            "\t\t\tp->scx.dsq_vtime = kit->vtime;",
            "\t\tdispatch_enqueue(dst_dsq, p, enq_flags);",
            "\t}",
            "",
            "\tif (kit->cursor.flags & __SCX_DSQ_ITER_HAS_SLICE)",
            "\t\tp->scx.slice = kit->slice;",
            "",
            "\tdispatched = true;",
            "out:",
            "\tif (in_balance) {",
            "\t\tif (this_rq != locked_rq) {",
            "\t\t\traw_spin_rq_unlock(locked_rq);",
            "\t\t\traw_spin_rq_lock(this_rq);",
            "\t\t}",
            "\t} else {",
            "\t\traw_spin_rq_unlock_irqrestore(locked_rq, flags);",
            "\t}",
            "",
            "\tkit->cursor.flags &= ~(__SCX_DSQ_ITER_HAS_SLICE |",
            "\t\t\t       __SCX_DSQ_ITER_HAS_VTIME);",
            "\treturn dispatched;",
            "}"
          ],
          "function_name": "scx_bpf_dsq_insert, scx_bpf_dispatch, scx_bpf_dsq_insert_vtime, scx_bpf_dispatch_vtime, scx_dsq_move",
          "description": "该代码块实现了基于BPF的调度扩展功能，包含任务队列操作接口及迁移逻辑。  \n`scx_bpf_dsq_insert`系列函数用于将任务插入指定数据队列（支持普通切片和带虚拟时间参数的场景），`scx_dsq_move`负责跨队列迁移任务并处理锁竞争与CPU亲和性。  \n代码未显式提及新API，但存在接口弃用警告（如`scx_bpf_dispatch`被`scx_bpf_dsq_insert`替代）。",
          "similarity": 0.5432388186454773
        },
        {
          "chunk_id": 32,
          "file_path": "kernel/sched/ext.c",
          "start_line": 6215,
          "end_line": 6316,
          "content": [
            "__bpf_kfunc u32 scx_bpf_dispatch_nr_slots(void)",
            "{",
            "\tif (!scx_kf_allowed(SCX_KF_DISPATCH))",
            "\t\treturn 0;",
            "",
            "\treturn scx_dsp_max_batch - __this_cpu_read(scx_dsp_ctx->cursor);",
            "}",
            "__bpf_kfunc void scx_bpf_dispatch_cancel(void)",
            "{",
            "\tstruct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);",
            "",
            "\tif (!scx_kf_allowed(SCX_KF_DISPATCH))",
            "\t\treturn;",
            "",
            "\tif (dspc->cursor > 0)",
            "\t\tdspc->cursor--;",
            "\telse",
            "\t\tscx_ops_error(\"dispatch buffer underflow\");",
            "}",
            "__bpf_kfunc bool scx_bpf_dsq_move_to_local(u64 dsq_id)",
            "{",
            "\tstruct scx_dsp_ctx *dspc = this_cpu_ptr(scx_dsp_ctx);",
            "\tstruct scx_dispatch_q *dsq;",
            "",
            "\tif (!scx_kf_allowed(SCX_KF_DISPATCH))",
            "\t\treturn false;",
            "",
            "\tflush_dispatch_buf(dspc->rq);",
            "",
            "\tdsq = find_user_dsq(dsq_id);",
            "\tif (unlikely(!dsq)) {",
            "\t\tscx_ops_error(\"invalid DSQ ID 0x%016llx\", dsq_id);",
            "\t\treturn false;",
            "\t}",
            "",
            "\tif (consume_dispatch_q(dspc->rq, dsq)) {",
            "\t\t/*",
            "\t\t * A successfully consumed task can be dequeued before it starts",
            "\t\t * running while the CPU is trying to migrate other dispatched",
            "\t\t * tasks. Bump nr_tasks to tell balance_scx() to retry on empty",
            "\t\t * local DSQ.",
            "\t\t */",
            "\t\tdspc->nr_tasks++;",
            "\t\treturn true;",
            "\t} else {",
            "\t\treturn false;",
            "\t}",
            "}",
            "__bpf_kfunc bool scx_bpf_consume(u64 dsq_id)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_consume() renamed to scx_bpf_dsq_move_to_local()\");",
            "\treturn scx_bpf_dsq_move_to_local(dsq_id);",
            "}",
            "__bpf_kfunc void scx_bpf_dsq_move_set_slice(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t\t    u64 slice)",
            "{",
            "\tstruct bpf_iter_scx_dsq_kern *kit = (void *)it__iter;",
            "",
            "\tkit->slice = slice;",
            "\tkit->cursor.flags |= __SCX_DSQ_ITER_HAS_SLICE;",
            "}",
            "__bpf_kfunc void scx_bpf_dispatch_from_dsq_set_slice(",
            "\t\t\tstruct bpf_iter_scx_dsq *it__iter, u64 slice)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_dispatch_from_dsq_set_slice() renamed to scx_bpf_dsq_move_set_slice()\");",
            "\tscx_bpf_dsq_move_set_slice(it__iter, slice);",
            "}",
            "__bpf_kfunc void scx_bpf_dsq_move_set_vtime(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t\t    u64 vtime)",
            "{",
            "\tstruct bpf_iter_scx_dsq_kern *kit = (void *)it__iter;",
            "",
            "\tkit->vtime = vtime;",
            "\tkit->cursor.flags |= __SCX_DSQ_ITER_HAS_VTIME;",
            "}",
            "__bpf_kfunc void scx_bpf_dispatch_from_dsq_set_vtime(",
            "\t\t\tstruct bpf_iter_scx_dsq *it__iter, u64 vtime)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_dispatch_from_dsq_set_vtime() renamed to scx_bpf_dsq_move_set_vtime()\");",
            "\tscx_bpf_dsq_move_set_vtime(it__iter, vtime);",
            "}",
            "__bpf_kfunc bool scx_bpf_dsq_move(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t  struct task_struct *p, u64 dsq_id,",
            "\t\t\t\t  u64 enq_flags)",
            "{",
            "\treturn scx_dsq_move((struct bpf_iter_scx_dsq_kern *)it__iter,",
            "\t\t\t    p, dsq_id, enq_flags);",
            "}",
            "__bpf_kfunc bool scx_bpf_dispatch_from_dsq(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t\t   struct task_struct *p, u64 dsq_id,",
            "\t\t\t\t\t   u64 enq_flags)",
            "{",
            "\tprintk_deferred_once(KERN_WARNING \"sched_ext: scx_bpf_dispatch_from_dsq() renamed to scx_bpf_dsq_move()\");",
            "\treturn scx_bpf_dsq_move(it__iter, p, dsq_id, enq_flags);",
            "}",
            "__bpf_kfunc bool scx_bpf_dsq_move_vtime(struct bpf_iter_scx_dsq *it__iter,",
            "\t\t\t\t\tstruct task_struct *p, u64 dsq_id,",
            "\t\t\t\t\tu64 enq_flags)",
            "{",
            "\treturn scx_dsq_move((struct bpf_iter_scx_dsq_kern *)it__iter,",
            "\t\t\t    p, dsq_id, enq_flags | SCX_ENQ_DSQ_PRIQ);",
            "}"
          ],
          "function_name": "scx_bpf_dispatch_nr_slots, scx_bpf_dispatch_cancel, scx_bpf_dsq_move_to_local, scx_bpf_consume, scx_bpf_dsq_move_set_slice, scx_bpf_dispatch_from_dsq_set_slice, scx_bpf_dsq_move_set_vtime, scx_bpf_dispatch_from_dsq_set_vtime, scx_bpf_dsq_move, scx_bpf_dispatch_from_dsq, scx_bpf_dsq_move_vtime",
          "description": "此代码段实现了调度扩展（SCX）框架中与DSQ（Dispatch Queue）操作相关的BPF辅助函数，核心功能包括任务分发槽位管理、DSQ任务迁移及参数配置。各函数通过权限校验后执行对应操作：`scx_bpf_dispatch_nr_slots`计算剩余分发容量，`scx_bpf_dsq_move_to_local`将任务从远程DSQ迁移到本地并触发负载均衡重试，其他函数用于设置迭代器参数或处理任务入队逻辑。由于缺少部分上下文（如`scx_kf_allowed`、`consume_dispatch_q`等关键实现），部分行为需结合外围代码理解。",
          "similarity": 0.5403447151184082
        }
      ]
    },
    {
      "source_file": "kernel/sched/build_policy.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:56:50\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\build_policy.c`\n\n---\n\n# `sched/build_policy.c` 技术文档\n\n## 1. 文件概述\n\n`build_policy.c` 是 Linux 内核调度子系统中的一个构建辅助文件，其主要作用是将多个与调度策略相关的源代码模块（如实时调度、截止时间调度、CPU 时间统计等）合并到一个编译单元中进行编译。这种设计并非用于实现具体调度逻辑，而是出于**构建效率优化**的目的：通过减少重复包含头文件的开销、平衡各编译单元的大小，从而缩短整体内核编译时间。该文件本身不包含任何函数或数据结构定义，仅通过 `#include` 指令聚合其他 `.c` 文件。\n\n## 2. 核心功能\n\n该文件本身**不定义任何函数或数据结构**，其“功能”体现在所包含的源文件模块中，主要包括：\n\n- **调度策略实现模块**：\n  - `idle.c`：空闲任务（idle task）的调度逻辑\n  - `rt.c`：实时调度类（SCHED_FIFO / SCHED_RR）的实现\n  - `deadline.c`：截止时间调度类（SCHED_DEADLINE）的实现\n  - `cpudeadline.c`（仅在 `CONFIG_SMP` 下）：SMP 架构下截止时间调度的 CPU 负载管理\n  - `ext.c`（仅在 `CONFIG_SCHED_CLASS_EXT` 下）：可扩展调度类支持\n\n- **辅助功能模块**：\n  - `cputime.c`：CPU 时间统计与账户管理\n  - `pelt.c`（仅在 `CONFIG_SMP` 下）：Per-Entity Load Tracking（PELT）负载跟踪机制\n  - `syscalls.c`：调度相关的系统调用（如 `sched_setattr`, `sched_getattr` 等）\n\n## 3. 关键实现\n\n- **单一编译单元聚合**：  \n  通过在一个 `.c` 文件中包含多个功能相关的 `.c` 文件，将原本分散的调度策略代码合并为一个较大的编译单元。这减少了每个源文件单独包含大量公共头文件（如 `sched.h`, `linux/sched/*.h` 等）所带来的重复解析开销。\n\n- **条件编译控制**：  \n  使用 `#ifdef CONFIG_SMP` 和 `#ifdef CONFIG_SCHED_CLASS_EXT` 等宏，确保仅在对应内核配置启用时才包含特定功能模块（如 `cpudeadline.c`、`pelt.c`、`ext.c`），保证构建的灵活性和配置适应性。\n\n- **构建时间平衡**：  \n  注释中明确指出，此编译单元的大小与 `core.c`（调度核心）和 `fair.c`（CFS 完全公平调度器）相当，有助于在并行编译时更均匀地分配工作负载，避免某些编译任务过重而拖慢整体构建速度。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 调度子系统内部头文件：`\"sched.h\"`, `\"smp.h\"`, `\"autogroup.h\"`, `\"stats.h\"`, `\"pelt.h\"`\n  - 内核通用子系统：`<linux/sched/*.h>`, `<linux/cpuidle.h>`, `<linux/psi.h>`, `<linux/rhashtable.h>` 等\n  - 用户态接口：`<uapi/linux/sched/types.h>`\n\n- **模块依赖**：\n  - 依赖 `core.c` 和 `fair.c` 提供的调度核心框架和 CFS 调度器（但这两者被单独编译）\n  - 所包含的模块（如 `rt.c`, `deadline.c`）依赖调度类注册机制、运行队列管理、负载均衡等核心调度基础设施\n  - `pelt.c` 依赖 SMP 架构下的负载跟踪和迁移逻辑\n\n## 5. 使用场景\n\n- **内核构建阶段**：  \n  该文件仅在内核编译过程中被使用，用于高效地编译调度策略相关代码。最终生成的内核镜像中不包含此文件的独立实体。\n\n- **调度策略运行时**：  \n  虽然 `build_policy.c` 本身不参与运行时逻辑，但它所聚合的模块（如实时调度、截止时间调度、CPU 时间统计等）在以下场景中被激活：\n  - 用户进程使用 `SCHED_FIFO`、`SCHED_RR` 或 `SCHED_DEADLINE` 策略\n  - 系统调用如 `sched_setattr()` 被调用以配置高级调度参数\n  - 内核进行 CPU 负载跟踪（PELT）、空闲 CPU 管理、CPU 热插拔时的调度状态迁移\n  - 能耗管理（如 cpuidle、suspend）与调度器协同工作时\n\n该文件是 Linux 内核构建系统优化的一个典型示例，体现了在大型项目中通过源码组织方式提升编译效率的设计思想。",
      "similarity": 0.5451319217681885,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/build_policy.c",
          "start_line": 1,
          "end_line": 66,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * These are the scheduling policy related scheduler files, built",
            " * in a single compilation unit for build efficiency reasons.",
            " *",
            " * ( Incidentally, the size of the compilation unit is roughly",
            " *   comparable to core.c and fair.c, the other two big",
            " *   compilation units. This helps balance build time, while",
            " *   coalescing source files to amortize header inclusion",
            " *   cost. )",
            " *",
            " * core.c and fair.c are built separately.",
            " */",
            "",
            "/* Headers: */",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/cputime.h>",
            "#include <linux/sched/hotplug.h>",
            "#include <linux/sched/isolation.h>",
            "#include <linux/sched/posix-timers.h>",
            "#include <linux/sched/rt.h>",
            "",
            "#include <linux/cpuidle.h>",
            "#include <linux/jiffies.h>",
            "#include <linux/kobject.h>",
            "#include <linux/livepatch.h>",
            "#include <linux/pm.h>",
            "#include <linux/psi.h>",
            "#include <linux/rhashtable.h>",
            "#include <linux/seq_buf.h>",
            "#include <linux/seqlock_api.h>",
            "#include <linux/slab.h>",
            "#include <linux/suspend.h>",
            "#include <linux/tsacct_kern.h>",
            "#include <linux/vtime.h>",
            "#include <linux/sysrq.h>",
            "#include <linux/percpu-rwsem.h>",
            "",
            "#include <uapi/linux/sched/types.h>",
            "",
            "#include \"sched.h\"",
            "#include \"smp.h\"",
            "",
            "#include \"autogroup.h\"",
            "#include \"stats.h\"",
            "#include \"pelt.h\"",
            "",
            "/* Source code modules: */",
            "",
            "#include \"idle.c\"",
            "",
            "#include \"rt.c\"",
            "",
            "#ifdef CONFIG_SMP",
            "# include \"cpudeadline.c\"",
            "# include \"pelt.c\"",
            "#endif",
            "",
            "#include \"cputime.c\"",
            "#include \"deadline.c\"",
            "",
            "#ifdef CONFIG_SCHED_CLASS_EXT",
            "# include \"ext.c\"",
            "#endif",
            "",
            "#include \"syscalls.c\""
          ],
          "function_name": null,
          "description": "此代码段是Linux内核调度器中与调度策略相关的编译单元，核心功能是整合多个调度类（如实时调度、截止时间调度等）的实现，通过条件编译包含不同模块（如`idle.c`、`rt.c`、`deadline.c`等）以提升构建效率。该文件作为调度策略逻辑的聚合点，协调各子模块的实现并管理其依赖关系。",
          "similarity": 0.513009250164032
        }
      ]
    }
  ]
}