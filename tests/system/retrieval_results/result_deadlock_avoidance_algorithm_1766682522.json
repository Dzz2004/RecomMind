{
  "query": "deadlock avoidance algorithm",
  "timestamp": "2025-12-26 01:08:42",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/osq_lock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:43:41\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\osq_lock.c`\n\n---\n\n# `locking/osq_lock.c` 技术文档\n\n## 1. 文件概述\n\n`osq_lock.c` 实现了一种专为**乐观自旋（Optimistic Spinning）**设计的轻量级排队自旋锁机制，称为 **OSQ（Optimistic Spin Queue）锁**。该机制主要用于支持如互斥锁（mutex）、读写信号量（rwsem）等**可睡眠锁**在争用时进行乐观自旋，以避免不必要的上下文切换和调度开销。OSQ 锁基于 MCS（Mellor-Crummey and Scott）锁的思想，但针对 Linux 内核的调度和抢占模型进行了优化，利用每个 CPU 的静态 per-CPU 节点结构，确保在禁用抢占的自旋上下文中安全使用。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct optimistic_spin_node`：每个 CPU 对应一个静态节点，包含：\n  - `cpu`：编码后的 CPU 编号（实际值 = CPU ID + 1）\n  - `locked`：布尔标志，表示是否已获得锁\n  - `next`：指向队列中下一个节点的指针\n  - `prev`：指向前一个节点的指针\n- `struct optimistic_spin_queue`：OSQ 锁结构体，仅包含一个原子变量 `tail`，用于指向队列尾部（编码后的 CPU 编号），`OSQ_UNLOCKED_VAL`（值为 0）表示无锁。\n\n### 主要函数\n- `bool osq_lock(struct optimistic_spin_queue *lock)`  \n  尝试获取 OSQ 锁。若成功获得锁或决定放弃自旋（如需要调度或前驱被抢占），返回 `true`；若成功排队但未获得锁且需继续等待，则返回 `false`（实际逻辑中，失败路径最终也返回 `false` 表示未获得锁）。\n  \n- `void osq_unlock(struct optimistic_spin_queue *lock)`  \n  释放 OSQ 锁，唤醒队列中的下一个等待者（若存在）。\n\n- `static inline struct optimistic_spin_node *osq_wait_next(...)`  \n  辅助函数，用于在解锁或取消排队时安全地获取下一个节点，并处理队列尾部的原子更新。\n\n- `encode_cpu()` / `decode_cpu()` / `node_cpu()`  \n  用于在 CPU 编号与 per-CPU 节点指针之间进行编码/解码转换，其中 CPU 编号 0 被编码为 1，以 0 表示“无 CPU”（即锁空闲）。\n\n## 3. 关键实现\n\n### Per-CPU 静态节点设计\n- 每个 CPU 拥有一个静态的 `osq_node`（通过 `DEFINE_PER_CPU_SHARED_ALIGNED` 定义），避免动态分配开销。\n- 由于 OSQ 仅在**禁用抢占**的上下文中使用（如 mutex 的乐观自旋阶段），且**不可在中断上下文调用**，因此 per-CPU 节点的生命周期安全。\n\n### 锁获取流程 (`osq_lock`)\n1. **初始化本地节点**：设置 `locked=0`、`next=NULL`，并确保 `cpu` 字段为当前 CPU 编码值。\n2. **原子交换尾指针**：通过 `atomic_xchg(&lock->tail, curr)` 尝试入队。若原值为 `OSQ_UNLOCKED_VAL`，直接获得锁。\n3. **链接到前驱**：若已有前驱（`prev`），通过 `smp_wmb()` 确保内存顺序后，设置 `prev->next = node`。\n4. **自旋等待**：使用 `smp_cond_load_relaxed()` 等待 `node->locked` 变为 1，或满足退出条件（`need_resched()` 或前驱 CPU 被抢占 `vcpu_is_preempted()`）。\n5. **取消排队（Unqueue）**：若需退出自旋：\n   - **Step A**：尝试将 `prev->next` 置为 `NULL`，断开链接。\n   - **Step B**：调用 `osq_wait_next()` 确定下一个节点，并可能将锁尾指针回退。\n   - **Step C**：若存在 `next`，将其与 `prev` 直接链接，完成队列修复。\n\n### 锁释放流程 (`osq_unlock`)\n1. **快速路径**：若当前 CPU 是唯一持有者（`tail == curr`），直接将 `tail` 设为 `OSQ_UNLOCKED_VAL`。\n2. **慢速路径**：\n   - 若本地节点的 `next` 非空，直接设置 `next->locked = 1` 唤醒后继。\n   - 否则调用 `osq_wait_next()` 获取下一个节点（处理并发取消排队的情况），再唤醒。\n\n### 内存屏障与原子操作\n- 使用 `atomic_xchg`、`atomic_cmpxchg_acquire/release` 确保对 `lock->tail` 的操作具有适当的内存序。\n- `smp_wmb()` 保证在设置 `prev->next` 前，本地节点的初始化对其他 CPU 可见。\n- `WRITE_ONCE`/`READ_ONCE` 防止编译器优化破坏并发访问语义。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/percpu.h>`：提供 per-CPU 变量支持（`this_cpu_ptr`, `per_cpu_ptr`）。\n  - `<linux/sched.h>`：提供调度相关函数（`need_resched()`）和虚拟 CPU 抢占检测（`vcpu_is_preempted()`）。\n  - `<linux/osq_lock.h>`：定义 `struct optimistic_spin_queue`、`struct optimistic_spin_node` 及 `OSQ_UNLOCKED_VAL`。\n- **架构依赖**：依赖底层架构的原子操作（`atomic_*`）、内存屏障（`smp_wmb`, `smp_load_acquire`）和 CPU ID 获取（`smp_processor_id()`）。\n- **调度器集成**：与内核调度器紧密协作，通过 `need_resched()` 和 `vcpu_is_preempted()` 决定是否继续自旋。\n\n## 5. 使用场景\n\nOSQ 锁主要用于**可睡眠锁的乐观自旋优化**，典型场景包括：\n- **Mutex（互斥锁）**：在 `mutex_spin_on_owner()` 中，若锁持有者正在运行，当前 CPU 会尝试 OSQ 自旋而非立即睡眠。\n- **Rwsem（读写信号量）**：在写者争用时，若满足条件，会使用 OSQ 进行乐观自旋。\n- **其他睡眠锁**：任何希望在锁争用时避免立即进入睡眠、以降低延迟的同步原语。\n\n其核心价值在于：当锁持有者很可能在**另一个 CPU 上运行且未被抢占**时，通过短暂自旋可避免昂贵的上下文切换，提升性能；同时通过 `vcpu_is_preempted()` 检测虚拟化环境中的抢占，避免在持有者已让出 CPU 时无效自旋。",
      "similarity": 0.566604733467102,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 20,
          "end_line": 149,
          "content": [
            "static inline int encode_cpu(int cpu_nr)",
            "{",
            "\treturn cpu_nr + 1;",
            "}",
            "static inline int node_cpu(struct optimistic_spin_node *node)",
            "{",
            "\treturn node->cpu - 1;",
            "}",
            "bool osq_lock(struct optimistic_spin_queue *lock)",
            "{",
            "\tstruct optimistic_spin_node *node = this_cpu_ptr(&osq_node);",
            "\tstruct optimistic_spin_node *prev, *next;",
            "\tint curr = encode_cpu(smp_processor_id());",
            "\tint old;",
            "",
            "\tnode->locked = 0;",
            "\tnode->next = NULL;",
            "\t/*",
            "\t * After this cpu member is initialized for the first time, it",
            "\t * would no longer change in fact. That could avoid cache misses",
            "\t * when spin and access the cpu member by other CPUs.",
            "\t */",
            "\tif (node->cpu != curr)",
            "\t\tnode->cpu = curr;",
            "",
            "\t/*",
            "\t * We need both ACQUIRE (pairs with corresponding RELEASE in",
            "\t * unlock() uncontended, or fastpath) and RELEASE (to publish",
            "\t * the node fields we just initialised) semantics when updating",
            "\t * the lock tail.",
            "\t */",
            "\told = atomic_xchg(&lock->tail, curr);",
            "\tif (old == OSQ_UNLOCKED_VAL)",
            "\t\treturn true;",
            "",
            "\tprev = decode_cpu(old);",
            "\tnode->prev = prev;",
            "",
            "\t/*",
            "\t * osq_lock()\t\t\tunqueue",
            "\t *",
            "\t * node->prev = prev\t\tosq_wait_next()",
            "\t * WMB\t\t\t\tMB",
            "\t * prev->next = node\t\tnext->prev = prev // unqueue-C",
            "\t *",
            "\t * Here 'node->prev' and 'next->prev' are the same variable and we need",
            "\t * to ensure these stores happen in-order to avoid corrupting the list.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tWRITE_ONCE(prev->next, node);",
            "",
            "\t/*",
            "\t * Normally @prev is untouchable after the above store; because at that",
            "\t * moment unlock can proceed and wipe the node element from stack.",
            "\t *",
            "\t * However, since our nodes are static per-cpu storage, we're",
            "\t * guaranteed their existence -- this allows us to apply",
            "\t * cmpxchg in an attempt to undo our queueing.",
            "\t */",
            "",
            "\t/*",
            "\t * Wait to acquire the lock or cancellation. Note that need_resched()",
            "\t * will come with an IPI, which will wake smp_cond_load_relaxed() if it",
            "\t * is implemented with a monitor-wait. vcpu_is_preempted() relies on",
            "\t * polling, be careful.",
            "\t */",
            "\tif (smp_cond_load_relaxed(&node->locked, VAL || need_resched() ||",
            "\t\t\t\t  vcpu_is_preempted(node_cpu(node->prev))))",
            "\t\treturn true;",
            "",
            "\t/* unqueue */",
            "\t/*",
            "\t * Step - A  -- stabilize @prev",
            "\t *",
            "\t * Undo our @prev->next assignment; this will make @prev's",
            "\t * unlock()/unqueue() wait for a next pointer since @lock points to us",
            "\t * (or later).",
            "\t */",
            "",
            "\tfor (;;) {",
            "\t\t/*",
            "\t\t * cpu_relax() below implies a compiler barrier which would",
            "\t\t * prevent this comparison being optimized away.",
            "\t\t */",
            "\t\tif (data_race(prev->next) == node &&",
            "\t\t    cmpxchg(&prev->next, node, NULL) == node)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * We can only fail the cmpxchg() racing against an unlock(),",
            "\t\t * in which case we should observe @node->locked becoming",
            "\t\t * true.",
            "\t\t */",
            "\t\tif (smp_load_acquire(&node->locked))",
            "\t\t\treturn true;",
            "",
            "\t\tcpu_relax();",
            "",
            "\t\t/*",
            "\t\t * Or we race against a concurrent unqueue()'s step-B, in which",
            "\t\t * case its step-C will write us a new @node->prev pointer.",
            "\t\t */",
            "\t\tprev = READ_ONCE(node->prev);",
            "\t}",
            "",
            "\t/*",
            "\t * Step - B -- stabilize @next",
            "\t *",
            "\t * Similar to unlock(), wait for @node->next or move @lock from @node",
            "\t * back to @prev.",
            "\t */",
            "",
            "\tnext = osq_wait_next(lock, node, prev);",
            "\tif (!next)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Step - C -- unlink",
            "\t *",
            "\t * @prev is stable because its still waiting for a new @prev->next",
            "\t * pointer, @next is stable because our @node->next pointer is NULL and",
            "\t * it will wait in Step-A.",
            "\t */",
            "",
            "\tWRITE_ONCE(next->prev, prev);",
            "\tWRITE_ONCE(prev->next, next);",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "encode_cpu, node_cpu, osq_lock",
          "description": "实现osq_lock函数，负责获取乐观自旋锁。通过原子操作将当前节点插入队列，利用内存屏障保证顺序一致性，并通过循环等待条件满足或被唤醒，最终完成锁的获取过程。",
          "similarity": 0.5520969033241272
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 213,
          "end_line": 238,
          "content": [
            "void osq_unlock(struct optimistic_spin_queue *lock)",
            "{",
            "\tstruct optimistic_spin_node *node, *next;",
            "\tint curr = encode_cpu(smp_processor_id());",
            "",
            "\t/*",
            "\t * Fast path for the uncontended case.",
            "\t */",
            "\tif (likely(atomic_cmpxchg_release(&lock->tail, curr,",
            "\t\t\t\t\t  OSQ_UNLOCKED_VAL) == curr))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Second most likely case.",
            "\t */",
            "\tnode = this_cpu_ptr(&osq_node);",
            "\tnext = xchg(&node->next, NULL);",
            "\tif (next) {",
            "\t\tWRITE_ONCE(next->locked, 1);",
            "\t\treturn;",
            "\t}",
            "",
            "\tnext = osq_wait_next(lock, node, NULL);",
            "\tif (next)",
            "\t\tWRITE_ONCE(next->locked, 1);",
            "}"
          ],
          "function_name": "osq_unlock",
          "description": "实现osq_unlock函数，处理锁的释放。通过原子比较交换操作快速处理无竞争情况，否则查找并唤醒下一个等待节点，确保锁状态的正确性与线程安全。",
          "similarity": 0.5508449077606201
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 1,
          "end_line": 19,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/percpu.h>",
            "#include <linux/sched.h>",
            "#include <linux/osq_lock.h>",
            "",
            "/*",
            " * An MCS like lock especially tailored for optimistic spinning for sleeping",
            " * lock implementations (mutex, rwsem, etc).",
            " *",
            " * Using a single mcs node per CPU is safe because sleeping locks should not be",
            " * called from interrupt context and we have preemption disabled while",
            " * spinning.",
            " */",
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);",
            "",
            "/*",
            " * We use the value 0 to represent \"no CPU\", thus the encoded value",
            " * will be the CPU number incremented by 1.",
            " */"
          ],
          "function_name": null,
          "description": "定义全局的per-CPU乐观自旋节点osq_node，用于支持多CPU环境下乐观自旋锁的实现。通过encode_cpu和node_cpu函数处理CPU编号转换，为后续锁操作提供基础设施。",
          "similarity": 0.4550100564956665
        }
      ]
    },
    {
      "source_file": "kernel/locking/spinlock_rt.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:55:06\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\spinlock_rt.c`\n\n---\n\n# `locking/spinlock_rt.c` 技术文档\n\n## 1. 文件概述\n\n`spinlock_rt.c` 是 Linux 内核中 **PREEMPT_RT（实时抢占）补丁** 的核心实现文件之一，用于在实时内核中替代传统的自旋锁（`spinlock_t`）和读写锁（`rwlock_t`）。  \n在 PREEMPT_RT 模型下，传统的“忙等”自旋锁语义被基于 **RT-Mutex（实时互斥锁）** 的可睡眠锁机制所取代，同时通过额外机制（如禁用迁移、RCU 读侧锁定）来**模拟原始自旋锁的语义行为**，确保实时性和正确性。\n\n该文件实现了：\n- 实时版本的自旋锁（`rt_spin_lock/unlock/trylock`）\n- 实时版本的读写锁（`rt_read/write_lock/unlock/trylock`）\n- 与 Lockdep（锁依赖验证器）和 RCU（读-拷贝-更新）机制的集成\n\n## 2. 核心功能\n\n### 主要函数\n\n#### 自旋锁相关\n- `rt_spin_lock(spinlock_t *lock)`：获取实时自旋锁\n- `rt_spin_unlock(spinlock_t *lock)`：释放实时自旋锁\n- `rt_spin_trylock(spinlock_t *lock)`：尝试非阻塞获取锁\n- `rt_spin_trylock_bh(spinlock_t *lock)`：在禁用软中断上下文中尝试获取锁\n- `rt_spin_lock_unlock(spinlock_t *lock)`：用于等待锁释放的辅助函数（先锁后立即释放）\n- `rt_spin_lock_nested()` / `rt_spin_lock_nest_lock()`：支持锁嵌套的调试版本\n- `__rt_spin_lock_init()`：锁初始化（调试模式）\n\n#### 读写锁相关\n- `rt_read_lock(rwlock_t *rwlock)` / `rt_write_lock(rwlock_t *rwlock)`：获取读/写锁\n- `rt_read_unlock(rwlock_t *rwlock)` / `rt_write_unlock(rwlock_t *rwlock)`：释放读/写锁\n- `rt_read_trylock(rwlock_t *rwlock)` / `rt_write_trylock(rwlock_t *rwlock)`：尝试获取读/写锁\n- `rt_write_lock_nested()`：支持写锁嵌套的调试版本\n- `__rt_rwlock_init()`：读写锁初始化（调试模式）\n\n### 关键内联函数与宏\n- `__rt_spin_lock()`：自旋锁获取的核心内联实现\n- `rtlock_lock()`：封装 RT-Mutex 获取逻辑\n- `rtlock_might_resched()`：用于 `might_sleep()` 检查的变体，考虑 RCU 嵌套\n- `rwbase_*` 系列宏：为 `rwbase_rt.c` 提供 RT 特定的底层操作接口\n\n## 3. 关键实现\n\n### 3.1 基于 RT-Mutex 的锁实现\n- 所有锁操作底层均使用 `rt_mutex_base` 结构。\n- 快速路径使用 `rt_mutex_cmpxchg_acquire/release` 原子操作尝试获取/释放锁。\n- 慢速路径（竞争时）调用 `rtlock_slowlock()`、`rt_mutex_slowunlock()` 等函数，这些函数定义在 `rtmutex.c` 中（通过 `#include \"rtmutex.c\"` 复用代码）。\n\n### 3.2 状态保存与恢复（State Preservation）\n- 当任务因锁竞争而阻塞时，**保存当前任务状态**（通过 `current_save_and_set_rtlock_wait_state()`）。\n- 在获取锁后**恢复原始状态**（通过 `current_restore_rtlock_saved_state()`）。\n- 此机制确保在阻塞期间的唤醒信号不会丢失，并维持任务状态一致性。\n\n### 3.3 模拟传统自旋锁语义\n传统自旋锁在持有期间会：\n- **禁用抢占** → 实时版本通过 `migrate_disable()` 禁用 CPU 迁移（等效于禁止负载均衡迁移，但允许抢占）。\n- **隐式 RCU 读侧临界区** → 实时版本显式调用 `rcu_read_lock()` / `rcu_read_unlock()`。\n\n### 3.4 与调度器集成\n- 阻塞时调用 `schedule_rtlock()`（由 `rwbase_schedule()` 宏定义），这是专为 RT 锁设计的调度点。\n- 使用 `TASK_RTLOCK_WAIT` 作为任务等待状态。\n\n### 3.5 Lockdep 集成\n- 所有锁操作均调用 `spin_acquire()` / `spin_release()` 或 `rwlock_acquire()` / `rwlock_release()`，向 Lockdep 提供锁依赖信息。\n- 支持锁类子类（`subclass`）和嵌套锁（`nest_lock`）的调试功能。\n\n### 3.6 RCU 感知的 `might_sleep` 检查\n- `rtlock_might_resched()` 宏在调用 `__might_resched()` 时传入 `RCU` 嵌套深度偏移量，避免在合法 RCU 临界区内误报睡眠警告。\n\n## 4. 依赖关系\n\n- **`rtmutex.c`**：通过 `#define RT_MUTEX_BUILD_SPINLOCKS` 和 `#include \"rtmutex.c\"` 复用 RT-Mutex 的慢速路径实现。\n- **`rwbase_rt.c`**：通过宏定义（如 `rwbase_rtmutex_lock_state`）提供读写锁的通用逻辑，本文件提供 RT 特定的底层操作。\n- **`<linux/spinlock.h>`**：定义 `spinlock_t`、`rwlock_t` 及相关 API。\n- **`<linux/rcupdate.h>`**：使用 `rcu_read_lock()` / `rcu_read_unlock()`。\n- **`<linux/migrate.h>`**：使用 `migrate_disable()` / `migrate_enable()`。\n- **Lockdep 子系统**：通过 `spin_acquire`/`release` 等接口集成锁依赖验证。\n- **调度器**：依赖 `schedule_rtlock()` 实现阻塞调度。\n\n## 5. 使用场景\n\n- **PREEMPT_RT 内核配置**：仅在 `CONFIG_PREEMPT_RT` 启用时编译和使用。\n- **替换传统自旋锁/读写锁**：内核中所有 `spin_lock()`、`read_lock()` 等调用在 RT 内核中会重定向到本文件中的 `rt_*` 函数。\n- **实时任务同步**：为高优先级实时任务提供可预测的锁行为，避免传统自旋锁导致的优先级反转和不可抢占问题。\n- **驱动和子系统开发**：开发者无需修改代码，PREEMPT_RT 会自动将锁语义转换为实时安全版本。\n- **调试支持**：在 `CONFIG_DEBUG_LOCK_ALLOC` 启用时，提供锁初始化、嵌套和依赖跟踪功能。",
      "similarity": 0.5264111757278442,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/spinlock_rt.c",
          "start_line": 1,
          "end_line": 37,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * PREEMPT_RT substitution for spin/rw_locks",
            " *",
            " * spinlocks and rwlocks on RT are based on rtmutexes, with a few twists to",
            " * resemble the non RT semantics:",
            " *",
            " * - Contrary to plain rtmutexes, spinlocks and rwlocks are state",
            " *   preserving. The task state is saved before blocking on the underlying",
            " *   rtmutex, and restored when the lock has been acquired. Regular wakeups",
            " *   during that time are redirected to the saved state so no wake up is",
            " *   missed.",
            " *",
            " * - Non RT spin/rwlocks disable preemption and eventually interrupts.",
            " *   Disabling preemption has the side effect of disabling migration and",
            " *   preventing RCU grace periods.",
            " *",
            " *   The RT substitutions explicitly disable migration and take",
            " *   rcu_read_lock() across the lock held section.",
            " */",
            "#include <linux/spinlock.h>",
            "#include <linux/export.h>",
            "",
            "#define RT_MUTEX_BUILD_SPINLOCKS",
            "#include \"rtmutex.c\"",
            "",
            "/*",
            " * __might_resched() skips the state check as rtlocks are state",
            " * preserving. Take RCU nesting into account as spin/read/write_lock() can",
            " * legitimately nest into an RCU read side critical section.",
            " */",
            "#define RTLOCK_RESCHED_OFFSETS\t\t\t\t\t\t\\",
            "\t(rcu_preempt_depth() << MIGHT_RESCHED_RCU_SHIFT)",
            "",
            "#define rtlock_might_resched()\t\t\t\t\t\t\\",
            "\t__might_resched(__FILE__, __LINE__, RTLOCK_RESCHED_OFFSETS)",
            ""
          ],
          "function_name": null,
          "description": "定义PREEMPT_RT下的自旋锁和读写锁实现，基于rtmutex并保留任务状态，通过宏定义和头文件引入核心机制，处理抢占禁用、迁移禁止及RCU嵌套问题",
          "similarity": 0.4922857880592346
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/spinlock_rt.c",
          "start_line": 38,
          "end_line": 138,
          "content": [
            "static __always_inline void rtlock_lock(struct rt_mutex_base *rtm)",
            "{",
            "\tlockdep_assert(!current->pi_blocked_on);",
            "",
            "\tif (unlikely(!rt_mutex_cmpxchg_acquire(rtm, NULL, current)))",
            "\t\trtlock_slowlock(rtm);",
            "}",
            "static __always_inline void __rt_spin_lock(spinlock_t *lock)",
            "{",
            "\trtlock_might_resched();",
            "\trtlock_lock(&lock->lock);",
            "\trcu_read_lock();",
            "\tmigrate_disable();",
            "}",
            "void __sched rt_spin_lock(spinlock_t *lock)",
            "{",
            "\tspin_acquire(&lock->dep_map, 0, 0, _RET_IP_);",
            "\t__rt_spin_lock(lock);",
            "}",
            "void __sched rt_spin_lock_nested(spinlock_t *lock, int subclass)",
            "{",
            "\tspin_acquire(&lock->dep_map, subclass, 0, _RET_IP_);",
            "\t__rt_spin_lock(lock);",
            "}",
            "void __sched rt_spin_lock_nest_lock(spinlock_t *lock,",
            "\t\t\t\t    struct lockdep_map *nest_lock)",
            "{",
            "\tspin_acquire_nest(&lock->dep_map, 0, 0, nest_lock, _RET_IP_);",
            "\t__rt_spin_lock(lock);",
            "}",
            "void __sched rt_spin_unlock(spinlock_t *lock)",
            "{",
            "\tspin_release(&lock->dep_map, _RET_IP_);",
            "\tmigrate_enable();",
            "\trcu_read_unlock();",
            "",
            "\tif (unlikely(!rt_mutex_cmpxchg_release(&lock->lock, current, NULL)))",
            "\t\trt_mutex_slowunlock(&lock->lock);",
            "}",
            "void __sched rt_spin_lock_unlock(spinlock_t *lock)",
            "{",
            "\tspin_lock(lock);",
            "\tspin_unlock(lock);",
            "}",
            "static __always_inline int __rt_spin_trylock(spinlock_t *lock)",
            "{",
            "\tint ret = 1;",
            "",
            "\tif (unlikely(!rt_mutex_cmpxchg_acquire(&lock->lock, NULL, current)))",
            "\t\tret = rt_mutex_slowtrylock(&lock->lock);",
            "",
            "\tif (ret) {",
            "\t\tspin_acquire(&lock->dep_map, 0, 1, _RET_IP_);",
            "\t\trcu_read_lock();",
            "\t\tmigrate_disable();",
            "\t}",
            "\treturn ret;",
            "}",
            "int __sched rt_spin_trylock(spinlock_t *lock)",
            "{",
            "\treturn __rt_spin_trylock(lock);",
            "}",
            "int __sched rt_spin_trylock_bh(spinlock_t *lock)",
            "{",
            "\tint ret;",
            "",
            "\tlocal_bh_disable();",
            "\tret = __rt_spin_trylock(lock);",
            "\tif (!ret)",
            "\t\tlocal_bh_enable();",
            "\treturn ret;",
            "}",
            "void __rt_spin_lock_init(spinlock_t *lock, const char *name,",
            "\t\t\t struct lock_class_key *key, bool percpu)",
            "{",
            "\tu8 type = percpu ? LD_LOCK_PERCPU : LD_LOCK_NORMAL;",
            "",
            "\tdebug_check_no_locks_freed((void *)lock, sizeof(*lock));",
            "\tlockdep_init_map_type(&lock->dep_map, name, key, 0, LD_WAIT_CONFIG,",
            "\t\t\t      LD_WAIT_INV, type);",
            "}",
            "static __always_inline int",
            "rwbase_rtmutex_lock_state(struct rt_mutex_base *rtm, unsigned int state)",
            "{",
            "\tif (unlikely(!rt_mutex_cmpxchg_acquire(rtm, NULL, current)))",
            "\t\trtlock_slowlock(rtm);",
            "\treturn 0;",
            "}",
            "static __always_inline int",
            "rwbase_rtmutex_slowlock_locked(struct rt_mutex_base *rtm, unsigned int state)",
            "{",
            "\trtlock_slowlock_locked(rtm);",
            "\treturn 0;",
            "}",
            "static __always_inline void rwbase_rtmutex_unlock(struct rt_mutex_base *rtm)",
            "{",
            "\tif (likely(rt_mutex_cmpxchg_acquire(rtm, current, NULL)))",
            "\t\treturn;",
            "",
            "\trt_mutex_slowunlock(rtm);",
            "}"
          ],
          "function_name": "rtlock_lock, __rt_spin_lock, rt_spin_lock, rt_spin_lock_nested, rt_spin_lock_nest_lock, rt_spin_unlock, rt_spin_lock_unlock, __rt_spin_trylock, rt_spin_trylock, rt_spin_trylock_bh, __rt_spin_lock_init, rwbase_rtmutex_lock_state, rwbase_rtmutex_slowlock_locked, rwbase_rtmutex_unlock",
          "description": "实现自旋锁的加锁/解锁逻辑，通过rt_mutex进行状态同步，禁用迁移并维护RCU读锁，在慢路径中处理阻塞唤醒，集成锁依赖检查和抢占状态管理",
          "similarity": 0.48978713154792786
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/spinlock_rt.c",
          "start_line": 179,
          "end_line": 253,
          "content": [
            "static __always_inline int  rwbase_rtmutex_trylock(struct rt_mutex_base *rtm)",
            "{",
            "\tif (likely(rt_mutex_cmpxchg_acquire(rtm, NULL, current)))",
            "\t\treturn 1;",
            "",
            "\treturn rt_mutex_slowtrylock(rtm);",
            "}",
            "int __sched rt_read_trylock(rwlock_t *rwlock)",
            "{",
            "\tint ret;",
            "",
            "\tret = rwbase_read_trylock(&rwlock->rwbase);",
            "\tif (ret) {",
            "\t\trwlock_acquire_read(&rwlock->dep_map, 0, 1, _RET_IP_);",
            "\t\trcu_read_lock();",
            "\t\tmigrate_disable();",
            "\t}",
            "\treturn ret;",
            "}",
            "int __sched rt_write_trylock(rwlock_t *rwlock)",
            "{",
            "\tint ret;",
            "",
            "\tret = rwbase_write_trylock(&rwlock->rwbase);",
            "\tif (ret) {",
            "\t\trwlock_acquire(&rwlock->dep_map, 0, 1, _RET_IP_);",
            "\t\trcu_read_lock();",
            "\t\tmigrate_disable();",
            "\t}",
            "\treturn ret;",
            "}",
            "void __sched rt_read_lock(rwlock_t *rwlock)",
            "{",
            "\trtlock_might_resched();",
            "\trwlock_acquire_read(&rwlock->dep_map, 0, 0, _RET_IP_);",
            "\trwbase_read_lock(&rwlock->rwbase, TASK_RTLOCK_WAIT);",
            "\trcu_read_lock();",
            "\tmigrate_disable();",
            "}",
            "void __sched rt_write_lock(rwlock_t *rwlock)",
            "{",
            "\trtlock_might_resched();",
            "\trwlock_acquire(&rwlock->dep_map, 0, 0, _RET_IP_);",
            "\trwbase_write_lock(&rwlock->rwbase, TASK_RTLOCK_WAIT);",
            "\trcu_read_lock();",
            "\tmigrate_disable();",
            "}",
            "void __sched rt_write_lock_nested(rwlock_t *rwlock, int subclass)",
            "{",
            "\trtlock_might_resched();",
            "\trwlock_acquire(&rwlock->dep_map, subclass, 0, _RET_IP_);",
            "\trwbase_write_lock(&rwlock->rwbase, TASK_RTLOCK_WAIT);",
            "\trcu_read_lock();",
            "\tmigrate_disable();",
            "}",
            "void __sched rt_read_unlock(rwlock_t *rwlock)",
            "{",
            "\trwlock_release(&rwlock->dep_map, _RET_IP_);",
            "\tmigrate_enable();",
            "\trcu_read_unlock();",
            "\trwbase_read_unlock(&rwlock->rwbase, TASK_RTLOCK_WAIT);",
            "}",
            "void __sched rt_write_unlock(rwlock_t *rwlock)",
            "{",
            "\trwlock_release(&rwlock->dep_map, _RET_IP_);",
            "\trcu_read_unlock();",
            "\tmigrate_enable();",
            "\trwbase_write_unlock(&rwlock->rwbase);",
            "}",
            "void __rt_rwlock_init(rwlock_t *rwlock, const char *name,",
            "\t\t      struct lock_class_key *key)",
            "{",
            "\tdebug_check_no_locks_freed((void *)rwlock, sizeof(*rwlock));",
            "\tlockdep_init_map_wait(&rwlock->dep_map, name, key, 0, LD_WAIT_CONFIG);",
            "}"
          ],
          "function_name": "rwbase_rtmutex_trylock, rt_read_trylock, rt_write_trylock, rt_read_lock, rt_write_lock, rt_write_lock_nested, rt_read_unlock, rt_write_unlock, __rt_rwlock_init",
          "description": "提供读写锁的获取/释放接口，基于rtmutex实现读写状态控制，强制RCU读锁和迁移禁止，在解锁时恢复RCU状态并释放底层互斥体，支持嵌套子类锁配置",
          "similarity": 0.4750785827636719
        }
      ]
    },
    {
      "source_file": "mm/workingset.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:34:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workingset.c`\n\n---\n\n# workingset.c 技术文档\n\n## 1. 文件概述\n\n`workingset.c` 实现了 Linux 内核中的 **工作集检测（Workingset Detection）** 机制，用于优化页面回收（page reclaim）策略。该机制通过跟踪页面的访问模式和重故障距离（refault distance），智能判断哪些页面应保留在内存中，从而减少系统颠簸（thrashing）并提升缓存效率。核心思想是：若一个被换出的页面在短时间内再次被访问（即重故障），且其重故障距离小于当前活跃页面数量，则应将其重新激活，以取代可能已不再活跃的现有活跃页面。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **Shadow Entry（影子条目）**：存储在页缓存槽位中的元数据，包含页面被驱逐时的时间戳（eviction counter 快照）、内存控制组 ID、节点 ID 和工作集标志。\n- **node->nonresident_age**：每个 NUMA 节点维护的计数器，记录非驻留页面的“年龄”，用于计算重故障距离。\n\n### 关键宏定义\n- `WORKINGSET_SHIFT`：工作集标识位偏移。\n- `EVICTION_SHIFT` / `EVICTION_MASK`：用于在 xarray 条目中紧凑编码驱逐时间戳的位操作参数。\n- `bucket_order`：当时间戳位数不足时，用于对驱逐事件进行分桶聚合的粒度。\n\n### 核心函数（部分实现）\n- `pack_shadow()`：将内存控制组 ID、节点指针、驱逐计数器值和工作集标志打包成一个 shadow entry。\n- （注：代码片段未完整展示其他关键函数如 `workingset_refault()`、`workingset_activation()` 等，但文档基于完整机制描述）\n\n## 3. 关键实现\n\n### 双 CLOCK 列表模型\n- 每个 NUMA 节点为文件页维护两个 LRU 列表：**inactive list**（不活跃）和 **active list**（活跃）。\n- 新缺页页面加入 inactive list 头部；回收从 inactive list 尾部扫描。\n- 在 inactive list 上被二次访问的页面晋升至 active list；active list 过长时，尾部页面降级到 inactive list。\n\n### 重故障距离（Refault Distance）算法\n1. **驱逐时记录**：页面被驱逐时，将其所在节点的 `nonresident_age` 计数器值（代表累计的驱逐+激活次数）作为时间戳存入 shadow entry。\n2. **重故障时计算**：\n   - 当缺页发生且存在对应 shadow entry 时，读取当前 `nonresident_age` 值（R）与 shadow 中存储的值（E）。\n   - 重故障距离 = `R - E`，表示页面不在内存期间发生的最小页面访问次数。\n3. **激活决策**：\n   - 若 `重故障距离 <= 当前活跃页面总数（file + anon）`，则认为若当时有足够 inactive 空间，该页面本可被激活而避免驱逐。\n   - 因此**乐观地激活**该重故障页面，使其与现有活跃页面竞争内存空间。\n\n### 影子条目压缩存储\n- 利用 xarray 条目的有限位宽（`BITS_PER_XA_VALUE`），通过位域拼接存储：\n  - 节点 ID（`NODES_SHIFT` 位）\n  - 内存控制组 ID（`MEM_CGROUP_ID_SHIFT` 位）\n  - 工作集标志（`WORKINGSET_SHIFT` 位）\n  - 驱逐时间戳（剩余位，必要时通过 `bucket_order` 降低精度）\n\n## 4. 依赖关系\n\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mm_inline.h>` — 提供页框、LRU 列表、页表操作等基础支持。\n- **内存控制组**：`<linux/memcontrol.h>` — 支持按 cgroup 隔离工作集统计。\n- **页缓存与交换**：`<linux/pagemap.h>`, `<linux/swap.h>`, `<linux/shmem_fs.h>` — 处理文件页、匿名页、tmpfs 页的回收逻辑。\n- **xarray 数据结构**：用于高效存储和检索 shadow entries（隐含在 `pack_shadow` 的位操作中）。\n- **DAX 支持**：`<linux/dax.h>` — 确保直接访问持久内存设备的页面也能参与工作集检测。\n\n## 5. 使用场景\n\n- **内存压力下的页面回收**：当系统内存紧张触发 kswapd 或直接回收时，工作集检测机制指导选择最优的牺牲页面。\n- **工作集切换检测**：识别应用程序工作集的动态变化（如新任务启动、旧任务结束），快速淘汰过时缓存。\n- **防止颠簸（Thrashing）**：在活跃工作集大小接近或超过可用内存时，通过重故障距离预测避免频繁换入换出。\n- **混合工作负载优化**：同时处理文件缓存（page cache）和匿名内存（anonymous pages）的工作集，平衡二者内存分配。\n- **容器化环境**：结合 memcg，在多租户系统中为每个容器独立维护工作集状态，避免相互干扰。",
      "similarity": 0.5241014361381531,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/workingset.c",
          "start_line": 418,
          "end_line": 526,
          "content": [
            "bool workingset_test_recent(void *shadow, bool file, bool *workingset,",
            "\t\t\t\tbool flush)",
            "{",
            "\tstruct mem_cgroup *eviction_memcg;",
            "\tstruct lruvec *eviction_lruvec;",
            "\tunsigned long refault_distance;",
            "\tunsigned long workingset_size;",
            "\tunsigned long refault;",
            "\tint memcgid;",
            "\tstruct pglist_data *pgdat;",
            "\tunsigned long eviction;",
            "",
            "\trcu_read_lock();",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tbool recent = lru_gen_test_recent(shadow, file,",
            "\t\t\t\t&eviction_lruvec, &eviction, workingset);",
            "",
            "\t\trcu_read_unlock();",
            "\t\treturn recent;",
            "\t}",
            "",
            "",
            "\tunpack_shadow(shadow, &memcgid, &pgdat, &eviction, workingset);",
            "\teviction <<= bucket_order;",
            "",
            "\t/*",
            "\t * Look up the memcg associated with the stored ID. It might",
            "\t * have been deleted since the folio's eviction.",
            "\t *",
            "\t * Note that in rare events the ID could have been recycled",
            "\t * for a new cgroup that refaults a shared folio. This is",
            "\t * impossible to tell from the available data. However, this",
            "\t * should be a rare and limited disturbance, and activations",
            "\t * are always speculative anyway. Ultimately, it's the aging",
            "\t * algorithm's job to shake out the minimum access frequency",
            "\t * for the active cache.",
            "\t *",
            "\t * XXX: On !CONFIG_MEMCG, this will always return NULL; it",
            "\t * would be better if the root_mem_cgroup existed in all",
            "\t * configurations instead.",
            "\t */",
            "\teviction_memcg = mem_cgroup_from_id(memcgid);",
            "\tif (!mem_cgroup_disabled() &&",
            "\t    (!eviction_memcg || !mem_cgroup_tryget(eviction_memcg))) {",
            "\t\trcu_read_unlock();",
            "\t\treturn false;",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * Flush stats (and potentially sleep) outside the RCU read section.",
            "\t *",
            "\t * Note that workingset_test_recent() itself might be called in RCU read",
            "\t * section (for e.g, in cachestat) - these callers need to skip flushing",
            "\t * stats (via the flush argument).",
            "\t *",
            "\t * XXX: With per-memcg flushing and thresholding, is ratelimiting",
            "\t * still needed here?",
            "\t */",
            "\tif (flush)",
            "\t\tmem_cgroup_flush_stats_ratelimited(eviction_memcg);",
            "",
            "\teviction_lruvec = mem_cgroup_lruvec(eviction_memcg, pgdat);",
            "\trefault = atomic_long_read(&eviction_lruvec->nonresident_age);",
            "",
            "\t/*",
            "\t * Calculate the refault distance",
            "\t *",
            "\t * The unsigned subtraction here gives an accurate distance",
            "\t * across nonresident_age overflows in most cases. There is a",
            "\t * special case: usually, shadow entries have a short lifetime",
            "\t * and are either refaulted or reclaimed along with the inode",
            "\t * before they get too old.  But it is not impossible for the",
            "\t * nonresident_age to lap a shadow entry in the field, which",
            "\t * can then result in a false small refault distance, leading",
            "\t * to a false activation should this old entry actually",
            "\t * refault again.  However, earlier kernels used to deactivate",
            "\t * unconditionally with *every* reclaim invocation for the",
            "\t * longest time, so the occasional inappropriate activation",
            "\t * leading to pressure on the active list is not a problem.",
            "\t */",
            "\trefault_distance = (refault - eviction) & EVICTION_MASK;",
            "",
            "\t/*",
            "\t * Compare the distance to the existing workingset size. We",
            "\t * don't activate pages that couldn't stay resident even if",
            "\t * all the memory was available to the workingset. Whether",
            "\t * workingset competition needs to consider anon or not depends",
            "\t * on having free swap space.",
            "\t */",
            "\tworkingset_size = lruvec_page_state(eviction_lruvec, NR_ACTIVE_FILE);",
            "\tif (!file) {",
            "\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_INACTIVE_FILE);",
            "\t}",
            "\tif (mem_cgroup_get_nr_swap_pages(eviction_memcg) > 0) {",
            "\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_ACTIVE_ANON);",
            "\t\tif (file) {",
            "\t\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_INACTIVE_ANON);",
            "\t\t}",
            "\t}",
            "",
            "\tmem_cgroup_put(eviction_memcg);",
            "\treturn refault_distance <= workingset_size;",
            "}"
          ],
          "function_name": "workingset_test_recent",
          "description": "实现工作集测试最近访问的判断逻辑，通过对比参考距离与当前工作集大小决定是否激活页面，支持内存组场景下的统计信息处理。",
          "similarity": 0.46994268894195557
        },
        {
          "chunk_id": 4,
          "file_path": "mm/workingset.c",
          "start_line": 712,
          "end_line": 833,
          "content": [
            "static enum lru_status shadow_lru_isolate(struct list_head *item,",
            "\t\t\t\t\t  struct list_lru_one *lru,",
            "\t\t\t\t\t  spinlock_t *lru_lock,",
            "\t\t\t\t\t  void *arg) __must_hold(lru_lock)",
            "{",
            "\tstruct xa_node *node = container_of(item, struct xa_node, private_list);",
            "\tstruct address_space *mapping;",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Page cache insertions and deletions synchronously maintain",
            "\t * the shadow node LRU under the i_pages lock and the",
            "\t * lru_lock.  Because the page cache tree is emptied before",
            "\t * the inode can be destroyed, holding the lru_lock pins any",
            "\t * address_space that has nodes on the LRU.",
            "\t *",
            "\t * We can then safely transition to the i_pages lock to",
            "\t * pin only the address_space of the particular node we want",
            "\t * to reclaim, take the node off-LRU, and drop the lru_lock.",
            "\t */",
            "",
            "\tmapping = container_of(node->array, struct address_space, i_pages);",
            "",
            "\t/* Coming from the list, invert the lock order */",
            "\tif (!xa_trylock(&mapping->i_pages)) {",
            "\t\tspin_unlock_irq(lru_lock);",
            "\t\tret = LRU_RETRY;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* For page cache we need to hold i_lock */",
            "\tif (mapping->host != NULL) {",
            "\t\tif (!spin_trylock(&mapping->host->i_lock)) {",
            "\t\t\txa_unlock(&mapping->i_pages);",
            "\t\t\tspin_unlock_irq(lru_lock);",
            "\t\t\tret = LRU_RETRY;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\tlist_lru_isolate(lru, item);",
            "\t__dec_node_page_state(virt_to_page(node), WORKINGSET_NODES);",
            "",
            "\tspin_unlock(lru_lock);",
            "",
            "\t/*",
            "\t * The nodes should only contain one or more shadow entries,",
            "\t * no pages, so we expect to be able to remove them all and",
            "\t * delete and free the empty node afterwards.",
            "\t */",
            "\tif (WARN_ON_ONCE(!node->nr_values))",
            "\t\tgoto out_invalid;",
            "\tif (WARN_ON_ONCE(node->count != node->nr_values))",
            "\t\tgoto out_invalid;",
            "\txa_delete_node(node, workingset_update_node);",
            "\t__inc_lruvec_kmem_state(node, WORKINGSET_NODERECLAIM);",
            "",
            "out_invalid:",
            "\txa_unlock_irq(&mapping->i_pages);",
            "\tif (mapping->host != NULL) {",
            "\t\tif (mapping_shrinkable(mapping))",
            "\t\t\tinode_add_lru(mapping->host);",
            "\t\tspin_unlock(&mapping->host->i_lock);",
            "\t}",
            "\tret = LRU_REMOVED_RETRY;",
            "out:",
            "\tcond_resched();",
            "\tspin_lock_irq(lru_lock);",
            "\treturn ret;",
            "}",
            "static unsigned long scan_shadow_nodes(struct shrinker *shrinker,",
            "\t\t\t\t       struct shrink_control *sc)",
            "{",
            "\t/* list_lru lock nests inside the IRQ-safe i_pages lock */",
            "\treturn list_lru_shrink_walk_irq(&shadow_nodes, sc, shadow_lru_isolate,",
            "\t\t\t\t\tNULL);",
            "}",
            "static int __init workingset_init(void)",
            "{",
            "\tstruct shrinker *workingset_shadow_shrinker;",
            "\tunsigned int timestamp_bits;",
            "\tunsigned int max_order;",
            "\tint ret = -ENOMEM;",
            "",
            "\tBUILD_BUG_ON(BITS_PER_LONG < EVICTION_SHIFT);",
            "\t/*",
            "\t * Calculate the eviction bucket size to cover the longest",
            "\t * actionable refault distance, which is currently half of",
            "\t * memory (totalram_pages/2). However, memory hotplug may add",
            "\t * some more pages at runtime, so keep working with up to",
            "\t * double the initial memory by using totalram_pages as-is.",
            "\t */",
            "\ttimestamp_bits = BITS_PER_LONG - EVICTION_SHIFT;",
            "\tmax_order = fls_long(totalram_pages() - 1);",
            "\tif (max_order > timestamp_bits)",
            "\t\tbucket_order = max_order - timestamp_bits;",
            "\tpr_info(\"workingset: timestamp_bits=%d max_order=%d bucket_order=%u\\n\",",
            "\t       timestamp_bits, max_order, bucket_order);",
            "",
            "\tworkingset_shadow_shrinker = shrinker_alloc(SHRINKER_NUMA_AWARE |",
            "\t\t\t\t\t\t    SHRINKER_MEMCG_AWARE,",
            "\t\t\t\t\t\t    \"mm-shadow\");",
            "\tif (!workingset_shadow_shrinker)",
            "\t\tgoto err;",
            "",
            "\tret = __list_lru_init(&shadow_nodes, true, &shadow_nodes_key,",
            "\t\t\t      workingset_shadow_shrinker);",
            "\tif (ret)",
            "\t\tgoto err_list_lru;",
            "",
            "\tworkingset_shadow_shrinker->count_objects = count_shadow_nodes;",
            "\tworkingset_shadow_shrinker->scan_objects = scan_shadow_nodes;",
            "\t/* ->count reports only fully expendable nodes */",
            "\tworkingset_shadow_shrinker->seeks = 0;",
            "",
            "\tshrinker_register(workingset_shadow_shrinker);",
            "\treturn 0;",
            "err_list_lru:",
            "\tshrinker_free(workingset_shadow_shrinker);",
            "err:",
            "\treturn ret;",
            "}"
          ],
          "function_name": "shadow_lru_isolate, scan_shadow_nodes, workingset_init",
          "description": "初始化工作集模块，注册收缩器管理影子节点，定义节点隔离和扫描机制，通过计算时间戳位宽确定桶阶参数以保证参考距离覆盖范围。",
          "similarity": 0.45672425627708435
        },
        {
          "chunk_id": 3,
          "file_path": "mm/workingset.c",
          "start_line": 537,
          "end_line": 689,
          "content": [
            "void workingset_refault(struct folio *folio, void *shadow)",
            "{",
            "\tbool file = folio_is_file_lru(folio);",
            "\tstruct pglist_data *pgdat;",
            "\tstruct mem_cgroup *memcg;",
            "\tstruct lruvec *lruvec;",
            "\tbool workingset;",
            "\tlong nr;",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tlru_gen_refault(folio, shadow);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * The activation decision for this folio is made at the level",
            "\t * where the eviction occurred, as that is where the LRU order",
            "\t * during folio reclaim is being determined.",
            "\t *",
            "\t * However, the cgroup that will own the folio is the one that",
            "\t * is actually experiencing the refault event. Make sure the folio is",
            "\t * locked to guarantee folio_memcg() stability throughout.",
            "\t */",
            "\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\tnr = folio_nr_pages(folio);",
            "\tmemcg = folio_memcg(folio);",
            "\tpgdat = folio_pgdat(folio);",
            "\tlruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "",
            "\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + file, nr);",
            "",
            "\tif (!workingset_test_recent(shadow, file, &workingset, true))",
            "\t\treturn;",
            "",
            "\tfolio_set_active(folio);",
            "\tworkingset_age_nonresident(lruvec, nr);",
            "\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + file, nr);",
            "",
            "\t/* Folio was active prior to eviction */",
            "\tif (workingset) {",
            "\t\tfolio_set_workingset(folio);",
            "\t\t/*",
            "\t\t * XXX: Move to folio_add_lru() when it supports new vs",
            "\t\t * putback",
            "\t\t */",
            "\t\tlru_note_cost_refault(folio);",
            "\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + file, nr);",
            "\t}",
            "}",
            "void workingset_activation(struct folio *folio)",
            "{",
            "\tstruct mem_cgroup *memcg;",
            "",
            "\trcu_read_lock();",
            "\t/*",
            "\t * Filter non-memcg pages here, e.g. unmap can call",
            "\t * mark_page_accessed() on VDSO pages.",
            "\t *",
            "\t * XXX: See workingset_refault() - this should return",
            "\t * root_mem_cgroup even for !CONFIG_MEMCG.",
            "\t */",
            "\tmemcg = folio_memcg_rcu(folio);",
            "\tif (!mem_cgroup_disabled() && !memcg)",
            "\t\tgoto out;",
            "\tworkingset_age_nonresident(folio_lruvec(folio), folio_nr_pages(folio));",
            "out:",
            "\trcu_read_unlock();",
            "}",
            "void workingset_update_node(struct xa_node *node)",
            "{",
            "\tstruct address_space *mapping;",
            "\tstruct page *page = virt_to_page(node);",
            "",
            "\t/*",
            "\t * Track non-empty nodes that contain only shadow entries;",
            "\t * unlink those that contain pages or are being freed.",
            "\t *",
            "\t * Avoid acquiring the list_lru lock when the nodes are",
            "\t * already where they should be. The list_empty() test is safe",
            "\t * as node->private_list is protected by the i_pages lock.",
            "\t */",
            "\tmapping = container_of(node->array, struct address_space, i_pages);",
            "\tlockdep_assert_held(&mapping->i_pages.xa_lock);",
            "",
            "\tif (node->count && node->count == node->nr_values) {",
            "\t\tif (list_empty(&node->private_list)) {",
            "\t\t\tlist_lru_add_obj(&shadow_nodes, &node->private_list);",
            "\t\t\t__inc_node_page_state(page, WORKINGSET_NODES);",
            "\t\t}",
            "\t} else {",
            "\t\tif (!list_empty(&node->private_list)) {",
            "\t\t\tlist_lru_del_obj(&shadow_nodes, &node->private_list);",
            "\t\t\t__dec_node_page_state(page, WORKINGSET_NODES);",
            "\t\t}",
            "\t}",
            "}",
            "static unsigned long count_shadow_nodes(struct shrinker *shrinker,",
            "\t\t\t\t\tstruct shrink_control *sc)",
            "{",
            "\tunsigned long max_nodes;",
            "\tunsigned long nodes;",
            "\tunsigned long pages;",
            "",
            "\tnodes = list_lru_shrink_count(&shadow_nodes, sc);",
            "\tif (!nodes)",
            "\t\treturn SHRINK_EMPTY;",
            "",
            "\t/*",
            "\t * Approximate a reasonable limit for the nodes",
            "\t * containing shadow entries. We don't need to keep more",
            "\t * shadow entries than possible pages on the active list,",
            "\t * since refault distances bigger than that are dismissed.",
            "\t *",
            "\t * The size of the active list converges toward 100% of",
            "\t * overall page cache as memory grows, with only a tiny",
            "\t * inactive list. Assume the total cache size for that.",
            "\t *",
            "\t * Nodes might be sparsely populated, with only one shadow",
            "\t * entry in the extreme case. Obviously, we cannot keep one",
            "\t * node for every eligible shadow entry, so compromise on a",
            "\t * worst-case density of 1/8th. Below that, not all eligible",
            "\t * refaults can be detected anymore.",
            "\t *",
            "\t * On 64-bit with 7 xa_nodes per page and 64 slots",
            "\t * each, this will reclaim shadow entries when they consume",
            "\t * ~1.8% of available memory:",
            "\t *",
            "\t * PAGE_SIZE / xa_nodes / node_entries * 8 / PAGE_SIZE",
            "\t */",
            "#ifdef CONFIG_MEMCG",
            "\tif (sc->memcg) {",
            "\t\tstruct lruvec *lruvec;",
            "\t\tint i;",
            "",
            "\t\tmem_cgroup_flush_stats_ratelimited(sc->memcg);",
            "\t\tlruvec = mem_cgroup_lruvec(sc->memcg, NODE_DATA(sc->nid));",
            "\t\tfor (pages = 0, i = 0; i < NR_LRU_LISTS; i++)",
            "\t\t\tpages += lruvec_page_state_local(lruvec,",
            "\t\t\t\t\t\t\t NR_LRU_BASE + i);",
            "\t\tpages += lruvec_page_state_local(",
            "\t\t\tlruvec, NR_SLAB_RECLAIMABLE_B) >> PAGE_SHIFT;",
            "\t\tpages += lruvec_page_state_local(",
            "\t\t\tlruvec, NR_SLAB_UNRECLAIMABLE_B) >> PAGE_SHIFT;",
            "\t} else",
            "#endif",
            "\t\tpages = node_present_pages(sc->nid);",
            "",
            "\tmax_nodes = pages >> (XA_CHUNK_SHIFT - 3);",
            "",
            "\tif (nodes <= max_nodes)",
            "\t\treturn 0;",
            "\treturn nodes - max_nodes;",
            "}"
          ],
          "function_name": "workingset_refault, workingset_activation, workingset_update_node, count_shadow_nodes",
          "description": "提供页面故障后的激活处理、节点更新及影子节点追踪功能，包含基于工作集状态更新节点计数器和触发页面重新激活的逻辑。",
          "similarity": 0.4432222843170166
        },
        {
          "chunk_id": 1,
          "file_path": "mm/workingset.c",
          "start_line": 209,
          "end_line": 314,
          "content": [
            "static void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,",
            "\t\t\t  unsigned long *evictionp, bool *workingsetp)",
            "{",
            "\tunsigned long entry = xa_to_value(shadow);",
            "\tint memcgid, nid;",
            "\tbool workingset;",
            "",
            "\tworkingset = entry & ((1UL << WORKINGSET_SHIFT) - 1);",
            "\tentry >>= WORKINGSET_SHIFT;",
            "\tnid = entry & ((1UL << NODES_SHIFT) - 1);",
            "\tentry >>= NODES_SHIFT;",
            "\tmemcgid = entry & ((1UL << MEM_CGROUP_ID_SHIFT) - 1);",
            "\tentry >>= MEM_CGROUP_ID_SHIFT;",
            "",
            "\t*memcgidp = memcgid;",
            "\t*pgdat = NODE_DATA(nid);",
            "\t*evictionp = entry;",
            "\t*workingsetp = workingset;",
            "}",
            "static bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,",
            "\t\t\t\tunsigned long *token, bool *workingset)",
            "{",
            "\tint memcg_id;",
            "\tunsigned long min_seq;",
            "\tstruct mem_cgroup *memcg;",
            "\tstruct pglist_data *pgdat;",
            "",
            "\tunpack_shadow(shadow, &memcg_id, &pgdat, token, workingset);",
            "",
            "\tmemcg = mem_cgroup_from_id(memcg_id);",
            "\t*lruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "",
            "\tmin_seq = READ_ONCE((*lruvec)->lrugen.min_seq[file]);",
            "\treturn (*token >> LRU_REFS_WIDTH) == (min_seq & (EVICTION_MASK >> LRU_REFS_WIDTH));",
            "}",
            "static void lru_gen_refault(struct folio *folio, void *shadow)",
            "{",
            "\tbool recent;",
            "\tint hist, tier, refs;",
            "\tbool workingset;",
            "\tunsigned long token;",
            "\tstruct lruvec *lruvec;",
            "\tstruct lru_gen_folio *lrugen;",
            "\tint type = folio_is_file_lru(folio);",
            "\tint delta = folio_nr_pages(folio);",
            "",
            "\trcu_read_lock();",
            "",
            "\trecent = lru_gen_test_recent(shadow, type, &lruvec, &token, &workingset);",
            "\tif (lruvec != folio_lruvec(folio))",
            "\t\tgoto unlock;",
            "",
            "\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + type, delta);",
            "",
            "\tif (!recent)",
            "\t\tgoto unlock;",
            "",
            "\tlrugen = &lruvec->lrugen;",
            "",
            "\thist = lru_hist_from_seq(READ_ONCE(lrugen->min_seq[type]));",
            "\t/* see the comment in folio_lru_refs() */",
            "\trefs = (token & (BIT(LRU_REFS_WIDTH) - 1)) + workingset;",
            "\ttier = lru_tier_from_refs(refs);",
            "",
            "\tatomic_long_add(delta, &lrugen->refaulted[hist][type][tier]);",
            "\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + type, delta);",
            "",
            "\t/*",
            "\t * Count the following two cases as stalls:",
            "\t * 1. For pages accessed through page tables, hotter pages pushed out",
            "\t *    hot pages which refaulted immediately.",
            "\t * 2. For pages accessed multiple times through file descriptors,",
            "\t *    they would have been protected by sort_folio().",
            "\t */",
            "\tif (lru_gen_in_fault() || refs >= BIT(LRU_REFS_WIDTH) - 1) {",
            "\t\tset_mask_bits(&folio->flags, 0, LRU_REFS_MASK | BIT(PG_workingset));",
            "\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + type, delta);",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "}",
            "static bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,",
            "\t\t\t\tunsigned long *token, bool *workingset)",
            "{",
            "\treturn false;",
            "}",
            "static void lru_gen_refault(struct folio *folio, void *shadow)",
            "{",
            "}",
            "void workingset_age_nonresident(struct lruvec *lruvec, unsigned long nr_pages)",
            "{",
            "\t/*",
            "\t * Reclaiming a cgroup means reclaiming all its children in a",
            "\t * round-robin fashion. That means that each cgroup has an LRU",
            "\t * order that is composed of the LRU orders of its child",
            "\t * cgroups; and every page has an LRU position not just in the",
            "\t * cgroup that owns it, but in all of that group's ancestors.",
            "\t *",
            "\t * So when the physical inactive list of a leaf cgroup ages,",
            "\t * the virtual inactive lists of all its parents, including",
            "\t * the root cgroup's, age as well.",
            "\t */",
            "\tdo {",
            "\t\tatomic_long_add(nr_pages, &lruvec->nonresident_age);",
            "\t} while ((lruvec = parent_lruvec(lruvec)));",
            "}"
          ],
          "function_name": "unpack_shadow, lru_gen_test_recent, lru_gen_refault, lru_gen_test_recent, lru_gen_refault, workingset_age_nonresident",
          "description": "包含解码影子条目、测试最近访问、处理页面故障的函数实现，但存在重复函数声明问题，实际功能涉及基于LRU生成的页面激活决策逻辑。",
          "similarity": 0.439840167760849
        },
        {
          "chunk_id": 0,
          "file_path": "mm/workingset.c",
          "start_line": 1,
          "end_line": 208,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Workingset detection",
            " *",
            " * Copyright (C) 2013 Red Hat, Inc., Johannes Weiner",
            " */",
            "",
            "#include <linux/memcontrol.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/writeback.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/atomic.h>",
            "#include <linux/module.h>",
            "#include <linux/swap.h>",
            "#include <linux/dax.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>",
            "",
            "/*",
            " *\t\tDouble CLOCK lists",
            " *",
            " * Per node, two clock lists are maintained for file pages: the",
            " * inactive and the active list.  Freshly faulted pages start out at",
            " * the head of the inactive list and page reclaim scans pages from the",
            " * tail.  Pages that are accessed multiple times on the inactive list",
            " * are promoted to the active list, to protect them from reclaim,",
            " * whereas active pages are demoted to the inactive list when the",
            " * active list grows too big.",
            " *",
            " *   fault ------------------------+",
            " *                                 |",
            " *              +--------------+   |            +-------------+",
            " *   reclaim <- |   inactive   | <-+-- demotion |    active   | <--+",
            " *              +--------------+                +-------------+    |",
            " *                     |                                           |",
            " *                     +-------------- promotion ------------------+",
            " *",
            " *",
            " *\t\tAccess frequency and refault distance",
            " *",
            " * A workload is thrashing when its pages are frequently used but they",
            " * are evicted from the inactive list every time before another access",
            " * would have promoted them to the active list.",
            " *",
            " * In cases where the average access distance between thrashing pages",
            " * is bigger than the size of memory there is nothing that can be",
            " * done - the thrashing set could never fit into memory under any",
            " * circumstance.",
            " *",
            " * However, the average access distance could be bigger than the",
            " * inactive list, yet smaller than the size of memory.  In this case,",
            " * the set could fit into memory if it weren't for the currently",
            " * active pages - which may be used more, hopefully less frequently:",
            " *",
            " *      +-memory available to cache-+",
            " *      |                           |",
            " *      +-inactive------+-active----+",
            " *  a b | c d e f g h i | J K L M N |",
            " *      +---------------+-----------+",
            " *",
            " * It is prohibitively expensive to accurately track access frequency",
            " * of pages.  But a reasonable approximation can be made to measure",
            " * thrashing on the inactive list, after which refaulting pages can be",
            " * activated optimistically to compete with the existing active pages.",
            " *",
            " * Approximating inactive page access frequency - Observations:",
            " *",
            " * 1. When a page is accessed for the first time, it is added to the",
            " *    head of the inactive list, slides every existing inactive page",
            " *    towards the tail by one slot, and pushes the current tail page",
            " *    out of memory.",
            " *",
            " * 2. When a page is accessed for the second time, it is promoted to",
            " *    the active list, shrinking the inactive list by one slot.  This",
            " *    also slides all inactive pages that were faulted into the cache",
            " *    more recently than the activated page towards the tail of the",
            " *    inactive list.",
            " *",
            " * Thus:",
            " *",
            " * 1. The sum of evictions and activations between any two points in",
            " *    time indicate the minimum number of inactive pages accessed in",
            " *    between.",
            " *",
            " * 2. Moving one inactive page N page slots towards the tail of the",
            " *    list requires at least N inactive page accesses.",
            " *",
            " * Combining these:",
            " *",
            " * 1. When a page is finally evicted from memory, the number of",
            " *    inactive pages accessed while the page was in cache is at least",
            " *    the number of page slots on the inactive list.",
            " *",
            " * 2. In addition, measuring the sum of evictions and activations (E)",
            " *    at the time of a page's eviction, and comparing it to another",
            " *    reading (R) at the time the page faults back into memory tells",
            " *    the minimum number of accesses while the page was not cached.",
            " *    This is called the refault distance.",
            " *",
            " * Because the first access of the page was the fault and the second",
            " * access the refault, we combine the in-cache distance with the",
            " * out-of-cache distance to get the complete minimum access distance",
            " * of this page:",
            " *",
            " *      NR_inactive + (R - E)",
            " *",
            " * And knowing the minimum access distance of a page, we can easily",
            " * tell if the page would be able to stay in cache assuming all page",
            " * slots in the cache were available:",
            " *",
            " *   NR_inactive + (R - E) <= NR_inactive + NR_active",
            " *",
            " * If we have swap we should consider about NR_inactive_anon and",
            " * NR_active_anon, so for page cache and anonymous respectively:",
            " *",
            " *   NR_inactive_file + (R - E) <= NR_inactive_file + NR_active_file",
            " *   + NR_inactive_anon + NR_active_anon",
            " *",
            " *   NR_inactive_anon + (R - E) <= NR_inactive_anon + NR_active_anon",
            " *   + NR_inactive_file + NR_active_file",
            " *",
            " * Which can be further simplified to:",
            " *",
            " *   (R - E) <= NR_active_file + NR_inactive_anon + NR_active_anon",
            " *",
            " *   (R - E) <= NR_active_anon + NR_inactive_file + NR_active_file",
            " *",
            " * Put into words, the refault distance (out-of-cache) can be seen as",
            " * a deficit in inactive list space (in-cache).  If the inactive list",
            " * had (R - E) more page slots, the page would not have been evicted",
            " * in between accesses, but activated instead.  And on a full system,",
            " * the only thing eating into inactive list space is active pages.",
            " *",
            " *",
            " *\t\tRefaulting inactive pages",
            " *",
            " * All that is known about the active list is that the pages have been",
            " * accessed more than once in the past.  This means that at any given",
            " * time there is actually a good chance that pages on the active list",
            " * are no longer in active use.",
            " *",
            " * So when a refault distance of (R - E) is observed and there are at",
            " * least (R - E) pages in the userspace workingset, the refaulting page",
            " * is activated optimistically in the hope that (R - E) pages are actually",
            " * used less frequently than the refaulting page - or even not used at",
            " * all anymore.",
            " *",
            " * That means if inactive cache is refaulting with a suitable refault",
            " * distance, we assume the cache workingset is transitioning and put",
            " * pressure on the current workingset.",
            " *",
            " * If this is wrong and demotion kicks in, the pages which are truly",
            " * used more frequently will be reactivated while the less frequently",
            " * used once will be evicted from memory.",
            " *",
            " * But if this is right, the stale pages will be pushed out of memory",
            " * and the used pages get to stay in cache.",
            " *",
            " *\t\tRefaulting active pages",
            " *",
            " * If on the other hand the refaulting pages have recently been",
            " * deactivated, it means that the active list is no longer protecting",
            " * actively used cache from reclaim. The cache is NOT transitioning to",
            " * a different workingset; the existing workingset is thrashing in the",
            " * space allocated to the page cache.",
            " *",
            " *",
            " *\t\tImplementation",
            " *",
            " * For each node's LRU lists, a counter for inactive evictions and",
            " * activations is maintained (node->nonresident_age).",
            " *",
            " * On eviction, a snapshot of this counter (along with some bits to",
            " * identify the node) is stored in the now empty page cache",
            " * slot of the evicted page.  This is called a shadow entry.",
            " *",
            " * On cache misses for which there are shadow entries, an eligible",
            " * refault distance will immediately activate the refaulting page.",
            " */",
            "",
            "#define WORKINGSET_SHIFT 1",
            "#define EVICTION_SHIFT\t((BITS_PER_LONG - BITS_PER_XA_VALUE) +\t\\",
            "\t\t\t WORKINGSET_SHIFT + NODES_SHIFT + \\",
            "\t\t\t MEM_CGROUP_ID_SHIFT)",
            "#define EVICTION_MASK\t(~0UL >> EVICTION_SHIFT)",
            "",
            "/*",
            " * Eviction timestamps need to be able to cover the full range of",
            " * actionable refaults. However, bits are tight in the xarray",
            " * entry, and after storing the identifier for the lruvec there might",
            " * not be enough left to represent every single actionable refault. In",
            " * that case, we have to sacrifice granularity for distance, and group",
            " * evictions into coarser buckets by shaving off lower timestamp bits.",
            " */",
            "static unsigned int bucket_order __read_mostly;",
            "",
            "static void *pack_shadow(int memcgid, pg_data_t *pgdat, unsigned long eviction,",
            "\t\t\t bool workingset)",
            "{",
            "\teviction &= EVICTION_MASK;",
            "\teviction = (eviction << MEM_CGROUP_ID_SHIFT) | memcgid;",
            "\teviction = (eviction << NODES_SHIFT) | pgdat->node_id;",
            "\teviction = (eviction << WORKINGSET_SHIFT) | workingset;",
            "",
            "\treturn xa_mk_value(eviction);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义影子条目打包函数，将内存组ID、节点ID、时间戳等信息编码到xarray值中，用于记录页面被驱逐时的状态信息。",
          "similarity": 0.4138628840446472
        }
      ]
    }
  ]
}