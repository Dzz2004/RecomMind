{
  "query": "操作系统中哲学家就餐问题的同步机制",
  "timestamp": "2025-12-26 01:00:28",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.5438988208770752,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.5143249034881592
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.5108987092971802
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.398768812417984
        }
      ]
    },
    {
      "source_file": "kernel/context_tracking.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:54:12\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `context_tracking.c`\n\n---\n\n# context_tracking.c 技术文档\n\n## 1. 文件概述\n\n`context_tracking.c` 实现了 Linux 内核中的上下文跟踪（Context Tracking）机制，用于探测 CPU 在高阶上下文边界（如内核态、用户态、虚拟机客户态或空闲状态）之间的切换。该机制的核心目的是支持 RCU（Read-Copy-Update）子系统在 CPU 处于用户态、空闲或客户态时进入“扩展静默状态”（Extended Quiescent State, EQS），从而允许 RCU 在这些状态下关闭周期性时钟滴答（tickless idle），降低功耗并提升可扩展性。\n\n该文件主要服务于 RCU 的动态滴答（dynticks）机制，确保在非内核执行期间 RCU 不需要依赖定时器中断来推进宽限期（grace period）。\n\n## 2. 核心功能\n\n### 数据结构\n\n- **`struct context_tracking`**（每 CPU 变量）  \n  定义在 `<linux/context_tracking.h>` 中，包含以下关键字段：\n  - `dynticks_nesting`：记录当前 CPU 是否处于 RCU 可观察状态（>0 表示在内核中，RCU 正在监视；0 表示在 EQS 中）。\n  - `dynticks_nmi_nesting`：跟踪 NMI（不可屏蔽中断）嵌套层级，用于处理 NMI 中断对 EQS 状态的干扰。\n  - `state`：原子变量，用于 RCU dynticks 状态同步，其最低位表示 RCU 是否正在监视当前 CPU。\n\n- **全局每 CPU 变量**：\n  ```c\n  DEFINE_PER_CPU(struct context_tracking, context_tracking)\n  ```\n  初始化时，若启用 `CONFIG_CONTEXT_TRACKING_IDLE`，则 `dynticks_nesting = 1`（表示初始处于内核态），`dynticks_nmi_nesting = DYNTICK_IRQ_NONIDLE`。\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `ct_kernel_exit(bool user, int offset)` | 进入扩展静默状态（如进入用户态或 idle），通知 RCU 停止监视当前 CPU。 |\n| `ct_kernel_enter(bool user, int offset)` | 退出扩展静默状态（如从用户态或 idle 返回内核），通知 RCU 恢复监视。 |\n| `ct_nmi_exit(void)` | 从 NMI 处理程序返回时调用，恢复被 NMI 中断的 EQS 状态（如适用）。 |\n| `ct_kernel_exit_state(int offset)` | 内部辅助函数，执行进入 EQS 的核心状态更新。 |\n| `ct_kernel_enter_state(int offset)` | 内部辅助函数，执行退出 EQS 的核心状态更新。 |\n| `rcu_dynticks_task_enter/exit()` | 在进入/退出 EQS 时记录当前任务信息（用于 Tasks RCU）。 |\n| `rcu_dynticks_task_trace_enter/exit()` | 管理 Tasks Trace RCU 的内存屏障需求标志。 |\n\n## 3. 关键实现\n\n### 扩展静默状态（EQS）管理\n\n- **状态切换逻辑**：\n  - 当 `dynticks_nesting` 从 1 减为 0 时，CPU 进入 EQS（如进入用户态或 idle），调用 `ct_kernel_exit_state()` 增加 `state` 值（通过 `ct_state_inc()`），使 `state` 变为偶数（`RCU_DYNTICKS_IDX` 为奇数掩码），表示 RCU 不再监视。\n  - 当 `dynticks_nesting` 从 0 增为 1 时，CPU 退出 EQS，调用 `ct_kernel_enter_state()`，使 `state` 变为奇数，表示 RCU 恢复监视。\n\n- **NMI 嵌套处理**：\n  - `dynticks_nmi_nesting` 初始为 `DYNTICK_IRQ_NONIDLE`（通常为 1）。\n  - 进入 EQS 时强制设为 0；退出 EQS 时重置为 `DYNTICK_IRQ_NONIDLE`。\n  - 在 NMI 中，若检测到 `dynticks_nmi_nesting == 1`，说明 NMI 中断了 EQS，需在 `ct_nmi_exit()` 中恢复 EQS 状态。\n\n### 内存屏障与指令排序\n\n- 使用 `WRITE_ONCE()` 避免编译器优化导致的存储撕裂（store tearing）。\n- 在状态变更前后调用 `rcu_dynticks_task_trace_enter/exit()`，确保 Tasks Trace RCU 的内存屏障需求正确设置。\n- `noinstr` 属性用于关键路径函数，防止 ftrace 等插桩干扰中断上下文。\n\n### 调试支持\n\n- 启用 `CONFIG_RCU_EQS_DEBUG` 时，通过 `WARN_ON_ONCE()` 验证状态一致性，例如：\n  - 进入 EQS 时必须处于用户态或 idle 任务。\n  - `dynticks_nesting` 和 `dynticks_nmi_nesting` 不能为负或非法值。\n  - 状态切换前后 RCU 监视状态必须符合预期。\n\n### 跟踪点（Tracepoint）\n\n- 使用 `trace_rcu_dyntick()` 记录状态转换事件（如 \"Start\"、\"End\"、\"Startirq\"），便于调试 RCU dynticks 行为。\n\n## 4. 依赖关系\n\n- **RCU 子系统**：  \n  本文件是 RCU dynticks 机制的核心组成部分，与 `kernel/rcu/tree.c` 共享状态定义和逻辑。\n- **调度器（SCHED）**：  \n  依赖 `current` 指针和 `is_idle_task()` 判断是否处于 idle 任务。\n- **中断子系统**：  \n  依赖 `hardirq.h` 中的中断状态判断（如 `raw_irqs_disabled()`）。\n- **Kprobes / Ftrace**：  \n  使用 `instrumentation_begin/end()` 和 `noinstr` 控制插桩行为。\n- **配置选项**：\n  - `CONFIG_CONTEXT_TRACKING_IDLE`：启用 idle/user 跟踪。\n  - `CONFIG_TASKS_RCU` / `CONFIG_TASKS_TRACE_RCU`：支持 Tasks RCU 相关功能。\n  - `CONFIG_NO_HZ_FULL`：全动态滴答模式，依赖此机制实现 tickless 用户态。\n\n## 5. 使用场景\n\n- **用户态执行**：  \n  当进程从内核态返回用户态时，调用 `ct_kernel_exit(true, ...)` 进入 EQS，允许 RCU 关闭本地时钟中断。\n  \n- **CPU 空闲（idle）**：  \n  在 idle 循环入口调用 `ct_kernel_exit(false, ...)`，使 CPU 进入低功耗状态，同时 RCU 不再依赖 tick。\n\n- **NMI 处理**：  \n  若 NMI 中断发生在 EQS 期间，`ct_nmi_exit()` 在 NMI 返回时恢复 EQS 状态，避免 RCU 误判 CPU 为活跃状态。\n\n- **虚拟化客户态（Guest）**：  \n  在 KVM 等虚拟化场景中，当 VCPU 运行客户代码时，可视为 EQS，减少宿主机 RCU 开销。\n\n- **Tasks RCU 支持**：  \n  在 `CONFIG_TASKS_RCU` 启用时，记录任务在 EQS 中的 CPU 信息，用于宽限期检测。",
      "similarity": 0.5382919311523438,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/context_tracking.c",
          "start_line": 365,
          "end_line": 487,
          "content": [
            "noinstr void ct_irq_enter(void)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "\tct_nmi_enter();",
            "}",
            "noinstr void ct_irq_exit(void)",
            "{",
            "\tlockdep_assert_irqs_disabled();",
            "\tct_nmi_exit();",
            "}",
            "void ct_irq_enter_irqson(void)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\tct_irq_enter();",
            "\tlocal_irq_restore(flags);",
            "}",
            "void ct_irq_exit_irqson(void)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\tct_irq_exit();",
            "\tlocal_irq_restore(flags);",
            "}",
            "static __always_inline void ct_kernel_exit(bool user, int offset) { }",
            "static __always_inline void ct_kernel_enter(bool user, int offset) { }",
            "static noinstr bool context_tracking_recursion_enter(void)",
            "{",
            "\tint recursion;",
            "",
            "\trecursion = __this_cpu_inc_return(context_tracking.recursion);",
            "\tif (recursion == 1)",
            "\t\treturn true;",
            "",
            "\tWARN_ONCE((recursion < 1), \"Invalid context tracking recursion value %d\\n\", recursion);",
            "\t__this_cpu_dec(context_tracking.recursion);",
            "",
            "\treturn false;",
            "}",
            "static __always_inline void context_tracking_recursion_exit(void)",
            "{",
            "\t__this_cpu_dec(context_tracking.recursion);",
            "}",
            "void noinstr __ct_user_enter(enum ctx_state state)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\t/* Kernel threads aren't supposed to go to userspace */",
            "\tWARN_ON_ONCE(!current->mm);",
            "",
            "\tif (!context_tracking_recursion_enter())",
            "\t\treturn;",
            "",
            "\tif (__ct_state() != state) {",
            "\t\tif (ct->active) {",
            "\t\t\t/*",
            "\t\t\t * At this stage, only low level arch entry code remains and",
            "\t\t\t * then we'll run in userspace. We can assume there won't be",
            "\t\t\t * any RCU read-side critical section until the next call to",
            "\t\t\t * user_exit() or ct_irq_enter(). Let's remove RCU's dependency",
            "\t\t\t * on the tick.",
            "\t\t\t */",
            "\t\t\tif (state == CT_STATE_USER) {",
            "\t\t\t\tinstrumentation_begin();",
            "\t\t\t\ttrace_user_enter(0);",
            "\t\t\t\tvtime_user_enter(current);",
            "\t\t\t\tinstrumentation_end();",
            "\t\t\t}",
            "\t\t\t/*",
            "\t\t\t * Other than generic entry implementation, we may be past the last",
            "\t\t\t * rescheduling opportunity in the entry code. Trigger a self IPI",
            "\t\t\t * that will fire and reschedule once we resume in user/guest mode.",
            "\t\t\t */",
            "\t\t\trcu_irq_work_resched();",
            "",
            "\t\t\t/*",
            "\t\t\t * Enter RCU idle mode right before resuming userspace.  No use of RCU",
            "\t\t\t * is permitted between this call and rcu_eqs_exit(). This way the",
            "\t\t\t * CPU doesn't need to maintain the tick for RCU maintenance purposes",
            "\t\t\t * when the CPU runs in userspace.",
            "\t\t\t */",
            "\t\t\tct_kernel_exit(true, RCU_DYNTICKS_IDX + state);",
            "",
            "\t\t\t/*",
            "\t\t\t * Special case if we only track user <-> kernel transitions for tickless",
            "\t\t\t * cputime accounting but we don't support RCU extended quiescent state.",
            "\t\t\t * In this we case we don't care about any concurrency/ordering.",
            "\t\t\t */",
            "\t\t\tif (!IS_ENABLED(CONFIG_CONTEXT_TRACKING_IDLE))",
            "\t\t\t\traw_atomic_set(&ct->state, state);",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * Even if context tracking is disabled on this CPU, because it's outside",
            "\t\t\t * the full dynticks mask for example, we still have to keep track of the",
            "\t\t\t * context transitions and states to prevent inconsistency on those of",
            "\t\t\t * other CPUs.",
            "\t\t\t * If a task triggers an exception in userspace, sleep on the exception",
            "\t\t\t * handler and then migrate to another CPU, that new CPU must know where",
            "\t\t\t * the exception returns by the time we call exception_exit().",
            "\t\t\t * This information can only be provided by the previous CPU when it called",
            "\t\t\t * exception_enter().",
            "\t\t\t * OTOH we can spare the calls to vtime and RCU when context_tracking.active",
            "\t\t\t * is false because we know that CPU is not tickless.",
            "\t\t\t */",
            "\t\t\tif (!IS_ENABLED(CONFIG_CONTEXT_TRACKING_IDLE)) {",
            "\t\t\t\t/* Tracking for vtime only, no concurrent RCU EQS accounting */",
            "\t\t\t\traw_atomic_set(&ct->state, state);",
            "\t\t\t} else {",
            "\t\t\t\t/*",
            "\t\t\t\t * Tracking for vtime and RCU EQS. Make sure we don't race",
            "\t\t\t\t * with NMIs. OTOH we don't care about ordering here since",
            "\t\t\t\t * RCU only requires RCU_DYNTICKS_IDX increments to be fully",
            "\t\t\t\t * ordered.",
            "\t\t\t\t */",
            "\t\t\t\traw_atomic_add(state, &ct->state);",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\tcontext_tracking_recursion_exit();",
            "}"
          ],
          "function_name": "ct_irq_enter, ct_irq_exit, ct_irq_enter_irqson, ct_irq_exit_irqson, ct_kernel_exit, ct_kernel_enter, context_tracking_recursion_enter, context_tracking_recursion_exit, __ct_user_enter",
          "description": "处理中断上下文转换及递归跟踪逻辑，协调用户态/内核态切换与RCU动态时钟跟踪状态的同步。",
          "similarity": 0.5588365793228149
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/context_tracking.c",
          "start_line": 1,
          "end_line": 41,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Context tracking: Probe on high level context boundaries such as kernel,",
            " * userspace, guest or idle.",
            " *",
            " * This is used by RCU to remove its dependency on the timer tick while a CPU",
            " * runs in idle, userspace or guest mode.",
            " *",
            " * User/guest tracking started by Frederic Weisbecker:",
            " *",
            " * Copyright (C) 2012 Red Hat, Inc., Frederic Weisbecker",
            " *",
            " * Many thanks to Gilad Ben-Yossef, Paul McKenney, Ingo Molnar, Andrew Morton,",
            " * Steven Rostedt, Peter Zijlstra for suggestions and improvements.",
            " *",
            " * RCU extended quiescent state bits imported from kernel/rcu/tree.c",
            " * where the relevant authorship may be found.",
            " */",
            "",
            "#include <linux/context_tracking.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/sched.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/export.h>",
            "#include <linux/kprobes.h>",
            "#include <trace/events/rcu.h>",
            "",
            "",
            "DEFINE_PER_CPU(struct context_tracking, context_tracking) = {",
            "#ifdef CONFIG_CONTEXT_TRACKING_IDLE",
            "\t.dynticks_nesting = 1,",
            "\t.dynticks_nmi_nesting = DYNTICK_IRQ_NONIDLE,",
            "#endif",
            "\t.state = ATOMIC_INIT(RCU_DYNTICKS_IDX),",
            "};",
            "EXPORT_SYMBOL_GPL(context_tracking);",
            "",
            "#ifdef CONFIG_CONTEXT_TRACKING_IDLE",
            "#define TPS(x)  tracepoint_string(x)",
            "",
            "/* Record the current task on dyntick-idle entry. */"
          ],
          "function_name": null,
          "description": "定义并初始化每个CPU的context_tracking结构体，设置动态时钟跟踪的初始状态，支持空闲模式下的RCU操作。",
          "similarity": 0.5525034666061401
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/context_tracking.c",
          "start_line": 42,
          "end_line": 154,
          "content": [
            "static __always_inline void rcu_dynticks_task_enter(void)",
            "{",
            "#if defined(CONFIG_TASKS_RCU) && defined(CONFIG_NO_HZ_FULL)",
            "\tWRITE_ONCE(current->rcu_tasks_idle_cpu, smp_processor_id());",
            "#endif /* #if defined(CONFIG_TASKS_RCU) && defined(CONFIG_NO_HZ_FULL) */",
            "}",
            "static __always_inline void rcu_dynticks_task_exit(void)",
            "{",
            "#if defined(CONFIG_TASKS_RCU) && defined(CONFIG_NO_HZ_FULL)",
            "\tWRITE_ONCE(current->rcu_tasks_idle_cpu, -1);",
            "#endif /* #if defined(CONFIG_TASKS_RCU) && defined(CONFIG_NO_HZ_FULL) */",
            "}",
            "static __always_inline void rcu_dynticks_task_trace_enter(void)",
            "{",
            "#ifdef CONFIG_TASKS_TRACE_RCU",
            "\tif (IS_ENABLED(CONFIG_TASKS_TRACE_RCU_READ_MB))",
            "\t\tcurrent->trc_reader_special.b.need_mb = true;",
            "#endif /* #ifdef CONFIG_TASKS_TRACE_RCU */",
            "}",
            "static __always_inline void rcu_dynticks_task_trace_exit(void)",
            "{",
            "#ifdef CONFIG_TASKS_TRACE_RCU",
            "\tif (IS_ENABLED(CONFIG_TASKS_TRACE_RCU_READ_MB))",
            "\t\tcurrent->trc_reader_special.b.need_mb = false;",
            "#endif /* #ifdef CONFIG_TASKS_TRACE_RCU */",
            "}",
            "static noinstr void ct_kernel_exit_state(int offset)",
            "{",
            "\tint seq;",
            "",
            "\t/*",
            "\t * CPUs seeing atomic_add_return() must see prior RCU read-side",
            "\t * critical sections, and we also must force ordering with the",
            "\t * next idle sojourn.",
            "\t */",
            "\trcu_dynticks_task_trace_enter();  // Before ->dynticks update!",
            "\tseq = ct_state_inc(offset);",
            "\t// RCU is no longer watching.  Better be in extended quiescent state!",
            "\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && (seq & RCU_DYNTICKS_IDX));",
            "}",
            "static noinstr void ct_kernel_enter_state(int offset)",
            "{",
            "\tint seq;",
            "",
            "\t/*",
            "\t * CPUs seeing atomic_add_return() must see prior idle sojourns,",
            "\t * and we also must force ordering with the next RCU read-side",
            "\t * critical section.",
            "\t */",
            "\tseq = ct_state_inc(offset);",
            "\t// RCU is now watching.  Better not be in an extended quiescent state!",
            "\trcu_dynticks_task_trace_exit();  // After ->dynticks update!",
            "\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !(seq & RCU_DYNTICKS_IDX));",
            "}",
            "static void noinstr ct_kernel_exit(bool user, int offset)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "",
            "\tWARN_ON_ONCE(ct_dynticks_nmi_nesting() != DYNTICK_IRQ_NONIDLE);",
            "\tWRITE_ONCE(ct->dynticks_nmi_nesting, 0);",
            "\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) &&",
            "\t\t     ct_dynticks_nesting() == 0);",
            "\tif (ct_dynticks_nesting() != 1) {",
            "\t\t// RCU will still be watching, so just do accounting and leave.",
            "\t\tct->dynticks_nesting--;",
            "\t\treturn;",
            "\t}",
            "",
            "\tinstrumentation_begin();",
            "\tlockdep_assert_irqs_disabled();",
            "\ttrace_rcu_dyntick(TPS(\"Start\"), ct_dynticks_nesting(), 0, ct_dynticks());",
            "\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !user && !is_idle_task(current));",
            "\trcu_preempt_deferred_qs(current);",
            "",
            "\t// instrumentation for the noinstr ct_kernel_exit_state()",
            "\tinstrument_atomic_write(&ct->state, sizeof(ct->state));",
            "",
            "\tinstrumentation_end();",
            "\tWRITE_ONCE(ct->dynticks_nesting, 0); /* Avoid irq-access tearing. */",
            "\t// RCU is watching here ...",
            "\tct_kernel_exit_state(offset);",
            "\t// ... but is no longer watching here.",
            "\trcu_dynticks_task_enter();",
            "}",
            "static void noinstr ct_kernel_enter(bool user, int offset)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "\tlong oldval;",
            "",
            "\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !raw_irqs_disabled());",
            "\toldval = ct_dynticks_nesting();",
            "\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && oldval < 0);",
            "\tif (oldval) {",
            "\t\t// RCU was already watching, so just do accounting and leave.",
            "\t\tct->dynticks_nesting++;",
            "\t\treturn;",
            "\t}",
            "\trcu_dynticks_task_exit();",
            "\t// RCU is not watching here ...",
            "\tct_kernel_enter_state(offset);",
            "\t// ... but is watching here.",
            "\tinstrumentation_begin();",
            "",
            "\t// instrumentation for the noinstr ct_kernel_enter_state()",
            "\tinstrument_atomic_write(&ct->state, sizeof(ct->state));",
            "",
            "\ttrace_rcu_dyntick(TPS(\"End\"), ct_dynticks_nesting(), 1, ct_dynticks());",
            "\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !user && !is_idle_task(current));",
            "\tWRITE_ONCE(ct->dynticks_nesting, 1);",
            "\tWARN_ON_ONCE(ct_dynticks_nmi_nesting());",
            "\tWRITE_ONCE(ct->dynticks_nmi_nesting, DYNTICK_IRQ_NONIDLE);",
            "\tinstrumentation_end();",
            "}"
          ],
          "function_name": "rcu_dynticks_task_enter, rcu_dynticks_task_exit, rcu_dynticks_task_trace_enter, rcu_dynticks_task_trace_exit, ct_kernel_exit_state, ct_kernel_enter_state, ct_kernel_exit, ct_kernel_enter",
          "description": "处理任务上下文切换时的RCU动态时钟跟踪状态变更，通过原子操作更新状态字段并触发相关追踪事件。",
          "similarity": 0.5227627754211426
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/context_tracking.c",
          "start_line": 204,
          "end_line": 304,
          "content": [
            "void noinstr ct_nmi_exit(void)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "",
            "\tinstrumentation_begin();",
            "\t/*",
            "\t * Check for ->dynticks_nmi_nesting underflow and bad ->dynticks.",
            "\t * (We are exiting an NMI handler, so RCU better be paying attention",
            "\t * to us!)",
            "\t */",
            "\tWARN_ON_ONCE(ct_dynticks_nmi_nesting() <= 0);",
            "\tWARN_ON_ONCE(rcu_dynticks_curr_cpu_in_eqs());",
            "",
            "\t/*",
            "\t * If the nesting level is not 1, the CPU wasn't RCU-idle, so",
            "\t * leave it in non-RCU-idle state.",
            "\t */",
            "\tif (ct_dynticks_nmi_nesting() != 1) {",
            "\t\ttrace_rcu_dyntick(TPS(\"--=\"), ct_dynticks_nmi_nesting(), ct_dynticks_nmi_nesting() - 2,",
            "\t\t\t\t  ct_dynticks());",
            "\t\tWRITE_ONCE(ct->dynticks_nmi_nesting, /* No store tearing. */",
            "\t\t\t   ct_dynticks_nmi_nesting() - 2);",
            "\t\tinstrumentation_end();",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* This NMI interrupted an RCU-idle CPU, restore RCU-idleness. */",
            "\ttrace_rcu_dyntick(TPS(\"Startirq\"), ct_dynticks_nmi_nesting(), 0, ct_dynticks());",
            "\tWRITE_ONCE(ct->dynticks_nmi_nesting, 0); /* Avoid store tearing. */",
            "",
            "\t// instrumentation for the noinstr ct_kernel_exit_state()",
            "\tinstrument_atomic_write(&ct->state, sizeof(ct->state));",
            "\tinstrumentation_end();",
            "",
            "\t// RCU is watching here ...",
            "\tct_kernel_exit_state(RCU_DYNTICKS_IDX);",
            "\t// ... but is no longer watching here.",
            "",
            "\tif (!in_nmi())",
            "\t\trcu_dynticks_task_enter();",
            "}",
            "void noinstr ct_nmi_enter(void)",
            "{",
            "\tlong incby = 2;",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "",
            "\t/* Complain about underflow. */",
            "\tWARN_ON_ONCE(ct_dynticks_nmi_nesting() < 0);",
            "",
            "\t/*",
            "\t * If idle from RCU viewpoint, atomically increment ->dynticks",
            "\t * to mark non-idle and increment ->dynticks_nmi_nesting by one.",
            "\t * Otherwise, increment ->dynticks_nmi_nesting by two.  This means",
            "\t * if ->dynticks_nmi_nesting is equal to one, we are guaranteed",
            "\t * to be in the outermost NMI handler that interrupted an RCU-idle",
            "\t * period (observation due to Andy Lutomirski).",
            "\t */",
            "\tif (rcu_dynticks_curr_cpu_in_eqs()) {",
            "",
            "\t\tif (!in_nmi())",
            "\t\t\trcu_dynticks_task_exit();",
            "",
            "\t\t// RCU is not watching here ...",
            "\t\tct_kernel_enter_state(RCU_DYNTICKS_IDX);",
            "\t\t// ... but is watching here.",
            "",
            "\t\tinstrumentation_begin();",
            "\t\t// instrumentation for the noinstr rcu_dynticks_curr_cpu_in_eqs()",
            "\t\tinstrument_atomic_read(&ct->state, sizeof(ct->state));",
            "\t\t// instrumentation for the noinstr ct_kernel_enter_state()",
            "\t\tinstrument_atomic_write(&ct->state, sizeof(ct->state));",
            "",
            "\t\tincby = 1;",
            "\t} else if (!in_nmi()) {",
            "\t\tinstrumentation_begin();",
            "\t\trcu_irq_enter_check_tick();",
            "\t} else  {",
            "\t\tinstrumentation_begin();",
            "\t}",
            "",
            "\ttrace_rcu_dyntick(incby == 1 ? TPS(\"Endirq\") : TPS(\"++=\"),",
            "\t\t\t  ct_dynticks_nmi_nesting(),",
            "\t\t\t  ct_dynticks_nmi_nesting() + incby, ct_dynticks());",
            "\tinstrumentation_end();",
            "\tWRITE_ONCE(ct->dynticks_nmi_nesting, /* Prevent store tearing. */",
            "\t\t   ct_dynticks_nmi_nesting() + incby);",
            "\tbarrier();",
            "}",
            "void noinstr ct_idle_enter(void)",
            "{",
            "\tWARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !raw_irqs_disabled());",
            "\tct_kernel_exit(false, RCU_DYNTICKS_IDX + CT_STATE_IDLE);",
            "}",
            "void noinstr ct_idle_exit(void)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_local_irq_save(flags);",
            "\tct_kernel_enter(false, RCU_DYNTICKS_IDX - CT_STATE_IDLE);",
            "\traw_local_irq_restore(flags);",
            "}"
          ],
          "function_name": "ct_nmi_exit, ct_nmi_enter, ct_idle_enter, ct_idle_exit",
          "description": "管理NMI中断处理期间的上下文状态转换，确保RCU观察状态与NMI嵌套层级保持一致。",
          "similarity": 0.4932815730571747
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/context_tracking.c",
          "start_line": 555,
          "end_line": 661,
          "content": [
            "void ct_user_enter(enum ctx_state state)",
            "{",
            "\tunsigned long flags;",
            "",
            "\t/*",
            "\t * Some contexts may involve an exception occuring in an irq,",
            "\t * leading to that nesting:",
            "\t * ct_irq_enter() rcu_eqs_exit(true) rcu_eqs_enter(true) ct_irq_exit()",
            "\t * This would mess up the dyntick_nesting count though. And rcu_irq_*()",
            "\t * helpers are enough to protect RCU uses inside the exception. So",
            "\t * just return immediately if we detect we are in an IRQ.",
            "\t */",
            "\tif (in_interrupt())",
            "\t\treturn;",
            "",
            "\tlocal_irq_save(flags);",
            "\t__ct_user_enter(state);",
            "\tlocal_irq_restore(flags);",
            "}",
            "void user_enter_callable(void)",
            "{",
            "\tuser_enter();",
            "}",
            "void noinstr __ct_user_exit(enum ctx_state state)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "",
            "\tif (!context_tracking_recursion_enter())",
            "\t\treturn;",
            "",
            "\tif (__ct_state() == state) {",
            "\t\tif (ct->active) {",
            "\t\t\t/*",
            "\t\t\t * Exit RCU idle mode while entering the kernel because it can",
            "\t\t\t * run a RCU read side critical section anytime.",
            "\t\t\t */",
            "\t\t\tct_kernel_enter(true, RCU_DYNTICKS_IDX - state);",
            "\t\t\tif (state == CT_STATE_USER) {",
            "\t\t\t\tinstrumentation_begin();",
            "\t\t\t\tvtime_user_exit(current);",
            "\t\t\t\ttrace_user_exit(0);",
            "\t\t\t\tinstrumentation_end();",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * Special case if we only track user <-> kernel transitions for tickless",
            "\t\t\t * cputime accounting but we don't support RCU extended quiescent state.",
            "\t\t\t * In this we case we don't care about any concurrency/ordering.",
            "\t\t\t */",
            "\t\t\tif (!IS_ENABLED(CONFIG_CONTEXT_TRACKING_IDLE))",
            "\t\t\t\traw_atomic_set(&ct->state, CT_STATE_KERNEL);",
            "",
            "\t\t} else {",
            "\t\t\tif (!IS_ENABLED(CONFIG_CONTEXT_TRACKING_IDLE)) {",
            "\t\t\t\t/* Tracking for vtime only, no concurrent RCU EQS accounting */",
            "\t\t\t\traw_atomic_set(&ct->state, CT_STATE_KERNEL);",
            "\t\t\t} else {",
            "\t\t\t\t/*",
            "\t\t\t\t * Tracking for vtime and RCU EQS. Make sure we don't race",
            "\t\t\t\t * with NMIs. OTOH we don't care about ordering here since",
            "\t\t\t\t * RCU only requires RCU_DYNTICKS_IDX increments to be fully",
            "\t\t\t\t * ordered.",
            "\t\t\t\t */",
            "\t\t\t\traw_atomic_sub(state, &ct->state);",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\tcontext_tracking_recursion_exit();",
            "}",
            "void ct_user_exit(enum ctx_state state)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tif (in_interrupt())",
            "\t\treturn;",
            "",
            "\tlocal_irq_save(flags);",
            "\t__ct_user_exit(state);",
            "\tlocal_irq_restore(flags);",
            "}",
            "void user_exit_callable(void)",
            "{",
            "\tuser_exit();",
            "}",
            "void __init ct_cpu_track_user(int cpu)",
            "{",
            "\tstatic __initdata bool initialized = false;",
            "",
            "\tif (!per_cpu(context_tracking.active, cpu)) {",
            "\t\tper_cpu(context_tracking.active, cpu) = true;",
            "\t\tstatic_branch_inc(&context_tracking_key);",
            "\t}",
            "",
            "\tif (initialized)",
            "\t\treturn;",
            "",
            "#ifdef CONFIG_HAVE_TIF_NOHZ",
            "\t/*",
            "\t * Set TIF_NOHZ to init/0 and let it propagate to all tasks through fork",
            "\t * This assumes that init is the only task at this early boot stage.",
            "\t */",
            "\tset_tsk_thread_flag(&init_task, TIF_NOHZ);",
            "#endif",
            "\tWARN_ON_ONCE(!tasklist_empty());",
            "",
            "\tinitialized = true;",
            "}"
          ],
          "function_name": "ct_user_enter, user_enter_callable, __ct_user_exit, ct_user_exit, user_exit_callable, ct_cpu_track_user",
          "description": "管理用户态与内核态转换的上下文跟踪，根据配置选项区分vtime统计与RCU扩展静止状态跟踪的不同行为。",
          "similarity": 0.4930676817893982
        }
      ]
    },
    {
      "source_file": "kernel/sched/autogroup.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:56:17\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sched\\autogroup.c`\n\n---\n\n# `sched/autogroup.c` 技术文档\n\n## 1. 文件概述\n\n`sched/autogroup.c` 实现了 Linux 内核中的 **自动任务组（autogroup）调度机制**。该机制旨在提升桌面交互式应用的响应性，通过为每个会话（session）自动创建独立的调度任务组（task group），并基于进程的 nice 值动态调整该组的 CPU 带宽分配。当启用时（默认开启），所有属于同一会话的进程共享相同的调度权重，从而避免一个 CPU 密集型任务拖慢整个用户会话的响应速度。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct autogroup`**（隐式定义，通过成员推断）：\n  - `tg`：指向关联的 `task_group`，用于 CFS 调度器的组调度。\n  - `kref`：引用计数，管理 autogroup 生命周期。\n  - `lock`：读写信号量，保护 autogroup 内部状态（如 nice 值）。\n  - `id`：唯一标识符，用于 `/proc` 接口显示。\n  - `nice`：当前 autogroup 的 nice 值（范围 -20 到 19）。\n\n- **全局变量**：\n  - `sysctl_sched_autogroup_enabled`：控制 autogroup 功能开关（可通过 `/proc/sys/kernel/sched_autogroup_enabled` 配置）。\n  - `autogroup_default`：默认 autogroup，初始任务（如 init）使用。\n  - `autogroup_seq_nr`：原子计数器，用于生成 autogroup 的唯一 ID。\n\n### 主要函数\n\n- **初始化与销毁**：\n  - `autogroup_init()`：初始化默认 autogroup 并关联到 init 任务。\n  - `autogroup_free()`：释放 task_group 关联的 autogroup 内存。\n  - `autogroup_destroy()`：当引用计数归零时，销毁 autogroup 及其 task_group。\n  \n- **Autogroup 管理**：\n  - `autogroup_create()`：创建新的 autogroup 及其 task_group。\n  - `autogroup_move_group()`：将任务及其线程组迁移到指定 autogroup。\n  - `autogroup_task_get()`：安全获取任务当前的 autogroup（带引用计数）。\n\n- **任务生命周期钩子**：\n  - `sched_autogroup_create_attach()`：为新会话创建并绑定 autogroup（如 shell 启动新进程组）。\n  - `sched_autogroup_detach()`：将任务移回默认 autogroup（当前无使用者）。\n  - `sched_autogroup_fork()`：fork 时子进程继承父进程的 autogroup。\n  - `sched_autogroup_exit()`：进程退出时释放 autogroup 引用。\n  - `sched_autogroup_exit_task()`：任务退出前确保调度器状态同步。\n\n- **用户接口**：\n  - `proc_sched_autogroup_set_nice()`：通过 `/proc/<pid>/autogroup` 设置 autogroup 的 nice 值。\n  - `proc_sched_autogroup_show_task()`：在 `/proc/<pid>/autogroup` 中显示 autogroup 信息。\n  - `autogroup_path()`：生成 autogroup 在 cgroup 层级中的路径名（如 `/autogroup-123`）。\n\n- **辅助函数**：\n  - `task_wants_autogroup()`：判断任务是否应使用 autogroup（排除非 root task_group 或退出中任务）。\n\n## 3. 关键实现\n\n### Autogroup 创建与绑定\n- 当新会话启动（如终端执行命令），`sched_autogroup_create_attach()` 被调用：\n  1. 调用 `autogroup_create()` 分配新 `autogroup` 结构。\n  2. 通过 `sched_create_group()` 创建关联的 `task_group`。\n  3. 对于 `CONFIG_RT_GROUP_SCHED`，实时任务被重定向到 root task_group，避免带宽分配复杂性。\n  4. 调用 `autogroup_move_group()` 将当前任务及其所有线程迁移到新 autogroup。\n  5. 释放 `autogroup_create()` 返回的额外引用。\n\n### 引用计数与生命周期\n- 每个 `autogroup` 使用 `kref` 管理生命周期：\n  - `autogroup_task_get()` 获取时增加引用。\n  - `autogroup_kref_put()` 释放时减少引用，归零则调用 `autogroup_destroy()`。\n- `signal_struct->autogroup` 指向会话的 autogroup，fork 时子进程通过 `sched_autogroup_fork()` 继承父进程的引用。\n- 进程退出时，`sched_autogroup_exit()` 释放 `signal_struct` 持有的引用。\n\n### 调度权重调整\n- 通过 `/proc/<pid>/autogroup` 写入 nice 值（-20~19）：\n  - 调用 `proc_sched_autogroup_set_nice()`。\n  - 非 root 用户需具备 `CAP_SYS_ADMIN` 或遵守速率限制（每 100ms 一次）。\n  - 将 nice 值转换为 CFS 调度权重（`sched_prio_to_weight`），通过 `sched_group_set_shares()` 应用到 autogroup 的 `task_group`。\n\n### 退出处理\n- 任务退出时，`sched_autogroup_exit_task()` 确保在 `exit_notify()` 前调用 `sched_move_task()`，避免调度器使用已失效的 `signal->autogroup`。\n- `task_wants_autogroup()` 通过检查 `PF_EXITING` 标志防止退出中任务被错误迁移。\n\n## 4. 依赖关系\n\n- **调度器核心**：\n  - 依赖 `kernel/sched/core.c` 提供的 `task_group` 管理接口（如 `sched_create_group()`, `sched_destroy_group()`）。\n  - 依赖 CFS 调度器的组调度功能（`CONFIG_FAIR_GROUP_SCHED`）。\n- **实时调度**：\n  - 若启用 `CONFIG_RT_GROUP_SCHED`，autogroup 会绕过 RT 带宽分配，将 RT 任务重定向到 root task_group。\n- **安全模块**：\n  - 调用 `security_task_setnice()` 进行权限检查（LSM 钩子）。\n- **系统控制**：\n  - 通过 `CONFIG_SYSCTL` 注册 `/proc/sys/kernel/sched_autogroup_enabled` 开关。\n- **Proc 文件系统**：\n  - 依赖 `CONFIG_PROC_FS` 提供 `/proc/<pid>/autogroup` 接口。\n\n## 5. 使用场景\n\n- **桌面交互式环境**：\n  - 默认启用时，每个终端会话（如 GNOME Terminal、xterm）自动获得独立的 autogroup。\n  - 用户在终端运行 CPU 密集型任务（如 `make -j`）不会显著影响其他会话（如浏览器、音乐播放器）的响应性。\n- **动态优先级调整**：\n  - 用户可通过 `echo 10 > /proc/<pid>/autogroup` 降低整个会话的 CPU 优先级（等效于对会话内所有进程设置 nice=10）。\n- **容器与轻量级隔离**：\n  - 在未使用 cgroup v1/v2 的场景下，autogroup 提供基于会话的轻量级 CPU 资源隔离。\n- **系统启动与关闭**：\n  - init 进程使用默认 autogroup（绑定到 root task_group）。\n  - 所有用户进程通过 fork/会话创建机制自动纳入 autogroup 管理。",
      "similarity": 0.5368828177452087,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/sched/autogroup.c",
          "start_line": 204,
          "end_line": 279,
          "content": [
            "void sched_autogroup_detach(struct task_struct *p)",
            "{",
            "\tautogroup_move_group(p, &autogroup_default);",
            "}",
            "void sched_autogroup_fork(struct signal_struct *sig)",
            "{",
            "\tsig->autogroup = autogroup_task_get(current);",
            "}",
            "void sched_autogroup_exit(struct signal_struct *sig)",
            "{",
            "\tautogroup_kref_put(sig->autogroup);",
            "}",
            "static int __init setup_autogroup(char *str)",
            "{",
            "\tsysctl_sched_autogroup_enabled = 0;",
            "",
            "\treturn 1;",
            "}",
            "int proc_sched_autogroup_set_nice(struct task_struct *p, int nice)",
            "{",
            "\tstatic unsigned long next = INITIAL_JIFFIES;",
            "\tstruct autogroup *ag;",
            "\tunsigned long shares;",
            "\tint err, idx;",
            "",
            "\tif (nice < MIN_NICE || nice > MAX_NICE)",
            "\t\treturn -EINVAL;",
            "",
            "\terr = security_task_setnice(current, nice);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (nice < 0 && !can_nice(current, nice))",
            "\t\treturn -EPERM;",
            "",
            "\t/* This is a heavy operation, taking global locks.. */",
            "\tif (!capable(CAP_SYS_ADMIN) && time_before(jiffies, next))",
            "\t\treturn -EAGAIN;",
            "",
            "\tnext = HZ / 10 + jiffies;",
            "\tag = autogroup_task_get(p);",
            "",
            "\tidx = array_index_nospec(nice + 20, 40);",
            "\tshares = scale_load(sched_prio_to_weight[idx]);",
            "",
            "\tdown_write(&ag->lock);",
            "\terr = sched_group_set_shares(ag->tg, shares);",
            "\tif (!err)",
            "\t\tag->nice = nice;",
            "\tup_write(&ag->lock);",
            "",
            "\tautogroup_kref_put(ag);",
            "",
            "\treturn err;",
            "}",
            "void proc_sched_autogroup_show_task(struct task_struct *p, struct seq_file *m)",
            "{",
            "\tstruct autogroup *ag = autogroup_task_get(p);",
            "",
            "\tif (!task_group_is_autogroup(ag->tg))",
            "\t\tgoto out;",
            "",
            "\tdown_read(&ag->lock);",
            "\tseq_printf(m, \"/autogroup-%ld nice %d\\n\", ag->id, ag->nice);",
            "\tup_read(&ag->lock);",
            "",
            "out:",
            "\tautogroup_kref_put(ag);",
            "}",
            "int autogroup_path(struct task_group *tg, char *buf, int buflen)",
            "{",
            "\tif (!task_group_is_autogroup(tg))",
            "\t\treturn 0;",
            "",
            "\treturn snprintf(buf, buflen, \"%s-%ld\", \"/autogroup\", tg->autogroup->id);",
            "}"
          ],
          "function_name": "sched_autogroup_detach, sched_autogroup_fork, sched_autogroup_exit, setup_autogroup, proc_sched_autogroup_set_nice, proc_sched_autogroup_show_task, autogroup_path",
          "description": "实现自动分组的分离、进程fork继承、退出清理功能，提供优先级调整接口及分组路径生成函数用于调度参数展示",
          "similarity": 0.49394315481185913
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sched/autogroup.c",
          "start_line": 24,
          "end_line": 124,
          "content": [
            "static void __init sched_autogroup_sysctl_init(void)",
            "{",
            "\tregister_sysctl_init(\"kernel\", sched_autogroup_sysctls);",
            "}",
            "void __init autogroup_init(struct task_struct *init_task)",
            "{",
            "\tautogroup_default.tg = &root_task_group;",
            "\tkref_init(&autogroup_default.kref);",
            "\tinit_rwsem(&autogroup_default.lock);",
            "\tinit_task->signal->autogroup = &autogroup_default;",
            "\tsched_autogroup_sysctl_init();",
            "}",
            "void autogroup_free(struct task_group *tg)",
            "{",
            "\tkfree(tg->autogroup);",
            "}",
            "static inline void autogroup_destroy(struct kref *kref)",
            "{",
            "\tstruct autogroup *ag = container_of(kref, struct autogroup, kref);",
            "",
            "#ifdef CONFIG_RT_GROUP_SCHED",
            "\t/* We've redirected RT tasks to the root task group... */",
            "\tag->tg->rt_se = NULL;",
            "\tag->tg->rt_rq = NULL;",
            "#endif",
            "\tsched_release_group(ag->tg);",
            "\tsched_destroy_group(ag->tg);",
            "}",
            "static inline void autogroup_kref_put(struct autogroup *ag)",
            "{",
            "\tkref_put(&ag->kref, autogroup_destroy);",
            "}",
            "bool task_wants_autogroup(struct task_struct *p, struct task_group *tg)",
            "{",
            "\tif (tg != &root_task_group)",
            "\t\treturn false;",
            "\t/*",
            "\t * If we race with autogroup_move_group() the caller can use the old",
            "\t * value of signal->autogroup but in this case sched_move_task() will",
            "\t * be called again before autogroup_kref_put().",
            "\t *",
            "\t * However, there is no way sched_autogroup_exit_task() could tell us",
            "\t * to avoid autogroup->tg, so we abuse PF_EXITING flag for this case.",
            "\t */",
            "\tif (p->flags & PF_EXITING)",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "void sched_autogroup_exit_task(struct task_struct *p)",
            "{",
            "\t/*",
            "\t * We are going to call exit_notify() and autogroup_move_group() can't",
            "\t * see this thread after that: we can no longer use signal->autogroup.",
            "\t * See the PF_EXITING check in task_wants_autogroup().",
            "\t */",
            "\tsched_move_task(p, true);",
            "}",
            "static void",
            "autogroup_move_group(struct task_struct *p, struct autogroup *ag)",
            "{",
            "\tstruct autogroup *prev;",
            "\tstruct task_struct *t;",
            "\tunsigned long flags;",
            "",
            "\tif (WARN_ON_ONCE(!lock_task_sighand(p, &flags)))",
            "\t\treturn;",
            "",
            "\tprev = p->signal->autogroup;",
            "\tif (prev == ag) {",
            "\t\tunlock_task_sighand(p, &flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\tp->signal->autogroup = autogroup_kref_get(ag);",
            "\t/*",
            "\t * We can't avoid sched_move_task() after we changed signal->autogroup,",
            "\t * this process can already run with task_group() == prev->tg or we can",
            "\t * race with cgroup code which can read autogroup = prev under rq->lock.",
            "\t * In the latter case for_each_thread() can not miss a migrating thread,",
            "\t * cpu_cgroup_attach() must not be possible after cgroup_exit() and it",
            "\t * can't be removed from thread list, we hold ->siglock.",
            "\t *",
            "\t * If an exiting thread was already removed from thread list we rely on",
            "\t * sched_autogroup_exit_task().",
            "\t */",
            "\tfor_each_thread(p, t)",
            "\t\tsched_move_task(t, true);",
            "",
            "\tunlock_task_sighand(p, &flags);",
            "\tautogroup_kref_put(prev);",
            "}",
            "void sched_autogroup_create_attach(struct task_struct *p)",
            "{",
            "\tstruct autogroup *ag = autogroup_create();",
            "",
            "\tautogroup_move_group(p, ag);",
            "",
            "\t/* Drop extra reference added by autogroup_create(): */",
            "\tautogroup_kref_put(ag);",
            "}"
          ],
          "function_name": "sched_autogroup_sysctl_init, autogroup_init, autogroup_free, autogroup_destroy, autogroup_kref_put, task_wants_autogroup, sched_autogroup_exit_task, autogroup_move_group, sched_autogroup_create_attach",
          "description": "初始化自动分组调度模块，建立默认分组结构并关联至进程信号组，实现分组销毁、引用计数管理和任务分组迁移逻辑",
          "similarity": 0.4564558267593384
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/sched/autogroup.c",
          "start_line": 1,
          "end_line": 23,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "",
            "/*",
            " * Auto-group scheduling implementation:",
            " */",
            "",
            "unsigned int __read_mostly sysctl_sched_autogroup_enabled;",
            "static struct autogroup autogroup_default;",
            "static atomic_t autogroup_seq_nr;",
            "",
            "#ifdef CONFIG_SYSCTL",
            "static struct ctl_table sched_autogroup_sysctls[] = {",
            "\t{",
            "\t\t.procname       = \"sched_autogroup_enabled\",",
            "\t\t.data           = &sysctl_sched_autogroup_enabled,",
            "\t\t.maxlen         = sizeof(unsigned int),",
            "\t\t.mode           = 0644,",
            "\t\t.proc_handler   = proc_dointvec_minmax,",
            "\t\t.extra1         = SYSCTL_ZERO,",
            "\t\t.extra2         = SYSCTL_ONE,",
            "\t},",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义自动分组调度的全局变量和sysctl表项，用于控制自动分组功能的启用状态及注册相关系统调参接口",
          "similarity": 0.40005284547805786
        }
      ]
    }
  ]
}