{
  "query": "内存映射文件的资源回收机制",
  "timestamp": "2025-12-26 01:10:06",
  "retrieved_files": [
    {
      "source_file": "mm/rmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:15:36\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rmap.c`\n\n---\n\n# rmap.c 技术文档\n\n## 1. 文件概述\n\n`rmap.c` 是 Linux 内核内存管理子系统中的核心文件，实现了**物理页到虚拟地址的反向映射（reverse mapping）机制**。该机制用于在给定一个物理页（或 folio）时，能够快速找到所有映射该页的虚拟内存区域（VMA），从而支持页面回收、迁移、共享内存管理、内存错误处理等关键功能。\n\n文件主要分为两部分：\n- **匿名页反向映射**：用于处理堆、栈、匿名 mmap 等不对应文件的内存页\n- **文件页反向映射**：用于处理映射自文件（如可执行文件、mmap 文件）的内存页\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct anon_vma`：匿名虚拟内存区域描述符，用于组织共享同一匿名页的所有 VMA\n- `struct anon_vma_chain`：连接 VMA 与 anon_vma 的链表节点\n- `anon_vma_cachep` / `anon_vma_chain_cachep`：专用 slab 缓存，用于高效分配上述结构\n\n### 主要函数\n- `__anon_vma_prepare()`：为 VMA 准备并关联 anon_vma 结构\n- `anon_vma_alloc()` / `anon_vma_free()`：anon_vma 的分配与释放\n- `anon_vma_chain_alloc()` / `anon_vma_chain_free()`：链表节点的分配与释放\n- `anon_vma_chain_link()`：将 anon_vma_chain 链接到 VMA 和 anon_vma\n- `lock_anon_vma_root()` / `unlock_anon_vma_root()`：安全地锁定 anon_vma 根节点\n\n### 辅助机制\n- 基于红黑树的区间树（`anon_vma_interval_tree_insert`）用于高效查找\n- RCU（Read-Copy-Update）机制支持无锁读取路径\n- 引用计数管理（`atomic_t refcount`）确保内存安全\n\n## 3. 关键实现\n\n### 锁定层次与并发控制\n文件严格遵循内核内存管理的**锁顺序规范**：\n```\nmm->mmap_lock → mapping->i_mmap_rwsem → anon_vma->rwsem → mm->page_table_lock\n```\n这种层次化锁定避免了死锁，并确保在并发环境下数据一致性。\n\n### Anon VMA 合并与复用\n`__anon_vma_prepare()` 实现了智能的 anon_vma 复用策略：\n- 优先查找相邻 VMA 是否有可合并的 anon_vma（通过 `find_mergeable_anon_vma()`）\n- 只有在无法复用时才分配新的 anon_vma，减少内存开销\n- 使用 `page_table_lock` 保护 VMA 的 anon_vma 字段更新\n\n### 安全释放机制\n`anon_vma_free()` 包含特殊的同步逻辑：\n- 检查根 anon_vma 的 rwsem 是否被持有\n- 如有必要，获取写锁再释放，确保 RCU 读端（如 `folio_lock_anon_vma_read()`）不会访问已释放内存\n- 依赖原子操作和内存屏障保证释放顺序\n\n### 内存分配策略\n- 所有 anon_vma 相关结构通过专用 slab 缓存分配，提高性能\n- 分配失败时进行适当的资源清理（`out_enomem` 路径）\n\n## 4. 依赖关系\n\n### 头文件依赖\n- `<linux/mm.h>`：核心内存管理接口\n- `<linux/rmap.h>`：反向映射公共接口\n- `<linux/swap.h>`：交换子系统集成\n- `<linux/hugetlb.h>`：大页支持\n- `<linux/memcontrol.h>`：内存控制组支持\n- `\"internal.h\"`：MM 子系统内部接口\n\n### 功能依赖\n- **VMA 管理**：依赖 `vm_area_struct` 和 `mm_struct` 的完整性\n- **页表操作**：需要 `page_table_lock` 保护页表修改\n- **RCU 机制**：用于无锁读取路径的安全性\n- **LRU 管理**：与页面回收子系统紧密集成\n- **内存迁移**：为 `migrate_pages()` 提供反向映射支持\n\n### 调用关系\n- **被调用**：由 `mmap()`、`fork()`、`mprotect()` 等系统调用间接调用\n- **调用**：调用 swap、hugetlb、memcg 等子系统的相关函数\n\n## 5. 使用场景\n\n### 页面回收（Page Reclaim）\n- 当内存压力触发页面回收时，需要通过反向映射找到所有映射该页的 PTE\n- 断开映射关系并更新页表，然后回收物理页\n\n### 内存迁移（Memory Migration）\n- 在 NUMA 平衡或内存压缩过程中，需要将页迁移到新位置\n- 通过反向映射更新所有相关 VMA 的 PTE 指向新地址\n\n### 写时复制（Copy-on-Write）\n- `fork()` 后子进程共享父进程页面，首次写入时触发 COW\n- 反向映射帮助确定哪些进程共享该页，以便正确分离\n\n### 内存错误处理（Memory Failure）\n- 当硬件检测到内存错误时，需要隔离损坏页面\n- 通过反向映射通知所有使用该页的进程\n\n### KSM（Kernel Samepage Merging）\n- 合并内容相同的匿名页以节省内存\n- 反向映射用于管理合并后页面的多个映射关系\n\n### 用户态缺页处理（Userfaultfd）\n- 支持用户态处理缺页异常\n- 需要准确的反向映射信息来定位相关 VMA",
      "similarity": 0.6552415490150452,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "mm/rmap.c",
          "start_line": 1075,
          "end_line": 1180,
          "content": [
            "static bool page_mkclean_one(struct folio *folio, struct vm_area_struct *vma,",
            "\t\t\t     unsigned long address, void *arg)",
            "{",
            "\tDEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, address, PVMW_SYNC);",
            "\tint *cleaned = arg;",
            "",
            "\t*cleaned += page_vma_mkclean_one(&pvmw);",
            "",
            "\treturn true;",
            "}",
            "static bool invalid_mkclean_vma(struct vm_area_struct *vma, void *arg)",
            "{",
            "\tif (vma->vm_flags & VM_SHARED)",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "int folio_mkclean(struct folio *folio)",
            "{",
            "\tint cleaned = 0;",
            "\tstruct address_space *mapping;",
            "\tstruct rmap_walk_control rwc = {",
            "\t\t.arg = (void *)&cleaned,",
            "\t\t.rmap_one = page_mkclean_one,",
            "\t\t.invalid_vma = invalid_mkclean_vma,",
            "\t};",
            "",
            "\tBUG_ON(!folio_test_locked(folio));",
            "",
            "\tif (!folio_mapped(folio))",
            "\t\treturn 0;",
            "",
            "\tmapping = folio_mapping(folio);",
            "\tif (!mapping)",
            "\t\treturn 0;",
            "",
            "\trmap_walk(folio, &rwc);",
            "",
            "\treturn cleaned;",
            "}",
            "int pfn_mkclean_range(unsigned long pfn, unsigned long nr_pages, pgoff_t pgoff,",
            "\t\t      struct vm_area_struct *vma)",
            "{",
            "\tstruct page_vma_mapped_walk pvmw = {",
            "\t\t.pfn\t\t= pfn,",
            "\t\t.nr_pages\t= nr_pages,",
            "\t\t.pgoff\t\t= pgoff,",
            "\t\t.vma\t\t= vma,",
            "\t\t.flags\t\t= PVMW_SYNC,",
            "\t};",
            "",
            "\tif (invalid_mkclean_vma(vma, NULL))",
            "\t\treturn 0;",
            "",
            "\tpvmw.address = vma_address(vma, pgoff, nr_pages);",
            "\tVM_BUG_ON_VMA(pvmw.address == -EFAULT, vma);",
            "",
            "\treturn page_vma_mkclean_one(&pvmw);",
            "}",
            "static __always_inline unsigned int __folio_add_rmap(struct folio *folio,",
            "\t\tstruct page *page, int nr_pages, enum rmap_level level,",
            "\t\tint *nr_pmdmapped)",
            "{",
            "\tatomic_t *mapped = &folio->_nr_pages_mapped;",
            "\tconst int orig_nr_pages = nr_pages;",
            "\tint first = 0, nr = 0;",
            "",
            "\t__folio_rmap_sanity_checks(folio, page, nr_pages, level);",
            "",
            "\tswitch (level) {",
            "\tcase RMAP_LEVEL_PTE:",
            "\t\tif (!folio_test_large(folio)) {",
            "\t\t\tnr = atomic_inc_and_test(&page->_mapcount);",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tdo {",
            "\t\t\tfirst += atomic_inc_and_test(&page->_mapcount);",
            "\t\t} while (page++, --nr_pages > 0);",
            "",
            "\t\tif (first &&",
            "\t\t    atomic_add_return_relaxed(first, mapped) < ENTIRELY_MAPPED)",
            "\t\t\tnr = first;",
            "",
            "\t\tatomic_add(orig_nr_pages, &folio->_large_mapcount);",
            "\t\tbreak;",
            "\tcase RMAP_LEVEL_PMD:",
            "\t\tfirst = atomic_inc_and_test(&folio->_entire_mapcount);",
            "\t\tif (first) {",
            "\t\t\tnr = atomic_add_return_relaxed(ENTIRELY_MAPPED, mapped);",
            "\t\t\tif (likely(nr < ENTIRELY_MAPPED + ENTIRELY_MAPPED)) {",
            "\t\t\t\t*nr_pmdmapped = folio_nr_pages(folio);",
            "\t\t\t\tnr = *nr_pmdmapped - (nr & FOLIO_PAGES_MAPPED);",
            "\t\t\t\t/* Raced ahead of a remove and another add? */",
            "\t\t\t\tif (unlikely(nr < 0))",
            "\t\t\t\t\tnr = 0;",
            "\t\t\t} else {",
            "\t\t\t\t/* Raced ahead of a remove of ENTIRELY_MAPPED */",
            "\t\t\t\tnr = 0;",
            "\t\t\t}",
            "\t\t}",
            "\t\tatomic_inc(&folio->_large_mapcount);",
            "\t\tbreak;",
            "\t}",
            "\treturn nr;",
            "}"
          ],
          "function_name": "page_mkclean_one, invalid_mkclean_vma, folio_mkclean, pfn_mkclean_range, __folio_add_rmap",
          "description": "该代码段核心功能是管理页面的清理与反向映射追踪。  \n`page_mkclean_one` 和 `invalid_mkclean_vma` 共同遍历 VMA 区间，过滤共享内存后对页面标记为干净；`folio_mkclean` 和 `pfn_mkclean_range` 分别通过 rmap 走查机制批量清理指定页面的脏标志。  \n`__folio_add_rmap` 根据 PTE/PMD 层级更新页面引用计数，维护 large 页面和 PMD 映射的统计状态，确保内存回收时正确判断页面引用情况。",
          "similarity": 0.5592846870422363
        },
        {
          "chunk_id": 13,
          "file_path": "mm/rmap.c",
          "start_line": 2637,
          "end_line": 2733,
          "content": [
            "static void rmap_walk_file(struct folio *folio,",
            "\t\tstruct rmap_walk_control *rwc, bool locked)",
            "{",
            "\tstruct address_space *mapping = folio_mapping(folio);",
            "\tpgoff_t pgoff_start, pgoff_end;",
            "\tstruct vm_area_struct *vma;",
            "",
            "\t/*",
            "\t * The page lock not only makes sure that page->mapping cannot",
            "\t * suddenly be NULLified by truncation, it makes sure that the",
            "\t * structure at mapping cannot be freed and reused yet,",
            "\t * so we can safely take mapping->i_mmap_rwsem.",
            "\t */",
            "\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "",
            "\tif (!mapping)",
            "\t\treturn;",
            "",
            "\tpgoff_start = folio_pgoff(folio);",
            "\tpgoff_end = pgoff_start + folio_nr_pages(folio) - 1;",
            "\tif (!locked) {",
            "\t\tif (i_mmap_trylock_read(mapping))",
            "\t\t\tgoto lookup;",
            "",
            "\t\tif (rwc->try_lock) {",
            "\t\t\trwc->contended = true;",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\ti_mmap_lock_read(mapping);",
            "\t}",
            "lookup:",
            "\tvma_interval_tree_foreach(vma, &mapping->i_mmap,",
            "\t\t\tpgoff_start, pgoff_end) {",
            "\t\tunsigned long address = vma_address(vma, pgoff_start,",
            "\t\t\t       folio_nr_pages(folio));",
            "",
            "\t\tVM_BUG_ON_VMA(address == -EFAULT, vma);",
            "\t\tcond_resched();",
            "",
            "\t\tif (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!rwc->rmap_one(folio, vma, address, rwc->arg))",
            "\t\t\tgoto done;",
            "\t\tif (rwc->done && rwc->done(folio))",
            "\t\t\tgoto done;",
            "\t}",
            "",
            "done:",
            "\tif (!locked)",
            "\t\ti_mmap_unlock_read(mapping);",
            "}",
            "void rmap_walk(struct folio *folio, struct rmap_walk_control *rwc)",
            "{",
            "\tif (unlikely(folio_test_ksm(folio)))",
            "\t\trmap_walk_ksm(folio, rwc);",
            "\telse if (folio_test_anon(folio))",
            "\t\trmap_walk_anon(folio, rwc, false);",
            "\telse",
            "\t\trmap_walk_file(folio, rwc, false);",
            "}",
            "void rmap_walk_locked(struct folio *folio, struct rmap_walk_control *rwc)",
            "{",
            "\t/* no ksm support for now */",
            "\tVM_BUG_ON_FOLIO(folio_test_ksm(folio), folio);",
            "\tif (folio_test_anon(folio))",
            "\t\trmap_walk_anon(folio, rwc, true);",
            "\telse",
            "\t\trmap_walk_file(folio, rwc, true);",
            "}",
            "void hugetlb_add_anon_rmap(struct folio *folio, struct vm_area_struct *vma,",
            "\t\tunsigned long address, rmap_t flags)",
            "{",
            "\tVM_WARN_ON_FOLIO(!folio_test_hugetlb(folio), folio);",
            "\tVM_WARN_ON_FOLIO(!folio_test_anon(folio), folio);",
            "",
            "\tatomic_inc(&folio->_entire_mapcount);",
            "\tatomic_inc(&folio->_large_mapcount);",
            "\tif (flags & RMAP_EXCLUSIVE)",
            "\t\tSetPageAnonExclusive(&folio->page);",
            "\tVM_WARN_ON_FOLIO(folio_entire_mapcount(folio) > 1 &&",
            "\t\t\t PageAnonExclusive(&folio->page), folio);",
            "}",
            "void hugetlb_add_new_anon_rmap(struct folio *folio,",
            "\t\tstruct vm_area_struct *vma, unsigned long address)",
            "{",
            "\tVM_WARN_ON_FOLIO(!folio_test_hugetlb(folio), folio);",
            "",
            "\tBUG_ON(address < vma->vm_start || address >= vma->vm_end);",
            "\t/* increment count (starts at -1) */",
            "\tatomic_set(&folio->_entire_mapcount, 0);",
            "\tatomic_set(&folio->_large_mapcount, 0);",
            "\tfolio_clear_hugetlb_restore_reserve(folio);",
            "\t__folio_set_anon(folio, vma, address, true);",
            "\tSetPageAnonExclusive(&folio->page);",
            "}"
          ],
          "function_name": "rmap_walk_file, rmap_walk, rmap_walk_locked, hugetlb_add_anon_rmap, hugetlb_add_new_anon_rmap",
          "description": "提供文件映射和匿名映射的通用rmap遍历框架，包含巨页添加匿名映射的辅助函数，维护页面到虚拟地址空间的关联关系。",
          "similarity": 0.5582676529884338
        },
        {
          "chunk_id": 0,
          "file_path": "mm/rmap.c",
          "start_line": 1,
          "end_line": 109,
          "content": [
            "/*",
            " * mm/rmap.c - physical to virtual reverse mappings",
            " *",
            " * Copyright 2001, Rik van Riel <riel@conectiva.com.br>",
            " * Released under the General Public License (GPL).",
            " *",
            " * Simple, low overhead reverse mapping scheme.",
            " * Please try to keep this thing as modular as possible.",
            " *",
            " * Provides methods for unmapping each kind of mapped page:",
            " * the anon methods track anonymous pages, and",
            " * the file methods track pages belonging to an inode.",
            " *",
            " * Original design by Rik van Riel <riel@conectiva.com.br> 2001",
            " * File methods by Dave McCracken <dmccr@us.ibm.com> 2003, 2004",
            " * Anonymous methods by Andrea Arcangeli <andrea@suse.de> 2004",
            " * Contributions by Hugh Dickins 2003, 2004",
            " */",
            "",
            "/*",
            " * Lock ordering in mm:",
            " *",
            " * inode->i_rwsem\t(while writing or truncating, not reading or faulting)",
            " *   mm->mmap_lock",
            " *     mapping->invalidate_lock (in filemap_fault)",
            " *       folio_lock",
            " *         hugetlbfs_i_mmap_rwsem_key (in huge_pmd_share, see hugetlbfs below)",
            " *           vma_start_write",
            " *             mapping->i_mmap_rwsem",
            " *               anon_vma->rwsem",
            " *                 mm->page_table_lock or pte_lock",
            " *                   swap_lock (in swap_duplicate, swap_info_get)",
            " *                     mmlist_lock (in mmput, drain_mmlist and others)",
            " *                     mapping->private_lock (in block_dirty_folio)",
            " *                         i_pages lock (widely used)",
            " *                           lruvec->lru_lock (in folio_lruvec_lock_irq)",
            " *                     inode->i_lock (in set_page_dirty's __mark_inode_dirty)",
            " *                     bdi.wb->list_lock (in set_page_dirty's __mark_inode_dirty)",
            " *                       sb_lock (within inode_lock in fs/fs-writeback.c)",
            " *                       i_pages lock (widely used, in set_page_dirty,",
            " *                                 in arch-dependent flush_dcache_mmap_lock,",
            " *                                 within bdi.wb->list_lock in __sync_single_inode)",
            " *",
            " * anon_vma->rwsem,mapping->i_mmap_rwsem   (memory_failure, collect_procs_anon)",
            " *   ->tasklist_lock",
            " *     pte map lock",
            " *",
            " * hugetlbfs PageHuge() take locks in this order:",
            " *   hugetlb_fault_mutex (hugetlbfs specific page fault mutex)",
            " *     vma_lock (hugetlb specific lock for pmd_sharing)",
            " *       mapping->i_mmap_rwsem (also used for hugetlb pmd sharing)",
            " *         folio_lock",
            " */",
            "",
            "#include <linux/mm.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/task.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/swap.h>",
            "#include <linux/swapops.h>",
            "#include <linux/slab.h>",
            "#include <linux/init.h>",
            "#include <linux/ksm.h>",
            "#include <linux/rmap.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/export.h>",
            "#include <linux/memcontrol.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/migrate.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/huge_mm.h>",
            "#include <linux/backing-dev.h>",
            "#include <linux/page_idle.h>",
            "#include <linux/memremap.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/oom.h>",
            "",
            "#include <asm/tlbflush.h>",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/tlb.h>",
            "#include <trace/events/migrate.h>",
            "",
            "#include \"internal.h\"",
            "",
            "static struct kmem_cache *anon_vma_cachep;",
            "static struct kmem_cache *anon_vma_chain_cachep;",
            "",
            "static inline struct anon_vma *anon_vma_alloc(void)",
            "{",
            "\tstruct anon_vma *anon_vma;",
            "",
            "\tanon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);",
            "\tif (anon_vma) {",
            "\t\tatomic_set(&anon_vma->refcount, 1);",
            "\t\tanon_vma->num_children = 0;",
            "\t\tanon_vma->num_active_vmas = 0;",
            "\t\tanon_vma->parent = anon_vma;",
            "\t\t/*",
            "\t\t * Initialise the anon_vma root to point to itself. If called",
            "\t\t * from fork, the root will be reset to the parents anon_vma.",
            "\t\t */",
            "\t\tanon_vma->root = anon_vma;",
            "\t}",
            "",
            "\treturn anon_vma;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义匿名页反向映射相关结构体和缓存，初始化anon_vma_cachep和anon_vma_chain_cachep，提供anon_vma_alloc函数用于分配匿名页映射结构体",
          "similarity": 0.549632728099823
        },
        {
          "chunk_id": 11,
          "file_path": "mm/rmap.c",
          "start_line": 2311,
          "end_line": 2428,
          "content": [
            "void try_to_migrate(struct folio *folio, enum ttu_flags flags)",
            "{",
            "\tstruct rmap_walk_control rwc = {",
            "\t\t.rmap_one = try_to_migrate_one,",
            "\t\t.arg = (void *)flags,",
            "\t\t.done = folio_not_mapped,",
            "\t\t.anon_lock = folio_lock_anon_vma_read,",
            "\t};",
            "",
            "\t/*",
            "\t * Migration always ignores mlock and only supports TTU_RMAP_LOCKED and",
            "\t * TTU_SPLIT_HUGE_PMD, TTU_SYNC, and TTU_BATCH_FLUSH flags.",
            "\t */",
            "\tif (WARN_ON_ONCE(flags & ~(TTU_RMAP_LOCKED | TTU_SPLIT_HUGE_PMD |",
            "\t\t\t\t\tTTU_SYNC | TTU_BATCH_FLUSH)))",
            "\t\treturn;",
            "",
            "\tif (folio_is_zone_device(folio) &&",
            "\t    (!folio_is_device_private(folio) && !folio_is_device_coherent(folio)))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * During exec, a temporary VMA is setup and later moved.",
            "\t * The VMA is moved under the anon_vma lock but not the",
            "\t * page tables leading to a race where migration cannot",
            "\t * find the migration ptes. Rather than increasing the",
            "\t * locking requirements of exec(), migration skips",
            "\t * temporary VMAs until after exec() completes.",
            "\t */",
            "\tif (!folio_test_ksm(folio) && folio_test_anon(folio))",
            "\t\trwc.invalid_vma = invalid_migration_vma;",
            "",
            "\tif (flags & TTU_RMAP_LOCKED)",
            "\t\trmap_walk_locked(folio, &rwc);",
            "\telse",
            "\t\trmap_walk(folio, &rwc);",
            "}",
            "static bool page_make_device_exclusive_one(struct folio *folio,",
            "\t\tstruct vm_area_struct *vma, unsigned long address, void *priv)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tDEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, address, 0);",
            "\tstruct make_exclusive_args *args = priv;",
            "\tpte_t pteval;",
            "\tstruct page *subpage;",
            "\tbool ret = true;",
            "\tstruct mmu_notifier_range range;",
            "\tswp_entry_t entry;",
            "\tpte_t swp_pte;",
            "\tpte_t ptent;",
            "",
            "\tmmu_notifier_range_init_owner(&range, MMU_NOTIFY_EXCLUSIVE, 0,",
            "\t\t\t\t      vma->vm_mm, address, min(vma->vm_end,",
            "\t\t\t\t      address + folio_size(folio)),",
            "\t\t\t\t      args->owner);",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "",
            "\twhile (page_vma_mapped_walk(&pvmw)) {",
            "\t\t/* Unexpected PMD-mapped THP? */",
            "\t\tVM_BUG_ON_FOLIO(!pvmw.pte, folio);",
            "",
            "\t\tptent = ptep_get(pvmw.pte);",
            "\t\tif (!pte_present(ptent)) {",
            "\t\t\tret = false;",
            "\t\t\tpage_vma_mapped_walk_done(&pvmw);",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tsubpage = folio_page(folio,",
            "\t\t\t\tpte_pfn(ptent) - folio_pfn(folio));",
            "\t\taddress = pvmw.address;",
            "",
            "\t\t/* Nuke the page table entry. */",
            "\t\tflush_cache_page(vma, address, pte_pfn(ptent));",
            "\t\tpteval = ptep_clear_flush(vma, address, pvmw.pte);",
            "",
            "\t\t/* Set the dirty flag on the folio now the pte is gone. */",
            "\t\tif (pte_dirty(pteval))",
            "\t\t\tfolio_mark_dirty(folio);",
            "",
            "\t\t/*",
            "\t\t * Check that our target page is still mapped at the expected",
            "\t\t * address.",
            "\t\t */",
            "\t\tif (args->mm == mm && args->address == address &&",
            "\t\t    pte_write(pteval))",
            "\t\t\targs->valid = true;",
            "",
            "\t\t/*",
            "\t\t * Store the pfn of the page in a special migration",
            "\t\t * pte. do_swap_page() will wait until the migration",
            "\t\t * pte is removed and then restart fault handling.",
            "\t\t */",
            "\t\tif (pte_write(pteval))",
            "\t\t\tentry = make_writable_device_exclusive_entry(",
            "\t\t\t\t\t\t\tpage_to_pfn(subpage));",
            "\t\telse",
            "\t\t\tentry = make_readable_device_exclusive_entry(",
            "\t\t\t\t\t\t\tpage_to_pfn(subpage));",
            "\t\tswp_pte = swp_entry_to_pte(entry);",
            "\t\tif (pte_soft_dirty(pteval))",
            "\t\t\tswp_pte = pte_swp_mksoft_dirty(swp_pte);",
            "\t\tif (pte_uffd_wp(pteval))",
            "\t\t\tswp_pte = pte_swp_mkuffd_wp(swp_pte);",
            "",
            "\t\tset_pte_at(mm, address, pvmw.pte, swp_pte);",
            "",
            "\t\t/*",
            "\t\t * There is a reference on the page for the swap entry which has",
            "\t\t * been removed, so shouldn't take another.",
            "\t\t */",
            "\t\tfolio_remove_rmap_pte(folio, subpage, vma);",
            "\t}",
            "",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "try_to_migrate, page_make_device_exclusive_one",
          "description": "尝试迁移页面到设备独占状态，通过遍历页表项清除原有映射并设置特殊迁移页表项，用于处理设备内存的exclusive访问需求。",
          "similarity": 0.5486389994621277
        },
        {
          "chunk_id": 9,
          "file_path": "mm/rmap.c",
          "start_line": 1600,
          "end_line": 1924,
          "content": [
            "void folio_remove_rmap_ptes(struct folio *folio, struct page *page,",
            "\t\tint nr_pages, struct vm_area_struct *vma)",
            "{",
            "\t__folio_remove_rmap(folio, page, nr_pages, vma, RMAP_LEVEL_PTE);",
            "}",
            "void folio_remove_rmap_pmd(struct folio *folio, struct page *page,",
            "\t\tstruct vm_area_struct *vma)",
            "{",
            "#ifdef CONFIG_TRANSPARENT_HUGEPAGE",
            "\t__folio_remove_rmap(folio, page, HPAGE_PMD_NR, vma, RMAP_LEVEL_PMD);",
            "#else",
            "\tWARN_ON_ONCE(true);",
            "#endif",
            "}",
            "static bool try_to_unmap_one(struct folio *folio, struct vm_area_struct *vma,",
            "\t\t     unsigned long address, void *arg)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tDEFINE_FOLIO_VMA_WALK(pvmw, folio, vma, address, 0);",
            "\tpte_t pteval;",
            "\tstruct page *subpage;",
            "\tbool anon_exclusive, ret = true;",
            "\tstruct mmu_notifier_range range;",
            "\tenum ttu_flags flags = (enum ttu_flags)(long)arg;",
            "\tunsigned long pfn;",
            "\tunsigned long hsz = 0;",
            "",
            "\t/*",
            "\t * When racing against e.g. zap_pte_range() on another cpu,",
            "\t * in between its ptep_get_and_clear_full() and folio_remove_rmap_*(),",
            "\t * try_to_unmap() may return before page_mapped() has become false,",
            "\t * if page table locking is skipped: use TTU_SYNC to wait for that.",
            "\t */",
            "\tif (flags & TTU_SYNC)",
            "\t\tpvmw.flags = PVMW_SYNC;",
            "",
            "\t/*",
            "\t * For THP, we have to assume the worse case ie pmd for invalidation.",
            "\t * For hugetlb, it could be much worse if we need to do pud",
            "\t * invalidation in the case of pmd sharing.",
            "\t *",
            "\t * Note that the folio can not be freed in this function as call of",
            "\t * try_to_unmap() must hold a reference on the folio.",
            "\t */",
            "\trange.end = vma_address_end(&pvmw);",
            "\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma->vm_mm,",
            "\t\t\t\taddress, range.end);",
            "\tif (folio_test_hugetlb(folio)) {",
            "\t\t/*",
            "\t\t * If sharing is possible, start and end will be adjusted",
            "\t\t * accordingly.",
            "\t\t */",
            "\t\tadjust_range_if_pmd_sharing_possible(vma, &range.start,",
            "\t\t\t\t\t\t     &range.end);",
            "",
            "\t\t/* We need the huge page size for set_huge_pte_at() */",
            "\t\thsz = huge_page_size(hstate_vma(vma));",
            "\t}",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "",
            "\twhile (page_vma_mapped_walk(&pvmw)) {",
            "\t\t/*",
            "\t\t * If the folio is in an mlock()d vma, we must not swap it out.",
            "\t\t */",
            "\t\tif (!(flags & TTU_IGNORE_MLOCK) &&",
            "\t\t    (vma->vm_flags & VM_LOCKED)) {",
            "\t\t\t/* Restore the mlock which got missed */",
            "\t\t\tif (!folio_test_large(folio))",
            "\t\t\t\tmlock_vma_folio(folio, vma);",
            "\t\t\tgoto walk_abort;",
            "\t\t}",
            "",
            "\t\tif (!pvmw.pte) {",
            "\t\t\tif (unmap_huge_pmd_locked(vma, pvmw.address, pvmw.pmd,",
            "\t\t\t\t\t\t  folio))",
            "\t\t\t\tgoto walk_done;",
            "",
            "\t\t\tif (flags & TTU_SPLIT_HUGE_PMD) {",
            "\t\t\t\t/*",
            "\t\t\t\t * We temporarily have to drop the PTL and",
            "\t\t\t\t * restart so we can process the PTE-mapped THP.",
            "\t\t\t\t */",
            "\t\t\t\tsplit_huge_pmd_locked(vma, pvmw.address,",
            "\t\t\t\t\t\t      pvmw.pmd, false, folio);",
            "\t\t\t\tflags &= ~TTU_SPLIT_HUGE_PMD;",
            "\t\t\t\tpage_vma_mapped_walk_restart(&pvmw);",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* Unexpected PMD-mapped THP? */",
            "\t\tVM_BUG_ON_FOLIO(!pvmw.pte, folio);",
            "",
            "\t\tpfn = pte_pfn(ptep_get(pvmw.pte));",
            "\t\tsubpage = folio_page(folio, pfn - folio_pfn(folio));",
            "\t\taddress = pvmw.address;",
            "\t\tanon_exclusive = folio_test_anon(folio) &&",
            "\t\t\t\t PageAnonExclusive(subpage);",
            "",
            "\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\tbool anon = folio_test_anon(folio);",
            "",
            "\t\t\t/*",
            "\t\t\t * The try_to_unmap() is only passed a hugetlb page",
            "\t\t\t * in the case where the hugetlb page is poisoned.",
            "\t\t\t */",
            "\t\t\tVM_BUG_ON_PAGE(!PageHWPoison(subpage), subpage);",
            "\t\t\t/*",
            "\t\t\t * huge_pmd_unshare may unmap an entire PMD page.",
            "\t\t\t * There is no way of knowing exactly which PMDs may",
            "\t\t\t * be cached for this mm, so we must flush them all.",
            "\t\t\t * start/end were already adjusted above to cover this",
            "\t\t\t * range.",
            "\t\t\t */",
            "\t\t\tflush_cache_range(vma, range.start, range.end);",
            "",
            "\t\t\t/*",
            "\t\t\t * To call huge_pmd_unshare, i_mmap_rwsem must be",
            "\t\t\t * held in write mode.  Caller needs to explicitly",
            "\t\t\t * do this outside rmap routines.",
            "\t\t\t *",
            "\t\t\t * We also must hold hugetlb vma_lock in write mode.",
            "\t\t\t * Lock order dictates acquiring vma_lock BEFORE",
            "\t\t\t * i_mmap_rwsem.  We can only try lock here and fail",
            "\t\t\t * if unsuccessful.",
            "\t\t\t */",
            "\t\t\tif (!anon) {",
            "\t\t\t\tVM_BUG_ON(!(flags & TTU_RMAP_LOCKED));",
            "\t\t\t\tif (!hugetlb_vma_trylock_write(vma))",
            "\t\t\t\t\tgoto walk_abort;",
            "\t\t\t\tif (huge_pmd_unshare(mm, vma, address, pvmw.pte)) {",
            "\t\t\t\t\thugetlb_vma_unlock_write(vma);",
            "\t\t\t\t\tflush_tlb_range(vma,",
            "\t\t\t\t\t\trange.start, range.end);",
            "\t\t\t\t\t/*",
            "\t\t\t\t\t * The ref count of the PMD page was",
            "\t\t\t\t\t * dropped which is part of the way map",
            "\t\t\t\t\t * counting is done for shared PMDs.",
            "\t\t\t\t\t * Return 'true' here.  When there is",
            "\t\t\t\t\t * no other sharing, huge_pmd_unshare",
            "\t\t\t\t\t * returns false and we will unmap the",
            "\t\t\t\t\t * actual page and drop map count",
            "\t\t\t\t\t * to zero.",
            "\t\t\t\t\t */",
            "\t\t\t\t\tgoto walk_done;",
            "\t\t\t\t}",
            "\t\t\t\thugetlb_vma_unlock_write(vma);",
            "\t\t\t}",
            "\t\t\tpteval = huge_ptep_clear_flush(vma, address, pvmw.pte);",
            "\t\t} else {",
            "\t\t\tflush_cache_page(vma, address, pfn);",
            "\t\t\t/* Nuke the page table entry. */",
            "\t\t\tif (should_defer_flush(mm, flags)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * We clear the PTE but do not flush so potentially",
            "\t\t\t\t * a remote CPU could still be writing to the folio.",
            "\t\t\t\t * If the entry was previously clean then the",
            "\t\t\t\t * architecture must guarantee that a clear->dirty",
            "\t\t\t\t * transition on a cached TLB entry is written through",
            "\t\t\t\t * and traps if the PTE is unmapped.",
            "\t\t\t\t */",
            "\t\t\t\tpteval = ptep_get_and_clear(mm, address, pvmw.pte);",
            "",
            "\t\t\t\tset_tlb_ubc_flush_pending(mm, pteval, address);",
            "\t\t\t} else {",
            "\t\t\t\tpteval = ptep_clear_flush(vma, address, pvmw.pte);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Now the pte is cleared. If this pte was uffd-wp armed,",
            "\t\t * we may want to replace a none pte with a marker pte if",
            "\t\t * it's file-backed, so we don't lose the tracking info.",
            "\t\t */",
            "\t\tpte_install_uffd_wp_if_needed(vma, address, pvmw.pte, pteval);",
            "",
            "\t\t/* Set the dirty flag on the folio now the pte is gone. */",
            "\t\tif (pte_dirty(pteval))",
            "\t\t\tfolio_mark_dirty(folio);",
            "",
            "\t\t/* Update high watermark before we lower rss */",
            "\t\tupdate_hiwater_rss(mm);",
            "",
            "\t\tif (PageHWPoison(subpage) && (flags & TTU_HWPOISON)) {",
            "\t\t\tpteval = swp_entry_to_pte(make_hwpoison_entry(subpage));",
            "\t\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\t\thugetlb_count_sub(folio_nr_pages(folio), mm);",
            "\t\t\t\tset_huge_pte_at(mm, address, pvmw.pte, pteval,",
            "\t\t\t\t\t\thsz);",
            "\t\t\t} else {",
            "\t\t\t\tdec_mm_counter(mm, mm_counter(folio));",
            "\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);",
            "\t\t\t}",
            "",
            "\t\t} else if (pte_unused(pteval) && !userfaultfd_armed(vma)) {",
            "\t\t\t/*",
            "\t\t\t * The guest indicated that the page content is of no",
            "\t\t\t * interest anymore. Simply discard the pte, vmscan",
            "\t\t\t * will take care of the rest.",
            "\t\t\t * A future reference will then fault in a new zero",
            "\t\t\t * page. When userfaultfd is active, we must not drop",
            "\t\t\t * this page though, as its main user (postcopy",
            "\t\t\t * migration) will not expect userfaults on already",
            "\t\t\t * copied pages.",
            "\t\t\t */",
            "\t\t\tdec_mm_counter(mm, mm_counter(folio));",
            "\t\t} else if (folio_test_anon(folio)) {",
            "\t\t\tswp_entry_t entry = page_swap_entry(subpage);",
            "\t\t\tpte_t swp_pte;",
            "\t\t\t/*",
            "\t\t\t * Store the swap location in the pte.",
            "\t\t\t * See handle_pte_fault() ...",
            "\t\t\t */",
            "\t\t\tif (unlikely(folio_test_swapbacked(folio) !=",
            "\t\t\t\t\tfolio_test_swapcache(folio))) {",
            "\t\t\t\tWARN_ON_ONCE(1);",
            "\t\t\t\tgoto walk_abort;",
            "\t\t\t}",
            "",
            "\t\t\t/* MADV_FREE page check */",
            "\t\t\tif (!folio_test_swapbacked(folio)) {",
            "\t\t\t\tint ref_count, map_count;",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * Synchronize with gup_pte_range():",
            "\t\t\t\t * - clear PTE; barrier; read refcount",
            "\t\t\t\t * - inc refcount; barrier; read PTE",
            "\t\t\t\t */",
            "\t\t\t\tsmp_mb();",
            "",
            "\t\t\t\tref_count = folio_ref_count(folio);",
            "\t\t\t\tmap_count = folio_mapcount(folio);",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * Order reads for page refcount and dirty flag",
            "\t\t\t\t * (see comments in __remove_mapping()).",
            "\t\t\t\t */",
            "\t\t\t\tsmp_rmb();",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * The only page refs must be one from isolation",
            "\t\t\t\t * plus the rmap(s) (dropped by discard:).",
            "\t\t\t\t */",
            "\t\t\t\tif (ref_count == 1 + map_count &&",
            "\t\t\t\t    !folio_test_dirty(folio)) {",
            "\t\t\t\t\tdec_mm_counter(mm, MM_ANONPAGES);",
            "\t\t\t\t\tgoto discard;",
            "\t\t\t\t}",
            "",
            "\t\t\t\t/*",
            "\t\t\t\t * If the folio was redirtied, it cannot be",
            "\t\t\t\t * discarded. Remap the page to page table.",
            "\t\t\t\t */",
            "\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);",
            "\t\t\t\tfolio_set_swapbacked(folio);",
            "\t\t\t\tgoto walk_abort;",
            "\t\t\t}",
            "",
            "\t\t\tif (swap_duplicate(entry) < 0) {",
            "\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);",
            "\t\t\t\tgoto walk_abort;",
            "\t\t\t}",
            "\t\t\tif (arch_unmap_one(mm, vma, address, pteval) < 0) {",
            "\t\t\t\tswap_free(entry);",
            "\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);",
            "\t\t\t\tgoto walk_abort;",
            "\t\t\t}",
            "",
            "\t\t\t/* See folio_try_share_anon_rmap(): clear PTE first. */",
            "\t\t\tif (anon_exclusive &&",
            "\t\t\t    folio_try_share_anon_rmap_pte(folio, subpage)) {",
            "\t\t\t\tswap_free(entry);",
            "\t\t\t\tset_pte_at(mm, address, pvmw.pte, pteval);",
            "\t\t\t\tgoto walk_abort;",
            "\t\t\t}",
            "\t\t\tif (list_empty(&mm->mmlist)) {",
            "\t\t\t\tspin_lock(&mmlist_lock);",
            "\t\t\t\tif (list_empty(&mm->mmlist))",
            "\t\t\t\t\tlist_add(&mm->mmlist, &init_mm.mmlist);",
            "\t\t\t\tspin_unlock(&mmlist_lock);",
            "\t\t\t}",
            "\t\t\tdec_mm_counter(mm, MM_ANONPAGES);",
            "\t\t\tinc_mm_counter(mm, MM_SWAPENTS);",
            "\t\t\tswp_pte = swp_entry_to_pte(entry);",
            "\t\t\tif (anon_exclusive)",
            "\t\t\t\tswp_pte = pte_swp_mkexclusive(swp_pte);",
            "\t\t\tif (pte_soft_dirty(pteval))",
            "\t\t\t\tswp_pte = pte_swp_mksoft_dirty(swp_pte);",
            "\t\t\tif (pte_uffd_wp(pteval))",
            "\t\t\t\tswp_pte = pte_swp_mkuffd_wp(swp_pte);",
            "\t\t\tset_pte_at(mm, address, pvmw.pte, swp_pte);",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * This is a locked file-backed folio,",
            "\t\t\t * so it cannot be removed from the page",
            "\t\t\t * cache and replaced by a new folio before",
            "\t\t\t * mmu_notifier_invalidate_range_end, so no",
            "\t\t\t * concurrent thread might update its page table",
            "\t\t\t * to point at a new folio while a device is",
            "\t\t\t * still using this folio.",
            "\t\t\t *",
            "\t\t\t * See Documentation/mm/mmu_notifier.rst",
            "\t\t\t */",
            "\t\t\tdec_mm_counter(mm, mm_counter_file(folio));",
            "\t\t}",
            "discard:",
            "\t\tif (unlikely(folio_test_hugetlb(folio)))",
            "\t\t\thugetlb_remove_rmap(folio);",
            "\t\telse",
            "\t\t\tfolio_remove_rmap_pte(folio, subpage, vma);",
            "\t\tif (vma->vm_flags & VM_LOCKED)",
            "\t\t\tmlock_drain_local();",
            "\t\tfolio_put(folio);",
            "\t\tcontinue;",
            "walk_abort:",
            "\t\tret = false;",
            "walk_done:",
            "\t\tpage_vma_mapped_walk_done(&pvmw);",
            "\t\tbreak;",
            "\t}",
            "",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "folio_remove_rmap_ptes, folio_remove_rmap_pmd, try_to_unmap_one",
          "description": "该代码段主要处理Linux内核中页面映射表项（PTE/PMD）的移除逻辑。  \n`folio_remove_rmap_ptes` 和 `folio_remove_rmap_pmd` 是辅助函数，分别用于移除普通页表项（PTE）和大页表项（PMD）的映射记录，均通过调用 `__folio_remove_rmap` 实现。  \n`try_to_unmap_one` 是核心函数，负责遍历指定VMA区域中的页表项，根据页面类型（普通/大页）、锁状态、脏页标记等条件，清除页表项、更新内存统计计数器，并处理异常情况（如HWPOISON、SWAP等），确保页面回收的安全性与一致性。",
          "similarity": 0.5478450059890747
        }
      ]
    },
    {
      "source_file": "kernel/iomem.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:45:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `iomem.c`\n\n---\n\n# iomem.c 技术文档\n\n## 1. 文件概述\n\n`iomem.c` 实现了通用的内存重映射（`memremap`）接口，用于将物理地址空间（特别是 I/O 内存资源）映射为可直接访问的内核虚拟地址。与传统的 `ioremap` 不同，`memremap` 专为**无 I/O 副作用**的内存区域设计（如持久内存 PMEM、设备内存等），并支持多种缓存策略（如写回 WB、写通 WT、写合并 WC）。该文件还提供了资源管理版本（`devm_memremap`），可自动在设备卸载时释放映射。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`memremap()`**  \n  核心映射函数，根据指定的缓存策略（`MEMREMAP_WB`/`WT`/`WC`）将物理地址映射为内核虚拟地址。若映射区域为系统 RAM 且请求 `MEMREMAP_WB`，则直接返回线性映射地址。\n\n- **`memunmap()`**  \n  释放由 `memremap()` 创建的映射。若地址来自 `ioremap` 系列函数，则调用 `iounmap()`；若为直接映射地址则无需操作。\n\n- **`devm_memremap()`**  \n  设备资源管理版本的 `memremap()`，将映射资源与设备生命周期绑定，设备卸载时自动释放。\n\n- **`devm_memunmap()`**  \n  显式释放由 `devm_memremap()` 分配的资源（通常无需手动调用）。\n\n### 辅助函数\n\n- **`try_ram_remap()`**  \n  尝试对系统 RAM 区域使用内核直接映射（`__va()`），避免创建新页表。\n\n- **`arch_memremap_wb()`**（弱符号）  \n  架构特定的写回（WB）映射实现，默认回退到 `ioremap_cache()` 或 `ioremap()`。\n\n- **`arch_memremap_can_ram_remap()`**（弱符号）  \n  架构特定的 RAM 重映射能力检查，默认返回 `true`。\n\n### 标志位（Flags）\n\n- `MEMREMAP_WB`：写回缓存（默认系统 RAM 策略）\n- `MEMREMAP_WT`：写通缓存（禁止用于系统 RAM）\n- `MEMREMAP_WC`：写合并（禁止用于系统 RAM）\n- `MEMREMAP_ENC`/`DEC`：加密/解密映射（代码中未直接处理，由底层 `ioremap` 实现）\n\n## 3. 关键实现\n\n### 内存区域类型检测\n- 使用 `region_intersects()` 检查物理地址范围是否与 `IORESOURCE_SYSTEM_RAM` 重叠，返回：\n  - `REGION_INTERSECTS`：完全或部分在系统 RAM 内\n  - `REGION_MIXED`：跨越 RAM 与非 RAM 区域（视为错误）\n  - `REGION_DISJOINT`：完全在非 RAM 区域\n\n### RAM 直接映射优化\n- 当请求 `MEMREMAP_WB` 且区域为系统 RAM 时：\n  1. 调用 `try_ram_remap()` 检查是否满足直接映射条件：\n     - 物理页帧有效（`pfn_valid()`）\n     - 非高端内存（`!PageHighMem()`）\n     - 架构允许 RAM 重映射（`arch_memremap_can_ram_remap()`）\n  2. 若满足，直接返回 `__va(offset)`（内核线性映射地址），避免页表开销。\n\n### 非 RAM 区域映射\n- 对于非 RAM 区域或非 WB 请求：\n  - `MEMREMAP_WT` → `ioremap_wt()`\n  - `MEMREMAP_WC` → `ioremap_wc()`\n  - `MEMREMAP_WB` → `arch_memremap_wb()`（最终调用 `ioremap_cache()` 或 `ioremap()`）\n\n### 安全限制\n- 禁止对系统 RAM 使用 `WT`/`WC` 映射（会触发 `WARN_ONCE` 并返回 `NULL`）\n- 禁止映射混合 RAM/非 RAM 区域（视为编程错误）\n\n### 资源管理\n- `devm_memremap()` 使用设备资源管理框架（`devres`）：\n  - 分配资源描述符（`devres_alloc_node`）\n  - 注册释放回调（`devm_memremap_release`）\n  - 设备卸载时自动调用 `memunmap()`\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/io.h>`：提供 `ioremap_*()` 系列函数\n  - `<linux/mm.h>`：提供 `pfn_valid()`、`PageHighMem()` 等内存管理接口\n  - `<linux/ioremap.h>`：定义 `ioremap` 相关类型和函数\n  - `<linux/device.h>`：提供设备资源管理（`devres`）接口\n\n- **架构依赖**：\n  - 依赖架构实现的 `ioremap_cache()`、`ioremap_wt()`、`ioremap_wc()`\n  - 可选覆盖 `arch_memremap_wb()` 和 `arch_memremap_can_ram_remap()`\n\n- **内核子系统**：\n  - 内存管理子系统（MM）：页表管理、直接映射\n  - 设备驱动模型：设备资源生命周期管理\n\n## 5. 使用场景\n\n- **持久内存（PMEM）驱动**：  \n  将持久内存设备的物理地址映射为可直接读写的内核虚拟地址（通常使用 `MEMREMAP_WB`）。\n\n- **设备内存（Device Memory）访问**：  \n  访问无 I/O 副作用的设备内存区域（如 GPU 显存、FPGA 内存），根据性能需求选择缓存策略。\n\n- **EFI 运行时服务内存**：  \n  映射 EFI 固件提供的内存区域（需确保无副作用）。\n\n- **设备驱动资源管理**：  \n  使用 `devm_memremap()` 简化驱动代码，避免手动释放映射（尤其适用于 probe/remove 场景）。\n\n- **内核子系统通用映射**：  \n  为需要高性能内存访问的子系统（如 DAX、HMM）提供统一映射接口。",
      "similarity": 0.6481207609176636,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/iomem.c",
          "start_line": 20,
          "end_line": 42,
          "content": [
            "static bool arch_memremap_can_ram_remap(resource_size_t offset, size_t size,",
            "\t\t\t\t\tunsigned long flags)",
            "{",
            "\treturn true;",
            "}",
            "void memunmap(void *addr)",
            "{",
            "\tif (is_ioremap_addr(addr))",
            "\t\tiounmap((void __iomem *) addr);",
            "}",
            "static void devm_memremap_release(struct device *dev, void *res)",
            "{",
            "\tmemunmap(*(void **)res);",
            "}",
            "static int devm_memremap_match(struct device *dev, void *res, void *match_data)",
            "{",
            "\treturn *(void **)res == match_data;",
            "}",
            "void devm_memunmap(struct device *dev, void *addr)",
            "{",
            "\tWARN_ON(devres_release(dev, devm_memremap_release,",
            "\t\t\t\tdevm_memremap_match, addr));",
            "}"
          ],
          "function_name": "arch_memremap_can_ram_remap, memunmap, devm_memremap_release, devm_memremap_match, devm_memunmap",
          "description": "实现内存映射释放相关函数，包含判断能否进行RAM重映射的钩子函数、解除ioremap地址映射的memunmap函数，以及设备资源管理中的动态内存映射释放匹配逻辑",
          "similarity": 0.6427969932556152
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/iomem.c",
          "start_line": 1,
          "end_line": 19,
          "content": [
            "/* SPDX-License-Identifier: GPL-2.0 */",
            "#include <linux/device.h>",
            "#include <linux/types.h>",
            "#include <linux/io.h>",
            "#include <linux/mm.h>",
            "#include <linux/ioremap.h>",
            "",
            "#ifndef arch_memremap_wb",
            "static void *arch_memremap_wb(resource_size_t offset, unsigned long size)",
            "{",
            "#ifdef ioremap_cache",
            "\treturn (__force void *)ioremap_cache(offset, size);",
            "#else",
            "\treturn (__force void *)ioremap(offset, size);",
            "#endif",
            "}",
            "#endif",
            "",
            "#ifndef arch_memremap_can_ram_remap"
          ],
          "function_name": null,
          "description": "定义arch_memremap_wb函数，根据ioremap_cache是否存在选择使用ioremap_cache或ioremap实现，用于创建带写回缓存策略的内存映射区域，上下文不完整",
          "similarity": 0.5800129175186157
        }
      ]
    },
    {
      "source_file": "mm/vmscan.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:33:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmscan.c`\n\n---\n\n# vmscan.c 技术文档\n\n## 1. 文件概述\n\n`vmscan.c` 是 Linux 内核内存管理子系统中的核心文件，主要负责**页面回收（page reclaim）**机制的实现。该文件实现了内核在内存压力下如何选择并释放不再活跃或可回收的物理页帧（pages），以维持系统可用内存水位。其核心功能包括：\n\n- 实现 `kswapd` 内核线程，用于后台异步回收内存\n- 提供直接回收（direct reclaim）路径，供分配器在内存不足时同步触发\n- 管理匿名页（anonymous pages）和文件缓存页（file-backed pages）的回收策略\n- 支持基于内存控制组（memcg）的层级化内存回收\n- 与交换（swap）、压缩（compaction）、OOM killer 等子系统协同工作\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct scan_control`**  \n  页面回收上下文控制结构，包含本次回收操作的所有参数和状态：\n  - `nr_to_reclaim`：目标回收页数\n  - `target_mem_cgroup`：目标内存 cgroup（用于 memcg 回收）\n  - `may_unmap` / `may_swap` / `may_writepage`：控制是否允许解除映射、交换、写回\n  - `priority`：扫描优先级（0~12，值越低压力越大）\n  - `order`：请求分配的阶数（影响回收激进程度）\n  - `nr_scanned` / `nr_reclaimed`：已扫描和已回收页数统计\n  - `anon_cost` / `file_cost`：用于平衡匿名页与文件页回收比例\n\n- **全局变量**\n  - `vm_swappiness`（默认 60）：控制系统倾向于回收匿名页（需 swap）还是文件页（可丢弃）\n\n### 主要函数（部分在代码片段中体现）\n\n- `cgroup_reclaim()` / `root_reclaim()`：判断当前回收是否针对特定 memcg 或全局\n- `writeback_throttling_sane()`：判断是否可使用标准脏页限流机制\n- `set_task_reclaim_state()` / `flush_reclaim_state()`：管理任务的 slab 回收状态\n- （注：核心回收函数如 `shrink_lruvec()`、`kswapd()` 等未在片段中展示）\n\n## 3. 关键实现\n\n### 内存回收控制逻辑\n\n- **回收目标决策**：通过 `scan_control` 结构传递回收上下文，区分直接回收（分配失败触发）与 kswapd 后台回收。\n- **LRU 链管理**：利用 `prefetchw_prev_lru_folio` 宏优化 LRU 链遍历时的 CPU 缓存预取性能。\n- **Memcg 集成**：\n  - 若 `target_mem_cgroup` 非空，则优先回收该 cgroup 的内存\n  - 支持 `memory.low` 保护机制：当常规回收无法满足需求且跳过受保护 cgroup 时，会触发二次强制回收（`memcg_low_reclaim`）\n- **脏页处理策略**：\n  - 在传统 memcg 模式下，禁用标准 `balance_dirty_pages()` 限流，改用直接阻塞回收（`writeback_throttling_sane()` 判断）\n  - 通过 `may_writepage` 控制是否在 laptop mode 下批量写回脏页\n\n### 回收统计与状态同步\n\n- **Slab 回收计数**：通过 `reclaim_state` 结构将非 LRU 回收（如 slab 释放）计入全局统计，但**仅在全局回收时计入**，避免 memcg 回收时高估实际效果导致欠回收。\n- **PSI/Trace 集成**：包含 `<trace/events/vmscan.h>` 用于性能分析，支持压力状态指示器（PSI）监控内存压力。\n\n## 4. 依赖关系\n\n### 头文件依赖\n\n- **核心内存管理**：`<linux/mm.h>`, `<linux/gfp.h>`, `<linux/swap.h>`, `<linux/vmstat.h>`\n- **LRU 与反向映射**：`<linux/rmap.h>`, `<linux/pagemap.h>`\n- **内存控制组**：`<linux/memcontrol.h>`\n- **IO 与写回**：`<linux/writeback.h>`, `<linux/backing-dev.h>`\n- **压缩与迁移**：`<linux/compaction.h>`, `<linux/migrate.h>`\n- **体系结构相关**：`<asm/tlbflush.h>`\n\n### 子系统交互\n\n- **Swap 子系统**：通过 `swapops.h` 和 `swap.h` 实现匿名页换出\n- **Slab 分配器**：通过 `reclaim_state` 接收 slab 回收通知\n- **OOM Killer**：当回收无法释放足够内存时触发\n- **Khugepaged**：大页合并/拆分与回收协同\n- **Memory Tiering**：支持分层内存架构中的页降级（demotion）控制\n\n## 5. 使用场景\n\n- **内存分配失败时的直接回收**：当 `alloc_pages()` 等分配函数无法满足请求时，同步调用回收路径。\n- **kswapd 后台回收**：当空闲内存低于 `watermark[low]` 时，唤醒 `kswapd` 线程异步回收至 `watermark[high]`。\n- **Memcg 内存超限时的层级回收**：当某个 cgroup 超过其内存限制时，仅回收该 cgroup 及其子树的页面。\n- **系统休眠（Hibernation）**：通过 `hibernation_mode` 标志优化休眠过程中的内存回收。\n- **主动内存回收（Proactive Reclaim）**：用户空间通过 `memory.reclaim` 接口触发预清回收。\n- **内存压缩准备**：当 `compaction_ready` 置位时，回收操作会为后续内存压缩腾出连续空间。",
      "similarity": 0.6205835938453674,
      "chunks": [
        {
          "chunk_id": 44,
          "file_path": "mm/vmscan.c",
          "start_line": 7321,
          "end_line": 7463,
          "content": [
            "static int __init kswapd_init(void)",
            "{",
            "\tint nid;",
            "",
            "\tswap_setup();",
            "\tfor_each_node_state(nid, N_MEMORY)",
            " \t\tkswapd_run(nid);",
            "\treturn 0;",
            "}",
            "static inline unsigned long node_unmapped_file_pages(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long file_mapped = node_page_state(pgdat, NR_FILE_MAPPED);",
            "\tunsigned long file_lru = node_page_state(pgdat, NR_INACTIVE_FILE) +",
            "\t\tnode_page_state(pgdat, NR_ACTIVE_FILE);",
            "",
            "\t/*",
            "\t * It's possible for there to be more file mapped pages than",
            "\t * accounted for by the pages on the file LRU lists because",
            "\t * tmpfs pages accounted for as ANON can also be FILE_MAPPED",
            "\t */",
            "\treturn (file_lru > file_mapped) ? (file_lru - file_mapped) : 0;",
            "}",
            "static unsigned long node_pagecache_reclaimable(struct pglist_data *pgdat)",
            "{",
            "\tunsigned long nr_pagecache_reclaimable;",
            "\tunsigned long delta = 0;",
            "",
            "\t/*",
            "\t * If RECLAIM_UNMAP is set, then all file pages are considered",
            "\t * potentially reclaimable. Otherwise, we have to worry about",
            "\t * pages like swapcache and node_unmapped_file_pages() provides",
            "\t * a better estimate",
            "\t */",
            "\tif (node_reclaim_mode & RECLAIM_UNMAP)",
            "\t\tnr_pagecache_reclaimable = node_page_state(pgdat, NR_FILE_PAGES);",
            "\telse",
            "\t\tnr_pagecache_reclaimable = node_unmapped_file_pages(pgdat);",
            "",
            "\t/* If we can't clean pages, remove dirty pages from consideration */",
            "\tif (!(node_reclaim_mode & RECLAIM_WRITE))",
            "\t\tdelta += node_page_state(pgdat, NR_FILE_DIRTY);",
            "",
            "\t/* Watch for any possible underflows due to delta */",
            "\tif (unlikely(delta > nr_pagecache_reclaimable))",
            "\t\tdelta = nr_pagecache_reclaimable;",
            "",
            "\treturn nr_pagecache_reclaimable - delta;",
            "}",
            "static int __node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)",
            "{",
            "\t/* Minimum pages needed in order to stay on node */",
            "\tconst unsigned long nr_pages = 1 << order;",
            "\tstruct task_struct *p = current;",
            "\tunsigned int noreclaim_flag;",
            "\tstruct scan_control sc = {",
            "\t\t.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),",
            "\t\t.gfp_mask = current_gfp_context(gfp_mask),",
            "\t\t.order = order,",
            "\t\t.priority = NODE_RECLAIM_PRIORITY,",
            "\t\t.may_writepage = !!(node_reclaim_mode & RECLAIM_WRITE),",
            "\t\t.may_unmap = !!(node_reclaim_mode & RECLAIM_UNMAP),",
            "\t\t.may_swap = 1,",
            "\t\t.reclaim_idx = gfp_zone(gfp_mask),",
            "\t};",
            "\tunsigned long pflags;",
            "",
            "\ttrace_mm_vmscan_node_reclaim_begin(pgdat->node_id, order,",
            "\t\t\t\t\t   sc.gfp_mask);",
            "",
            "\tcond_resched();",
            "\tpsi_memstall_enter(&pflags);",
            "\tfs_reclaim_acquire(sc.gfp_mask);",
            "\t/*",
            "\t * We need to be able to allocate from the reserves for RECLAIM_UNMAP",
            "\t */",
            "\tnoreclaim_flag = memalloc_noreclaim_save();",
            "\tset_task_reclaim_state(p, &sc.reclaim_state);",
            "",
            "\tif (node_pagecache_reclaimable(pgdat) > pgdat->min_unmapped_pages ||",
            "\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) > pgdat->min_slab_pages) {",
            "\t\t/*",
            "\t\t * Free memory by calling shrink node with increasing",
            "\t\t * priorities until we have enough memory freed.",
            "\t\t */",
            "\t\tdo {",
            "\t\t\tshrink_node(pgdat, &sc);",
            "\t\t} while (sc.nr_reclaimed < nr_pages && --sc.priority >= 0);",
            "\t}",
            "",
            "\tset_task_reclaim_state(p, NULL);",
            "\tmemalloc_noreclaim_restore(noreclaim_flag);",
            "\tfs_reclaim_release(sc.gfp_mask);",
            "\tpsi_memstall_leave(&pflags);",
            "",
            "\ttrace_mm_vmscan_node_reclaim_end(sc.nr_reclaimed);",
            "",
            "\treturn sc.nr_reclaimed >= nr_pages;",
            "}",
            "int node_reclaim(struct pglist_data *pgdat, gfp_t gfp_mask, unsigned int order)",
            "{",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Node reclaim reclaims unmapped file backed pages and",
            "\t * slab pages if we are over the defined limits.",
            "\t *",
            "\t * A small portion of unmapped file backed pages is needed for",
            "\t * file I/O otherwise pages read by file I/O will be immediately",
            "\t * thrown out if the node is overallocated. So we do not reclaim",
            "\t * if less than a specified percentage of the node is used by",
            "\t * unmapped file backed pages.",
            "\t */",
            "\tif (node_pagecache_reclaimable(pgdat) <= pgdat->min_unmapped_pages &&",
            "\t    node_page_state_pages(pgdat, NR_SLAB_RECLAIMABLE_B) <=",
            "\t    pgdat->min_slab_pages)",
            "\t\treturn NODE_RECLAIM_FULL;",
            "",
            "\t/*",
            "\t * Do not scan if the allocation should not be delayed.",
            "\t */",
            "\tif (!gfpflags_allow_blocking(gfp_mask) || (current->flags & PF_MEMALLOC))",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\t/*",
            "\t * Only run node reclaim on the local node or on nodes that do not",
            "\t * have associated processors. This will favor the local processor",
            "\t * over remote processors and spread off node memory allocations",
            "\t * as wide as possible.",
            "\t */",
            "\tif (node_state(pgdat->node_id, N_CPU) && pgdat->node_id != numa_node_id())",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\tif (test_and_set_bit(PGDAT_RECLAIM_LOCKED, &pgdat->flags))",
            "\t\treturn NODE_RECLAIM_NOSCAN;",
            "",
            "\tret = __node_reclaim(pgdat, gfp_mask, order);",
            "\tclear_bit_unlock(PGDAT_RECLAIM_LOCKED, &pgdat->flags);",
            "",
            "\tif (!ret)",
            "\t\tcount_vm_event(PGSCAN_ZONE_RECLAIM_FAILED);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "kswapd_init, node_unmapped_file_pages, node_pagecache_reclaimable, __node_reclaim, node_reclaim",
          "description": "实现节点级别内存回收策略，kswapd_init初始化kswapd线程，node_unmapped_file_pages计算未映射文件页，node_pagecache_reclaimable评估可回收页面数量，__node_reclaim执行节点级回收，node_reclaim决定是否触发回收操作。",
          "similarity": 0.6883939504623413
        },
        {
          "chunk_id": 1,
          "file_path": "mm/vmscan.c",
          "start_line": 195,
          "end_line": 305,
          "content": [
            "static bool cgroup_reclaim(struct scan_control *sc)",
            "{",
            "\treturn sc->target_mem_cgroup;",
            "}",
            "static bool root_reclaim(struct scan_control *sc)",
            "{",
            "\treturn !sc->target_mem_cgroup || mem_cgroup_is_root(sc->target_mem_cgroup);",
            "}",
            "static bool writeback_throttling_sane(struct scan_control *sc)",
            "{",
            "\tif (!cgroup_reclaim(sc))",
            "\t\treturn true;",
            "#ifdef CONFIG_CGROUP_WRITEBACK",
            "\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys))",
            "\t\treturn true;",
            "#endif",
            "\treturn false;",
            "}",
            "static bool cgroup_reclaim(struct scan_control *sc)",
            "{",
            "\treturn false;",
            "}",
            "static bool root_reclaim(struct scan_control *sc)",
            "{",
            "\treturn true;",
            "}",
            "static bool writeback_throttling_sane(struct scan_control *sc)",
            "{",
            "\treturn true;",
            "}",
            "static void set_task_reclaim_state(struct task_struct *task,",
            "\t\t\t\t   struct reclaim_state *rs)",
            "{",
            "\t/* Check for an overwrite */",
            "\tWARN_ON_ONCE(rs && task->reclaim_state);",
            "",
            "\t/* Check for the nulling of an already-nulled member */",
            "\tWARN_ON_ONCE(!rs && !task->reclaim_state);",
            "",
            "\ttask->reclaim_state = rs;",
            "}",
            "static void flush_reclaim_state(struct scan_control *sc)",
            "{",
            "\t/*",
            "\t * Currently, reclaim_state->reclaimed includes three types of pages",
            "\t * freed outside of vmscan:",
            "\t * (1) Slab pages.",
            "\t * (2) Clean file pages from pruned inodes (on highmem systems).",
            "\t * (3) XFS freed buffer pages.",
            "\t *",
            "\t * For all of these cases, we cannot universally link the pages to a",
            "\t * single memcg. For example, a memcg-aware shrinker can free one object",
            "\t * charged to the target memcg, causing an entire page to be freed.",
            "\t * If we count the entire page as reclaimed from the memcg, we end up",
            "\t * overestimating the reclaimed amount (potentially under-reclaiming).",
            "\t *",
            "\t * Only count such pages for global reclaim to prevent under-reclaiming",
            "\t * from the target memcg; preventing unnecessary retries during memcg",
            "\t * charging and false positives from proactive reclaim.",
            "\t *",
            "\t * For uncommon cases where the freed pages were actually mostly",
            "\t * charged to the target memcg, we end up underestimating the reclaimed",
            "\t * amount. This should be fine. The freed pages will be uncharged",
            "\t * anyway, even if they are not counted here properly, and we will be",
            "\t * able to make forward progress in charging (which is usually in a",
            "\t * retry loop).",
            "\t *",
            "\t * We can go one step further, and report the uncharged objcg pages in",
            "\t * memcg reclaim, to make reporting more accurate and reduce",
            "\t * underestimation, but it's probably not worth the complexity for now.",
            "\t */",
            "\tif (current->reclaim_state && root_reclaim(sc)) {",
            "\t\tsc->nr_reclaimed += current->reclaim_state->reclaimed;",
            "\t\tcurrent->reclaim_state->reclaimed = 0;",
            "\t}",
            "}",
            "static bool can_demote(int nid, struct scan_control *sc)",
            "{",
            "\tif (!numa_demotion_enabled)",
            "\t\treturn false;",
            "\tif (sc && sc->no_demotion)",
            "\t\treturn false;",
            "\tif (next_demotion_node(nid) == NUMA_NO_NODE)",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static inline bool can_reclaim_anon_pages(struct mem_cgroup *memcg,",
            "\t\t\t\t\t  int nid,",
            "\t\t\t\t\t  struct scan_control *sc)",
            "{",
            "\tif (memcg == NULL) {",
            "\t\t/*",
            "\t\t * For non-memcg reclaim, is there",
            "\t\t * space in any swap device?",
            "\t\t */",
            "\t\tif (get_nr_swap_pages() > 0)",
            "\t\t\treturn true;",
            "\t} else {",
            "\t\t/* Is the memcg below its swap limit? */",
            "\t\tif (mem_cgroup_get_nr_swap_pages(memcg) > 0)",
            "\t\t\treturn true;",
            "\t}",
            "",
            "\t/*",
            "\t * The page can not be swapped.",
            "\t *",
            "\t * Can it be reclaimed from this node via demotion?",
            "\t */",
            "\treturn can_demote(nid, sc);",
            "}"
          ],
          "function_name": "cgroup_reclaim, root_reclaim, writeback_throttling_sane, cgroup_reclaim, root_reclaim, writeback_throttling_sane, set_task_reclaim_state, flush_reclaim_state, can_demote, can_reclaim_anon_pages",
          "description": "提供内存组回收判定逻辑与回收状态管理接口，包含判断是否针对特定内存组回收、是否允许写回操作、设置/刷新任务回收状态等功能，支持多层级回收场景决策。",
          "similarity": 0.6810420751571655
        },
        {
          "chunk_id": 2,
          "file_path": "mm/vmscan.c",
          "start_line": 343,
          "end_line": 443,
          "content": [
            "unsigned long zone_reclaimable_pages(struct zone *zone)",
            "{",
            "\tunsigned long nr;",
            "",
            "\tnr = zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_FILE) +",
            "\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_FILE);",
            "\tif (can_reclaim_anon_pages(NULL, zone_to_nid(zone), NULL))",
            "\t\tnr += zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_ANON) +",
            "\t\t\tzone_page_state_snapshot(zone, NR_ZONE_ACTIVE_ANON);",
            "\t/*",
            "\t * If there are no reclaimable file-backed or anonymous pages,",
            "\t * ensure zones with sufficient free pages are not skipped.",
            "\t * This prevents zones like DMA32 from being ignored in reclaim",
            "\t * scenarios where they can still help alleviate memory pressure.",
            "\t */",
            "\tif (nr == 0)",
            "\t\tnr = zone_page_state_snapshot(zone, NR_FREE_PAGES);",
            "\treturn nr;",
            "}",
            "static unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru,",
            "\t\t\t\t     int zone_idx)",
            "{",
            "\tunsigned long size = 0;",
            "\tint zid;",
            "",
            "\tfor (zid = 0; zid <= zone_idx; zid++) {",
            "\t\tstruct zone *zone = &lruvec_pgdat(lruvec)->node_zones[zid];",
            "",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!mem_cgroup_disabled())",
            "\t\t\tsize += mem_cgroup_get_zone_lru_size(lruvec, lru, zid);",
            "\t\telse",
            "\t\t\tsize += zone_page_state(zone, NR_ZONE_LRU_BASE + lru);",
            "\t}",
            "\treturn size;",
            "}",
            "static unsigned long drop_slab_node(int nid)",
            "{",
            "\tunsigned long freed = 0;",
            "\tstruct mem_cgroup *memcg = NULL;",
            "",
            "\tmemcg = mem_cgroup_iter(NULL, NULL, NULL);",
            "\tdo {",
            "\t\tfreed += shrink_slab(GFP_KERNEL, nid, memcg, 0);",
            "\t} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);",
            "",
            "\treturn freed;",
            "}",
            "void drop_slab(void)",
            "{",
            "\tint nid;",
            "\tint shift = 0;",
            "\tunsigned long freed;",
            "",
            "\tdo {",
            "\t\tfreed = 0;",
            "\t\tfor_each_online_node(nid) {",
            "\t\t\tif (fatal_signal_pending(current))",
            "\t\t\t\treturn;",
            "",
            "\t\t\tfreed += drop_slab_node(nid);",
            "\t\t}",
            "\t} while ((freed >> shift++) > 1);",
            "}",
            "static int reclaimer_offset(void)",
            "{",
            "\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=",
            "\t\t\tPGDEMOTE_DIRECT - PGDEMOTE_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=",
            "\t\t\tPGDEMOTE_KHUGEPAGED - PGDEMOTE_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_DIRECT - PGSTEAL_KSWAPD !=",
            "\t\t\tPGSCAN_DIRECT - PGSCAN_KSWAPD);",
            "\tBUILD_BUG_ON(PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD !=",
            "\t\t\tPGSCAN_KHUGEPAGED - PGSCAN_KSWAPD);",
            "",
            "\tif (current_is_kswapd())",
            "\t\treturn 0;",
            "\tif (current_is_khugepaged())",
            "\t\treturn PGSTEAL_KHUGEPAGED - PGSTEAL_KSWAPD;",
            "\treturn PGSTEAL_DIRECT - PGSTEAL_KSWAPD;",
            "}",
            "static inline int is_page_cache_freeable(struct folio *folio)",
            "{",
            "\t/*",
            "\t * A freeable page cache folio is referenced only by the caller",
            "\t * that isolated the folio, the page cache and optional filesystem",
            "\t * private data at folio->private.",
            "\t */",
            "\treturn folio_ref_count(folio) - folio_test_private(folio) ==",
            "\t\t1 + folio_nr_pages(folio);",
            "}",
            "static void handle_write_error(struct address_space *mapping,",
            "\t\t\t\tstruct folio *folio, int error)",
            "{",
            "\tfolio_lock(folio);",
            "\tif (folio_mapping(folio) == mapping)",
            "\t\tmapping_set_error(mapping, error);",
            "\tfolio_unlock(folio);",
            "}"
          ],
          "function_name": "zone_reclaimable_pages, lruvec_lru_size, drop_slab_node, drop_slab, reclaimer_offset, is_page_cache_freeable, handle_write_error",
          "description": "实现区域可回收页数计算、Slab对象释放及回收偏移量调整逻辑，通过遍历各内存区域统计潜在可回收页数，提供Slab内存碎片回收机制并维护回收进程优先级偏移量。",
          "similarity": 0.6505593061447144
        },
        {
          "chunk_id": 35,
          "file_path": "mm/vmscan.c",
          "start_line": 5914,
          "end_line": 6023,
          "content": [
            "static void shrink_node(pg_data_t *pgdat, struct scan_control *sc)",
            "{",
            "\tunsigned long nr_reclaimed, nr_scanned, nr_node_reclaimed;",
            "\tstruct lruvec *target_lruvec;",
            "\tbool reclaimable = false;",
            "",
            "\tif (lru_gen_enabled() && root_reclaim(sc)) {",
            "\t\tlru_gen_shrink_node(pgdat, sc);",
            "\t\treturn;",
            "\t}",
            "",
            "\ttarget_lruvec = mem_cgroup_lruvec(sc->target_mem_cgroup, pgdat);",
            "",
            "again:",
            "\tmemset(&sc->nr, 0, sizeof(sc->nr));",
            "",
            "\tnr_reclaimed = sc->nr_reclaimed;",
            "\tnr_scanned = sc->nr_scanned;",
            "",
            "\tprepare_scan_control(pgdat, sc);",
            "",
            "\tshrink_node_memcgs(pgdat, sc);",
            "",
            "\tflush_reclaim_state(sc);",
            "",
            "\tnr_node_reclaimed = sc->nr_reclaimed - nr_reclaimed;",
            "",
            "\t/* Record the subtree's reclaim efficiency */",
            "\tif (!sc->proactive)",
            "\t\tvmpressure(sc->gfp_mask, sc->target_mem_cgroup, true,",
            "\t\t\t   sc->nr_scanned - nr_scanned, nr_node_reclaimed);",
            "",
            "\tif (nr_node_reclaimed)",
            "\t\treclaimable = true;",
            "",
            "\tif (current_is_kswapd()) {",
            "\t\t/*",
            "\t\t * If reclaim is isolating dirty pages under writeback,",
            "\t\t * it implies that the long-lived page allocation rate",
            "\t\t * is exceeding the page laundering rate. Either the",
            "\t\t * global limits are not being effective at throttling",
            "\t\t * processes due to the page distribution throughout",
            "\t\t * zones or there is heavy usage of a slow backing",
            "\t\t * device. The only option is to throttle from reclaim",
            "\t\t * context which is not ideal as there is no guarantee",
            "\t\t * the dirtying process is throttled in the same way",
            "\t\t * balance_dirty_pages() manages.",
            "\t\t *",
            "\t\t * Once a node is flagged PGDAT_WRITEBACK, kswapd will",
            "\t\t * count the number of pages under pages flagged for",
            "\t\t * immediate reclaim and stall if any are encountered",
            "\t\t * in the nr_immediate check below.",
            "\t\t */",
            "\t\tif (sc->nr.writeback && sc->nr.writeback == sc->nr.taken)",
            "\t\t\tset_bit(PGDAT_WRITEBACK, &pgdat->flags);",
            "",
            "\t\t/* Allow kswapd to start writing pages during reclaim.*/",
            "\t\tif (sc->nr.unqueued_dirty == sc->nr.file_taken)",
            "\t\t\tset_bit(PGDAT_DIRTY, &pgdat->flags);",
            "",
            "\t\t/*",
            "\t\t * If kswapd scans pages marked for immediate",
            "\t\t * reclaim and under writeback (nr_immediate), it",
            "\t\t * implies that pages are cycling through the LRU",
            "\t\t * faster than they are written so forcibly stall",
            "\t\t * until some pages complete writeback.",
            "\t\t */",
            "\t\tif (sc->nr.immediate)",
            "\t\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_WRITEBACK);",
            "\t}",
            "",
            "\t/*",
            "\t * Tag a node/memcg as congested if all the dirty pages were marked",
            "\t * for writeback and immediate reclaim (counted in nr.congested).",
            "\t *",
            "\t * Legacy memcg will stall in page writeback so avoid forcibly",
            "\t * stalling in reclaim_throttle().",
            "\t */",
            "\tif (sc->nr.dirty && sc->nr.dirty == sc->nr.congested) {",
            "\t\tif (cgroup_reclaim(sc) && writeback_throttling_sane(sc))",
            "\t\t\tset_bit(LRUVEC_CGROUP_CONGESTED, &target_lruvec->flags);",
            "",
            "\t\tif (current_is_kswapd())",
            "\t\t\tset_bit(LRUVEC_NODE_CONGESTED, &target_lruvec->flags);",
            "\t}",
            "",
            "\t/*",
            "\t * Stall direct reclaim for IO completions if the lruvec is",
            "\t * node is congested. Allow kswapd to continue until it",
            "\t * starts encountering unqueued dirty pages or cycling through",
            "\t * the LRU too quickly.",
            "\t */",
            "\tif (!current_is_kswapd() && current_may_throttle() &&",
            "\t    !sc->hibernation_mode &&",
            "\t    (test_bit(LRUVEC_CGROUP_CONGESTED, &target_lruvec->flags) ||",
            "\t     test_bit(LRUVEC_NODE_CONGESTED, &target_lruvec->flags)))",
            "\t\treclaim_throttle(pgdat, VMSCAN_THROTTLE_CONGESTED);",
            "",
            "\tif (should_continue_reclaim(pgdat, nr_node_reclaimed, sc))",
            "\t\tgoto again;",
            "",
            "\t/*",
            "\t * Kswapd gives up on balancing particular nodes after too",
            "\t * many failures to reclaim anything from them and goes to",
            "\t * sleep. On reclaim progress, reset the failure counter. A",
            "\t * successful direct reclaim run will revive a dormant kswapd.",
            "\t */",
            "\tif (reclaimable)",
            "\t\tpgdat->kswapd_failures = 0;",
            "}"
          ],
          "function_name": "shrink_node",
          "description": "该函数处理节点级别的内存回收，通过遍历各zone进行页面回收，并根据回收效率设置节点状态标志位，如PGDAT_WRITEBACK和CONGESTED。若当前为kswapd线程，则根据writeback和dirty页面状态触发节流机制。",
          "similarity": 0.6257942914962769
        },
        {
          "chunk_id": 3,
          "file_path": "mm/vmscan.c",
          "start_line": 469,
          "end_line": 588,
          "content": [
            "static bool skip_throttle_noprogress(pg_data_t *pgdat)",
            "{",
            "\tint reclaimable = 0, write_pending = 0;",
            "\tint i;",
            "",
            "\t/*",
            "\t * If kswapd is disabled, reschedule if necessary but do not",
            "\t * throttle as the system is likely near OOM.",
            "\t */",
            "\tif (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * If there are a lot of dirty/writeback folios then do not",
            "\t * throttle as throttling will occur when the folios cycle",
            "\t * towards the end of the LRU if still under writeback.",
            "\t */",
            "\tfor (i = 0; i < MAX_NR_ZONES; i++) {",
            "\t\tstruct zone *zone = pgdat->node_zones + i;",
            "",
            "\t\tif (!managed_zone(zone))",
            "\t\t\tcontinue;",
            "",
            "\t\treclaimable += zone_reclaimable_pages(zone);",
            "\t\twrite_pending += zone_page_state_snapshot(zone,",
            "\t\t\t\t\t\t  NR_ZONE_WRITE_PENDING);",
            "\t}",
            "\tif (2 * write_pending <= reclaimable)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "void reclaim_throttle(pg_data_t *pgdat, enum vmscan_throttle_state reason)",
            "{",
            "\twait_queue_head_t *wqh = &pgdat->reclaim_wait[reason];",
            "\tlong timeout, ret;",
            "\tDEFINE_WAIT(wait);",
            "",
            "\t/*",
            "\t * Do not throttle user workers, kthreads other than kswapd or",
            "\t * workqueues. They may be required for reclaim to make",
            "\t * forward progress (e.g. journalling workqueues or kthreads).",
            "\t */",
            "\tif (!current_is_kswapd() &&",
            "\t    current->flags & (PF_USER_WORKER|PF_KTHREAD)) {",
            "\t\tcond_resched();",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * These figures are pulled out of thin air.",
            "\t * VMSCAN_THROTTLE_ISOLATED is a transient condition based on too many",
            "\t * parallel reclaimers which is a short-lived event so the timeout is",
            "\t * short. Failing to make progress or waiting on writeback are",
            "\t * potentially long-lived events so use a longer timeout. This is shaky",
            "\t * logic as a failure to make progress could be due to anything from",
            "\t * writeback to a slow device to excessive referenced folios at the tail",
            "\t * of the inactive LRU.",
            "\t */",
            "\tswitch(reason) {",
            "\tcase VMSCAN_THROTTLE_WRITEBACK:",
            "\t\ttimeout = HZ/10;",
            "",
            "\t\tif (atomic_inc_return(&pgdat->nr_writeback_throttled) == 1) {",
            "\t\t\tWRITE_ONCE(pgdat->nr_reclaim_start,",
            "\t\t\t\tnode_page_state(pgdat, NR_THROTTLED_WRITTEN));",
            "\t\t}",
            "",
            "\t\tbreak;",
            "\tcase VMSCAN_THROTTLE_CONGESTED:",
            "\t\tfallthrough;",
            "\tcase VMSCAN_THROTTLE_NOPROGRESS:",
            "\t\tif (skip_throttle_noprogress(pgdat)) {",
            "\t\t\tcond_resched();",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\ttimeout = 1;",
            "",
            "\t\tbreak;",
            "\tcase VMSCAN_THROTTLE_ISOLATED:",
            "\t\ttimeout = HZ/50;",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(1);",
            "\t\ttimeout = HZ;",
            "\t\tbreak;",
            "\t}",
            "",
            "\tprepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);",
            "\tret = schedule_timeout(timeout);",
            "\tfinish_wait(wqh, &wait);",
            "",
            "\tif (reason == VMSCAN_THROTTLE_WRITEBACK)",
            "\t\tatomic_dec(&pgdat->nr_writeback_throttled);",
            "",
            "\ttrace_mm_vmscan_throttled(pgdat->node_id, jiffies_to_usecs(timeout),",
            "\t\t\t\tjiffies_to_usecs(timeout - ret),",
            "\t\t\t\treason);",
            "}",
            "void __acct_reclaim_writeback(pg_data_t *pgdat, struct folio *folio,",
            "\t\t\t\t\t\t\tint nr_throttled)",
            "{",
            "\tunsigned long nr_written;",
            "",
            "\tnode_stat_add_folio(folio, NR_THROTTLED_WRITTEN);",
            "",
            "\t/*",
            "\t * This is an inaccurate read as the per-cpu deltas may not",
            "\t * be synchronised. However, given that the system is",
            "\t * writeback throttled, it is not worth taking the penalty",
            "\t * of getting an accurate count. At worst, the throttle",
            "\t * timeout guarantees forward progress.",
            "\t */",
            "\tnr_written = node_page_state(pgdat, NR_THROTTLED_WRITTEN) -",
            "\t\tREAD_ONCE(pgdat->nr_reclaim_start);",
            "",
            "\tif (nr_written > SWAP_CLUSTER_MAX * nr_throttled)",
            "\t\twake_up(&pgdat->reclaim_wait[VMSCAN_THROTTLE_WRITEBACK]);",
            "}"
          ],
          "function_name": "skip_throttle_noprogress, reclaim_throttle, __acct_reclaim_writeback",
          "description": "包含回收节流控制逻辑，通过检测写回Pending页数与可回收页数比例决定是否触发节流，管理回收进程休眠唤醒机制，并统计写回数据量以优化回收策略执行时机。",
          "similarity": 0.6231153011322021
        }
      ]
    }
  ]
}