{
  "query": "RPC implementation",
  "timestamp": "2025-12-26 01:21:29",
  "retrieved_files": [
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.535504162311554,
      "chunks": [
        {
          "chunk_id": 22,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4072,
          "end_line": 4226,
          "content": [
            "static void rcu_barrier_trace(const char *s, int cpu, unsigned long done)",
            "{",
            "\ttrace_rcu_barrier(rcu_state.name, s, cpu,",
            "\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);",
            "}",
            "static void rcu_barrier_callback(struct rcu_head *rhp)",
            "{",
            "\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;",
            "",
            "\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {",
            "\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "\t} else {",
            "\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);",
            "\t}",
            "}",
            "static void rcu_barrier_entrain(struct rcu_data *rdp)",
            "{",
            "\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);",
            "\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);",
            "\tbool wake_nocb = false;",
            "\tbool was_alldone = false;",
            "",
            "\tlockdep_assert_held(&rcu_state.barrier_lock);",
            "\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))",
            "\t\treturn;",
            "\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);",
            "\trdp->barrier_head.func = rcu_barrier_callback;",
            "\tdebug_rcu_head_queue(&rdp->barrier_head);",
            "\trcu_nocb_lock(rdp);",
            "\t/*",
            "\t * Flush bypass and wakeup rcuog if we add callbacks to an empty regular",
            "\t * queue. This way we don't wait for bypass timer that can reach seconds",
            "\t * if it's fully lazy.",
            "\t */",
            "\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));",
            "\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {",
            "\t\tatomic_inc(&rcu_state.barrier_cpu_count);",
            "\t} else {",
            "\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);",
            "\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);",
            "\t}",
            "\trcu_nocb_unlock(rdp);",
            "\tif (wake_nocb)",
            "\t\twake_nocb_gp(rdp, false);",
            "\tsmp_store_release(&rdp->barrier_seq_snap, gseq);",
            "}",
            "static void rcu_barrier_handler(void *cpu_in)",
            "{",
            "\tuintptr_t cpu = (uintptr_t)cpu_in;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\tWARN_ON_ONCE(cpu != rdp->cpu);",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\trcu_barrier_entrain(rdp);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "}",
            "void rcu_barrier(void)",
            "{",
            "\tuintptr_t cpu;",
            "\tunsigned long flags;",
            "\tunsigned long gseq;",
            "\tstruct rcu_data *rdp;",
            "\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);",
            "",
            "\trcu_barrier_trace(TPS(\"Begin\"), -1, s);",
            "",
            "\t/* Take mutex to serialize concurrent rcu_barrier() requests. */",
            "\tmutex_lock(&rcu_state.barrier_mutex);",
            "",
            "\t/* Did someone else do our work for us? */",
            "\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {",
            "\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);",
            "\t\tsmp_mb(); /* caller's subsequent code after above check. */",
            "\t\tmutex_unlock(&rcu_state.barrier_mutex);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Mark the start of the barrier operation. */",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\trcu_seq_start(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);",
            "",
            "\t/*",
            "\t * Initialize the count to two rather than to zero in order",
            "\t * to avoid a too-soon return to zero in case of an immediate",
            "\t * invocation of the just-enqueued callback (or preemption of",
            "\t * this task).  Exclude CPU-hotplug operations to ensure that no",
            "\t * offline non-offloaded CPU has callbacks queued.",
            "\t */",
            "\tinit_completion(&rcu_state.barrier_completion);",
            "\tatomic_set(&rcu_state.barrier_cpu_count, 2);",
            "\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "",
            "\t/*",
            "\t * Force each CPU with callbacks to register a new callback.",
            "\t * When that callback is invoked, we will know that all of the",
            "\t * corresponding CPU's preceding callbacks have been invoked.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "retry:",
            "\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)",
            "\t\t\tcontinue;",
            "\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {",
            "\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (!rcu_rdp_cpu_online(rdp)) {",
            "\t\t\trcu_barrier_entrain(rdp);",
            "\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {",
            "\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);",
            "\t}",
            "",
            "\t/*",
            "\t * Now that we have an rcu_barrier_callback() callback on each",
            "\t * CPU, and thus each counted, remove the initial count.",
            "\t */",
            "\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "",
            "\t/* Wait for all rcu_barrier_callback() callbacks to be invoked. */",
            "\twait_for_completion(&rcu_state.barrier_completion);",
            "",
            "\t/* Mark the end of the barrier operation. */",
            "\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);",
            "\trcu_seq_end(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t}",
            "",
            "\t/* Other rcu_barrier() invocations can now safely proceed. */",
            "\tmutex_unlock(&rcu_state.barrier_mutex);",
            "}"
          ],
          "function_name": "rcu_barrier_trace, rcu_barrier_callback, rcu_barrier_entrain, rcu_barrier_handler, rcu_barrier",
          "description": "实现RCU屏障功能，通过分发回调函数强制所有CPU完成当前RCU操作，使用原子计数器跟踪完成状态，通过completion等待所有回调完成",
          "similarity": 0.5270720720291138
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 908,
          "end_line": 1026,
          "content": [
            "static void trace_rcu_this_gp(struct rcu_node *rnp, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req, const char *s)",
            "{",
            "\ttrace_rcu_future_grace_period(rcu_state.name, READ_ONCE(rnp->gp_seq),",
            "\t\t\t\t      gp_seq_req, rnp->level,",
            "\t\t\t\t      rnp->grplo, rnp->grphi, s);",
            "}",
            "static bool rcu_start_this_gp(struct rcu_node *rnp_start, struct rcu_data *rdp,",
            "\t\t\t      unsigned long gp_seq_req)",
            "{",
            "\tbool ret = false;",
            "\tstruct rcu_node *rnp;",
            "",
            "\t/*",
            "\t * Use funnel locking to either acquire the root rcu_node",
            "\t * structure's lock or bail out if the need for this grace period",
            "\t * has already been recorded -- or if that grace period has in",
            "\t * fact already started.  If there is already a grace period in",
            "\t * progress in a non-leaf node, no recording is needed because the",
            "\t * end of the grace period will scan the leaf rcu_node structures.",
            "\t * Note that rnp_start->lock must not be released.",
            "\t */",
            "\traw_lockdep_assert_held_rcu_node(rnp_start);",
            "\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req, TPS(\"Startleaf\"));",
            "\tfor (rnp = rnp_start; 1; rnp = rnp->parent) {",
            "\t\tif (rnp != rnp_start)",
            "\t\t\traw_spin_lock_rcu_node(rnp);",
            "\t\tif (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||",
            "\t\t    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||",
            "\t\t    (rnp != rnp_start &&",
            "\t\t     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {",
            "\t\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Prestarted\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tWRITE_ONCE(rnp->gp_seq_needed, gp_seq_req);",
            "\t\tif (rcu_seq_state(rcu_seq_current(&rnp->gp_seq))) {",
            "\t\t\t/*",
            "\t\t\t * We just marked the leaf or internal node, and a",
            "\t\t\t * grace period is in progress, which means that",
            "\t\t\t * rcu_gp_cleanup() will see the marking.  Bail to",
            "\t\t\t * reduce contention.",
            "\t\t\t */",
            "\t\t\ttrace_rcu_this_gp(rnp_start, rdp, gp_seq_req,",
            "\t\t\t\t\t  TPS(\"Startedleaf\"));",
            "\t\t\tgoto unlock_out;",
            "\t\t}",
            "\t\tif (rnp != rnp_start && rnp->parent != NULL)",
            "\t\t\traw_spin_unlock_rcu_node(rnp);",
            "\t\tif (!rnp->parent)",
            "\t\t\tbreak;  /* At root, and perhaps also leaf. */",
            "\t}",
            "",
            "\t/* If GP already in progress, just leave, otherwise start one. */",
            "\tif (rcu_gp_in_progress()) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedleafroot\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"Startedroot\"));",
            "\tWRITE_ONCE(rcu_state.gp_flags, rcu_state.gp_flags | RCU_GP_FLAG_INIT);",
            "\tWRITE_ONCE(rcu_state.gp_req_activity, jiffies);",
            "\tif (!READ_ONCE(rcu_state.gp_kthread)) {",
            "\t\ttrace_rcu_this_gp(rnp, rdp, gp_seq_req, TPS(\"NoGPkthread\"));",
            "\t\tgoto unlock_out;",
            "\t}",
            "\ttrace_rcu_grace_period(rcu_state.name, data_race(rcu_state.gp_seq), TPS(\"newreq\"));",
            "\tret = true;  /* Caller must wake GP kthread. */",
            "unlock_out:",
            "\t/* Push furthest requested GP to leaf node and rcu_data structure. */",
            "\tif (ULONG_CMP_LT(gp_seq_req, rnp->gp_seq_needed)) {",
            "\t\tWRITE_ONCE(rnp_start->gp_seq_needed, rnp->gp_seq_needed);",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\t}",
            "\tif (rnp != rnp_start)",
            "\t\traw_spin_unlock_rcu_node(rnp);",
            "\treturn ret;",
            "}",
            "static bool rcu_future_gp_cleanup(struct rcu_node *rnp)",
            "{",
            "\tbool needmore;",
            "\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);",
            "",
            "\tneedmore = ULONG_CMP_LT(rnp->gp_seq, rnp->gp_seq_needed);",
            "\tif (!needmore)",
            "\t\trnp->gp_seq_needed = rnp->gp_seq; /* Avoid counter wrap. */",
            "\ttrace_rcu_this_gp(rnp, rdp, rnp->gp_seq,",
            "\t\t\t  needmore ? TPS(\"CleanupMore\") : TPS(\"Cleanup\"));",
            "\treturn needmore;",
            "}",
            "static void swake_up_one_online_ipi(void *arg)",
            "{",
            "\tstruct swait_queue_head *wqh = arg;",
            "",
            "\tswake_up_one(wqh);",
            "}",
            "static void swake_up_one_online(struct swait_queue_head *wqh)",
            "{",
            "\tint cpu = get_cpu();",
            "",
            "\t/*",
            "\t * If called from rcutree_report_cpu_starting(), wake up",
            "\t * is dangerous that late in the CPU-down hotplug process. The",
            "\t * scheduler might queue an ignored hrtimer. Defer the wake up",
            "\t * to an online CPU instead.",
            "\t */",
            "\tif (unlikely(cpu_is_offline(cpu))) {",
            "\t\tint target;",
            "",
            "\t\ttarget = cpumask_any_and(housekeeping_cpumask(HK_TYPE_RCU),",
            "\t\t\t\t\t cpu_online_mask);",
            "",
            "\t\tsmp_call_function_single(target, swake_up_one_online_ipi,",
            "\t\t\t\t\t wqh, 0);",
            "\t\tput_cpu();",
            "\t} else {",
            "\t\tput_cpu();",
            "\t\tswake_up_one(wqh);",
            "\t}",
            "}"
          ],
          "function_name": "trace_rcu_this_gp, rcu_start_this_gp, rcu_future_gp_cleanup, swake_up_one_online_ipi, swake_up_one_online",
          "description": "实现RCU grace period事件追踪、新grace period启动逻辑及未来grace period清理机制，支持跨层级节点的同步状态传播。",
          "similarity": 0.5098980665206909
        },
        {
          "chunk_id": 13,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2406,
          "end_line": 2512,
          "content": [
            "static void strict_work_handler(struct work_struct *work)",
            "{",
            "\trcu_read_lock();",
            "\trcu_read_unlock();",
            "}",
            "static __latent_entropy void rcu_core(void)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp = raw_cpu_ptr(&rcu_data);",
            "\tstruct rcu_node *rnp = rdp->mynode;",
            "\t/*",
            "\t * On RT rcu_core() can be preempted when IRQs aren't disabled.",
            "\t * Therefore this function can race with concurrent NOCB (de-)offloading",
            "\t * on this CPU and the below condition must be considered volatile.",
            "\t * However if we race with:",
            "\t *",
            "\t * _ Offloading:   In the worst case we accelerate or process callbacks",
            "\t *                 concurrently with NOCB kthreads. We are guaranteed to",
            "\t *                 call rcu_nocb_lock() if that happens.",
            "\t *",
            "\t * _ Deoffloading: In the worst case we miss callbacks acceleration or",
            "\t *                 processing. This is fine because the early stage",
            "\t *                 of deoffloading invokes rcu_core() after setting",
            "\t *                 SEGCBLIST_RCU_CORE. So we guarantee that we'll process",
            "\t *                 what could have been dismissed without the need to wait",
            "\t *                 for the next rcu_pending() check in the next jiffy.",
            "\t */",
            "\tconst bool do_batch = !rcu_segcblist_completely_offloaded(&rdp->cblist);",
            "",
            "\tif (cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "\ttrace_rcu_utilization(TPS(\"Start RCU core\"));",
            "\tWARN_ON_ONCE(!rdp->beenonline);",
            "",
            "\t/* Report any deferred quiescent states if preemption enabled. */",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_COUNT) && (!(preempt_count() & PREEMPT_MASK))) {",
            "\t\trcu_preempt_deferred_qs(current);",
            "\t} else if (rcu_preempt_need_deferred_qs(current)) {",
            "\t\tset_tsk_need_resched(current);",
            "\t\tset_preempt_need_resched();",
            "\t}",
            "",
            "\t/* Update RCU state based on any recent quiescent states. */",
            "\trcu_check_quiescent_state(rdp);",
            "",
            "\t/* No grace period and unregistered callbacks? */",
            "\tif (!rcu_gp_in_progress() &&",
            "\t    rcu_segcblist_is_enabled(&rdp->cblist) && do_batch) {",
            "\t\trcu_nocb_lock_irqsave(rdp, flags);",
            "\t\tif (!rcu_segcblist_restempty(&rdp->cblist, RCU_NEXT_READY_TAIL))",
            "\t\t\trcu_accelerate_cbs_unlocked(rnp, rdp);",
            "\t\trcu_nocb_unlock_irqrestore(rdp, flags);",
            "\t}",
            "",
            "\trcu_check_gp_start_stall(rnp, rdp, rcu_jiffies_till_stall_check());",
            "",
            "\t/* If there are callbacks ready, invoke them. */",
            "\tif (do_batch && rcu_segcblist_ready_cbs(&rdp->cblist) &&",
            "\t    likely(READ_ONCE(rcu_scheduler_fully_active))) {",
            "\t\trcu_do_batch(rdp);",
            "\t\t/* Re-invoke RCU core processing if there are callbacks remaining. */",
            "\t\tif (rcu_segcblist_ready_cbs(&rdp->cblist))",
            "\t\t\tinvoke_rcu_core();",
            "\t}",
            "",
            "\t/* Do any needed deferred wakeups of rcuo kthreads. */",
            "\tdo_nocb_deferred_wakeup(rdp);",
            "\ttrace_rcu_utilization(TPS(\"End RCU core\"));",
            "",
            "\t// If strict GPs, schedule an RCU reader in a clean environment.",
            "\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))",
            "\t\tqueue_work_on(rdp->cpu, rcu_gp_wq, &rdp->strict_work);",
            "}",
            "static void rcu_core_si(struct softirq_action *h)",
            "{",
            "\trcu_core();",
            "}",
            "static void rcu_wake_cond(struct task_struct *t, int status)",
            "{",
            "\t/*",
            "\t * If the thread is yielding, only wake it when this",
            "\t * is invoked from idle",
            "\t */",
            "\tif (t && (status != RCU_KTHREAD_YIELDING || is_idle_task(current)))",
            "\t\twake_up_process(t);",
            "}",
            "static void invoke_rcu_core_kthread(void)",
            "{",
            "\tstruct task_struct *t;",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\t__this_cpu_write(rcu_data.rcu_cpu_has_work, 1);",
            "\tt = __this_cpu_read(rcu_data.rcu_cpu_kthread_task);",
            "\tif (t != NULL && t != current)",
            "\t\trcu_wake_cond(t, __this_cpu_read(rcu_data.rcu_cpu_kthread_status));",
            "\tlocal_irq_restore(flags);",
            "}",
            "static void invoke_rcu_core(void)",
            "{",
            "\tif (!cpu_online(smp_processor_id()))",
            "\t\treturn;",
            "\tif (use_softirq)",
            "\t\traise_softirq(RCU_SOFTIRQ);",
            "\telse",
            "\t\tinvoke_rcu_core_kthread();",
            "}"
          ],
          "function_name": "strict_work_handler, rcu_core, rcu_core_si, rcu_wake_cond, invoke_rcu_core_kthread, invoke_rcu_core",
          "description": "定义RCU核心处理流程，包含软中断模式下的回调处理、grace period状态验证及条件唤醒机制，支持严格grace period场景的特殊处理",
          "similarity": 0.5078021883964539
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2273,
          "end_line": 2388,
          "content": [
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tunsigned long j;",
            "",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU)) {",
            "\t\tj = jiffies;",
            "\t\tWARN_ON_ONCE(time_before(j, __this_cpu_read(rcu_data.last_sched_clock)));",
            "\t\t__this_cpu_write(rcu_data.last_sched_clock, j);",
            "\t}",
            "\ttrace_rcu_utilization(TPS(\"Start scheduler-tick\"));",
            "\tlockdep_assert_irqs_disabled();",
            "\traw_cpu_inc(rcu_data.ticks_this_gp);",
            "\t/* The load-acquire pairs with the store-release setting to true. */",
            "\tif (smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {",
            "\t\t/* Idle and userspace execution already are quiescent states. */",
            "\t\tif (!rcu_is_cpu_rrupt_from_idle() && !user) {",
            "\t\t\tset_tsk_need_resched(current);",
            "\t\t\tset_preempt_need_resched();",
            "\t\t}",
            "\t\t__this_cpu_write(rcu_data.rcu_urgent_qs, false);",
            "\t}",
            "\trcu_flavor_sched_clock_irq(user);",
            "\tif (rcu_pending(user))",
            "\t\tinvoke_rcu_core();",
            "\tif (user || rcu_is_cpu_rrupt_from_idle())",
            "\t\trcu_note_voluntary_context_switch(current);",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\ttrace_rcu_utilization(TPS(\"End scheduler-tick\"));",
            "}",
            "static void force_qs_rnp(int (*f)(struct rcu_data *rdp))",
            "{",
            "\tint cpu;",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trcu_state.cbovld = rcu_state.cbovldnext;",
            "\trcu_state.cbovldnext = false;",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tunsigned long mask = 0;",
            "\t\tunsigned long rsmask = 0;",
            "",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trcu_state.cbovldnext |= !!rnp->cbovldmask;",
            "\t\tif (rnp->qsmask == 0) {",
            "\t\t\tif (rcu_preempt_blocked_readers_cgp(rnp)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No point in scanning bits because they",
            "\t\t\t\t * are all zero.  But we might need to",
            "\t\t\t\t * priority-boost blocked readers.",
            "\t\t\t\t */",
            "\t\t\t\trcu_initiate_boost(rnp, flags);",
            "\t\t\t\t/* rcu_initiate_boost() releases rnp->lock */",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->qsmask) {",
            "\t\t\tstruct rcu_data *rdp;",
            "\t\t\tint ret;",
            "",
            "\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\t\t\tret = f(rdp);",
            "\t\t\tif (ret > 0) {",
            "\t\t\t\tmask |= rdp->grpmask;",
            "\t\t\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t\t}",
            "\t\t\tif (ret < 0)",
            "\t\t\t\trsmask |= rdp->grpmask;",
            "\t\t}",
            "\t\tif (mask != 0) {",
            "\t\t\t/* Idle/offline CPUs, report (releases rnp->lock). */",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t} else {",
            "\t\t\t/* Nothing to do here, so just drop the lock. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t}",
            "",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rsmask)",
            "\t\t\tresched_cpu(cpu);",
            "\t}",
            "}",
            "void rcu_force_quiescent_state(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool ret;",
            "\tstruct rcu_node *rnp;",
            "\tstruct rcu_node *rnp_old = NULL;",
            "",
            "\t/* Funnel through hierarchy to reduce memory contention. */",
            "\trnp = raw_cpu_read(rcu_data.mynode);",
            "\tfor (; rnp != NULL; rnp = rnp->parent) {",
            "\t\tret = (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) ||",
            "\t\t       !raw_spin_trylock(&rnp->fqslock);",
            "\t\tif (rnp_old != NULL)",
            "\t\t\traw_spin_unlock(&rnp_old->fqslock);",
            "\t\tif (ret)",
            "\t\t\treturn;",
            "\t\trnp_old = rnp;",
            "\t}",
            "\t/* rnp_old == rcu_get_root(), rnp == NULL. */",
            "",
            "\t/* Reached the root of the rcu_node tree, acquire lock. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp_old, flags);",
            "\traw_spin_unlock(&rnp_old->fqslock);",
            "\tif (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\t\treturn;  /* Someone beat us to it. */",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\trcu_gp_kthread_wake();",
            "}"
          ],
          "function_name": "rcu_sched_clock_irq, force_qs_rnp, rcu_force_quiescent_state",
          "description": "实现强制quiescent状态触发与调度器中断处理，包含优先级提升、回调加速及grace period推进等关键控制流，维护RCU状态一致性",
          "similarity": 0.5005949139595032
        },
        {
          "chunk_id": 24,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4444,
          "end_line": 4559,
          "content": [
            "static void __init",
            "rcu_boot_init_percpu_data(int cpu)",
            "{",
            "\tstruct context_tracking *ct = this_cpu_ptr(&context_tracking);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\trdp->grpmask = leaf_node_cpu_bit(rdp->mynode, cpu);",
            "\tINIT_WORK(&rdp->strict_work, strict_work_handler);",
            "\tWARN_ON_ONCE(ct->dynticks_nesting != 1);",
            "\tWARN_ON_ONCE(rcu_dynticks_in_eqs(rcu_dynticks_snap(cpu)));",
            "\trdp->barrier_seq_snap = rcu_state.barrier_sequence;",
            "\trdp->rcu_ofl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_ofl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->rcu_onl_gp_seq = rcu_state.gp_seq;",
            "\trdp->rcu_onl_gp_flags = RCU_GP_CLEANED;",
            "\trdp->last_sched_clock = jiffies;",
            "\trdp->cpu = cpu;",
            "\trcu_boot_init_nocb_percpu_data(rdp);",
            "}",
            "int rcutree_prepare_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct context_tracking *ct = per_cpu_ptr(&context_tracking, cpu);",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "\tstruct rcu_node *rnp = rcu_get_root();",
            "",
            "\t/* Set up local state, ensuring consistent view of global state. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trdp->qlen_last_fqs_check = 0;",
            "\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\trdp->blimit = blimit;",
            "\tct->dynticks_nesting = 1;\t/* CPU not up, no tearing. */",
            "\traw_spin_unlock_rcu_node(rnp);\t\t/* irqs remain disabled. */",
            "",
            "\t/*",
            "\t * Only non-NOCB CPUs that didn't have early-boot callbacks need to be",
            "\t * (re-)initialized.",
            "\t */",
            "\tif (!rcu_segcblist_is_enabled(&rdp->cblist))",
            "\t\trcu_segcblist_init(&rdp->cblist);  /* Re-enable callbacks. */",
            "",
            "\t/*",
            "\t * Add CPU to leaf rcu_node pending-online bitmask.  Any needed",
            "\t * propagation up the rcu_node tree will happen at the beginning",
            "\t * of the next grace period.",
            "\t */",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_rcu_node(rnp);\t\t/* irqs already disabled. */",
            "\trdp->gp_seq = READ_ONCE(rnp->gp_seq);",
            "\trdp->gp_seq_needed = rdp->gp_seq;",
            "\trdp->cpu_no_qs.b.norm = true;",
            "\trdp->core_needs_qs = false;",
            "\trdp->rcu_iw_pending = false;",
            "\trdp->rcu_iw = IRQ_WORK_INIT_HARD(rcu_iw_handler);",
            "\trdp->rcu_iw_gp_seq = rdp->gp_seq - 1;",
            "\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuonl\"));",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcu_preempt_deferred_qs_init(rdp);",
            "\trcu_spawn_one_boost_kthread(rnp);",
            "\trcu_spawn_cpu_nocb_kthread(cpu);",
            "\tWRITE_ONCE(rcu_state.n_online_cpus, rcu_state.n_online_cpus + 1);",
            "",
            "\treturn 0;",
            "}",
            "static void rcutree_affinity_setting(unsigned int cpu, int outgoing)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\trcu_boost_kthread_setaffinity(rdp->mynode, outgoing);",
            "}",
            "bool rcu_cpu_beenfullyonline(int cpu)",
            "{",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\treturn smp_load_acquire(&rdp->beenonline);",
            "}",
            "int rcutree_online_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask |= rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\tif (rcu_scheduler_active == RCU_SCHEDULER_INACTIVE)",
            "\t\treturn 0; /* Too early in boot for scheduler work. */",
            "\tsync_sched_exp_online_cleanup(cpu);",
            "\trcutree_affinity_setting(cpu, -1);",
            "",
            "\t// Stop-machine done, so allow nohz_full to disable tick.",
            "\ttick_dep_clear(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}",
            "int rcutree_offline_cpu(unsigned int cpu)",
            "{",
            "\tunsigned long flags;",
            "\tstruct rcu_data *rdp;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\trnp = rdp->mynode;",
            "\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\trnp->ffmask &= ~rdp->grpmask;",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "",
            "\trcutree_affinity_setting(cpu, cpu);",
            "",
            "\t// nohz_full CPUs need the tick for stop-machine to work quickly",
            "\ttick_dep_set(TICK_DEP_BIT_RCU);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rcu_boot_init_percpu_data, rcutree_prepare_cpu, rcutree_affinity_setting, rcu_cpu_beenfullyonline, rcutree_online_cpu, rcutree_offline_cpu",
          "description": "初始化每个CPU的RCU私有数据结构，处理CPU上线/下线时的RCU状态同步，配置中断亲和性，更新全局在线CPU计数器",
          "similarity": 0.4994823634624481
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree_plugin.h",
      "md_summary": "> 自动生成时间: 2025-10-25 15:48:59\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree_plugin.h`\n\n---\n\n# `rcu/tree_plugin.h` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree_plugin.h` 是 Linux 内核中 **树形 RCU（Read-Copy Update）机制** 的内部头文件，用于实现基于分层树结构的 RCU 互斥机制。该文件定义了适用于 **经典 RCU** 或 **可抢占 RCU（PREEMPT_RCU）** 的内部非公开接口和辅助函数，主要服务于 `kernel/rcu/tree.c` 等核心 RCU 实现模块。其核心目标是在大规模 CPU 系统中高效管理宽限期（Grace Period）的检测与回调处理，同时支持 NOCB（No-CBs，即回调卸载）等高级特性。\n\n## 2. 核心功能\n\n### 主要函数\n\n- **`rcu_rdp_is_offloaded(struct rcu_data *rdp)`**  \n  安全地判断指定 CPU 的 `rcu_data` 是否启用了 NOCB（回调卸载）模式。该函数包含严格的锁依赖检查（通过 `RCU_LOCKDEP_WARN`），确保在读取 `offloaded` 状态时不会因并发修改导致数据不一致。\n\n- **`rcu_bootup_announce_oddness(void)`**  \n  在内核启动阶段检测并打印所有非默认或调试相关的 RCU 配置参数，用于诊断和性能调优。涵盖内容包括：扇出（fanout）设置、回调水位线、FQS（Force Quiescent State）延迟、软中断处理方式、调试选项等。\n\n- **`rcu_bootup_announce(void)`**（仅 `CONFIG_PREEMPT_RCU`）  \n  启动时声明当前使用的是“可抢占的分层 RCU 实现”，并调用 `rcu_bootup_announce_oddness()` 输出配置异常信息。\n\n- **`rcu_preempt_ctxt_queue(struct rcu_node *rnp, struct rcu_data *rdp)`**（仅 `CONFIG_PREEMPT_RCU`）  \n  将当前被抢占且处于 RCU 读侧临界区的任务插入到 `rcu_node` 的阻塞任务链表（`blkd_tasks`）中的合适位置。其插入策略基于当前是否存在普通或加速宽限期（GP/EXP GP），以及当前 CPU 是否被这些宽限期阻塞，以最小化对已有宽限期的不必要阻塞。\n\n### 关键宏定义（仅 `CONFIG_PREEMPT_RCU`）\n\n- **`RCU_GP_TASKS` / `RCU_EXP_TASKS` / `RCU_GP_BLKD` / `RCU_EXP_BLKD`**  \n  用于构建决策表，表示 `rcu_node` 中普通/加速宽限期的等待状态及当前 CPU 的阻塞状态，指导 `rcu_preempt_ctxt_queue()` 的任务插入逻辑。\n\n## 3. 关键实现\n\n### 安全读取 NOCB 状态\n`rcu_rdp_is_offloaded()` 通过 `RCU_LOCKDEP_WARN` 强制要求调用者必须持有以下任一同步原语：\n- `rcu_state.barrier_mutex`\n- CPU 热插拔锁（读/写）\n- 对应 `rdp` 的 NOCB 锁\n- 在本地 CPU 且不可抢占（非 `CONFIG_PREEMPT_COUNT` 或不可抢占上下文）\n- 当前为 NOCB 内核线程  \n这确保了在读取 `rdp->cblist` 的 `offloaded` 标志时，其值不会被并发修改。\n\n### 可抢占 RCU 的任务阻塞队列策略\n在 `CONFIG_PREEMPT_RCU` 下，当任务在 RCU 读侧临界区内被抢占时，需将其加入 `rcu_node->blkd_tasks` 链表。`rcu_preempt_ctxt_queue()` 使用 **状态决策表**（基于 `blkd_state` 的 4 位组合）决定插入位置：\n- **插入链表头部**：当任务不会阻塞任何**已存在的**宽限期（尤其是加速宽限期）时，避免延长已有宽限期。\n- **插入链表尾部**（代码未完整显示，但逻辑隐含）：当任务会阻塞已有宽限期时，需排在末尾以确保正确性。  \n该策略优先保护**加速宽限期**的低延迟特性，即使可能轻微延长普通宽限期。\n\n### 启动配置诊断\n`rcu_bootup_announce_oddness()` 系统性地检查数十个编译时和运行时 RCU 参数，对任何非默认值或启用的调试功能输出 `pr_info` 日志。这为系统管理员和开发者提供了 RCU 行为的透明视图，便于性能分析和问题排查。\n\n## 4. 依赖关系\n\n- **内部依赖**：\n  - `../locking/rtmutex_common.h`：提供 `lockdep_is_cpus_held()` 等锁依赖检查宏。\n  - `rcu_segcblist_is_offloaded()`：来自 RCU 回调段管理模块，用于查询 NOCB 状态。\n  - `rcu_lockdep_is_held_nocb()`、`rcu_current_is_nocb_kthread()`：NOCB 相关的锁依赖和上下文检查函数。\n  - `rcupdate_announce_bootup_oddness()`：来自 `kernel/rcu/update.c`，用于打印通用 RCU 启动信息。\n\n- **配置依赖**：\n  - `CONFIG_PREEMPT_RCU`：启用可抢占 RCU 的特定逻辑（如任务阻塞队列）。\n  - `CONFIG_RCU_TRACE`、`CONFIG_PROVE_RCU`、`CONFIG_RCU_BOOST` 等：控制启动诊断信息的输出。\n  - `CONFIG_HOTPLUG_CPU`：影响 CPU 热插拔锁的检查逻辑。\n\n- **数据结构依赖**：\n  - `struct rcu_data`、`struct rcu_node`：RCU 核心数据结构，定义在 `kernel/rcu/tree.h`。\n  - `rcu_state`：全局 RCU 状态结构体。\n\n## 5. 使用场景\n\n- **内核启动阶段**：  \n  `rcu_bootup_announce()` 和 `rcu_bootup_announce_oddness()` 在 RCU 初始化时被调用，输出配置诊断信息，帮助确认 RCU 子系统按预期配置。\n\n- **NOCB（回调卸载）模式运行时**：  \n  当系统启用 `CONFIG_RCU_NOCB_CPU` 时，`rcu_rdp_is_offloaded()` 被频繁调用（如在回调处理、宽限期推进路径中），以安全判断当前 CPU 的回调是否由专用内核线程处理。\n\n- **可抢占内核中的任务调度**：  \n  在 `CONFIG_PREEMPT_RCU` 系统中，当任务在 RCU 读侧临界区内被抢占时，调度器路径会调用 `rcu_preempt_ctxt_queue()`，将任务加入阻塞链表，确保宽限期能正确等待该任务退出临界区。\n\n- **调试与性能分析**：  \n  启动时的“oddness”日志为 RCU 调优提供依据；`RCU_LOCKDEP_WARN` 等检查帮助开发者发现 RCU 状态访问的同步错误。",
      "similarity": 0.5352299809455872,
      "chunks": []
    },
    {
      "source_file": "kernel/rcu/tiny.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:45:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tiny.c`\n\n---\n\n# `rcu/tiny.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tiny.c` 是 Linux 内核中 **RCU（Read-Copy Update）机制的“精简版”（Tiny RCU）实现**，专为单处理器（UP）或资源受限系统（如嵌入式设备）设计。该实现去除了复杂的状态机、CPU 间通信和动态负载均衡等开销，仅保留 RCU 的核心语义：**在读端无锁、写端延迟回收**。由于系统只有一个 CPU，任何上下文切换或中断返回用户态都天然构成“宽限期”（Grace Period），因此无需复杂的跨 CPU 同步逻辑。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_ctrlblk`**：RCU 控制块，全局唯一，用于管理回调链表和宽限期状态。\n  - `rcucblist`：待处理的 RCU 回调链表头。\n  - `donetail`：指向最后一个已完成宽限期的回调的 `next` 指针。\n  - `curtail`：指向链表最后一个回调的 `next` 指针。\n  - `gp_seq`：宽限期序列号，每次宽限期结束递增 2（偶数表示完成状态）。\n\n### 主要函数\n\n| 函数 | 功能说明 |\n|------|--------|\n| `rcu_qs(void)` | 记录当前 CPU 的静默状态（Quiescent State），推进已完成回调指针并触发软中断处理回调。 |\n| `rcu_sched_clock_irq(int user)` | 调度时钟中断处理函数，若处于用户态则调用 `rcu_qs()`；否则标记当前任务需重新调度以尽快进入静默状态。 |\n| `call_rcu(struct rcu_head *head, rcu_callback_t func)` | 注册一个 RCU 回调，在下一个宽限期结束后执行。 |\n| `synchronize_rcu(void)` | 等待当前宽限期结束（在 UP 系统中立即完成，仅更新 `gp_seq`）。 |\n| `rcu_process_callbacks(struct softirq_action *unused)` | RCU 软中断处理函数，批量执行已完成宽限期的回调。 |\n| `rcu_barrier(void)` | 等待所有已注册的 RCU 回调执行完毕。 |\n| `get_state_synchronize_rcu()` / `start_poll_synchronize_rcu()` / `poll_state_synchronize_rcu()` | 支持轮询式宽限期检测的 API。 |\n| `rcu_init(void)` | RCU 子系统初始化，注册 RCU 软中断处理函数。 |\n\n## 3. 关键实现\n\n### 宽限期管理\n- **单 CPU 假设**：由于系统只有一个 CPU，任何从内核态返回用户态、发生上下文切换或空闲任务运行，都视为一个完整的宽限期。\n- **`gp_seq` 计数器**：初始值为 `0 - 300UL`（负数，确保早期调用 `get_state_synchronize_rcu()` 返回有效值）。每次调用 `rcu_qs()` 或 `synchronize_rcu()` 时递增 2，偶数值表示宽限期已完成。\n- **无实际等待**：`synchronize_rcu()` 不阻塞，仅更新 `gp_seq`，因为调用者本身已处于静默状态。\n\n### 回调队列管理\n- **双指针链表**：使用 `donetail` 和 `curtail` 实现无锁（在中断禁用下）的回调入队和出队。\n  - 新回调通过 `curtail` 追加到链表尾部。\n  - `rcu_qs()` 将 `donetail` 移至 `curtail`，表示此前所有回调已完成宽限期。\n  - 软中断 `rcu_process_callbacks()` 将 `donetail` 之前的所有回调移出并执行。\n\n### 回调执行\n- **软中断上下文**：回调在 `RCU_SOFTIRQ` 中执行，确保不在原子上下文。\n- **支持 `kvfree`**：通过 `__is_kvfree_rcu_offset` 判断是否为 `kvfree_rcu` 回调，若是则直接释放内存而非调用函数指针。\n- **调试支持**：包含双释放检测（`debug_rcu_head_queue`）和内存泄漏防护（`tiny_rcu_leak_callback`）。\n\n### 轮询 API 实现\n- `get_state_synchronize_rcu()` 返回当前 `gp_seq`。\n- `poll_state_synchronize_rcu(oldstate)` 判断 `oldstate` 是否已完成：若 `oldstate == RCU_GET_STATE_COMPLETED`（特殊值）或当前 `gp_seq != oldstate`，则返回 `true`。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/rcupdate_wait.h>`：提供 `wait_rcu_gp()` 实现 `rcu_barrier()`。\n  - `\"rcu.h\"`：RCU 内部头文件，定义调试宏、trace 点等。\n  - 其他通用内核头文件（如 `sched.h`, `softirq.h`, `slab.h` 等）。\n- **内核配置**：\n  - 仅在 `CONFIG_TINY_RCU` 或 `CONFIG_TINY_SRCU` 启用时编译。\n  - 与 `CONFIG_PREEMPT`、`CONFIG_SMP` 互斥（Tiny RCU 用于非抢占式 UP 系统）。\n- **软中断子系统**：依赖 `open_softirq()` 注册 `RCU_SOFTIRQ`。\n- **内存管理**：`kvfree_call_rcu()` 依赖 KASAN 的辅助栈记录（`CONFIG_KASAN_GENERIC`）。\n\n## 5. 使用场景\n\n- **单处理器嵌入式系统**：资源受限设备（如 IoT 设备、微控制器）中替代 Tree RCU，显著减少代码体积和运行时开销。\n- **内核测试与调试**：作为 RCU 行为的简化模型，用于验证 RCU 语义正确性。\n- **RCU 基础功能提供**：\n  - 为内核其他子系统（如网络、文件系统、设备驱动）提供 `call_rcu()` 和 `synchronize_rcu()` 接口。\n  - 支持延迟内存回收（如 `kfree_rcu()`）。\n  - 通过 `rcu_barrier()` 确保模块卸载前所有回调完成。\n- **轮询式同步**：适用于不能阻塞的上下文（如中断处理程序），通过 `poll_state_synchronize_rcu()` 轮询宽限期状态。",
      "similarity": 0.5307047963142395,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/rcu/tiny.c",
          "start_line": 45,
          "end_line": 161,
          "content": [
            "void rcu_barrier(void)",
            "{",
            "\twait_rcu_gp(call_rcu_hurry);",
            "}",
            "void rcu_qs(void)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (rcu_ctrlblk.donetail != rcu_ctrlblk.curtail) {",
            "\t\trcu_ctrlblk.donetail = rcu_ctrlblk.curtail;",
            "\t\traise_softirq_irqoff(RCU_SOFTIRQ);",
            "\t}",
            "\tWRITE_ONCE(rcu_ctrlblk.gp_seq, rcu_ctrlblk.gp_seq + 2);",
            "\tlocal_irq_restore(flags);",
            "}",
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tif (user) {",
            "\t\trcu_qs();",
            "\t} else if (rcu_ctrlblk.donetail != rcu_ctrlblk.curtail) {",
            "\t\tset_tsk_need_resched(current);",
            "\t\tset_preempt_need_resched();",
            "\t}",
            "}",
            "static inline bool rcu_reclaim_tiny(struct rcu_head *head)",
            "{",
            "\trcu_callback_t f;",
            "\tunsigned long offset = (unsigned long)head->func;",
            "",
            "\trcu_lock_acquire(&rcu_callback_map);",
            "\tif (__is_kvfree_rcu_offset(offset)) {",
            "\t\ttrace_rcu_invoke_kvfree_callback(\"\", head, offset);",
            "\t\tkvfree((void *)head - offset);",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "\t\treturn true;",
            "\t}",
            "",
            "\ttrace_rcu_invoke_callback(\"\", head);",
            "\tf = head->func;",
            "\tdebug_rcu_head_callback(head);",
            "\tWRITE_ONCE(head->func, (rcu_callback_t)0L);",
            "\tf(head);",
            "\trcu_lock_release(&rcu_callback_map);",
            "\treturn false;",
            "}",
            "static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused)",
            "{",
            "\tstruct rcu_head *next, *list;",
            "\tunsigned long flags;",
            "",
            "\t/* Move the ready-to-invoke callbacks to a local list. */",
            "\tlocal_irq_save(flags);",
            "\tif (rcu_ctrlblk.donetail == &rcu_ctrlblk.rcucblist) {",
            "\t\t/* No callbacks ready, so just leave. */",
            "\t\tlocal_irq_restore(flags);",
            "\t\treturn;",
            "\t}",
            "\tlist = rcu_ctrlblk.rcucblist;",
            "\trcu_ctrlblk.rcucblist = *rcu_ctrlblk.donetail;",
            "\t*rcu_ctrlblk.donetail = NULL;",
            "\tif (rcu_ctrlblk.curtail == rcu_ctrlblk.donetail)",
            "\t\trcu_ctrlblk.curtail = &rcu_ctrlblk.rcucblist;",
            "\trcu_ctrlblk.donetail = &rcu_ctrlblk.rcucblist;",
            "\tlocal_irq_restore(flags);",
            "",
            "\t/* Invoke the callbacks on the local list. */",
            "\twhile (list) {",
            "\t\tnext = list->next;",
            "\t\tprefetch(next);",
            "\t\tdebug_rcu_head_unqueue(list);",
            "\t\tlocal_bh_disable();",
            "\t\trcu_reclaim_tiny(list);",
            "\t\tlocal_bh_enable();",
            "\t\tlist = next;",
            "\t}",
            "}",
            "void synchronize_rcu(void)",
            "{",
            "\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_lock_map) ||",
            "\t\t\t lock_is_held(&rcu_sched_lock_map),",
            "\t\t\t \"Illegal synchronize_rcu() in RCU read-side critical section\");",
            "\tWRITE_ONCE(rcu_ctrlblk.gp_seq, rcu_ctrlblk.gp_seq + 2);",
            "}",
            "static void tiny_rcu_leak_callback(struct rcu_head *rhp)",
            "{",
            "}",
            "void call_rcu(struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\tstatic atomic_t doublefrees;",
            "\tunsigned long flags;",
            "",
            "\tif (debug_rcu_head_queue(head)) {",
            "\t\tif (atomic_inc_return(&doublefrees) < 4) {",
            "\t\t\tpr_err(\"%s(): Double-freed CB %p->%pS()!!!  \", __func__, head, head->func);",
            "\t\t\tmem_dump_obj(head);",
            "\t\t}",
            "",
            "\t\tif (!__is_kvfree_rcu_offset((unsigned long)head->func))",
            "\t\t\tWRITE_ONCE(head->func, tiny_rcu_leak_callback);",
            "\t\treturn;",
            "\t}",
            "",
            "\thead->func = func;",
            "\thead->next = NULL;",
            "",
            "\tlocal_irq_save(flags);",
            "\t*rcu_ctrlblk.curtail = head;",
            "\trcu_ctrlblk.curtail = &head->next;",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (unlikely(is_idle_task(current))) {",
            "\t\t/* force scheduling for rcu_qs() */",
            "\t\tresched_cpu(0);",
            "\t}",
            "}"
          ],
          "function_name": "rcu_barrier, rcu_qs, rcu_sched_clock_irq, rcu_reclaim_tiny, rcu_process_callbacks, synchronize_rcu, tiny_rcu_leak_callback, call_rcu",
          "description": "实现RCU核心函数，包括rcu_barrier触发同步屏障、rcu_process_callbacks处理回调队列、synchronize_rcu更新grace period序列号、call_rcu添加回调到链表并处理空闲任务调度。",
          "similarity": 0.5048158764839172
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/rcu/tiny.c",
          "start_line": 206,
          "end_line": 239,
          "content": [
            "void get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp)",
            "{",
            "\trgosp->rgos_norm = RCU_GET_STATE_COMPLETED;",
            "}",
            "unsigned long get_state_synchronize_rcu(void)",
            "{",
            "\treturn READ_ONCE(rcu_ctrlblk.gp_seq);",
            "}",
            "unsigned long start_poll_synchronize_rcu(void)",
            "{",
            "\tunsigned long gp_seq = get_state_synchronize_rcu();",
            "",
            "\tif (unlikely(is_idle_task(current))) {",
            "\t\t/* force scheduling for rcu_qs() */",
            "\t\tresched_cpu(0);",
            "\t}",
            "\treturn gp_seq;",
            "}",
            "bool poll_state_synchronize_rcu(unsigned long oldstate)",
            "{",
            "\treturn oldstate == RCU_GET_STATE_COMPLETED || READ_ONCE(rcu_ctrlblk.gp_seq) != oldstate;",
            "}",
            "void kvfree_call_rcu(struct rcu_head *head, void *ptr)",
            "{",
            "\tif (head)",
            "\t\tkasan_record_aux_stack_noalloc(ptr);",
            "",
            "\t__kvfree_call_rcu(head, ptr);",
            "}",
            "void __init rcu_init(void)",
            "{",
            "\topen_softirq(RCU_SOFTIRQ, rcu_process_callbacks);",
            "\trcu_early_boot_tests();",
            "}"
          ],
          "function_name": "get_completed_synchronize_rcu_full, get_state_synchronize_rcu, start_poll_synchronize_rcu, poll_state_synchronize_rcu, kvfree_call_rcu, rcu_init",
          "description": "提供RCU状态查询接口，get_state_synchronize_rcu读取当前grace period序号，start_poll_synchronize_rcu用于轮询同步状态，rcu_init初始化RCU软中断处理函数注册。",
          "similarity": 0.48910993337631226
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/rcu/tiny.c",
          "start_line": 1,
          "end_line": 44,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0+",
            "/*",
            " * Read-Copy Update mechanism for mutual exclusion, the Bloatwatch edition.",
            " *",
            " * Copyright IBM Corporation, 2008",
            " *",
            " * Author: Paul E. McKenney <paulmck@linux.ibm.com>",
            " *",
            " * For detailed explanation of Read-Copy Update mechanism see -",
            " *\t\tDocumentation/RCU",
            " */",
            "#include <linux/completion.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/notifier.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/mutex.h>",
            "#include <linux/sched.h>",
            "#include <linux/types.h>",
            "#include <linux/init.h>",
            "#include <linux/time.h>",
            "#include <linux/cpu.h>",
            "#include <linux/prefetch.h>",
            "#include <linux/slab.h>",
            "#include <linux/mm.h>",
            "",
            "#include \"rcu.h\"",
            "",
            "/* Global control variables for rcupdate callback mechanism. */",
            "struct rcu_ctrlblk {",
            "\tstruct rcu_head *rcucblist;\t/* List of pending callbacks (CBs). */",
            "\tstruct rcu_head **donetail;\t/* ->next pointer of last \"done\" CB. */",
            "\tstruct rcu_head **curtail;\t/* ->next pointer of last CB. */",
            "\tunsigned long gp_seq;\t\t/* Grace-period counter. */",
            "};",
            "",
            "/* Definition for rcupdate control block. */",
            "static struct rcu_ctrlblk rcu_ctrlblk = {",
            "\t.donetail\t= &rcu_ctrlblk.rcucblist,",
            "\t.curtail\t= &rcu_ctrlblk.rcucblist,",
            "\t.gp_seq\t\t= 0 - 300UL,",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义RCU控制块结构体rcu_ctrlblk，用于管理回调链表、尾指针及 grace period 序列号，初始化时设置 donetail 和 curtail 指向同一位置，gp_seq 初始化为负值以标记初始状态。",
          "similarity": 0.4474949240684509
        }
      ]
    }
  ]
}