{
  "query": "同步机制在并发编程中的应用",
  "timestamp": "2025-12-25 23:42:58",
  "retrieved_files": [
    {
      "source_file": "kernel/async.c",
      "md_summary": "> 自动生成时间: 2025-10-25 11:49:14\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `async.c`\n\n---\n\n# async.c 技术文档\n\n## 1. 文件概述\n\n`async.c` 实现了 Linux 内核中的异步函数调用机制，主要用于优化系统启动性能。该机制允许在内核初始化阶段将原本串行执行的、相互独立的硬件探测和初始化操作并行化，从而显著缩短启动时间。其核心思想是在保持对外可见操作顺序一致性的前提下，内部执行过程可乱序进行，类似于乱序执行 CPU 的“按序提交”语义。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct async_entry`**：表示一个异步任务条目，包含：\n  - `domain_list` / `global_list`：分别链接到所属域和全局的待处理链表\n  - `work`：关联的 workqueue 工作项\n  - `cookie`：序列号，用于同步控制\n  - `func` / `data`：要执行的函数及其参数\n  - `domain`：所属的异步域\n\n- **`struct async_domain`**：异步执行域，用于将异步任务分组管理，默认使用 `async_dfl_domain`\n\n- **全局变量**：\n  - `next_cookie`：单调递增的序列号生成器\n  - `async_global_pending`：所有已注册域的全局待处理任务链表\n  - `async_dfl_domain`：默认异步域\n  - `async_lock`：保护异步任务队列的自旋锁\n  - `entry_count`：当前挂起的异步任务计数\n\n### 主要函数\n\n- **`async_schedule_node_domain()`**：在指定 NUMA 节点和异步域中调度异步函数\n- **`async_schedule_node()`**：在指定 NUMA 节点上调度异步函数（使用默认域）\n- **`async_schedule_dev_nocall()`**：基于设备的 NUMA 信息调度异步函数（失败时不回退到同步执行）\n- **`lowest_in_progress()`**：获取指定域或全局中最早（最小 cookie）的未完成任务\n- **`async_run_entry_fn()`**：workqueue 回调函数，实际执行异步任务并清理资源\n\n## 3. 关键实现\n\n### 序列 Cookie 机制\n- 每个异步任务分配一个单调递增的 `async_cookie_t`（64 位无符号整数）\n- 任务执行前可通过 `async_synchronize_cookie()` 等待所有小于等于指定 cookie 的任务完成\n- 保证对外部可见操作（如设备注册）的顺序一致性\n\n### 内存与负载控制\n- 使用 `GFP_ATOMIC` 分配内存，支持原子上下文调用\n- 当内存不足或挂起任务超过 `MAX_WORK`（32768）时，自动回退到同步执行\n- 通过 `entry_count` 原子计数器跟踪挂起任务数量\n\n### 双链表管理\n- 每个任务同时链接到：\n  - 所属域的 `domain->pending` 链表（按 cookie 顺序）\n  - 全局 `async_global_pending` 链表（仅当域已注册）\n- 保证域内和全局的同步操作都能正确等待\n\n### NUMA 感知调度\n- 通过 `queue_work_node()` 将任务调度到指定 NUMA 节点\n- 若节点无效则自动分发到可用 CPU\n\n### 资源清理与通知\n- 任务执行完成后从链表移除并释放内存\n- 通过 `wake_up(&async_done)` 唤醒等待同步完成的线程\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/async.h>`：异步 API 定义\n  - `<linux/workqueue.h>`：工作队列机制\n  - `\"workqueue_internal.h\"`：内部 workqueue 接口\n  - 其他基础内核头文件（atomic、slab、wait 等）\n\n- **核心子系统**：\n  - **Workqueue 子系统**：实际执行异步任务的底层机制\n  - **内存管理子系统**：任务结构体内存分配\n  - **调度器**：NUMA 节点感知的任务调度\n\n- **导出符号**：\n  - `async_schedule_node_domain`\n  - `async_schedule_node`\n\n## 5. 使用场景\n\n- **内核启动优化**：\n  - 并行执行设备探测（如 PCI、USB 控制器初始化）\n  - 异步加载固件或执行硬件自检\n\n- **驱动初始化**：\n  - 驱动可将耗时的初始化操作（如 PHY 配置、固件加载）放入异步任务\n  - 通过 `async_synchronize_full()` 确保在模块初始化完成前所有异步任务结束\n\n- **NUMA 优化**：\n  - 将设备相关的初始化任务调度到设备所在 NUMA 节点，减少远程内存访问\n\n- **资源受限环境**：\n  - 在内存压力下自动回退到同步执行，保证系统稳定性\n  - 通过 `MAX_WORK` 限制防止异步任务无限堆积",
      "similarity": 0.6018568277359009,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/async.c",
          "start_line": 241,
          "end_line": 290,
          "content": [
            "async_cookie_t async_schedule_node(async_func_t func, void *data, int node)",
            "{",
            "\treturn async_schedule_node_domain(func, data, node, &async_dfl_domain);",
            "}",
            "bool async_schedule_dev_nocall(async_func_t func, struct device *dev)",
            "{",
            "\tstruct async_entry *entry;",
            "",
            "\tentry = kzalloc(sizeof(struct async_entry), GFP_KERNEL);",
            "",
            "\t/* Give up if there is no memory or too much work. */",
            "\tif (!entry || atomic_read(&entry_count) > MAX_WORK) {",
            "\t\tkfree(entry);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t__async_schedule_node_domain(func, dev, dev_to_node(dev),",
            "\t\t\t\t     &async_dfl_domain, entry);",
            "\treturn true;",
            "}",
            "void async_synchronize_full(void)",
            "{",
            "\tasync_synchronize_full_domain(NULL);",
            "}",
            "void async_synchronize_full_domain(struct async_domain *domain)",
            "{",
            "\tasync_synchronize_cookie_domain(ASYNC_COOKIE_MAX, domain);",
            "}",
            "void async_synchronize_cookie_domain(async_cookie_t cookie, struct async_domain *domain)",
            "{",
            "\tktime_t starttime;",
            "",
            "\tpr_debug(\"async_waiting @ %i\\n\", task_pid_nr(current));",
            "\tstarttime = ktime_get();",
            "",
            "\twait_event(async_done, lowest_in_progress(domain) >= cookie);",
            "",
            "\tpr_debug(\"async_continuing @ %i after %lli usec\\n\", task_pid_nr(current),",
            "\t\t microseconds_since(starttime));",
            "}",
            "void async_synchronize_cookie(async_cookie_t cookie)",
            "{",
            "\tasync_synchronize_cookie_domain(cookie, &async_dfl_domain);",
            "}",
            "bool current_is_async(void)",
            "{",
            "\tstruct worker *worker = current_wq_worker();",
            "",
            "\treturn worker && worker->current_func == async_run_entry_fn;",
            "}"
          ],
          "function_name": "async_schedule_node, async_schedule_dev_nocall, async_synchronize_full, async_synchronize_full_domain, async_synchronize_cookie_domain, async_synchronize_cookie, current_is_async",
          "description": "提供同步屏障接口和运行态检测，确保异步操作有序完成",
          "similarity": 0.650922954082489
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/async.c",
          "start_line": 82,
          "end_line": 201,
          "content": [
            "static long long microseconds_since(ktime_t start)",
            "{",
            "\tktime_t now = ktime_get();",
            "\treturn ktime_to_ns(ktime_sub(now, start)) >> 10;",
            "}",
            "static async_cookie_t lowest_in_progress(struct async_domain *domain)",
            "{",
            "\tstruct async_entry *first = NULL;",
            "\tasync_cookie_t ret = ASYNC_COOKIE_MAX;",
            "\tunsigned long flags;",
            "",
            "\tspin_lock_irqsave(&async_lock, flags);",
            "",
            "\tif (domain) {",
            "\t\tif (!list_empty(&domain->pending))",
            "\t\t\tfirst = list_first_entry(&domain->pending,",
            "\t\t\t\t\tstruct async_entry, domain_list);",
            "\t} else {",
            "\t\tif (!list_empty(&async_global_pending))",
            "\t\t\tfirst = list_first_entry(&async_global_pending,",
            "\t\t\t\t\tstruct async_entry, global_list);",
            "\t}",
            "",
            "\tif (first)",
            "\t\tret = first->cookie;",
            "",
            "\tspin_unlock_irqrestore(&async_lock, flags);",
            "\treturn ret;",
            "}",
            "static void async_run_entry_fn(struct work_struct *work)",
            "{",
            "\tstruct async_entry *entry =",
            "\t\tcontainer_of(work, struct async_entry, work);",
            "\tunsigned long flags;",
            "\tktime_t calltime;",
            "",
            "\t/* 1) run (and print duration) */",
            "\tpr_debug(\"calling  %lli_%pS @ %i\\n\", (long long)entry->cookie,",
            "\t\t entry->func, task_pid_nr(current));",
            "\tcalltime = ktime_get();",
            "",
            "\tentry->func(entry->data, entry->cookie);",
            "",
            "\tpr_debug(\"initcall %lli_%pS returned after %lld usecs\\n\",",
            "\t\t (long long)entry->cookie, entry->func,",
            "\t\t microseconds_since(calltime));",
            "",
            "\t/* 2) remove self from the pending queues */",
            "\tspin_lock_irqsave(&async_lock, flags);",
            "\tlist_del_init(&entry->domain_list);",
            "\tlist_del_init(&entry->global_list);",
            "",
            "\t/* 3) free the entry */",
            "\tkfree(entry);",
            "\tatomic_dec(&entry_count);",
            "",
            "\tspin_unlock_irqrestore(&async_lock, flags);",
            "",
            "\t/* 4) wake up any waiters */",
            "\twake_up(&async_done);",
            "}",
            "static async_cookie_t __async_schedule_node_domain(async_func_t func,",
            "\t\t\t\t\t\t   void *data, int node,",
            "\t\t\t\t\t\t   struct async_domain *domain,",
            "\t\t\t\t\t\t   struct async_entry *entry)",
            "{",
            "\tasync_cookie_t newcookie;",
            "\tunsigned long flags;",
            "",
            "\tINIT_LIST_HEAD(&entry->domain_list);",
            "\tINIT_LIST_HEAD(&entry->global_list);",
            "\tINIT_WORK(&entry->work, async_run_entry_fn);",
            "\tentry->func = func;",
            "\tentry->data = data;",
            "\tentry->domain = domain;",
            "",
            "\tspin_lock_irqsave(&async_lock, flags);",
            "",
            "\t/* allocate cookie and queue */",
            "\tnewcookie = entry->cookie = next_cookie++;",
            "",
            "\tlist_add_tail(&entry->domain_list, &domain->pending);",
            "\tif (domain->registered)",
            "\t\tlist_add_tail(&entry->global_list, &async_global_pending);",
            "",
            "\tatomic_inc(&entry_count);",
            "\tspin_unlock_irqrestore(&async_lock, flags);",
            "",
            "\t/* schedule for execution */",
            "\tqueue_work_node(node, system_unbound_wq, &entry->work);",
            "",
            "\treturn newcookie;",
            "}",
            "async_cookie_t async_schedule_node_domain(async_func_t func, void *data,",
            "\t\t\t\t\t  int node, struct async_domain *domain)",
            "{",
            "\tstruct async_entry *entry;",
            "\tunsigned long flags;",
            "\tasync_cookie_t newcookie;",
            "",
            "\t/* allow irq-off callers */",
            "\tentry = kzalloc(sizeof(struct async_entry), GFP_ATOMIC);",
            "",
            "\t/*",
            "\t * If we're out of memory or if there's too much work",
            "\t * pending already, we execute synchronously.",
            "\t */",
            "\tif (!entry || atomic_read(&entry_count) > MAX_WORK) {",
            "\t\tkfree(entry);",
            "\t\tspin_lock_irqsave(&async_lock, flags);",
            "\t\tnewcookie = next_cookie++;",
            "\t\tspin_unlock_irqrestore(&async_lock, flags);",
            "",
            "\t\t/* low on memory.. run synchronously */",
            "\t\tfunc(data, newcookie);",
            "\t\treturn newcookie;",
            "\t}",
            "",
            "\treturn __async_schedule_node_domain(func, data, node, domain, entry);",
            "}"
          ],
          "function_name": "microseconds_since, lowest_in_progress, async_run_entry_fn, __async_schedule_node_domain, async_schedule_node_domain",
          "description": "实现异步任务调度与执行逻辑，包含时间测量、任务排队及工作队列调度",
          "similarity": 0.6064783334732056
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/async.c",
          "start_line": 1,
          "end_line": 81,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * async.c: Asynchronous function calls for boot performance",
            " *",
            " * (C) Copyright 2009 Intel Corporation",
            " * Author: Arjan van de Ven <arjan@linux.intel.com>",
            " */",
            "",
            "",
            "/*",
            "",
            "Goals and Theory of Operation",
            "",
            "The primary goal of this feature is to reduce the kernel boot time,",
            "by doing various independent hardware delays and discovery operations",
            "decoupled and not strictly serialized.",
            "",
            "More specifically, the asynchronous function call concept allows",
            "certain operations (primarily during system boot) to happen",
            "asynchronously, out of order, while these operations still",
            "have their externally visible parts happen sequentially and in-order.",
            "(not unlike how out-of-order CPUs retire their instructions in order)",
            "",
            "Key to the asynchronous function call implementation is the concept of",
            "a \"sequence cookie\" (which, although it has an abstracted type, can be",
            "thought of as a monotonically incrementing number).",
            "",
            "The async core will assign each scheduled event such a sequence cookie and",
            "pass this to the called functions.",
            "",
            "The asynchronously called function should before doing a globally visible",
            "operation, such as registering device numbers, call the",
            "async_synchronize_cookie() function and pass in its own cookie. The",
            "async_synchronize_cookie() function will make sure that all asynchronous",
            "operations that were scheduled prior to the operation corresponding with the",
            "cookie have completed.",
            "",
            "Subsystem/driver initialization code that scheduled asynchronous probe",
            "functions, but which shares global resources with other drivers/subsystems",
            "that do not use the asynchronous call feature, need to do a full",
            "synchronization with the async_synchronize_full() function, before returning",
            "from their init function. This is to maintain strict ordering between the",
            "asynchronous and synchronous parts of the kernel.",
            "",
            "*/",
            "",
            "#include <linux/async.h>",
            "#include <linux/atomic.h>",
            "#include <linux/export.h>",
            "#include <linux/ktime.h>",
            "#include <linux/pid.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/wait.h>",
            "#include <linux/workqueue.h>",
            "",
            "#include \"workqueue_internal.h\"",
            "",
            "static async_cookie_t next_cookie = 1;",
            "",
            "#define MAX_WORK\t\t32768",
            "#define ASYNC_COOKIE_MAX\tULLONG_MAX\t/* infinity cookie */",
            "",
            "static LIST_HEAD(async_global_pending);\t/* pending from all registered doms */",
            "static ASYNC_DOMAIN(async_dfl_domain);",
            "static DEFINE_SPINLOCK(async_lock);",
            "",
            "struct async_entry {",
            "\tstruct list_head\tdomain_list;",
            "\tstruct list_head\tglobal_list;",
            "\tstruct work_struct\twork;",
            "\tasync_cookie_t\t\tcookie;",
            "\tasync_func_t\t\tfunc;",
            "\tvoid\t\t\t*data;",
            "\tstruct async_domain\t*domain;",
            "};",
            "",
            "static DECLARE_WAIT_QUEUE_HEAD(async_done);",
            "",
            "static atomic_t entry_count;",
            ""
          ],
          "function_name": null,
          "description": "定义异步任务结构体和核心变量，支持多域异步调度",
          "similarity": 0.5278224945068359
        }
      ]
    },
    {
      "source_file": "kernel/rcu/sync.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:44:18\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\sync.c`\n\n---\n\n# rcu/sync.c 技术文档\n\n## 文件概述\n\n`rcu/sync.c` 实现了一个基于 RCU（Read-Copy-Update）机制的轻量级读写同步基础设施，称为 `rcu_sync`。该机制允许写者（更新者）在需要时强制所有读者切换到“慢路径”（slow path），并在更新完成后经过一个 RCU 宽限期（grace period）后，允许读者重新使用“快路径”（fast path）。该设计特别适用于需要频繁但短暂地禁用读者快路径的场景，避免了传统读写锁的开销，同时利用 RCU 的无锁读取特性提升性能。\n\n## 核心功能\n\n### 数据结构\n\n- **`struct rcu_sync`**  \n  核心同步控制结构，包含以下关键字段：\n  - `gp_state`：当前同步状态（`GP_IDLE`, `GP_ENTER`, `GP_PASSED`, `GP_EXIT`, `GP_REPLAY`）\n  - `gp_count`：嵌套的 `rcu_sync_enter()` 调用计数\n  - `cb_head`：用于 RCU 回调的 `rcu_head`\n  - `gp_wait`：等待队列，用于阻塞等待状态转换完成\n\n### 主要函数\n\n| 函数 | 功能描述 |\n|------|--------|\n| `rcu_sync_init()` | 初始化 `rcu_sync` 结构体 |\n| `rcu_sync_enter_start()` | 预激活同步机制，使 `rcu_sync_is_idle()` 返回 false，且后续 enter/exit 成为 NO-OP |\n| `rcu_sync_enter()` | 强制读者进入慢路径，确保后续读者不会使用快路径 |\n| `rcu_sync_exit()` | 标记更新结束，安排在宽限期后恢复读者快路径 |\n| `rcu_sync_dtor()` | 销毁 `rcu_sync` 结构，确保所有 RCU 回调已完成 |\n| `rcu_sync_func()` | RCU 回调函数，根据当前状态推进状态机 |\n\n## 关键实现\n\n### 状态机设计\n\n`rcu_sync` 使用五种状态实现高效的状态转换：\n\n- **`GP_IDLE`**：初始状态，读者可使用快路径。\n- **`GP_ENTER`**：正在进入同步状态，需等待宽限期。\n- **`GP_PASSED`**：宽限期已过，读者已全部进入慢路径。\n- **`GP_EXIT`**：正在退出同步，需等待另一个宽限期以恢复快路径。\n- **`GP_REPLAY`**：在退出过程中又有新的 enter/exit 对发生，需重新调度回调。\n\n### 嵌套与优化\n\n- **嵌套支持**：通过 `gp_count` 支持 `rcu_sync_enter()` 的嵌套调用。只有当 `gp_count` 从 1 递减到 0 时，才触发退出流程。\n- **宽限期合并**：连续的 `enter/exit` 调用可避免多次等待宽限期。例如：\n  - 若在 `GP_PASSED` 状态下调用 `exit`，直接进入 `GP_EXIT` 并调度回调。\n  - 若在回调执行前再次调用 `enter/exit`，状态转为 `GP_REPLAY`，并在回调中重新调度，避免冗余宽限期。\n- **快速路径优化**：首次 `enter` 时若处于 `GP_IDLE`，直接调用 `synchronize_rcu()` 而非异步 `call_rcu()`，可利用 `rcu_expedited` 或 `rcu_blocking_is_gp()` 加速。\n\n### 同步与唤醒\n\n- 写者调用 `rcu_sync_enter()` 后，若非首次进入，会阻塞在 `wait_event()`，直到状态变为 `GP_PASSED` 或更高。\n- `rcu_sync_func()` 在宽限期后执行，根据 `gp_count` 和当前状态决定是唤醒等待者、重调度回调，还是恢复到 `GP_IDLE`。\n\n## 依赖关系\n\n- **`<linux/rcu_sync.h>`**：定义 `struct rcu_sync` 及相关 API。\n- **`<linux/sched.h>`**：提供 `wait_event()`、`wake_up_locked()` 等调度和等待队列原语。\n- **RCU 子系统**：\n  - `call_rcu_hurry()` / `call_rcu()`：用于注册宽限期后的回调。\n  - `synchronize_rcu()`：用于同步等待宽限期。\n  - `rcu_barrier()`：在析构时确保所有回调完成。\n- **自旋锁**：使用 `spin_lock_irqsave()` 保护状态和计数器，确保中断上下文安全。\n\n## 使用场景\n\n- **文件系统元数据更新**：如 overlayfs、btrfs 等在修改共享元数据结构时，临时禁止读者使用快路径缓存。\n- **动态配置更新**：内核模块或子系统在热更新全局配置时，确保读者看到一致状态。\n- **轻量级写者同步**：适用于写操作较少但需高效读者路径的场景，避免传统 rwlock 的读者竞争开销。\n- **替代 `synchronize_rcu()` 的批量操作**：当多个连续更新可合并为一次宽限期等待时，提升性能。",
      "similarity": 0.597471296787262,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/rcu/sync.c",
          "start_line": 21,
          "end_line": 136,
          "content": [
            "void rcu_sync_init(struct rcu_sync *rsp)",
            "{",
            "\tmemset(rsp, 0, sizeof(*rsp));",
            "\tinit_waitqueue_head(&rsp->gp_wait);",
            "}",
            "void rcu_sync_enter_start(struct rcu_sync *rsp)",
            "{",
            "\trsp->gp_count++;",
            "\trsp->gp_state = GP_PASSED;",
            "}",
            "static void rcu_sync_call(struct rcu_sync *rsp)",
            "{",
            "\tcall_rcu_hurry(&rsp->cb_head, rcu_sync_func);",
            "}",
            "static void rcu_sync_func(struct rcu_head *rhp)",
            "{",
            "\tstruct rcu_sync *rsp = container_of(rhp, struct rcu_sync, cb_head);",
            "\tunsigned long flags;",
            "",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_IDLE);",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_PASSED);",
            "",
            "\tspin_lock_irqsave(&rsp->rss_lock, flags);",
            "\tif (rsp->gp_count) {",
            "\t\t/*",
            "\t\t * We're at least a GP after the GP_IDLE->GP_ENTER transition.",
            "\t\t */",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_PASSED);",
            "\t\twake_up_locked(&rsp->gp_wait);",
            "\t} else if (rsp->gp_state == GP_REPLAY) {",
            "\t\t/*",
            "\t\t * A new rcu_sync_exit() has happened; requeue the callback to",
            "\t\t * catch a later GP.",
            "\t\t */",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_EXIT);",
            "\t\trcu_sync_call(rsp);",
            "\t} else {",
            "\t\t/*",
            "\t\t * We're at least a GP after the last rcu_sync_exit(); everybody",
            "\t\t * will now have observed the write side critical section.",
            "\t\t * Let 'em rip!",
            "\t\t */",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_IDLE);",
            "\t}",
            "\tspin_unlock_irqrestore(&rsp->rss_lock, flags);",
            "}",
            "void rcu_sync_enter(struct rcu_sync *rsp)",
            "{",
            "\tint gp_state;",
            "",
            "\tspin_lock_irq(&rsp->rss_lock);",
            "\tgp_state = rsp->gp_state;",
            "\tif (gp_state == GP_IDLE) {",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_ENTER);",
            "\t\tWARN_ON_ONCE(rsp->gp_count);",
            "\t\t/*",
            "\t\t * Note that we could simply do rcu_sync_call(rsp) here and",
            "\t\t * avoid the \"if (gp_state == GP_IDLE)\" block below.",
            "\t\t *",
            "\t\t * However, synchronize_rcu() can be faster if rcu_expedited",
            "\t\t * or rcu_blocking_is_gp() is true.",
            "\t\t *",
            "\t\t * Another reason is that we can't wait for rcu callback if",
            "\t\t * we are called at early boot time but this shouldn't happen.",
            "\t\t */",
            "\t}",
            "\trsp->gp_count++;",
            "\tspin_unlock_irq(&rsp->rss_lock);",
            "",
            "\tif (gp_state == GP_IDLE) {",
            "\t\t/*",
            "\t\t * See the comment above, this simply does the \"synchronous\"",
            "\t\t * call_rcu(rcu_sync_func) which does GP_ENTER -> GP_PASSED.",
            "\t\t */",
            "\t\tsynchronize_rcu();",
            "\t\trcu_sync_func(&rsp->cb_head);",
            "\t\t/* Not really needed, wait_event() would see GP_PASSED. */",
            "\t\treturn;",
            "\t}",
            "",
            "\twait_event(rsp->gp_wait, READ_ONCE(rsp->gp_state) >= GP_PASSED);",
            "}",
            "void rcu_sync_exit(struct rcu_sync *rsp)",
            "{",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_IDLE);",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_count) == 0);",
            "",
            "\tspin_lock_irq(&rsp->rss_lock);",
            "\tif (!--rsp->gp_count) {",
            "\t\tif (rsp->gp_state == GP_PASSED) {",
            "\t\t\tWRITE_ONCE(rsp->gp_state, GP_EXIT);",
            "\t\t\trcu_sync_call(rsp);",
            "\t\t} else if (rsp->gp_state == GP_EXIT) {",
            "\t\t\tWRITE_ONCE(rsp->gp_state, GP_REPLAY);",
            "\t\t}",
            "\t}",
            "\tspin_unlock_irq(&rsp->rss_lock);",
            "}",
            "void rcu_sync_dtor(struct rcu_sync *rsp)",
            "{",
            "\tint gp_state;",
            "",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_count));",
            "\tWARN_ON_ONCE(READ_ONCE(rsp->gp_state) == GP_PASSED);",
            "",
            "\tspin_lock_irq(&rsp->rss_lock);",
            "\tif (rsp->gp_state == GP_REPLAY)",
            "\t\tWRITE_ONCE(rsp->gp_state, GP_EXIT);",
            "\tgp_state = rsp->gp_state;",
            "\tspin_unlock_irq(&rsp->rss_lock);",
            "",
            "\tif (gp_state != GP_IDLE) {",
            "\t\trcu_barrier();",
            "\t\tWARN_ON_ONCE(rsp->gp_state != GP_IDLE);",
            "\t}",
            "}"
          ],
          "function_name": "rcu_sync_init, rcu_sync_enter_start, rcu_sync_call, rcu_sync_func, rcu_sync_enter, rcu_sync_exit, rcu_sync_dtor",
          "description": "实现了RCU同步核心函数，包括初始化、状态管理、回调触发和退出处理，通过spinlock保护状态机并利用RCU回调实现延迟同步，用于协调读者-写者并发访问的安全转换",
          "similarity": 0.6511929631233215
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/rcu/sync.c",
          "start_line": 1,
          "end_line": 20,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0+",
            "/*",
            " * RCU-based infrastructure for lightweight reader-writer locking",
            " *",
            " * Copyright (c) 2015, Red Hat, Inc.",
            " *",
            " * Author: Oleg Nesterov <oleg@redhat.com>",
            " */",
            "",
            "#include <linux/rcu_sync.h>",
            "#include <linux/sched.h>",
            "",
            "enum { GP_IDLE = 0, GP_ENTER, GP_PASSED, GP_EXIT, GP_REPLAY };",
            "",
            "#define\trss_lock\tgp_wait.lock",
            "",
            "/**",
            " * rcu_sync_init() - Initialize an rcu_sync structure",
            " * @rsp: Pointer to rcu_sync structure to be initialized",
            " */"
          ],
          "function_name": null,
          "description": "定义了RCU同步基础设施的枚举常量和rcu_sync_init函数声明，用于初始化rcu_sync结构体，但代码上下文不完整",
          "similarity": 0.5536758899688721
        }
      ]
    },
    {
      "source_file": "kernel/padata.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:13:27\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `padata.c`\n\n---\n\n# padata.c 技术文档\n\n## 文件概述\n\n`padata.c` 实现了 Linux 内核中的 **padata**（parallel data）框架，提供了一个通用接口，用于在多个 CPU 上并行处理数据流，同时保证结果按照原始顺序串行化输出。该机制常用于需要高吞吐量并行计算但又要求结果有序的场景（如加密、压缩等）。padata 框架通过将任务分发到多个 CPU 并行执行，并在专用的串行化 CPU 上按序回调，实现了并行与有序的统一。\n\n## 核心功能\n\n### 主要数据结构\n\n- `struct padata_work`：封装工作项，用于将并行任务提交到工作队列。\n- `struct padata_mt_job_state`：用于多线程作业（multi-threaded job）的状态管理，包含完成通知、工作计数和数据块大小。\n- `struct parallel_data`：并行数据上下文，包含 CPU 掩码、序列号、重排序队列等关键状态。\n- `struct padata_priv`：用户传递的任务私有数据结构，必须包含 `.parallel` 和 `.serial` 回调函数。\n\n### 主要函数\n\n- `padata_do_parallel()`：核心入口函数，将任务分发到并行工作队列。\n- `padata_parallel_worker()`：并行工作线程执行函数，调用用户提供的 `.parallel` 回调。\n- `padata_find_next()`：在重排序队列中查找下一个应被串行化的任务。\n- `padata_cpu_hash()`：根据序列号将任务哈希到特定的并行 CPU。\n- `padata_index_to_cpu()`：将逻辑 CPU 索引映射到实际的 CPU ID。\n- `padata_work_alloc()` / `padata_work_free()`：管理工作项的分配与回收。\n- `padata_work_alloc_mt()` / `padata_works_free()`：用于多线程作业的批量工作项管理。\n\n## 关键实现\n\n### 1. 并行-串行模型\npadata 采用“并行执行 + 有序串行化”模型：\n- **并行阶段**：任务通过 `padata_do_parallel()` 分发到 `parallel_wq` 工作队列，在 `pd->cpumask.pcpu` 指定的 CPU 上并行执行（BH 关闭）。\n- **串行阶段**：任务完成后进入 per-CPU 重排序队列，由 `padata_reorder()` 机制按 `seq_nr` 顺序触发 `.serial` 回调，回调在 `pd->cpumask.cbcpu` 指定的 CPU 上执行。\n\n### 2. 顺序保证机制\n- 每个任务分配唯一递增的 `seq_nr`。\n- 任务完成后按 `seq_nr % weight(pcpu_mask)` 哈希到特定 CPU 的重排序队列。\n- `padata_find_next()` 仅当 `seq_nr == pd->processed` 时才取出任务，确保严格 FIFO 顺序。\n\n### 3. 工作项管理\n- 全局预分配 `padata_work` 对象池（`padata_free_works` 链表），避免运行时内存分配。\n- 使用自旋锁 `padata_works_lock` 保护工作项分配/回收，支持 BH 上下文安全。\n\n### 4. CPU 掩码处理\n- 支持动态 CPU 掩码（`pcpu` 用于并行，`cbcpu` 用于串行回调）。\n- 若用户指定的 `cb_cpu` 不在 `cbcpu` 掩码中，自动选择回退 CPU（通过模运算和 `cpumask_next`）。\n\n### 5. 引用计数与生命周期\n- `parallel_data` 使用 `refcount_t` 管理生命周期，`padata_get_pd()` / `padata_put_pd()` 控制引用。\n- RCU 保护 `pd` 指针的读取（`rcu_dereference_bh`），确保并发安全。\n\n## 依赖关系\n\n- **内核子系统**：\n  - `workqueue`：依赖内核工作队列机制执行并行任务。\n  - `RCU`：用于 `parallel_data` 结构的并发读取保护。\n  - `percpu`：使用 per-CPU 变量存储重排序队列（`reorder_list`）。\n  - `sysfs`：支持通过 sysfs 接口配置 padata 实例（未在片段中体现，但头文件包含）。\n- **头文件依赖**：\n  - `<linux/padata.h>`：定义用户接口和核心结构。\n  - `<linux/completion.h>`、`<linux/cpumask.h>`、`<linux/rcupdate.h>` 等提供基础功能。\n\n## 使用场景\n\n1. **加密子系统**：如 IPsec 或 dm-crypt 使用 padata 并行处理多个数据块的加密/解密，同时保证输出顺序。\n2. **压缩/解压缩**：在需要高吞吐量的压缩场景中并行处理数据流。\n3. **批量数据处理**：任何需要将大数据集分片并行处理，但要求结果按输入顺序交付的内核模块。\n4. **多线程作业辅助**：通过 `padata_work_alloc_mt()` 接口，辅助实现内核态多线程任务分发（如大内存初始化）。\n\n> **注意**：所有通过 `padata_do_parallel()` 提交的任务**必须**调用 `padata_do_serial()` 完成串行化阶段，否则会导致资源泄漏和顺序错乱。",
      "similarity": 0.5911160707473755,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/padata.c",
          "start_line": 50,
          "end_line": 203,
          "content": [
            "static inline void padata_get_pd(struct parallel_data *pd)",
            "{",
            "\trefcount_inc(&pd->refcnt);",
            "}",
            "static inline void padata_put_pd_cnt(struct parallel_data *pd, int cnt)",
            "{",
            "\tif (refcount_sub_and_test(cnt, &pd->refcnt))",
            "\t\tpadata_free_pd(pd);",
            "}",
            "static inline void padata_put_pd(struct parallel_data *pd)",
            "{",
            "\tpadata_put_pd_cnt(pd, 1);",
            "}",
            "static int padata_index_to_cpu(struct parallel_data *pd, int cpu_index)",
            "{",
            "\tint cpu, target_cpu;",
            "",
            "\ttarget_cpu = cpumask_first(pd->cpumask.pcpu);",
            "\tfor (cpu = 0; cpu < cpu_index; cpu++)",
            "\t\ttarget_cpu = cpumask_next(target_cpu, pd->cpumask.pcpu);",
            "",
            "\treturn target_cpu;",
            "}",
            "static int padata_cpu_hash(struct parallel_data *pd, unsigned int seq_nr)",
            "{",
            "\t/*",
            "\t * Hash the sequence numbers to the cpus by taking",
            "\t * seq_nr mod. number of cpus in use.",
            "\t */",
            "\tint cpu_index = seq_nr % cpumask_weight(pd->cpumask.pcpu);",
            "",
            "\treturn padata_index_to_cpu(pd, cpu_index);",
            "}",
            "static void __ref padata_work_init(struct padata_work *pw, work_func_t work_fn,",
            "\t\t\t\t   void *data, int flags)",
            "{",
            "\tif (flags & PADATA_WORK_ONSTACK)",
            "\t\tINIT_WORK_ONSTACK(&pw->pw_work, work_fn);",
            "\telse",
            "\t\tINIT_WORK(&pw->pw_work, work_fn);",
            "\tpw->pw_data = data;",
            "}",
            "static int __init padata_work_alloc_mt(int nworks, void *data,",
            "\t\t\t\t       struct list_head *head)",
            "{",
            "\tint i;",
            "",
            "\tspin_lock_bh(&padata_works_lock);",
            "\t/* Start at 1 because the current task participates in the job. */",
            "\tfor (i = 1; i < nworks; ++i) {",
            "\t\tstruct padata_work *pw = padata_work_alloc();",
            "",
            "\t\tif (!pw)",
            "\t\t\tbreak;",
            "\t\tpadata_work_init(pw, padata_mt_helper, data, 0);",
            "\t\tlist_add(&pw->pw_list, head);",
            "\t}",
            "\tspin_unlock_bh(&padata_works_lock);",
            "",
            "\treturn i;",
            "}",
            "static void padata_work_free(struct padata_work *pw)",
            "{",
            "\tlockdep_assert_held(&padata_works_lock);",
            "\tlist_add(&pw->pw_list, &padata_free_works);",
            "}",
            "static void __init padata_works_free(struct list_head *works)",
            "{",
            "\tstruct padata_work *cur, *next;",
            "",
            "\tif (list_empty(works))",
            "\t\treturn;",
            "",
            "\tspin_lock_bh(&padata_works_lock);",
            "\tlist_for_each_entry_safe(cur, next, works, pw_list) {",
            "\t\tlist_del(&cur->pw_list);",
            "\t\tpadata_work_free(cur);",
            "\t}",
            "\tspin_unlock_bh(&padata_works_lock);",
            "}",
            "static void padata_parallel_worker(struct work_struct *parallel_work)",
            "{",
            "\tstruct padata_work *pw = container_of(parallel_work, struct padata_work,",
            "\t\t\t\t\t      pw_work);",
            "\tstruct padata_priv *padata = pw->pw_data;",
            "",
            "\tlocal_bh_disable();",
            "\tpadata->parallel(padata);",
            "\tspin_lock(&padata_works_lock);",
            "\tpadata_work_free(pw);",
            "\tspin_unlock(&padata_works_lock);",
            "\tlocal_bh_enable();",
            "}",
            "int padata_do_parallel(struct padata_shell *ps,",
            "\t\t       struct padata_priv *padata, int *cb_cpu)",
            "{",
            "\tstruct padata_instance *pinst = ps->pinst;",
            "\tint i, cpu, cpu_index, err;",
            "\tstruct parallel_data *pd;",
            "\tstruct padata_work *pw;",
            "",
            "\trcu_read_lock_bh();",
            "",
            "\tpd = rcu_dereference_bh(ps->pd);",
            "",
            "\terr = -EINVAL;",
            "\tif (!(pinst->flags & PADATA_INIT) || pinst->flags & PADATA_INVALID)",
            "\t\tgoto out;",
            "",
            "\tif (!cpumask_test_cpu(*cb_cpu, pd->cpumask.cbcpu)) {",
            "\t\tif (cpumask_empty(pd->cpumask.cbcpu))",
            "\t\t\tgoto out;",
            "",
            "\t\t/* Select an alternate fallback CPU and notify the caller. */",
            "\t\tcpu_index = *cb_cpu % cpumask_weight(pd->cpumask.cbcpu);",
            "",
            "\t\tcpu = cpumask_first(pd->cpumask.cbcpu);",
            "\t\tfor (i = 0; i < cpu_index; i++)",
            "\t\t\tcpu = cpumask_next(cpu, pd->cpumask.cbcpu);",
            "",
            "\t\t*cb_cpu = cpu;",
            "\t}",
            "",
            "\terr = -EBUSY;",
            "\tif ((pinst->flags & PADATA_RESET))",
            "\t\tgoto out;",
            "",
            "\tpadata_get_pd(pd);",
            "\tpadata->pd = pd;",
            "\tpadata->cb_cpu = *cb_cpu;",
            "",
            "\tspin_lock(&padata_works_lock);",
            "\tpadata->seq_nr = ++pd->seq_nr;",
            "\tpw = padata_work_alloc();",
            "\tspin_unlock(&padata_works_lock);",
            "",
            "\tif (!pw) {",
            "\t\t/* Maximum works limit exceeded, run in the current task. */",
            "\t\tpadata->parallel(padata);",
            "\t}",
            "",
            "\trcu_read_unlock_bh();",
            "",
            "\tif (pw) {",
            "\t\tpadata_work_init(pw, padata_parallel_worker, padata, 0);",
            "\t\tqueue_work(pinst->parallel_wq, &pw->pw_work);",
            "\t}",
            "",
            "\treturn 0;",
            "out:",
            "\trcu_read_unlock_bh();",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "padata_get_pd, padata_put_pd_cnt, padata_put_pd, padata_index_to_cpu, padata_cpu_hash, padata_work_init, padata_work_alloc_mt, padata_work_free, padata_works_free, padata_parallel_worker, padata_do_parallel",
          "description": "提供并行数据引用计数、CPU映射算法、工作项初始化与分配逻辑，以及并行任务执行流程",
          "similarity": 0.6601133346557617
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/padata.c",
          "start_line": 296,
          "end_line": 402,
          "content": [
            "static void padata_reorder(struct padata_priv *padata)",
            "{",
            "\tstruct parallel_data *pd = padata->pd;",
            "\tstruct padata_instance *pinst = pd->ps->pinst;",
            "\tunsigned int processed;",
            "\tint cpu;",
            "",
            "\tprocessed = pd->processed;",
            "\tcpu = pd->cpu;",
            "",
            "\tdo {",
            "\t\tstruct padata_serial_queue *squeue;",
            "\t\tint cb_cpu;",
            "",
            "\t\tcpu = cpumask_next_wrap(cpu, pd->cpumask.pcpu, -1, false);",
            "\t\tprocessed++;",
            "",
            "\t\tcb_cpu = padata->cb_cpu;",
            "\t\tsqueue = per_cpu_ptr(pd->squeue, cb_cpu);",
            "",
            "\t\tspin_lock(&squeue->serial.lock);",
            "\t\tlist_add_tail(&padata->list, &squeue->serial.list);",
            "\t\tqueue_work_on(cb_cpu, pinst->serial_wq, &squeue->work);",
            "",
            "\t\t/*",
            "\t\t * If the next object that needs serialization is parallel",
            "\t\t * processed by another cpu and is still on it's way to the",
            "\t\t * cpu's reorder queue, end the loop.",
            "\t\t */",
            "\t\tpadata = padata_find_next(pd, cpu, processed);",
            "\t\tspin_unlock(&squeue->serial.lock);",
            "\t} while (padata);",
            "}",
            "static void padata_serial_worker(struct work_struct *serial_work)",
            "{",
            "\tstruct padata_serial_queue *squeue;",
            "\tstruct parallel_data *pd;",
            "\tLIST_HEAD(local_list);",
            "\tint cnt;",
            "",
            "\tlocal_bh_disable();",
            "\tsqueue = container_of(serial_work, struct padata_serial_queue, work);",
            "\tpd = squeue->pd;",
            "",
            "\tspin_lock(&squeue->serial.lock);",
            "\tlist_replace_init(&squeue->serial.list, &local_list);",
            "\tspin_unlock(&squeue->serial.lock);",
            "",
            "\tcnt = 0;",
            "",
            "\twhile (!list_empty(&local_list)) {",
            "\t\tstruct padata_priv *padata;",
            "",
            "\t\tpadata = list_entry(local_list.next,",
            "\t\t\t\t    struct padata_priv, list);",
            "",
            "\t\tlist_del_init(&padata->list);",
            "",
            "\t\tpadata->serial(padata);",
            "\t\tcnt++;",
            "\t}",
            "\tlocal_bh_enable();",
            "",
            "\tpadata_put_pd_cnt(pd, cnt);",
            "}",
            "void padata_do_serial(struct padata_priv *padata)",
            "{",
            "\tstruct parallel_data *pd = padata->pd;",
            "\tint hashed_cpu = padata_cpu_hash(pd, padata->seq_nr);",
            "\tstruct padata_list *reorder = per_cpu_ptr(pd->reorder_list, hashed_cpu);",
            "\tstruct padata_priv *cur;",
            "\tstruct list_head *pos;",
            "\tbool gotit = true;",
            "",
            "\tspin_lock(&reorder->lock);",
            "\t/* Sort in ascending order of sequence number. */",
            "\tlist_for_each_prev(pos, &reorder->list) {",
            "\t\tcur = list_entry(pos, struct padata_priv, list);",
            "\t\t/* Compare by difference to consider integer wrap around */",
            "\t\tif ((signed int)(cur->seq_nr - padata->seq_nr) < 0)",
            "\t\t\tbreak;",
            "\t}",
            "\tif (padata->seq_nr != pd->processed) {",
            "\t\tgotit = false;",
            "\t\tlist_add(&padata->list, pos);",
            "\t}",
            "\tspin_unlock(&reorder->lock);",
            "",
            "\tif (gotit)",
            "\t\tpadata_reorder(padata);",
            "}",
            "static int padata_setup_cpumasks(struct padata_instance *pinst)",
            "{",
            "\tstruct workqueue_attrs *attrs;",
            "\tint err;",
            "",
            "\tattrs = alloc_workqueue_attrs();",
            "\tif (!attrs)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/* Restrict parallel_wq workers to pd->cpumask.pcpu. */",
            "\tcpumask_copy(attrs->cpumask, pinst->cpumask.pcpu);",
            "\terr = apply_workqueue_attrs(pinst->parallel_wq, attrs);",
            "\tfree_workqueue_attrs(attrs);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "padata_reorder, padata_serial_worker, padata_do_serial, padata_setup_cpumasks",
          "description": "实现串行化队列的排序插入、工作项处理函数及CPU掩码配置功能",
          "similarity": 0.5655077695846558
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/padata.c",
          "start_line": 539,
          "end_line": 643,
          "content": [
            "static void padata_init_reorder_list(struct parallel_data *pd)",
            "{",
            "\tint cpu;",
            "\tstruct padata_list *list;",
            "",
            "\tfor_each_cpu(cpu, pd->cpumask.pcpu) {",
            "\t\tlist = per_cpu_ptr(pd->reorder_list, cpu);",
            "\t\t__padata_list_init(list);",
            "\t}",
            "}",
            "static void padata_free_pd(struct parallel_data *pd)",
            "{",
            "\tfree_cpumask_var(pd->cpumask.pcpu);",
            "\tfree_cpumask_var(pd->cpumask.cbcpu);",
            "\tfree_percpu(pd->reorder_list);",
            "\tfree_percpu(pd->squeue);",
            "\tkfree(pd);",
            "}",
            "static void __padata_start(struct padata_instance *pinst)",
            "{",
            "\tpinst->flags |= PADATA_INIT;",
            "}",
            "static void __padata_stop(struct padata_instance *pinst)",
            "{",
            "\tif (!(pinst->flags & PADATA_INIT))",
            "\t\treturn;",
            "",
            "\tpinst->flags &= ~PADATA_INIT;",
            "",
            "\tsynchronize_rcu();",
            "}",
            "static int padata_replace_one(struct padata_shell *ps)",
            "{",
            "\tstruct parallel_data *pd_new;",
            "",
            "\tpd_new = padata_alloc_pd(ps);",
            "\tif (!pd_new)",
            "\t\treturn -ENOMEM;",
            "",
            "\tps->opd = rcu_dereference_protected(ps->pd, 1);",
            "\trcu_assign_pointer(ps->pd, pd_new);",
            "",
            "\treturn 0;",
            "}",
            "static int padata_replace(struct padata_instance *pinst)",
            "{",
            "\tstruct padata_shell *ps;",
            "\tint err = 0;",
            "",
            "\tpinst->flags |= PADATA_RESET;",
            "",
            "\tlist_for_each_entry(ps, &pinst->pslist, list) {",
            "\t\terr = padata_replace_one(ps);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tsynchronize_rcu();",
            "",
            "\tlist_for_each_entry_continue_reverse(ps, &pinst->pslist, list)",
            "\t\tpadata_put_pd(ps->opd);",
            "",
            "\tpinst->flags &= ~PADATA_RESET;",
            "",
            "\treturn err;",
            "}",
            "static bool padata_validate_cpumask(struct padata_instance *pinst,",
            "\t\t\t\t    const struct cpumask *cpumask)",
            "{",
            "\tif (!cpumask_intersects(cpumask, cpu_online_mask)) {",
            "\t\tpinst->flags |= PADATA_INVALID;",
            "\t\treturn false;",
            "\t}",
            "",
            "\tpinst->flags &= ~PADATA_INVALID;",
            "\treturn true;",
            "}",
            "static int __padata_set_cpumasks(struct padata_instance *pinst,",
            "\t\t\t\t cpumask_var_t pcpumask,",
            "\t\t\t\t cpumask_var_t cbcpumask)",
            "{",
            "\tint valid;",
            "\tint err;",
            "",
            "\tvalid = padata_validate_cpumask(pinst, pcpumask);",
            "\tif (!valid) {",
            "\t\t__padata_stop(pinst);",
            "\t\tgoto out_replace;",
            "\t}",
            "",
            "\tvalid = padata_validate_cpumask(pinst, cbcpumask);",
            "\tif (!valid)",
            "\t\t__padata_stop(pinst);",
            "",
            "out_replace:",
            "\tcpumask_copy(pinst->cpumask.pcpu, pcpumask);",
            "\tcpumask_copy(pinst->cpumask.cbcpu, cbcpumask);",
            "",
            "\terr = padata_setup_cpumasks(pinst) ?: padata_replace(pinst);",
            "",
            "\tif (valid)",
            "\t\t__padata_start(pinst);",
            "",
            "\treturn err;",
            "}"
          ],
          "function_name": "padata_init_reorder_list, padata_free_pd, __padata_start, __padata_stop, padata_replace_one, padata_replace, padata_validate_cpumask, __padata_set_cpumasks",
          "description": "初始化与释放重排列表资源，实现CPU掩码动态配置及有效性校验机制",
          "similarity": 0.5538628697395325
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/padata.c",
          "start_line": 1,
          "end_line": 49,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * padata.c - generic interface to process data streams in parallel",
            " *",
            " * See Documentation/core-api/padata.rst for more information.",
            " *",
            " * Copyright (C) 2008, 2009 secunet Security Networks AG",
            " * Copyright (C) 2008, 2009 Steffen Klassert <steffen.klassert@secunet.com>",
            " *",
            " * Copyright (c) 2020 Oracle and/or its affiliates.",
            " * Author: Daniel Jordan <daniel.m.jordan@oracle.com>",
            " */",
            "",
            "#include <linux/completion.h>",
            "#include <linux/export.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/err.h>",
            "#include <linux/cpu.h>",
            "#include <linux/padata.h>",
            "#include <linux/mutex.h>",
            "#include <linux/sched.h>",
            "#include <linux/slab.h>",
            "#include <linux/sysfs.h>",
            "#include <linux/rcupdate.h>",
            "",
            "#define\tPADATA_WORK_ONSTACK\t1\t/* Work's memory is on stack */",
            "",
            "struct padata_work {",
            "\tstruct work_struct\tpw_work;",
            "\tstruct list_head\tpw_list;  /* padata_free_works linkage */",
            "\tvoid\t\t\t*pw_data;",
            "};",
            "",
            "static DEFINE_SPINLOCK(padata_works_lock);",
            "static struct padata_work *padata_works;",
            "static LIST_HEAD(padata_free_works);",
            "",
            "struct padata_mt_job_state {",
            "\tspinlock_t\t\tlock;",
            "\tstruct completion\tcompletion;",
            "\tstruct padata_mt_job\t*job;",
            "\tint\t\t\tnworks;",
            "\tint\t\t\tnworks_fini;",
            "\tunsigned long\t\tchunk_size;",
            "};",
            "",
            "static void padata_free_pd(struct parallel_data *pd);",
            "static void __init padata_mt_helper(struct work_struct *work);",
            ""
          ],
          "function_name": null,
          "description": "定义并行处理工作的数据结构和全局资源，包括工作项结构体、锁及用于管理工作项的链表",
          "similarity": 0.5502939820289612
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/padata.c",
          "start_line": 416,
          "end_line": 526,
          "content": [
            "static void __init padata_mt_helper(struct work_struct *w)",
            "{",
            "\tstruct padata_work *pw = container_of(w, struct padata_work, pw_work);",
            "\tstruct padata_mt_job_state *ps = pw->pw_data;",
            "\tstruct padata_mt_job *job = ps->job;",
            "\tbool done;",
            "",
            "\tspin_lock(&ps->lock);",
            "",
            "\twhile (job->size > 0) {",
            "\t\tunsigned long start, size, end;",
            "",
            "\t\tstart = job->start;",
            "\t\t/* So end is chunk size aligned if enough work remains. */",
            "\t\tsize = roundup(start + 1, ps->chunk_size) - start;",
            "\t\tsize = min(size, job->size);",
            "\t\tend = start + size;",
            "",
            "\t\tjob->start = end;",
            "\t\tjob->size -= size;",
            "",
            "\t\tspin_unlock(&ps->lock);",
            "\t\tjob->thread_fn(start, end, job->fn_arg);",
            "\t\tspin_lock(&ps->lock);",
            "\t}",
            "",
            "\t++ps->nworks_fini;",
            "\tdone = (ps->nworks_fini == ps->nworks);",
            "\tspin_unlock(&ps->lock);",
            "",
            "\tif (done)",
            "\t\tcomplete(&ps->completion);",
            "}",
            "void __init padata_do_multithreaded(struct padata_mt_job *job)",
            "{",
            "\t/* In case threads finish at different times. */",
            "\tstatic const unsigned long load_balance_factor = 4;",
            "\tstruct padata_work my_work, *pw;",
            "\tstruct padata_mt_job_state ps;",
            "\tLIST_HEAD(works);",
            "\tint nworks;",
            "",
            "\tif (job->size == 0)",
            "\t\treturn;",
            "",
            "\t/* Ensure at least one thread when size < min_chunk. */",
            "\tnworks = max(job->size / max(job->min_chunk, job->align), 1ul);",
            "\tnworks = min(nworks, job->max_threads);",
            "",
            "\tif (nworks == 1) {",
            "\t\t/* Single thread, no coordination needed, cut to the chase. */",
            "\t\tjob->thread_fn(job->start, job->start + job->size, job->fn_arg);",
            "\t\treturn;",
            "\t}",
            "",
            "\tspin_lock_init(&ps.lock);",
            "\tinit_completion(&ps.completion);",
            "\tps.job\t       = job;",
            "\tps.nworks      = padata_work_alloc_mt(nworks, &ps, &works);",
            "\tps.nworks_fini = 0;",
            "",
            "\t/*",
            "\t * Chunk size is the amount of work a helper does per call to the",
            "\t * thread function.  Load balance large jobs between threads by",
            "\t * increasing the number of chunks, guarantee at least the minimum",
            "\t * chunk size from the caller, and honor the caller's alignment.",
            "\t * Ensure chunk_size is at least 1 to prevent divide-by-0",
            "\t * panic in padata_mt_helper().",
            "\t */",
            "\tps.chunk_size = job->size / (ps.nworks * load_balance_factor);",
            "\tps.chunk_size = max(ps.chunk_size, job->min_chunk);",
            "\tps.chunk_size = max(ps.chunk_size, 1ul);",
            "\tps.chunk_size = roundup(ps.chunk_size, job->align);",
            "",
            "\t/*",
            "\t * chunk_size can be 0 if the caller sets min_chunk to 0. So force it",
            "\t * to at least 1 to prevent divide-by-0 panic in padata_mt_helper().`",
            "\t */",
            "\tif (!ps.chunk_size)",
            "\t\tps.chunk_size = 1U;",
            "",
            "\tlist_for_each_entry(pw, &works, pw_list)",
            "\t\tqueue_work(system_unbound_wq, &pw->pw_work);",
            "",
            "\t/* Use the current thread, which saves starting a workqueue worker. */",
            "\tpadata_work_init(&my_work, padata_mt_helper, &ps, PADATA_WORK_ONSTACK);",
            "\tpadata_mt_helper(&my_work.pw_work);",
            "",
            "\t/* Wait for all the helpers to finish. */",
            "\twait_for_completion(&ps.completion);",
            "",
            "\tdestroy_work_on_stack(&my_work.pw_work);",
            "\tpadata_works_free(&works);",
            "}",
            "static void __padata_list_init(struct padata_list *pd_list)",
            "{",
            "\tINIT_LIST_HEAD(&pd_list->list);",
            "\tspin_lock_init(&pd_list->lock);",
            "}",
            "static void padata_init_squeues(struct parallel_data *pd)",
            "{",
            "\tint cpu;",
            "\tstruct padata_serial_queue *squeue;",
            "",
            "\tfor_each_cpu(cpu, pd->cpumask.cbcpu) {",
            "\t\tsqueue = per_cpu_ptr(pd->squeue, cpu);",
            "\t\tsqueue->pd = pd;",
            "\t\t__padata_list_init(&squeue->serial);",
            "\t\tINIT_WORK(&squeue->work, padata_serial_worker);",
            "\t}",
            "}"
          ],
          "function_name": "padata_mt_helper, padata_do_multithreaded, __padata_list_init, padata_init_squeues",
          "description": "支持多线程任务分片处理，通过工作队列分发任务并协调多线程执行",
          "similarity": 0.5403096675872803
        }
      ]
    }
  ]
}