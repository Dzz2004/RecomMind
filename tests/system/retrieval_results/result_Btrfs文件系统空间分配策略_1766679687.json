{
  "query": "Btrfs文件系统空间分配策略",
  "timestamp": "2025-12-26 00:21:27",
  "retrieved_files": [
    {
      "source_file": "kernel/bpf/bpf_local_storage.c",
      "md_summary": "> 自动生成时间: 2025-10-25 11:58:54\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\bpf_local_storage.c`\n\n---\n\n# bpf_local_storage.c 技术文档\n\n## 文件概述\n\n`bpf_local_storage.c` 实现了 BPF（Berkeley Packet Filter）本地存储（local storage）机制的核心功能。该机制允许 BPF 程序为内核对象（如 socket、task 等）动态关联私有数据，实现高效、安全的 per-object 存储管理。文件提供了存储元素（`bpf_local_storage_elem`）和存储容器（`bpf_local_storage`）的分配、释放、链接管理以及内存回收机制，并支持与 BPF 内存分配器（`bpf_mem_alloc`）集成以提升性能。\n\n## 核心功能\n\n### 主要数据结构\n- `struct bpf_local_storage_elem`：BPF 本地存储元素，包含指向 map、owner 和实际数据的指针\n- `struct bpf_local_storage`：BPF 本地存储容器，用于管理特定 owner 的所有存储元素\n- `struct bpf_local_storage_map`：扩展的 BPF map 类型，用于管理本地存储\n\n### 主要函数\n- `bpf_selem_alloc()`：分配并初始化 BPF 本地存储元素\n- `bpf_selem_free()`：释放 BPF 本地存储元素\n- `bpf_local_storage_free()`：释放 BPF 本地存储容器\n- `select_bucket()`：根据存储元素选择哈希桶\n- `mem_charge()` / `mem_uncharge()`：内存资源计费/释放\n- `owner_storage()`：获取 owner 对象的存储指针\n- `selem_linked_to_*()`：检查存储元素的链接状态\n\n## 关键实现\n\n### 内存管理策略\n- 支持两种内存分配模式：传统 `bpf_map_kzalloc` 和高性能 `bpf_mem_cache_alloc`\n- 通过 `smap->bpf_ma` 标志区分是否使用 BPF 内存分配器\n- 实现了精细的内存计费机制，通过 `map_local_storage_charge/uncharge` 回调\n\n### RCU 安全回收机制\n- 针对不同场景实现多种 RCU 回收策略：\n  - 普通 RCU (`call_rcu`)\n  - RCU Tasks Trace (`call_rcu_tasks_trace`)\n  - 直接释放（`reuse_now` 模式）\n- 根据 `rcu_trace_implies_rcu_gp()` 动态选择合适的释放方式\n- 支持立即重用模式（`reuse_now`），在对象销毁时直接回收内存\n\n### 存储元素管理\n- 使用哈希表组织存储元素，通过 `hash_ptr()` 计算桶位置\n- 通过 `hlist_unhashed()` 检查元素是否已链接到存储结构\n- 实现了安全的双向链接管理（map 链和 storage 链）\n\n### 对象生命周期管理\n- 在分配时支持值初始化和 uptr 交换\n- 释放时正确处理 BPF 对象字段的清理（`bpf_obj_free_fields`）\n- 支持克隆操作（通过 `BPF_F_CLONE` 标志）\n\n## 依赖关系\n\n### 内核头文件依赖\n- **RCU 相关**：`<linux/rculist.h>`, `<linux/rcupdate.h>`, `<linux/rcupdate_trace.h>`\n- **数据结构**：`<linux/list.h>`, `<linux/hash.h>`, `<linux/spinlock.h>`\n- **BPF 核心**：`<linux/bpf.h>`, `<linux/bpf_local_storage.h>`, `<linux/btf_ids.h>`\n- **网络子系统**：`<net/sock.h>`, `<uapi/linux/sock_diag.h>`\n- **内存管理**：`<linux/bpf_mem_alloc.h>`（隐式通过 bpf_mem_cache_* 函数）\n\n### 功能依赖\n- 依赖 BPF map 操作接口（`map->ops->map_owner_storage_ptr` 等）\n- 依赖 BPF 内存分配器（`bpf_mem_cache_*` 系列函数）\n- 依赖 BPF 对象引用管理（`bpf_obj_free_fields`, `bpf_obj_swap_uptrs`）\n\n## 使用场景\n\n### BPF 程序数据存储\n- BPF 程序需要为特定内核对象（如 socket、task）维护私有状态信息\n- 通过 `bpf_sk_storage_get()` 等 helper 函数访问本地存储\n\n### 网络监控和跟踪\n- Socket 本地存储用于网络连接的 per-socket 状态跟踪\n- Task 本地存储用于进程级别的监控和策略实施\n\n### 性能关键路径\n- 在高性能网络数据路径中，通过预分配和内存池减少分配开销\n- 利用 RCU Tasks Trace 实现低延迟的内存回收\n\n### 资源隔离和计费\n- 为不同 BPF map 实例提供独立的内存计费\n- 防止恶意 BPF 程序耗尽系统内存资源\n\n### 对象生命周期集成\n- 与内核对象（socket、task）的销毁流程紧密集成\n- 确保在对象销毁时正确清理关联的 BPF 存储数据",
      "similarity": 0.5789237022399902,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/bpf_local_storage.c",
          "start_line": 1,
          "end_line": 26,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/* Copyright (c) 2019 Facebook  */",
            "#include <linux/rculist.h>",
            "#include <linux/list.h>",
            "#include <linux/hash.h>",
            "#include <linux/types.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/bpf.h>",
            "#include <linux/btf_ids.h>",
            "#include <linux/bpf_local_storage.h>",
            "#include <net/sock.h>",
            "#include <uapi/linux/sock_diag.h>",
            "#include <uapi/linux/btf.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/rcupdate_trace.h>",
            "#include <linux/rcupdate_wait.h>",
            "",
            "#define BPF_LOCAL_STORAGE_CREATE_FLAG_MASK (BPF_F_NO_PREALLOC | BPF_F_CLONE)",
            "",
            "static struct bpf_local_storage_map_bucket *",
            "select_bucket(struct bpf_local_storage_map *smap,",
            "\t      struct bpf_local_storage_elem *selem)",
            "{",
            "\treturn &smap->buckets[hash_ptr(selem, smap->bucket_log)];",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义BPF本地存储相关头文件和宏，提供`select_bucket`函数用于根据元素哈希值选择对应的桶结构。",
          "similarity": 0.5530078411102295
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/bpf_local_storage.c",
          "start_line": 730,
          "end_line": 892,
          "content": [
            "static void bpf_local_storage_cache_idx_free(struct bpf_local_storage_cache *cache,",
            "\t\t\t\t\t     u16 idx)",
            "{",
            "\tspin_lock(&cache->idx_lock);",
            "\tcache->idx_usage_counts[idx]--;",
            "\tspin_unlock(&cache->idx_lock);",
            "}",
            "int bpf_local_storage_map_alloc_check(union bpf_attr *attr)",
            "{",
            "\tif (attr->map_flags & ~BPF_LOCAL_STORAGE_CREATE_FLAG_MASK ||",
            "\t    !(attr->map_flags & BPF_F_NO_PREALLOC) ||",
            "\t    attr->max_entries ||",
            "\t    attr->key_size != sizeof(int) || !attr->value_size ||",
            "\t    /* Enforce BTF for userspace sk dumping */",
            "\t    !attr->btf_key_type_id || !attr->btf_value_type_id)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (attr->value_size > BPF_LOCAL_STORAGE_MAX_VALUE_SIZE)",
            "\t\treturn -E2BIG;",
            "",
            "\treturn 0;",
            "}",
            "int bpf_local_storage_map_check_btf(const struct bpf_map *map,",
            "\t\t\t\t    const struct btf *btf,",
            "\t\t\t\t    const struct btf_type *key_type,",
            "\t\t\t\t    const struct btf_type *value_type)",
            "{",
            "\tu32 int_data;",
            "",
            "\tif (BTF_INFO_KIND(key_type->info) != BTF_KIND_INT)",
            "\t\treturn -EINVAL;",
            "",
            "\tint_data = *(u32 *)(key_type + 1);",
            "\tif (BTF_INT_BITS(int_data) != 32 || BTF_INT_OFFSET(int_data))",
            "\t\treturn -EINVAL;",
            "",
            "\treturn 0;",
            "}",
            "void bpf_local_storage_destroy(struct bpf_local_storage *local_storage)",
            "{",
            "\tstruct bpf_local_storage_map *storage_smap;",
            "\tstruct bpf_local_storage_elem *selem;",
            "\tbool bpf_ma, free_storage = false;",
            "\tHLIST_HEAD(free_selem_list);",
            "\tstruct hlist_node *n;",
            "\tunsigned long flags;",
            "",
            "\tstorage_smap = rcu_dereference_check(local_storage->smap, bpf_rcu_lock_held());",
            "\tbpf_ma = check_storage_bpf_ma(local_storage, storage_smap, NULL);",
            "",
            "\t/* Neither the bpf_prog nor the bpf_map's syscall",
            "\t * could be modifying the local_storage->list now.",
            "\t * Thus, no elem can be added to or deleted from the",
            "\t * local_storage->list by the bpf_prog or by the bpf_map's syscall.",
            "\t *",
            "\t * It is racing with bpf_local_storage_map_free() alone",
            "\t * when unlinking elem from the local_storage->list and",
            "\t * the map's bucket->list.",
            "\t */",
            "\traw_spin_lock_irqsave(&local_storage->lock, flags);",
            "\thlist_for_each_entry_safe(selem, n, &local_storage->list, snode) {",
            "\t\t/* Always unlink from map before unlinking from",
            "\t\t * local_storage.",
            "\t\t */",
            "\t\tbpf_selem_unlink_map(selem);",
            "\t\t/* If local_storage list has only one element, the",
            "\t\t * bpf_selem_unlink_storage_nolock() will return true.",
            "\t\t * Otherwise, it will return false. The current loop iteration",
            "\t\t * intends to remove all local storage. So the last iteration",
            "\t\t * of the loop will set the free_cgroup_storage to true.",
            "\t\t */",
            "\t\tfree_storage = bpf_selem_unlink_storage_nolock(",
            "\t\t\tlocal_storage, selem, true, &free_selem_list);",
            "\t}",
            "\traw_spin_unlock_irqrestore(&local_storage->lock, flags);",
            "",
            "\tbpf_selem_free_list(&free_selem_list, true);",
            "",
            "\tif (free_storage)",
            "\t\tbpf_local_storage_free(local_storage, storage_smap, bpf_ma, true);",
            "}",
            "u64 bpf_local_storage_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tstruct bpf_local_storage_map *smap = (struct bpf_local_storage_map *)map;",
            "\tu64 usage = sizeof(*smap);",
            "",
            "\t/* The dynamically callocated selems are not counted currently. */",
            "\tusage += sizeof(*smap->buckets) * (1ULL << smap->bucket_log);",
            "\treturn usage;",
            "}",
            "void bpf_local_storage_map_free(struct bpf_map *map,",
            "\t\t\t\tstruct bpf_local_storage_cache *cache,",
            "\t\t\t\tint __percpu *busy_counter)",
            "{",
            "\tstruct bpf_local_storage_map_bucket *b;",
            "\tstruct bpf_local_storage_elem *selem;",
            "\tstruct bpf_local_storage_map *smap;",
            "\tunsigned int i;",
            "",
            "\tsmap = (struct bpf_local_storage_map *)map;",
            "\tbpf_local_storage_cache_idx_free(cache, smap->cache_idx);",
            "",
            "\t/* Note that this map might be concurrently cloned from",
            "\t * bpf_sk_storage_clone. Wait for any existing bpf_sk_storage_clone",
            "\t * RCU read section to finish before proceeding. New RCU",
            "\t * read sections should be prevented via bpf_map_inc_not_zero.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\t/* bpf prog and the userspace can no longer access this map",
            "\t * now.  No new selem (of this map) can be added",
            "\t * to the owner->storage or to the map bucket's list.",
            "\t *",
            "\t * The elem of this map can be cleaned up here",
            "\t * or when the storage is freed e.g.",
            "\t * by bpf_sk_storage_free() during __sk_destruct().",
            "\t */",
            "\tfor (i = 0; i < (1U << smap->bucket_log); i++) {",
            "\t\tb = &smap->buckets[i];",
            "",
            "\t\trcu_read_lock();",
            "\t\t/* No one is adding to b->list now */",
            "\t\twhile ((selem = hlist_entry_safe(",
            "\t\t\t\trcu_dereference_raw(hlist_first_rcu(&b->list)),",
            "\t\t\t\tstruct bpf_local_storage_elem, map_node))) {",
            "\t\t\tif (busy_counter) {",
            "\t\t\t\tmigrate_disable();",
            "\t\t\t\tthis_cpu_inc(*busy_counter);",
            "\t\t\t}",
            "\t\t\tbpf_selem_unlink(selem, true);",
            "\t\t\tif (busy_counter) {",
            "\t\t\t\tthis_cpu_dec(*busy_counter);",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t}",
            "\t\t\tcond_resched_rcu();",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t}",
            "",
            "\t/* While freeing the storage we may still need to access the map.",
            "\t *",
            "\t * e.g. when bpf_sk_storage_free() has unlinked selem from the map",
            "\t * which then made the above while((selem = ...)) loop",
            "\t * exit immediately.",
            "\t *",
            "\t * However, while freeing the storage one still needs to access the",
            "\t * smap->elem_size to do the uncharging in",
            "\t * bpf_selem_unlink_storage_nolock().",
            "\t *",
            "\t * Hence, wait another rcu grace period for the storage to be freed.",
            "\t */",
            "\tsynchronize_rcu();",
            "",
            "\tif (smap->bpf_ma) {",
            "\t\trcu_barrier_tasks_trace();",
            "\t\tif (!rcu_trace_implies_rcu_gp())",
            "\t\t\trcu_barrier();",
            "\t\tbpf_mem_alloc_destroy(&smap->selem_ma);",
            "\t\tbpf_mem_alloc_destroy(&smap->storage_ma);",
            "\t}",
            "\tkvfree(smap->buckets);",
            "\tbpf_map_area_free(smap);",
            "}"
          ],
          "function_name": "bpf_local_storage_cache_idx_free, bpf_local_storage_map_alloc_check, bpf_local_storage_map_check_btf, bpf_local_storage_destroy, bpf_local_storage_map_mem_usage, bpf_local_storage_map_free",
          "description": "该代码块实现BPF本地存储模块的内存管理和资源释放逻辑，核心功能包含参数校验、BTF类型验证、元素清理及内存统计。  \n`bpf_local_storage_map_alloc_check` 和 `bpf_local_storage_map_check_btf` 分别用于验证创建参数合法性及BTF类型约束，而 `bpf_local_storage_destroy` 通过RCU机制安全遍历并销毁存储元素。  \n部分函数依赖未展示的辅助函数（如 `bpf_selem_unlink_map`），且涉及复杂RCU同步策略，上下文存在一定缺失。",
          "similarity": 0.5366055369377136
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/bpf/bpf_local_storage.c",
          "start_line": 499,
          "end_line": 605,
          "content": [
            "static int check_flags(const struct bpf_local_storage_data *old_sdata,",
            "\t\t       u64 map_flags)",
            "{",
            "\tif (old_sdata && (map_flags & ~BPF_F_LOCK) == BPF_NOEXIST)",
            "\t\t/* elem already exists */",
            "\t\treturn -EEXIST;",
            "",
            "\tif (!old_sdata && (map_flags & ~BPF_F_LOCK) == BPF_EXIST)",
            "\t\t/* elem doesn't exist, cannot update it */",
            "\t\treturn -ENOENT;",
            "",
            "\treturn 0;",
            "}",
            "int bpf_local_storage_alloc(void *owner,",
            "\t\t\t    struct bpf_local_storage_map *smap,",
            "\t\t\t    struct bpf_local_storage_elem *first_selem,",
            "\t\t\t    gfp_t gfp_flags)",
            "{",
            "\tstruct bpf_local_storage *prev_storage, *storage;",
            "\tstruct bpf_local_storage **owner_storage_ptr;",
            "\tint err;",
            "",
            "\terr = mem_charge(smap, owner, sizeof(*storage));",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (smap->bpf_ma) {",
            "\t\tmigrate_disable();",
            "\t\tstorage = bpf_mem_cache_alloc_flags(&smap->storage_ma, gfp_flags);",
            "\t\tmigrate_enable();",
            "\t} else {",
            "\t\tstorage = bpf_map_kzalloc(&smap->map, sizeof(*storage),",
            "\t\t\t\t\t  gfp_flags | __GFP_NOWARN);",
            "\t}",
            "",
            "\tif (!storage) {",
            "\t\terr = -ENOMEM;",
            "\t\tgoto uncharge;",
            "\t}",
            "",
            "\tRCU_INIT_POINTER(storage->smap, smap);",
            "\tINIT_HLIST_HEAD(&storage->list);",
            "\traw_spin_lock_init(&storage->lock);",
            "\tstorage->owner = owner;",
            "",
            "\tbpf_selem_link_storage_nolock(storage, first_selem);",
            "\tbpf_selem_link_map(smap, first_selem);",
            "",
            "\towner_storage_ptr =",
            "\t\t(struct bpf_local_storage **)owner_storage(smap, owner);",
            "\t/* Publish storage to the owner.",
            "\t * Instead of using any lock of the kernel object (i.e. owner),",
            "\t * cmpxchg will work with any kernel object regardless what",
            "\t * the running context is, bh, irq...etc.",
            "\t *",
            "\t * From now on, the owner->storage pointer (e.g. sk->sk_bpf_storage)",
            "\t * is protected by the storage->lock.  Hence, when freeing",
            "\t * the owner->storage, the storage->lock must be held before",
            "\t * setting owner->storage ptr to NULL.",
            "\t */",
            "\tprev_storage = cmpxchg(owner_storage_ptr, NULL, storage);",
            "\tif (unlikely(prev_storage)) {",
            "\t\tbpf_selem_unlink_map(first_selem);",
            "\t\terr = -EAGAIN;",
            "\t\tgoto uncharge;",
            "",
            "\t\t/* Note that even first_selem was linked to smap's",
            "\t\t * bucket->list, first_selem can be freed immediately",
            "\t\t * (instead of kfree_rcu) because",
            "\t\t * bpf_local_storage_map_free() does a",
            "\t\t * synchronize_rcu_mult (waiting for both sleepable and",
            "\t\t * normal programs) before walking the bucket->list.",
            "\t\t * Hence, no one is accessing selem from the",
            "\t\t * bucket->list under rcu_read_lock().",
            "\t\t */",
            "\t}",
            "",
            "\treturn 0;",
            "",
            "uncharge:",
            "\tbpf_local_storage_free(storage, smap, smap->bpf_ma, true);",
            "\tmem_uncharge(smap, owner, sizeof(*storage));",
            "\treturn err;",
            "}",
            "static u16 bpf_local_storage_cache_idx_get(struct bpf_local_storage_cache *cache)",
            "{",
            "\tu64 min_usage = U64_MAX;",
            "\tu16 i, res = 0;",
            "",
            "\tspin_lock(&cache->idx_lock);",
            "",
            "\tfor (i = 0; i < BPF_LOCAL_STORAGE_CACHE_SIZE; i++) {",
            "\t\tif (cache->idx_usage_counts[i] < min_usage) {",
            "\t\t\tmin_usage = cache->idx_usage_counts[i];",
            "\t\t\tres = i;",
            "",
            "\t\t\t/* Found a free cache_idx */",
            "\t\t\tif (!min_usage)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\tcache->idx_usage_counts[res]++;",
            "",
            "\tspin_unlock(&cache->idx_lock);",
            "",
            "\treturn res;",
            "}"
          ],
          "function_name": "check_flags, bpf_local_storage_alloc, bpf_local_storage_cache_idx_get",
          "description": "提供本地存储分配接口和缓存索引分配算法，包含冲突检测逻辑和基于内存缓存的分配实现。",
          "similarity": 0.5068400502204895
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/bpf_local_storage.c",
          "start_line": 343,
          "end_line": 449,
          "content": [
            "static bool check_storage_bpf_ma(struct bpf_local_storage *local_storage,",
            "\t\t\t\t struct bpf_local_storage_map *storage_smap,",
            "\t\t\t\t struct bpf_local_storage_elem *selem)",
            "{",
            "",
            "\tstruct bpf_local_storage_map *selem_smap;",
            "",
            "\t/* local_storage->smap may be NULL. If it is, get the bpf_ma",
            "\t * from any selem in the local_storage->list. The bpf_ma of all",
            "\t * local_storage and selem should have the same value",
            "\t * for the same map type.",
            "\t *",
            "\t * If the local_storage->list is already empty, the caller will not",
            "\t * care about the bpf_ma value also because the caller is not",
            "\t * responsibile to free the local_storage.",
            "\t */",
            "",
            "\tif (storage_smap)",
            "\t\treturn storage_smap->bpf_ma;",
            "",
            "\tif (!selem) {",
            "\t\tstruct hlist_node *n;",
            "",
            "\t\tn = rcu_dereference_check(hlist_first_rcu(&local_storage->list),",
            "\t\t\t\t\t  bpf_rcu_lock_held());",
            "\t\tif (!n)",
            "\t\t\treturn false;",
            "",
            "\t\tselem = hlist_entry(n, struct bpf_local_storage_elem, snode);",
            "\t}",
            "\tselem_smap = rcu_dereference_check(SDATA(selem)->smap, bpf_rcu_lock_held());",
            "",
            "\treturn selem_smap->bpf_ma;",
            "}",
            "static void bpf_selem_unlink_storage(struct bpf_local_storage_elem *selem,",
            "\t\t\t\t     bool reuse_now)",
            "{",
            "\tstruct bpf_local_storage_map *storage_smap;",
            "\tstruct bpf_local_storage *local_storage;",
            "\tbool bpf_ma, free_local_storage = false;",
            "\tHLIST_HEAD(selem_free_list);",
            "\tunsigned long flags;",
            "",
            "\tif (unlikely(!selem_linked_to_storage_lockless(selem)))",
            "\t\t/* selem has already been unlinked from sk */",
            "\t\treturn;",
            "",
            "\tlocal_storage = rcu_dereference_check(selem->local_storage,",
            "\t\t\t\t\t      bpf_rcu_lock_held());",
            "\tstorage_smap = rcu_dereference_check(local_storage->smap,",
            "\t\t\t\t\t     bpf_rcu_lock_held());",
            "\tbpf_ma = check_storage_bpf_ma(local_storage, storage_smap, selem);",
            "",
            "\traw_spin_lock_irqsave(&local_storage->lock, flags);",
            "\tif (likely(selem_linked_to_storage(selem)))",
            "\t\tfree_local_storage = bpf_selem_unlink_storage_nolock(",
            "\t\t\tlocal_storage, selem, true, &selem_free_list);",
            "\traw_spin_unlock_irqrestore(&local_storage->lock, flags);",
            "",
            "\tbpf_selem_free_list(&selem_free_list, reuse_now);",
            "",
            "\tif (free_local_storage)",
            "\t\tbpf_local_storage_free(local_storage, storage_smap, bpf_ma, reuse_now);",
            "}",
            "void bpf_selem_link_storage_nolock(struct bpf_local_storage *local_storage,",
            "\t\t\t\t   struct bpf_local_storage_elem *selem)",
            "{",
            "\tRCU_INIT_POINTER(selem->local_storage, local_storage);",
            "\thlist_add_head_rcu(&selem->snode, &local_storage->list);",
            "}",
            "static void bpf_selem_unlink_map(struct bpf_local_storage_elem *selem)",
            "{",
            "\tstruct bpf_local_storage_map *smap;",
            "\tstruct bpf_local_storage_map_bucket *b;",
            "\tunsigned long flags;",
            "",
            "\tif (unlikely(!selem_linked_to_map_lockless(selem)))",
            "\t\t/* selem has already be unlinked from smap */",
            "\t\treturn;",
            "",
            "\tsmap = rcu_dereference_check(SDATA(selem)->smap, bpf_rcu_lock_held());",
            "\tb = select_bucket(smap, selem);",
            "\traw_spin_lock_irqsave(&b->lock, flags);",
            "\tif (likely(selem_linked_to_map(selem)))",
            "\t\thlist_del_init_rcu(&selem->map_node);",
            "\traw_spin_unlock_irqrestore(&b->lock, flags);",
            "}",
            "void bpf_selem_link_map(struct bpf_local_storage_map *smap,",
            "\t\t\tstruct bpf_local_storage_elem *selem)",
            "{",
            "\tstruct bpf_local_storage_map_bucket *b = select_bucket(smap, selem);",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&b->lock, flags);",
            "\tRCU_INIT_POINTER(SDATA(selem)->smap, smap);",
            "\thlist_add_head_rcu(&selem->map_node, &b->list);",
            "\traw_spin_unlock_irqrestore(&b->lock, flags);",
            "}",
            "void bpf_selem_unlink(struct bpf_local_storage_elem *selem, bool reuse_now)",
            "{",
            "\t/* Always unlink from map before unlinking from local_storage",
            "\t * because selem will be freed after successfully unlinked from",
            "\t * the local_storage.",
            "\t */",
            "\tbpf_selem_unlink_map(selem);",
            "\tbpf_selem_unlink_storage(selem, reuse_now);",
            "}"
          ],
          "function_name": "check_storage_bpf_ma, bpf_selem_unlink_storage, bpf_selem_link_storage_nolock, bpf_selem_unlink_map, bpf_selem_link_map, bpf_selem_unlink",
          "description": "实现元素与存储结构的绑定/解除绑定操作，包含RCU锁保护的链表操作和存储结构生命周期管理。",
          "similarity": 0.410377562046051
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/bpf_local_storage.c",
          "start_line": 27,
          "end_line": 135,
          "content": [
            "static int mem_charge(struct bpf_local_storage_map *smap, void *owner, u32 size)",
            "{",
            "\tstruct bpf_map *map = &smap->map;",
            "",
            "\tif (!map->ops->map_local_storage_charge)",
            "\t\treturn 0;",
            "",
            "\treturn map->ops->map_local_storage_charge(smap, owner, size);",
            "}",
            "static void mem_uncharge(struct bpf_local_storage_map *smap, void *owner,",
            "\t\t\t u32 size)",
            "{",
            "\tstruct bpf_map *map = &smap->map;",
            "",
            "\tif (map->ops->map_local_storage_uncharge)",
            "\t\tmap->ops->map_local_storage_uncharge(smap, owner, size);",
            "}",
            "static bool selem_linked_to_storage_lockless(const struct bpf_local_storage_elem *selem)",
            "{",
            "\treturn !hlist_unhashed_lockless(&selem->snode);",
            "}",
            "static bool selem_linked_to_storage(const struct bpf_local_storage_elem *selem)",
            "{",
            "\treturn !hlist_unhashed(&selem->snode);",
            "}",
            "static bool selem_linked_to_map_lockless(const struct bpf_local_storage_elem *selem)",
            "{",
            "\treturn !hlist_unhashed_lockless(&selem->map_node);",
            "}",
            "static bool selem_linked_to_map(const struct bpf_local_storage_elem *selem)",
            "{",
            "\treturn !hlist_unhashed(&selem->map_node);",
            "}",
            "static void __bpf_local_storage_free_trace_rcu(struct rcu_head *rcu)",
            "{",
            "\tstruct bpf_local_storage *local_storage;",
            "",
            "\t/* If RCU Tasks Trace grace period implies RCU grace period, do",
            "\t * kfree(), else do kfree_rcu().",
            "\t */",
            "\tlocal_storage = container_of(rcu, struct bpf_local_storage, rcu);",
            "\tif (rcu_trace_implies_rcu_gp())",
            "\t\tkfree(local_storage);",
            "\telse",
            "\t\tkfree_rcu(local_storage, rcu);",
            "}",
            "static void bpf_local_storage_free_rcu(struct rcu_head *rcu)",
            "{",
            "\tstruct bpf_local_storage *local_storage;",
            "",
            "\tlocal_storage = container_of(rcu, struct bpf_local_storage, rcu);",
            "\tbpf_mem_cache_raw_free(local_storage);",
            "}",
            "static void bpf_local_storage_free_trace_rcu(struct rcu_head *rcu)",
            "{",
            "\tif (rcu_trace_implies_rcu_gp())",
            "\t\tbpf_local_storage_free_rcu(rcu);",
            "\telse",
            "\t\tcall_rcu(rcu, bpf_local_storage_free_rcu);",
            "}",
            "static void __bpf_local_storage_free(struct bpf_local_storage *local_storage,",
            "\t\t\t\t     bool vanilla_rcu)",
            "{",
            "\tif (vanilla_rcu)",
            "\t\tkfree_rcu(local_storage, rcu);",
            "\telse",
            "\t\tcall_rcu_tasks_trace(&local_storage->rcu,",
            "\t\t\t\t     __bpf_local_storage_free_trace_rcu);",
            "}",
            "static void bpf_local_storage_free(struct bpf_local_storage *local_storage,",
            "\t\t\t\t   struct bpf_local_storage_map *smap,",
            "\t\t\t\t   bool bpf_ma, bool reuse_now)",
            "{",
            "\tif (!local_storage)",
            "\t\treturn;",
            "",
            "\tif (!bpf_ma) {",
            "\t\t__bpf_local_storage_free(local_storage, reuse_now);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!reuse_now) {",
            "\t\tcall_rcu_tasks_trace(&local_storage->rcu,",
            "\t\t\t\t     bpf_local_storage_free_trace_rcu);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (smap) {",
            "\t\tmigrate_disable();",
            "\t\tbpf_mem_cache_free(&smap->storage_ma, local_storage);",
            "\t\tmigrate_enable();",
            "\t} else {",
            "\t\t/* smap could be NULL if the selem that triggered",
            "\t\t * this 'local_storage' creation had been long gone.",
            "\t\t * In this case, directly do call_rcu().",
            "\t\t */",
            "\t\tcall_rcu(&local_storage->rcu, bpf_local_storage_free_rcu);",
            "\t}",
            "}",
            "static void __bpf_selem_free_trace_rcu(struct rcu_head *rcu)",
            "{",
            "\tstruct bpf_local_storage_elem *selem;",
            "",
            "\tselem = container_of(rcu, struct bpf_local_storage_elem, rcu);",
            "\tif (rcu_trace_implies_rcu_gp())",
            "\t\tkfree(selem);",
            "\telse",
            "\t\tkfree_rcu(selem, rcu);",
            "}"
          ],
          "function_name": "mem_charge, mem_uncharge, selem_linked_to_storage_lockless, selem_linked_to_storage, selem_linked_to_map_lockless, selem_linked_to_map, __bpf_local_storage_free_trace_rcu, bpf_local_storage_free_rcu, bpf_local_storage_free_trace_rcu, __bpf_local_storage_free, bpf_local_storage_free, __bpf_selem_free_trace_rcu",
          "description": "实现内存充放电接口及元素链接状态检测，包含RCU安全的元素释放逻辑和不同RCU模式的内存回收方法。",
          "similarity": 0.3869357705116272
        }
      ]
    },
    {
      "source_file": "kernel/bpf/memalloc.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:19:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\memalloc.c`\n\n---\n\n# `bpf/memalloc.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/memalloc.c` 实现了一个专用于 BPF（Berkeley Packet Filter）程序的内存分配器，支持在任意上下文（包括 NMI、中断、不可抢占上下文等）中安全地分配和释放小块内存。该分配器通过每 CPU 的多级缓存桶（per-CPU per-bucket free list）机制，避免在 BPF 程序执行路径中直接调用可能不安全的 `kmalloc()`。缓存桶的填充和回收由 `irq_work` 异步完成，确保主执行路径的低延迟和高可靠性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_mem_cache`**  \n  每个缓存桶的核心结构，包含：\n  - `free_llist` / `free_llist_extra`：无锁链表（llist），用于存储空闲对象。\n  - `active`：本地原子计数器，用于保护对 `free_llist` 的并发访问。\n  - `refill_work`：`irq_work` 结构，用于触发异步填充。\n  - `objcg`：对象 cgroup 指针，用于内存记账。\n  - `unit_size`：该缓存桶中对象的固定大小。\n  - `free_cnt`、`low_watermark`、`high_watermark`、`batch`：缓存管理参数。\n  - `percpu_size`：标识是否为 per-CPU 分配。\n  - RCU 相关字段（`free_by_rcu`、`rcu` 等）：用于延迟释放内存，避免在不可睡眠上下文中调用 `kfree`。\n\n- **`struct bpf_mem_caches`**  \n  包含 `NUM_CACHES`（11 个）不同大小的 `bpf_mem_cache` 实例，对应预定义的内存块尺寸。\n\n- **`sizes[NUM_CACHES]`**  \n  定义了 11 种支持的分配尺寸：`{96, 192, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096}` 字节。\n\n- **`size_index[24]`**  \n  查找表，将请求大小（≤192 字节）映射到对应的缓存桶索引。\n\n### 主要函数\n\n- **`bpf_mem_cache_idx(size_t size)`**  \n  根据请求大小返回对应的缓存桶索引（0~10），超出 `BPF_MEM_ALLOC_SIZE_MAX`（4096）则返回 -1。\n\n- **`__alloc()`**  \n  底层分配函数，根据是否为 per-CPU 类型调用 `kmalloc_node` 或 `__alloc_percpu_gfp`。\n\n- **`add_obj_to_free_list()`**  \n  将对象安全地加入当前 CPU 的空闲链表，使用 `active` 计数器保护。\n\n- **`alloc_bulk()`**  \n  批量分配对象并填充缓存桶，优先从延迟释放队列（如 `free_by_rcu_ttrace`）回收，再尝试从全局分配器分配。\n\n- **`free_one()` / `free_all()`**  \n  释放单个或多个对象，区分普通和 per-CPU 类型。\n\n- **`__free_rcu()` / `__free_rcu_tasks_trace()`**  \n  RCU 回调函数，用于在宽限期结束后真正释放内存。\n\n- **`enque_to_free()` / `do_call_rcu_ttrace()`**  \n  将待释放对象加入 RCU 延迟队列，并触发 RCU 宽限期。\n\n## 3. 关键实现\n\n### 内存布局与对齐\n- 每个分配的对象末尾附加 8 字节的 `struct llist_node`，用于无锁链表管理。\n- 所有分配均对齐至 8 字节边界。\n\n### 并发控制\n- 使用 `local_t active` 计数器保护对 `free_llist` 的访问。在分配/释放时，通过 `inc_active()`/`dec_active()` 禁用中断（尤其在 `CONFIG_PREEMPT_RT` 下），确保 NMI 或中断上下文不会破坏链表结构。\n- `free_llist_extra` 用于在 `active` 忙时暂存释放对象，避免失败。\n\n### 异步填充机制\n- 当缓存桶水位低于 `low_watermark` 时，通过 `irq_work` 触发 `alloc_bulk()`。\n- `alloc_bulk()` 优先从 RCU 延迟释放队列中回收对象，减少全局分配压力。\n- 使用 `set_active_memcg()` 确保内存分配计入正确的 memcg。\n\n### RCU 延迟释放\n- 在不可睡眠上下文（如 NMI）中释放内存时，对象被加入 `free_by_rcu_ttrace` 队列。\n- 通过 `call_rcu_tasks_trace()` 或 `call_rcu()` 触发宽限期，之后在软中断上下文中真正释放。\n- 支持 `rcu_trace_implies_rcu_gp()` 优化，避免双重 RCU 调用。\n\n### 尺寸映射策略\n- 对 ≤192 字节的请求，使用 `size_index` 查找表快速定位桶。\n- 对 >192 字节的请求，使用 `fls(size - 1) - 2` 计算桶索引，覆盖 256~4096 字节范围。\n\n## 4. 依赖关系\n\n- **内存管理**：依赖 `<linux/mm.h>`、`<linux/memcontrol.h>` 进行底层分配和 memcg 记账。\n- **BPF 子系统**：通过 `<linux/bpf.h>` 和 `<linux/bpf_mem_alloc.h>` 与 BPF 运行时集成。\n- **无锁数据结构**：使用 `<linux/llist.h>` 提供的无锁链表。\n- **中断与延迟执行**：依赖 `<linux/irq_work.h>` 实现异步填充。\n- **RCU 机制**：使用 RCU 和 RCU Tasks Trace 宽限期实现安全延迟释放。\n- **架构相关**：使用 `<asm/local.h>` 的 per-CPU 原子操作。\n\n## 5. 使用场景\n\n- **BPF tracing 程序**：当 BPF 程序 attach 到 `kprobe`、`fentry` 等 hook 点时，可能运行在任意内核上下文（包括 NMI、中断、不可抢占区域）。此时标准 `kmalloc` 不安全，必须使用本分配器。\n- **高可靠性内存分配**：在不允许睡眠、不能触发内存回收的上下文中，提供确定性的内存分配能力。\n- **低延迟要求**：通过 per-CPU 缓存避免锁竞争和全局分配器开销，满足 BPF 程序对性能的严苛要求。\n- **内存隔离与记账**：支持通过 `objcg` 将 BPF 内存消耗计入特定 cgroup，便于资源控制。",
      "similarity": 0.5734316110610962,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 689,
          "end_line": 807,
          "content": [
            "static void free_mem_alloc(struct bpf_mem_alloc *ma)",
            "{",
            "\t/* waiting_for_gp[_ttrace] lists were drained, but RCU callbacks",
            "\t * might still execute. Wait for them.",
            "\t *",
            "\t * rcu_barrier_tasks_trace() doesn't imply synchronize_rcu_tasks_trace(),",
            "\t * but rcu_barrier_tasks_trace() and rcu_barrier() below are only used",
            "\t * to wait for the pending __free_rcu_tasks_trace() and __free_rcu(),",
            "\t * so if call_rcu(head, __free_rcu) is skipped due to",
            "\t * rcu_trace_implies_rcu_gp(), it will be OK to skip rcu_barrier() by",
            "\t * using rcu_trace_implies_rcu_gp() as well.",
            "\t */",
            "\trcu_barrier(); /* wait for __free_by_rcu */",
            "\trcu_barrier_tasks_trace(); /* wait for __free_rcu */",
            "\tif (!rcu_trace_implies_rcu_gp())",
            "\t\trcu_barrier();",
            "\tfree_mem_alloc_no_barrier(ma);",
            "}",
            "static void free_mem_alloc_deferred(struct work_struct *work)",
            "{",
            "\tstruct bpf_mem_alloc *ma = container_of(work, struct bpf_mem_alloc, work);",
            "",
            "\tfree_mem_alloc(ma);",
            "\tkfree(ma);",
            "}",
            "static void destroy_mem_alloc(struct bpf_mem_alloc *ma, int rcu_in_progress)",
            "{",
            "\tstruct bpf_mem_alloc *copy;",
            "",
            "\tif (!rcu_in_progress) {",
            "\t\t/* Fast path. No callbacks are pending, hence no need to do",
            "\t\t * rcu_barrier-s.",
            "\t\t */",
            "\t\tfree_mem_alloc_no_barrier(ma);",
            "\t\treturn;",
            "\t}",
            "",
            "\tcopy = kmemdup(ma, sizeof(*ma), GFP_KERNEL);",
            "\tif (!copy) {",
            "\t\t/* Slow path with inline barrier-s */",
            "\t\tfree_mem_alloc(ma);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Defer barriers into worker to let the rest of map memory to be freed */",
            "\tmemset(ma, 0, sizeof(*ma));",
            "\tINIT_WORK(&copy->work, free_mem_alloc_deferred);",
            "\tqueue_work(system_unbound_wq, &copy->work);",
            "}",
            "void bpf_mem_alloc_destroy(struct bpf_mem_alloc *ma)",
            "{",
            "\tstruct bpf_mem_caches *cc;",
            "\tstruct bpf_mem_cache *c;",
            "\tint cpu, i, rcu_in_progress;",
            "",
            "\tif (ma->cache) {",
            "\t\trcu_in_progress = 0;",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(ma->cache, cpu);",
            "\t\t\tWRITE_ONCE(c->draining, true);",
            "\t\t\tirq_work_sync(&c->refill_work);",
            "\t\t\tdrain_mem_cache(c);",
            "\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
            "\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
            "\t\t}",
            "\t\tobj_cgroup_put(ma->objcg);",
            "\t\tdestroy_mem_alloc(ma, rcu_in_progress);",
            "\t}",
            "\tif (ma->caches) {",
            "\t\trcu_in_progress = 0;",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tcc = per_cpu_ptr(ma->caches, cpu);",
            "\t\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\t\tc = &cc->cache[i];",
            "\t\t\t\tWRITE_ONCE(c->draining, true);",
            "\t\t\t\tirq_work_sync(&c->refill_work);",
            "\t\t\t\tdrain_mem_cache(c);",
            "\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
            "\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
            "\t\t\t}",
            "\t\t}",
            "\t\tobj_cgroup_put(ma->objcg);",
            "\t\tdestroy_mem_alloc(ma, rcu_in_progress);",
            "\t}",
            "}",
            "static void notrace unit_free(struct bpf_mem_cache *c, void *ptr)",
            "{",
            "\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;",
            "\tunsigned long flags;",
            "\tint cnt = 0;",
            "",
            "\tBUILD_BUG_ON(LLIST_NODE_SZ > 8);",
            "",
            "\t/*",
            "\t * Remember bpf_mem_cache that allocated this object.",
            "\t * The hint is not accurate.",
            "\t */",
            "\tc->tgt = *(struct bpf_mem_cache **)llnode;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (local_inc_return(&c->active) == 1) {",
            "\t\t__llist_add(llnode, &c->free_llist);",
            "\t\tcnt = ++c->free_cnt;",
            "\t} else {",
            "\t\t/* unit_free() cannot fail. Therefore add an object to atomic",
            "\t\t * llist. free_bulk() will drain it. Though free_llist_extra is",
            "\t\t * a per-cpu list we have to use atomic llist_add here, since",
            "\t\t * it also can be interrupted by bpf nmi prog that does another",
            "\t\t * unit_free() into the same free_llist_extra.",
            "\t\t */",
            "\t\tllist_add(llnode, &c->free_llist_extra);",
            "\t}",
            "\tlocal_dec(&c->active);",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (cnt > c->high_watermark)",
            "\t\t/* free few objects from current cpu into global kmalloc pool */",
            "\t\tirq_work_raise(c);",
            "}"
          ],
          "function_name": "free_mem_alloc, free_mem_alloc_deferred, destroy_mem_alloc, bpf_mem_alloc_destroy, unit_free",
          "description": "实现BPF内存分配销毁逻辑，通过RCU屏障等待回调完成并安全释放资源，deferred路径利用workqueue异步释放，destroy_mem_alloc处理缓存清理和RCU状态同步。",
          "similarity": 0.5741695165634155
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 1,
          "end_line": 67,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2022 Meta Platforms, Inc. and affiliates. */",
            "#include <linux/mm.h>",
            "#include <linux/llist.h>",
            "#include <linux/bpf.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/bpf_mem_alloc.h>",
            "#include <linux/memcontrol.h>",
            "#include <asm/local.h>",
            "",
            "/* Any context (including NMI) BPF specific memory allocator.",
            " *",
            " * Tracing BPF programs can attach to kprobe and fentry. Hence they",
            " * run in unknown context where calling plain kmalloc() might not be safe.",
            " *",
            " * Front-end kmalloc() with per-cpu per-bucket cache of free elements.",
            " * Refill this cache asynchronously from irq_work.",
            " *",
            " * CPU_0 buckets",
            " * 16 32 64 96 128 196 256 512 1024 2048 4096",
            " * ...",
            " * CPU_N buckets",
            " * 16 32 64 96 128 196 256 512 1024 2048 4096",
            " *",
            " * The buckets are prefilled at the start.",
            " * BPF programs always run with migration disabled.",
            " * It's safe to allocate from cache of the current cpu with irqs disabled.",
            " * Free-ing is always done into bucket of the current cpu as well.",
            " * irq_work trims extra free elements from buckets with kfree",
            " * and refills them with kmalloc, so global kmalloc logic takes care",
            " * of freeing objects allocated by one cpu and freed on another.",
            " *",
            " * Every allocated objected is padded with extra 8 bytes that contains",
            " * struct llist_node.",
            " */",
            "#define LLIST_NODE_SZ sizeof(struct llist_node)",
            "",
            "#define BPF_MEM_ALLOC_SIZE_MAX 4096",
            "",
            "/* similar to kmalloc, but sizeof == 8 bucket is gone */",
            "static u8 size_index[24] __ro_after_init = {",
            "\t3,\t/* 8 */",
            "\t3,\t/* 16 */",
            "\t4,\t/* 24 */",
            "\t4,\t/* 32 */",
            "\t5,\t/* 40 */",
            "\t5,\t/* 48 */",
            "\t5,\t/* 56 */",
            "\t5,\t/* 64 */",
            "\t1,\t/* 72 */",
            "\t1,\t/* 80 */",
            "\t1,\t/* 88 */",
            "\t1,\t/* 96 */",
            "\t6,\t/* 104 */",
            "\t6,\t/* 112 */",
            "\t6,\t/* 120 */",
            "\t6,\t/* 128 */",
            "\t2,\t/* 136 */",
            "\t2,\t/* 144 */",
            "\t2,\t/* 152 */",
            "\t2,\t/* 160 */",
            "\t2,\t/* 168 */",
            "\t2,\t/* 176 */",
            "\t2,\t/* 184 */",
            "\t2\t/* 192 */",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义BPF专用内存分配器基础结构，通过per-CPU缓存和异步填充机制管理小对象分配。建立大小到缓存索引的映射表，预分配不同大小的桶并注册到各CPU，支持NMI和中断上下文的安全内存分配。",
          "similarity": 0.5732136964797974
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 859,
          "end_line": 938,
          "content": [
            "static void notrace unit_free_rcu(struct bpf_mem_cache *c, void *ptr)",
            "{",
            "\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;",
            "\tunsigned long flags;",
            "",
            "\tc->tgt = *(struct bpf_mem_cache **)llnode;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (local_inc_return(&c->active) == 1) {",
            "\t\tif (__llist_add(llnode, &c->free_by_rcu))",
            "\t\t\tc->free_by_rcu_tail = llnode;",
            "\t} else {",
            "\t\tllist_add(llnode, &c->free_llist_extra_rcu);",
            "\t}",
            "\tlocal_dec(&c->active);",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (!atomic_read(&c->call_rcu_in_progress))",
            "\t\tirq_work_raise(c);",
            "}",
            "void notrace bpf_mem_free(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tstruct bpf_mem_cache *c;",
            "\tint idx;",
            "",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tc = *(void **)(ptr - LLIST_NODE_SZ);",
            "\tidx = bpf_mem_cache_idx(c->unit_size);",
            "\tif (WARN_ON_ONCE(idx < 0))",
            "\t\treturn;",
            "",
            "\tunit_free(this_cpu_ptr(ma->caches)->cache + idx, ptr);",
            "}",
            "void notrace bpf_mem_free_rcu(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tstruct bpf_mem_cache *c;",
            "\tint idx;",
            "",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tc = *(void **)(ptr - LLIST_NODE_SZ);",
            "\tidx = bpf_mem_cache_idx(c->unit_size);",
            "\tif (WARN_ON_ONCE(idx < 0))",
            "\t\treturn;",
            "",
            "\tunit_free_rcu(this_cpu_ptr(ma->caches)->cache + idx, ptr);",
            "}",
            "void notrace bpf_mem_cache_free(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tunit_free(this_cpu_ptr(ma->cache), ptr);",
            "}",
            "void notrace bpf_mem_cache_free_rcu(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tunit_free_rcu(this_cpu_ptr(ma->cache), ptr);",
            "}",
            "void bpf_mem_cache_raw_free(void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tkfree(ptr - LLIST_NODE_SZ);",
            "}",
            "int bpf_mem_alloc_check_size(bool percpu, size_t size)",
            "{",
            "\t/* The size of percpu allocation doesn't have LLIST_NODE_SZ overhead */",
            "\tif ((percpu && size > BPF_MEM_ALLOC_SIZE_MAX) ||",
            "\t    (!percpu && size > BPF_MEM_ALLOC_SIZE_MAX - LLIST_NODE_SZ))",
            "\t\treturn -E2BIG;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "unit_free_rcu, bpf_mem_free, bpf_mem_free_rcu, bpf_mem_cache_free, bpf_mem_cache_free_rcu, bpf_mem_cache_raw_free, bpf_mem_alloc_check_size",
          "description": "提供基于RCU的内存释放接口，unit_free系列函数将对象加入链表实现批量回收，bpf_mem_free系列根据上下文选择普通或RCU释放路径，check_size验证分配尺寸合法性",
          "similarity": 0.5336741805076599
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 68,
          "end_line": 168,
          "content": [
            "static int bpf_mem_cache_idx(size_t size)",
            "{",
            "\tif (!size || size > BPF_MEM_ALLOC_SIZE_MAX)",
            "\t\treturn -1;",
            "",
            "\tif (size <= 192)",
            "\t\treturn size_index[(size - 1) / 8] - 1;",
            "",
            "\treturn fls(size - 1) - 2;",
            "}",
            "static void inc_active(struct bpf_mem_cache *c, unsigned long *flags)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\t/* In RT irq_work runs in per-cpu kthread, so disable",
            "\t\t * interrupts to avoid preemption and interrupts and",
            "\t\t * reduce the chance of bpf prog executing on this cpu",
            "\t\t * when active counter is busy.",
            "\t\t */",
            "\t\tlocal_irq_save(*flags);",
            "\t/* alloc_bulk runs from irq_work which will not preempt a bpf",
            "\t * program that does unit_alloc/unit_free since IRQs are",
            "\t * disabled there. There is no race to increment 'active'",
            "\t * counter. It protects free_llist from corruption in case NMI",
            "\t * bpf prog preempted this loop.",
            "\t */",
            "\tWARN_ON_ONCE(local_inc_return(&c->active) != 1);",
            "}",
            "static void dec_active(struct bpf_mem_cache *c, unsigned long *flags)",
            "{",
            "\tlocal_dec(&c->active);",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tlocal_irq_restore(*flags);",
            "}",
            "static void add_obj_to_free_list(struct bpf_mem_cache *c, void *obj)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tinc_active(c, &flags);",
            "\t__llist_add(obj, &c->free_llist);",
            "\tc->free_cnt++;",
            "\tdec_active(c, &flags);",
            "}",
            "static void alloc_bulk(struct bpf_mem_cache *c, int cnt, int node, bool atomic)",
            "{",
            "\tstruct mem_cgroup *memcg = NULL, *old_memcg;",
            "\tgfp_t gfp;",
            "\tvoid *obj;",
            "\tint i;",
            "",
            "\tgfp = __GFP_NOWARN | __GFP_ACCOUNT;",
            "\tgfp |= atomic ? GFP_NOWAIT : GFP_KERNEL;",
            "",
            "\tfor (i = 0; i < cnt; i++) {",
            "\t\t/*",
            "\t\t * For every 'c' llist_del_first(&c->free_by_rcu_ttrace); is",
            "\t\t * done only by one CPU == current CPU. Other CPUs might",
            "\t\t * llist_add() and llist_del_all() in parallel.",
            "\t\t */",
            "\t\tobj = llist_del_first(&c->free_by_rcu_ttrace);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tif (i >= cnt)",
            "\t\treturn;",
            "",
            "\tfor (; i < cnt; i++) {",
            "\t\tobj = llist_del_first(&c->waiting_for_gp_ttrace);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tif (i >= cnt)",
            "\t\treturn;",
            "",
            "\tmemcg = get_memcg(c);",
            "\told_memcg = set_active_memcg(memcg);",
            "\tfor (; i < cnt; i++) {",
            "\t\t/* Allocate, but don't deplete atomic reserves that typical",
            "\t\t * GFP_ATOMIC would do. irq_work runs on this cpu and kmalloc",
            "\t\t * will allocate from the current numa node which is what we",
            "\t\t * want here.",
            "\t\t */",
            "\t\tobj = __alloc(c, node, gfp);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tset_active_memcg(old_memcg);",
            "\tmem_cgroup_put(memcg);",
            "}",
            "static void free_one(void *obj, bool percpu)",
            "{",
            "\tif (percpu) {",
            "\t\tfree_percpu(((void __percpu **)obj)[1]);",
            "\t\tkfree(obj);",
            "\t\treturn;",
            "\t}",
            "",
            "\tkfree(obj);",
            "}"
          ],
          "function_name": "bpf_mem_cache_idx, inc_active, dec_active, add_obj_to_free_list, alloc_bulk, free_one",
          "description": "实现BPF内存缓存核心控制逻辑，包含大小索引计算、活跃计数器管理、对象回收到自由链表、批量分配与释放流程。通过irq_work异步补充缓存，处理多CPU间的内存对象迁移与回收。",
          "similarity": 0.514157235622406
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 576,
          "end_line": 682,
          "content": [
            "int bpf_mem_alloc_percpu_init(struct bpf_mem_alloc *ma, struct obj_cgroup *objcg)",
            "{",
            "\tstruct bpf_mem_caches __percpu *pcc;",
            "",
            "\tpcc = __alloc_percpu_gfp(sizeof(struct bpf_mem_caches), 8, GFP_KERNEL);",
            "\tif (!pcc)",
            "\t\treturn -ENOMEM;",
            "",
            "\tma->caches = pcc;",
            "\tma->objcg = objcg;",
            "\tma->percpu = true;",
            "\treturn 0;",
            "}",
            "int bpf_mem_alloc_percpu_unit_init(struct bpf_mem_alloc *ma, int size)",
            "{",
            "\tstruct bpf_mem_caches *cc; struct bpf_mem_caches __percpu *pcc;",
            "\tint cpu, i, unit_size, percpu_size;",
            "\tstruct obj_cgroup *objcg;",
            "\tstruct bpf_mem_cache *c;",
            "",
            "\ti = bpf_mem_cache_idx(size);",
            "\tif (i < 0)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* room for llist_node and per-cpu pointer */",
            "\tpercpu_size = LLIST_NODE_SZ + sizeof(void *);",
            "",
            "\tunit_size = sizes[i];",
            "\tobjcg = ma->objcg;",
            "\tpcc = ma->caches;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tcc = per_cpu_ptr(pcc, cpu);",
            "\t\tc = &cc->cache[i];",
            "\t\tif (cpu == 0 && c->unit_size)",
            "\t\t\tbreak;",
            "",
            "\t\tc->unit_size = unit_size;",
            "\t\tc->objcg = objcg;",
            "\t\tc->percpu_size = percpu_size;",
            "\t\tc->tgt = c;",
            "",
            "\t\tinit_refill_work(c);",
            "\t\tprefill_mem_cache(c, cpu);",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static void drain_mem_cache(struct bpf_mem_cache *c)",
            "{",
            "\tbool percpu = !!c->percpu_size;",
            "",
            "\t/* No progs are using this bpf_mem_cache, but htab_map_free() called",
            "\t * bpf_mem_cache_free() for all remaining elements and they can be in",
            "\t * free_by_rcu_ttrace or in waiting_for_gp_ttrace lists, so drain those lists now.",
            "\t *",
            "\t * Except for waiting_for_gp_ttrace list, there are no concurrent operations",
            "\t * on these lists, so it is safe to use __llist_del_all().",
            "\t */",
            "\tfree_all(llist_del_all(&c->free_by_rcu_ttrace), percpu);",
            "\tfree_all(llist_del_all(&c->waiting_for_gp_ttrace), percpu);",
            "\tfree_all(__llist_del_all(&c->free_llist), percpu);",
            "\tfree_all(__llist_del_all(&c->free_llist_extra), percpu);",
            "\tfree_all(__llist_del_all(&c->free_by_rcu), percpu);",
            "\tfree_all(__llist_del_all(&c->free_llist_extra_rcu), percpu);",
            "\tfree_all(llist_del_all(&c->waiting_for_gp), percpu);",
            "}",
            "static void check_mem_cache(struct bpf_mem_cache *c)",
            "{",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_by_rcu_ttrace));",
            "\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp_ttrace));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_llist));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_llist_extra));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_by_rcu));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_llist_extra_rcu));",
            "\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));",
            "}",
            "static void check_leaked_objs(struct bpf_mem_alloc *ma)",
            "{",
            "\tstruct bpf_mem_caches *cc;",
            "\tstruct bpf_mem_cache *c;",
            "\tint cpu, i;",
            "",
            "\tif (ma->cache) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(ma->cache, cpu);",
            "\t\t\tcheck_mem_cache(c);",
            "\t\t}",
            "\t}",
            "\tif (ma->caches) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tcc = per_cpu_ptr(ma->caches, cpu);",
            "\t\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\t\tc = &cc->cache[i];",
            "\t\t\t\tcheck_mem_cache(c);",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "}",
            "static void free_mem_alloc_no_barrier(struct bpf_mem_alloc *ma)",
            "{",
            "\tcheck_leaked_objs(ma);",
            "\tfree_percpu(ma->cache);",
            "\tfree_percpu(ma->caches);",
            "\tma->cache = NULL;",
            "\tma->caches = NULL;",
            "}"
          ],
          "function_name": "bpf_mem_alloc_percpu_init, bpf_mem_alloc_percpu_unit_init, drain_mem_cache, check_mem_cache, check_leaked_objs, free_mem_alloc_no_barrier",
          "description": "实现Per-CPU内存分配器的初始化与清理逻辑，包含Per-CPU缓存初始化、对象回收链表遍历、内存泄漏检测及资源释放。提供模块卸载时的强制清理接口，确保所有残留对象被正确释放。",
          "similarity": 0.4336933195590973
        }
      ]
    },
    {
      "source_file": "kernel/bpf/hashtab.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:10:56\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\hashtab.c`\n\n---\n\n# bpf/hashtab.c 技术文档\n\n## 1. 文件概述\n\n`bpf/hashtab.c` 是 Linux 内核中 BPF（Berkeley Packet Filter）子系统的核心实现文件之一，负责提供基于哈希表（hash table）的 BPF map 类型支持。该文件实现了多种 BPF map 类型，包括普通哈希表（`BPF_MAP_TYPE_HASH`）、LRU 哈希表（`BPF_MAP_TYPE_LRU_HASH`）、每 CPU 哈希表（`BPF_MAP_TYPE_PERCPU_HASH`）及其 LRU 变体。它支持预分配（pre-allocated）和动态分配（non-preallocated）两种内存管理模式，并集成了 BPF 内存分配器（`bpf_mem_alloc`）、LRU 驱逐机制、每 CPU 自由列表（percpu freelist）等高级特性，以满足高性能、低延迟的 BPF 程序需求。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bucket`**  \n  哈希桶结构，包含一个 `hlist_nulls_head` 链表头和一个 `raw_spinlock_t` 原始自旋锁，用于保护桶内元素的并发访问。\n\n- **`struct bpf_htab`**  \n  BPF 哈希表的主控制结构，继承自 `struct bpf_map`，包含：\n  - 桶数组指针 `buckets`\n  - 元素存储区 `elems`\n  - 内存分配器 `ma`（主）和 `pcpu_ma`（每 CPU）\n  - LRU 或 percpu_freelist 联合体\n  - 元素计数器（`pcount` 或 `count`）\n  - 哈希种子 `hashrnd`\n  - 锁依赖类键 `lockdep_key`\n  - 每 CPU 锁状态数组 `map_locked`（用于防止递归）\n\n- **`struct htab_elem`**  \n  哈希表元素结构，包含：\n  - 哈希链表节点 `hash_node`\n  - LRU 节点或自由列表节点\n  - 指向每 CPU 指针的指针（用于 per-CPU map）\n  - 哈希值 `hash`\n  - 可变长键 `key[]`（后接值或 per-CPU 指针）\n\n### 关键辅助函数\n\n- `htab_is_prealloc()`：判断是否为预分配模式\n- `htab_is_lru()` / `htab_is_percpu()`：判断 map 类型是否为 LRU 或 per-CPU\n- `htab_init_buckets()`：初始化所有哈希桶\n- `htab_lock_bucket()` / `htab_unlock_bucket()`：带递归保护的桶锁操作\n- `htab_elem_set_ptr()` / `htab_elem_get_ptr()`：操作 per-CPU 指针\n- `get_htab_elem()`：从预分配区域获取第 i 个元素\n- `htab_has_extra_elems()`：判断是否包含额外元素（用于 per-CPU 扩展）\n- `htab_free_prealloced_timers_and_wq()`：释放预分配元素中的 BPF 定时器和工作队列资源\n\n### 批量操作宏\n\n- `BATCH_OPS(_name)`：定义批量操作函数指针，如 `map_lookup_batch`、`map_update_batch` 等。\n\n## 3. 关键实现\n\n### 并发控制与死锁预防\n\n- 使用 **原始自旋锁（`raw_spinlock_t`）** 保护每个哈希桶，确保在任意上下文（如 kprobe、perf、tracepoint）中安全使用。\n- 引入 **每 CPU 递归计数器 `map_locked[]`**，防止 BPF 程序在持有桶锁时再次进入（例如通过 `sys_bpf()` 或嵌套 BPF 调用），避免死锁。\n- 在 `PREEMPT_RT` 实时内核上，由于普通自旋锁可能睡眠，必须使用 `raw_spinlock` 以保证原子性；结合 `bpf_mem_alloc` 后，即使非预分配模式也可安全使用原始锁。\n\n### 内存管理\n\n- **预分配模式（`BPF_F_NO_PREALLOC` 未设置）**：启动时一次性分配所有元素，使用 `pcpu_freelist` 管理空闲元素。\n- **非预分配模式**：按需通过 `bpf_mem_alloc` 动态分配元素，支持 NUMA 感知和内存回收。\n- **Per-CPU 支持**：对于 `PERCPU_HASH` 类型，每个键对应一个 per-CPU 值数组，通过 `htab_elem_get_ptr()` 访问。\n\n### LRU 驱逐机制\n\n- 当 map 类型为 `LRU_HASH` 或 `LRU_PERCPU_HASH` 时，使用 `bpf_lru` 子系统管理元素生命周期，自动驱逐最近最少使用的条目以维持 `max_entries` 限制。\n\n### 扩展字段支持\n\n- 支持 BTF（BPF Type Format）描述的复杂值类型，如 `BPF_TIMER` 和 `BPF_WORKQUEUE`，在销毁 map 时自动释放相关资源（见 `htab_free_prealloced_timers_and_wq`）。\n\n### 哈希与对齐\n\n- 使用 `jhash` 算法计算键的哈希值，并通过 `hashrnd` 引入随机种子防止哈希碰撞攻击。\n- 键和值之间按 8 字节对齐（`__aligned(8)`），确保 per-CPU 指针正确对齐。\n\n## 4. 依赖关系\n\n- **内核头文件**：\n  - `<linux/bpf.h>`、`<linux/btf.h>`：BPF 和 BTF 核心接口\n  - `<linux/jhash.h>`：哈希函数\n  - `<linux/rculist_nulls.h>`：RCU 安全的空指针链表\n  - `<linux/percpu_freelist.h>`、`<linux/bpf_lru_list.h>`：内存管理子系统\n  - `<linux/bpf_mem_alloc.h>`：BPF 专用内存分配器\n\n- **内部模块**：\n  - `map_in_map.h`：支持 map-in-map 功能\n  - `bpf_lru_list.c`：LRU 驱逐实现\n  - `percpu_freelist.c`：每 CPU 自由列表管理\n\n- **BPF 子系统**：\n  - 与 `bpf_map` 通用框架集成，通过 `bpf_map_ops` 注册操作函数\n  - 依赖 `bpf_prog_active` 机制防止 BPF 递归\n\n## 5. 使用场景\n\n- **网络数据包过滤与监控**：eBPF 程序使用 `BPF_MAP_TYPE_HASH` 存储连接状态、统计信息等。\n- **性能分析**：通过 `PERCPU_HASH` 收集每 CPU 的性能计数器，避免锁竞争。\n- **资源限制与缓存**：`LRU_HASH` 用于实现有界缓存（如 DNS 缓存、会话表），自动淘汰旧条目。\n- **内核跟踪**：kprobe、tracepoint 等 attach 的 BPF 程序频繁读写哈希表，要求低延迟和高并发。\n- **用户空间交互**：通过 `bpf(2)` 系统调用进行 map 的创建、更新、查询和删除，支持批量操作提升效率。\n- **高级 BPF 功能**：支持包含定时器（`bpf_timer`）或工作队列（`bpf_workqueue`）的复杂 map 值类型，用于异步任务调度。",
      "similarity": 0.5699801445007324,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1,
          "end_line": 131,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com",
            " * Copyright (c) 2016 Facebook",
            " */",
            "#include <linux/bpf.h>",
            "#include <linux/btf.h>",
            "#include <linux/jhash.h>",
            "#include <linux/filter.h>",
            "#include <linux/rculist_nulls.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/random.h>",
            "#include <uapi/linux/btf.h>",
            "#include <linux/rcupdate_trace.h>",
            "#include <linux/btf_ids.h>",
            "#include \"percpu_freelist.h\"",
            "#include \"bpf_lru_list.h\"",
            "#include \"map_in_map.h\"",
            "#include <linux/bpf_mem_alloc.h>",
            "",
            "#define HTAB_CREATE_FLAG_MASK\t\t\t\t\t\t\\",
            "\t(BPF_F_NO_PREALLOC | BPF_F_NO_COMMON_LRU | BPF_F_NUMA_NODE |\t\\",
            "\t BPF_F_ACCESS_MASK | BPF_F_ZERO_SEED)",
            "",
            "#define BATCH_OPS(_name)\t\t\t\\",
            "\t.map_lookup_batch =\t\t\t\\",
            "\t_name##_map_lookup_batch,\t\t\\",
            "\t.map_lookup_and_delete_batch =\t\t\\",
            "\t_name##_map_lookup_and_delete_batch,\t\\",
            "\t.map_update_batch =\t\t\t\\",
            "\tgeneric_map_update_batch,\t\t\\",
            "\t.map_delete_batch =\t\t\t\\",
            "\tgeneric_map_delete_batch",
            "",
            "/*",
            " * The bucket lock has two protection scopes:",
            " *",
            " * 1) Serializing concurrent operations from BPF programs on different",
            " *    CPUs",
            " *",
            " * 2) Serializing concurrent operations from BPF programs and sys_bpf()",
            " *",
            " * BPF programs can execute in any context including perf, kprobes and",
            " * tracing. As there are almost no limits where perf, kprobes and tracing",
            " * can be invoked from the lock operations need to be protected against",
            " * deadlocks. Deadlocks can be caused by recursion and by an invocation in",
            " * the lock held section when functions which acquire this lock are invoked",
            " * from sys_bpf(). BPF recursion is prevented by incrementing the per CPU",
            " * variable bpf_prog_active, which prevents BPF programs attached to perf",
            " * events, kprobes and tracing to be invoked before the prior invocation",
            " * from one of these contexts completed. sys_bpf() uses the same mechanism",
            " * by pinning the task to the current CPU and incrementing the recursion",
            " * protection across the map operation.",
            " *",
            " * This has subtle implications on PREEMPT_RT. PREEMPT_RT forbids certain",
            " * operations like memory allocations (even with GFP_ATOMIC) from atomic",
            " * contexts. This is required because even with GFP_ATOMIC the memory",
            " * allocator calls into code paths which acquire locks with long held lock",
            " * sections. To ensure the deterministic behaviour these locks are regular",
            " * spinlocks, which are converted to 'sleepable' spinlocks on RT. The only",
            " * true atomic contexts on an RT kernel are the low level hardware",
            " * handling, scheduling, low level interrupt handling, NMIs etc. None of",
            " * these contexts should ever do memory allocations.",
            " *",
            " * As regular device interrupt handlers and soft interrupts are forced into",
            " * thread context, the existing code which does",
            " *   spin_lock*(); alloc(GFP_ATOMIC); spin_unlock*();",
            " * just works.",
            " *",
            " * In theory the BPF locks could be converted to regular spinlocks as well,",
            " * but the bucket locks and percpu_freelist locks can be taken from",
            " * arbitrary contexts (perf, kprobes, tracepoints) which are required to be",
            " * atomic contexts even on RT. Before the introduction of bpf_mem_alloc,",
            " * it is only safe to use raw spinlock for preallocated hash map on a RT kernel,",
            " * because there is no memory allocation within the lock held sections. However",
            " * after hash map was fully converted to use bpf_mem_alloc, there will be",
            " * non-synchronous memory allocation for non-preallocated hash map, so it is",
            " * safe to always use raw spinlock for bucket lock.",
            " */",
            "struct bucket {",
            "\tstruct hlist_nulls_head head;",
            "\traw_spinlock_t raw_lock;",
            "};",
            "",
            "#define HASHTAB_MAP_LOCK_COUNT 8",
            "#define HASHTAB_MAP_LOCK_MASK (HASHTAB_MAP_LOCK_COUNT - 1)",
            "",
            "struct bpf_htab {",
            "\tstruct bpf_map map;",
            "\tstruct bpf_mem_alloc ma;",
            "\tstruct bpf_mem_alloc pcpu_ma;",
            "\tstruct bucket *buckets;",
            "\tvoid *elems;",
            "\tunion {",
            "\t\tstruct pcpu_freelist freelist;",
            "\t\tstruct bpf_lru lru;",
            "\t};",
            "\tstruct htab_elem *__percpu *extra_elems;",
            "\t/* number of elements in non-preallocated hashtable are kept",
            "\t * in either pcount or count",
            "\t */",
            "\tstruct percpu_counter pcount;",
            "\tatomic_t count;",
            "\tbool use_percpu_counter;",
            "\tu32 n_buckets;\t/* number of hash buckets */",
            "\tu32 elem_size;\t/* size of each element in bytes */",
            "\tu32 hashrnd;",
            "\tstruct lock_class_key lockdep_key;",
            "\tint __percpu *map_locked[HASHTAB_MAP_LOCK_COUNT];",
            "};",
            "",
            "/* each htab element is struct htab_elem + key + value */",
            "struct htab_elem {",
            "\tunion {",
            "\t\tstruct hlist_nulls_node hash_node;",
            "\t\tstruct {",
            "\t\t\tvoid *padding;",
            "\t\t\tunion {",
            "\t\t\t\tstruct pcpu_freelist_node fnode;",
            "\t\t\t\tstruct htab_elem *batch_flink;",
            "\t\t\t};",
            "\t\t};",
            "\t};",
            "\tunion {",
            "\t\t/* pointer to per-cpu pointer */",
            "\t\tvoid *ptr_to_pptr;",
            "\t\tstruct bpf_lru_node lru_node;",
            "\t};",
            "\tu32 hash;",
            "\tchar key[] __aligned(8);",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义BPF哈希表相关的头文件和宏常量，声明bucket结构体及bpf_htab结构体，包含哈希表的桶锁、元素存储、LRU/PCPU管理等核心成员变量，描述了哈希表的并发控制机制和内存分配策略。",
          "similarity": 0.5122296810150146
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1215,
          "end_line": 1341,
          "content": [
            "static void htab_lru_push_free(struct bpf_htab *htab, struct htab_elem *elem)",
            "{",
            "\tcheck_and_free_fields(htab, elem);",
            "\tbpf_map_dec_elem_count(&htab->map);",
            "\tbpf_lru_push_free(&htab->lru, &elem->lru_node);",
            "}",
            "static long htab_lru_map_update_elem(struct bpf_map *map, void *key, void *value,",
            "\t\t\t\t     u64 map_flags)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tstruct htab_elem *l_new, *l_old = NULL;",
            "\tstruct hlist_nulls_head *head;",
            "\tunsigned long flags;",
            "\tstruct bucket *b;",
            "\tu32 key_size, hash;",
            "\tint ret;",
            "",
            "\tif (unlikely(map_flags > BPF_EXIST))",
            "\t\t/* unknown flags */",
            "\t\treturn -EINVAL;",
            "",
            "\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
            "\t\t     !rcu_read_lock_bh_held());",
            "",
            "\tkey_size = map->key_size;",
            "",
            "\thash = htab_map_hash(key, key_size, htab->hashrnd);",
            "",
            "\tb = __select_bucket(htab, hash);",
            "\thead = &b->head;",
            "",
            "\t/* For LRU, we need to alloc before taking bucket's",
            "\t * spinlock because getting free nodes from LRU may need",
            "\t * to remove older elements from htab and this removal",
            "\t * operation will need a bucket lock.",
            "\t */",
            "\tl_new = prealloc_lru_pop(htab, key, hash);",
            "\tif (!l_new)",
            "\t\treturn -ENOMEM;",
            "\tcopy_map_value(&htab->map,",
            "\t\t       l_new->key + round_up(map->key_size, 8), value);",
            "",
            "\tret = htab_lock_bucket(htab, b, hash, &flags);",
            "\tif (ret)",
            "\t\tgoto err_lock_bucket;",
            "",
            "\tl_old = lookup_elem_raw(head, hash, key, key_size);",
            "",
            "\tret = check_flags(htab, l_old, map_flags);",
            "\tif (ret)",
            "\t\tgoto err;",
            "",
            "\t/* add new element to the head of the list, so that",
            "\t * concurrent search will find it before old elem",
            "\t */",
            "\thlist_nulls_add_head_rcu(&l_new->hash_node, head);",
            "\tif (l_old) {",
            "\t\tbpf_lru_node_set_ref(&l_new->lru_node);",
            "\t\thlist_nulls_del_rcu(&l_old->hash_node);",
            "\t}",
            "\tret = 0;",
            "",
            "err:",
            "\thtab_unlock_bucket(htab, b, hash, flags);",
            "",
            "err_lock_bucket:",
            "\tif (ret)",
            "\t\thtab_lru_push_free(htab, l_new);",
            "\telse if (l_old)",
            "\t\thtab_lru_push_free(htab, l_old);",
            "",
            "\treturn ret;",
            "}",
            "static long __htab_percpu_map_update_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\t  void *value, u64 map_flags,",
            "\t\t\t\t\t  bool onallcpus)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tstruct htab_elem *l_new = NULL, *l_old;",
            "\tstruct hlist_nulls_head *head;",
            "\tunsigned long flags;",
            "\tstruct bucket *b;",
            "\tu32 key_size, hash;",
            "\tint ret;",
            "",
            "\tif (unlikely(map_flags > BPF_EXIST))",
            "\t\t/* unknown flags */",
            "\t\treturn -EINVAL;",
            "",
            "\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&",
            "\t\t     !rcu_read_lock_bh_held());",
            "",
            "\tkey_size = map->key_size;",
            "",
            "\thash = htab_map_hash(key, key_size, htab->hashrnd);",
            "",
            "\tb = __select_bucket(htab, hash);",
            "\thead = &b->head;",
            "",
            "\tret = htab_lock_bucket(htab, b, hash, &flags);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tl_old = lookup_elem_raw(head, hash, key, key_size);",
            "",
            "\tret = check_flags(htab, l_old, map_flags);",
            "\tif (ret)",
            "\t\tgoto err;",
            "",
            "\tif (l_old) {",
            "\t\t/* per-cpu hash map can update value in-place */",
            "\t\tpcpu_copy_value(htab, htab_elem_get_ptr(l_old, key_size),",
            "\t\t\t\tvalue, onallcpus);",
            "\t} else {",
            "\t\tl_new = alloc_htab_elem(htab, key, value, key_size,",
            "\t\t\t\t\thash, true, onallcpus, NULL);",
            "\t\tif (IS_ERR(l_new)) {",
            "\t\t\tret = PTR_ERR(l_new);",
            "\t\t\tgoto err;",
            "\t\t}",
            "\t\thlist_nulls_add_head_rcu(&l_new->hash_node, head);",
            "\t}",
            "\tret = 0;",
            "err:",
            "\thtab_unlock_bucket(htab, b, hash, flags);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "htab_lru_push_free, htab_lru_map_update_elem, __htab_percpu_map_update_elem",
          "description": "实现LRU策略的元素管理，包含元素推送至自由列表、带LRU特性的更新操作及Per-CPU哈希表的值复制逻辑",
          "similarity": 0.46711134910583496
        },
        {
          "chunk_id": 11,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 1701,
          "end_line": 1938,
          "content": [
            "static int htab_lru_percpu_map_lookup_and_delete_elem(struct bpf_map *map,",
            "\t\t\t\t\t\t      void *key, void *value,",
            "\t\t\t\t\t\t      u64 flags)",
            "{",
            "\treturn __htab_map_lookup_and_delete_elem(map, key, value, true, true,",
            "\t\t\t\t\t\t flags);",
            "}",
            "static int",
            "__htab_map_lookup_and_delete_batch(struct bpf_map *map,",
            "\t\t\t\t   const union bpf_attr *attr,",
            "\t\t\t\t   union bpf_attr __user *uattr,",
            "\t\t\t\t   bool do_delete, bool is_lru_map,",
            "\t\t\t\t   bool is_percpu)",
            "{",
            "\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);",
            "\tu32 bucket_cnt, total, key_size, value_size, roundup_key_size;",
            "\tvoid *keys = NULL, *values = NULL, *value, *dst_key, *dst_val;",
            "\tvoid __user *uvalues = u64_to_user_ptr(attr->batch.values);",
            "\tvoid __user *ukeys = u64_to_user_ptr(attr->batch.keys);",
            "\tvoid __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);",
            "\tu32 batch, max_count, size, bucket_size, map_id;",
            "\tstruct htab_elem *node_to_free = NULL;",
            "\tu64 elem_map_flags, map_flags;",
            "\tstruct hlist_nulls_head *head;",
            "\tstruct hlist_nulls_node *n;",
            "\tunsigned long flags = 0;",
            "\tbool locked = false;",
            "\tstruct htab_elem *l;",
            "\tstruct bucket *b;",
            "\tint ret = 0;",
            "",
            "\telem_map_flags = attr->batch.elem_flags;",
            "\tif ((elem_map_flags & ~BPF_F_LOCK) ||",
            "\t    ((elem_map_flags & BPF_F_LOCK) && !btf_record_has_field(map->record, BPF_SPIN_LOCK)))",
            "\t\treturn -EINVAL;",
            "",
            "\tmap_flags = attr->batch.flags;",
            "\tif (map_flags)",
            "\t\treturn -EINVAL;",
            "",
            "\tmax_count = attr->batch.count;",
            "\tif (!max_count)",
            "\t\treturn 0;",
            "",
            "\tif (put_user(0, &uattr->batch.count))",
            "\t\treturn -EFAULT;",
            "",
            "\tbatch = 0;",
            "\tif (ubatch && copy_from_user(&batch, ubatch, sizeof(batch)))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (batch >= htab->n_buckets)",
            "\t\treturn -ENOENT;",
            "",
            "\tkey_size = htab->map.key_size;",
            "\troundup_key_size = round_up(htab->map.key_size, 8);",
            "\tvalue_size = htab->map.value_size;",
            "\tsize = round_up(value_size, 8);",
            "\tif (is_percpu)",
            "\t\tvalue_size = size * num_possible_cpus();",
            "\ttotal = 0;",
            "\t/* while experimenting with hash tables with sizes ranging from 10 to",
            "\t * 1000, it was observed that a bucket can have up to 5 entries.",
            "\t */",
            "\tbucket_size = 5;",
            "",
            "alloc:",
            "\t/* We cannot do copy_from_user or copy_to_user inside",
            "\t * the rcu_read_lock. Allocate enough space here.",
            "\t */",
            "\tkeys = kvmalloc_array(key_size, bucket_size, GFP_USER | __GFP_NOWARN);",
            "\tvalues = kvmalloc_array(value_size, bucket_size, GFP_USER | __GFP_NOWARN);",
            "\tif (!keys || !values) {",
            "\t\tret = -ENOMEM;",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "again:",
            "\tbpf_disable_instrumentation();",
            "\trcu_read_lock();",
            "again_nocopy:",
            "\tdst_key = keys;",
            "\tdst_val = values;",
            "\tb = &htab->buckets[batch];",
            "\thead = &b->head;",
            "\t/* do not grab the lock unless need it (bucket_cnt > 0). */",
            "\tif (locked) {",
            "\t\tret = htab_lock_bucket(htab, b, batch, &flags);",
            "\t\tif (ret) {",
            "\t\t\trcu_read_unlock();",
            "\t\t\tbpf_enable_instrumentation();",
            "\t\t\tgoto after_loop;",
            "\t\t}",
            "\t}",
            "",
            "\tbucket_cnt = 0;",
            "\thlist_nulls_for_each_entry_rcu(l, n, head, hash_node)",
            "\t\tbucket_cnt++;",
            "",
            "\tif (bucket_cnt && !locked) {",
            "\t\tlocked = true;",
            "\t\tgoto again_nocopy;",
            "\t}",
            "",
            "\tif (bucket_cnt > (max_count - total)) {",
            "\t\tif (total == 0)",
            "\t\t\tret = -ENOSPC;",
            "\t\t/* Note that since bucket_cnt > 0 here, it is implicit",
            "\t\t * that the locked was grabbed, so release it.",
            "\t\t */",
            "\t\thtab_unlock_bucket(htab, b, batch, flags);",
            "\t\trcu_read_unlock();",
            "\t\tbpf_enable_instrumentation();",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "\tif (bucket_cnt > bucket_size) {",
            "\t\tbucket_size = bucket_cnt;",
            "\t\t/* Note that since bucket_cnt > 0 here, it is implicit",
            "\t\t * that the locked was grabbed, so release it.",
            "\t\t */",
            "\t\thtab_unlock_bucket(htab, b, batch, flags);",
            "\t\trcu_read_unlock();",
            "\t\tbpf_enable_instrumentation();",
            "\t\tkvfree(keys);",
            "\t\tkvfree(values);",
            "\t\tgoto alloc;",
            "\t}",
            "",
            "\t/* Next block is only safe to run if you have grabbed the lock */",
            "\tif (!locked)",
            "\t\tgoto next_batch;",
            "",
            "\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {",
            "\t\tmemcpy(dst_key, l->key, key_size);",
            "",
            "\t\tif (is_percpu) {",
            "\t\t\tint off = 0, cpu;",
            "\t\t\tvoid __percpu *pptr;",
            "",
            "\t\t\tpptr = htab_elem_get_ptr(l, map->key_size);",
            "\t\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\t\tcopy_map_value_long(&htab->map, dst_val + off, per_cpu_ptr(pptr, cpu));",
            "\t\t\t\tcheck_and_init_map_value(&htab->map, dst_val + off);",
            "\t\t\t\toff += size;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tvalue = l->key + roundup_key_size;",
            "\t\t\tif (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {",
            "\t\t\t\tstruct bpf_map **inner_map = value;",
            "",
            "\t\t\t\t /* Actual value is the id of the inner map */",
            "\t\t\t\tmap_id = map->ops->map_fd_sys_lookup_elem(*inner_map);",
            "\t\t\t\tvalue = &map_id;",
            "\t\t\t}",
            "",
            "\t\t\tif (elem_map_flags & BPF_F_LOCK)",
            "\t\t\t\tcopy_map_value_locked(map, dst_val, value,",
            "\t\t\t\t\t\t      true);",
            "\t\t\telse",
            "\t\t\t\tcopy_map_value(map, dst_val, value);",
            "\t\t\t/* Zeroing special fields in the temp buffer */",
            "\t\t\tcheck_and_init_map_value(map, dst_val);",
            "\t\t}",
            "\t\tif (do_delete) {",
            "\t\t\thlist_nulls_del_rcu(&l->hash_node);",
            "",
            "\t\t\t/* bpf_lru_push_free() will acquire lru_lock, which",
            "\t\t\t * may cause deadlock. See comments in function",
            "\t\t\t * prealloc_lru_pop(). Let us do bpf_lru_push_free()",
            "\t\t\t * after releasing the bucket lock.",
            "\t\t\t *",
            "\t\t\t * For htab of maps, htab_put_fd_value() in",
            "\t\t\t * free_htab_elem() may acquire a spinlock with bucket",
            "\t\t\t * lock being held and it violates the lock rule, so",
            "\t\t\t * invoke free_htab_elem() after unlock as well.",
            "\t\t\t */",
            "\t\t\tl->batch_flink = node_to_free;",
            "\t\t\tnode_to_free = l;",
            "\t\t}",
            "\t\tdst_key += key_size;",
            "\t\tdst_val += value_size;",
            "\t}",
            "",
            "\thtab_unlock_bucket(htab, b, batch, flags);",
            "\tlocked = false;",
            "",
            "\twhile (node_to_free) {",
            "\t\tl = node_to_free;",
            "\t\tnode_to_free = node_to_free->batch_flink;",
            "\t\tif (is_lru_map)",
            "\t\t\thtab_lru_push_free(htab, l);",
            "\t\telse",
            "\t\t\tfree_htab_elem(htab, l);",
            "\t}",
            "",
            "next_batch:",
            "\t/* If we are not copying data, we can go to next bucket and avoid",
            "\t * unlocking the rcu.",
            "\t */",
            "\tif (!bucket_cnt && (batch + 1 < htab->n_buckets)) {",
            "\t\tbatch++;",
            "\t\tgoto again_nocopy;",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "\tbpf_enable_instrumentation();",
            "\tif (bucket_cnt && (copy_to_user(ukeys + total * key_size, keys,",
            "\t    key_size * bucket_cnt) ||",
            "\t    copy_to_user(uvalues + total * value_size, values,",
            "\t    value_size * bucket_cnt))) {",
            "\t\tret = -EFAULT;",
            "\t\tgoto after_loop;",
            "\t}",
            "",
            "\ttotal += bucket_cnt;",
            "\tbatch++;",
            "\tif (batch >= htab->n_buckets) {",
            "\t\tret = -ENOENT;",
            "\t\tgoto after_loop;",
            "\t}",
            "\tgoto again;",
            "",
            "after_loop:",
            "\tif (ret == -EFAULT)",
            "\t\tgoto out;",
            "",
            "\t/* copy # of entries and next batch */",
            "\tubatch = u64_to_user_ptr(attr->batch.out_batch);",
            "\tif (copy_to_user(ubatch, &batch, sizeof(batch)) ||",
            "\t    put_user(total, &uattr->batch.count))",
            "\t\tret = -EFAULT;",
            "",
            "out:",
            "\tkvfree(keys);",
            "\tkvfree(values);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "htab_lru_percpu_map_lookup_and_delete_elem, __htab_map_lookup_and_delete_batch",
          "description": "处理批量查找删除操作，通过RCU读锁遍历指定桶内元素，支持普通/PERCPU/LRU类型。动态分配缓冲区复制键值，处理锁竞争和内存溢出情况，返回操作结果及统计信息。",
          "similarity": 0.46204131841659546
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 275,
          "end_line": 381,
          "content": [
            "static void htab_free_elems(struct bpf_htab *htab)",
            "{",
            "\tint i;",
            "",
            "\tif (!htab_is_percpu(htab))",
            "\t\tgoto free_elems;",
            "",
            "\tfor (i = 0; i < htab->map.max_entries; i++) {",
            "\t\tvoid __percpu *pptr;",
            "",
            "\t\tpptr = htab_elem_get_ptr(get_htab_elem(htab, i),",
            "\t\t\t\t\t htab->map.key_size);",
            "\t\tfree_percpu(pptr);",
            "\t\tcond_resched();",
            "\t}",
            "free_elems:",
            "\tbpf_map_area_free(htab->elems);",
            "}",
            "static int prealloc_init(struct bpf_htab *htab)",
            "{",
            "\tu32 num_entries = htab->map.max_entries;",
            "\tint err = -ENOMEM, i;",
            "",
            "\tif (htab_has_extra_elems(htab))",
            "\t\tnum_entries += num_possible_cpus();",
            "",
            "\thtab->elems = bpf_map_area_alloc((u64)htab->elem_size * num_entries,",
            "\t\t\t\t\t htab->map.numa_node);",
            "\tif (!htab->elems)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (!htab_is_percpu(htab))",
            "\t\tgoto skip_percpu_elems;",
            "",
            "\tfor (i = 0; i < num_entries; i++) {",
            "\t\tu32 size = round_up(htab->map.value_size, 8);",
            "\t\tvoid __percpu *pptr;",
            "",
            "\t\tpptr = bpf_map_alloc_percpu(&htab->map, size, 8,",
            "\t\t\t\t\t    GFP_USER | __GFP_NOWARN);",
            "\t\tif (!pptr)",
            "\t\t\tgoto free_elems;",
            "\t\thtab_elem_set_ptr(get_htab_elem(htab, i), htab->map.key_size,",
            "\t\t\t\t  pptr);",
            "\t\tcond_resched();",
            "\t}",
            "",
            "skip_percpu_elems:",
            "\tif (htab_is_lru(htab))",
            "\t\terr = bpf_lru_init(&htab->lru,",
            "\t\t\t\t   htab->map.map_flags & BPF_F_NO_COMMON_LRU,",
            "\t\t\t\t   offsetof(struct htab_elem, hash) -",
            "\t\t\t\t   offsetof(struct htab_elem, lru_node),",
            "\t\t\t\t   htab_lru_map_delete_node,",
            "\t\t\t\t   htab);",
            "\telse",
            "\t\terr = pcpu_freelist_init(&htab->freelist);",
            "",
            "\tif (err)",
            "\t\tgoto free_elems;",
            "",
            "\tif (htab_is_lru(htab))",
            "\t\tbpf_lru_populate(&htab->lru, htab->elems,",
            "\t\t\t\t offsetof(struct htab_elem, lru_node),",
            "\t\t\t\t htab->elem_size, num_entries);",
            "\telse",
            "\t\tpcpu_freelist_populate(&htab->freelist,",
            "\t\t\t\t       htab->elems + offsetof(struct htab_elem, fnode),",
            "\t\t\t\t       htab->elem_size, num_entries);",
            "",
            "\treturn 0;",
            "",
            "free_elems:",
            "\thtab_free_elems(htab);",
            "\treturn err;",
            "}",
            "static void prealloc_destroy(struct bpf_htab *htab)",
            "{",
            "\thtab_free_elems(htab);",
            "",
            "\tif (htab_is_lru(htab))",
            "\t\tbpf_lru_destroy(&htab->lru);",
            "\telse",
            "\t\tpcpu_freelist_destroy(&htab->freelist);",
            "}",
            "static int alloc_extra_elems(struct bpf_htab *htab)",
            "{",
            "\tstruct htab_elem *__percpu *pptr, *l_new;",
            "\tstruct pcpu_freelist_node *l;",
            "\tint cpu;",
            "",
            "\tpptr = bpf_map_alloc_percpu(&htab->map, sizeof(struct htab_elem *), 8,",
            "\t\t\t\t    GFP_USER | __GFP_NOWARN);",
            "\tif (!pptr)",
            "\t\treturn -ENOMEM;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tl = pcpu_freelist_pop(&htab->freelist);",
            "\t\t/* pop will succeed, since prealloc_init()",
            "\t\t * preallocated extra num_possible_cpus elements",
            "\t\t */",
            "\t\tl_new = container_of(l, struct htab_elem, fnode);",
            "\t\t*per_cpu_ptr(pptr, cpu) = l_new;",
            "\t}",
            "\thtab->extra_elems = pptr;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "htab_free_elems, prealloc_init, prealloc_destroy, alloc_extra_elems",
          "description": "实现预分配元素的初始化、销毁和额外元素分配逻辑，包含内存分配、指针对齐处理、PCPU资源管理等细节，负责构建哈希表的物理存储结构。",
          "similarity": 0.4608001708984375
        },
        {
          "chunk_id": 15,
          "file_path": "kernel/bpf/hashtab.c",
          "start_line": 2555,
          "end_line": 2611,
          "content": [
            "int bpf_fd_htab_map_lookup_elem(struct bpf_map *map, void *key, u32 *value)",
            "{",
            "\tvoid **ptr;",
            "\tint ret = 0;",
            "",
            "\tif (!map->ops->map_fd_sys_lookup_elem)",
            "\t\treturn -ENOTSUPP;",
            "",
            "\trcu_read_lock();",
            "\tptr = htab_map_lookup_elem(map, key);",
            "\tif (ptr)",
            "\t\t*value = map->ops->map_fd_sys_lookup_elem(READ_ONCE(*ptr));",
            "\telse",
            "\t\tret = -ENOENT;",
            "\trcu_read_unlock();",
            "",
            "\treturn ret;",
            "}",
            "int bpf_fd_htab_map_update_elem(struct bpf_map *map, struct file *map_file,",
            "\t\t\t\tvoid *key, void *value, u64 map_flags)",
            "{",
            "\tvoid *ptr;",
            "\tint ret;",
            "\tu32 ufd = *(u32 *)value;",
            "",
            "\tptr = map->ops->map_fd_get_ptr(map, map_file, ufd);",
            "\tif (IS_ERR(ptr))",
            "\t\treturn PTR_ERR(ptr);",
            "",
            "\tret = htab_map_update_elem(map, key, &ptr, map_flags);",
            "\tif (ret)",
            "\t\tmap->ops->map_fd_put_ptr(map, ptr, false);",
            "",
            "\treturn ret;",
            "}",
            "static int htab_of_map_gen_lookup(struct bpf_map *map,",
            "\t\t\t\t  struct bpf_insn *insn_buf)",
            "{",
            "\tstruct bpf_insn *insn = insn_buf;",
            "\tconst int ret = BPF_REG_0;",
            "",
            "\tBUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,",
            "\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));",
            "\t*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);",
            "\t*insn++ = BPF_JMP_IMM(BPF_JEQ, ret, 0, 2);",
            "\t*insn++ = BPF_ALU64_IMM(BPF_ADD, ret,",
            "\t\t\t\toffsetof(struct htab_elem, key) +",
            "\t\t\t\tround_up(map->key_size, 8));",
            "\t*insn++ = BPF_LDX_MEM(BPF_DW, ret, ret, 0);",
            "",
            "\treturn insn - insn_buf;",
            "}",
            "static void htab_of_map_free(struct bpf_map *map)",
            "{",
            "\tbpf_map_meta_free(map->inner_map_meta);",
            "\tfd_htab_map_free(map);",
            "}"
          ],
          "function_name": "bpf_fd_htab_map_lookup_elem, bpf_fd_htab_map_update_elem, htab_of_map_gen_lookup, htab_of_map_free",
          "description": "该代码段实现了基于文件描述符的哈希表操作，包含查找、更新及释放逻辑。  \n`bpf_fd_htab_map_lookup_elem` 和 `bpf_fd_htab_map_update_elem` 分别用于通过文件描述符键查找和更新哈希表项，依赖于 `map->ops` 中的回调函数。  \n`htab_of_map_gen_lookup` 生成 eBPF 指令以调用哈希表查找逻辑，`htab_of_map_free` 释放哈希表相关元数据；部分底层函数（如 `htab_map_lookup_elem`）未展示，上下文不完整。",
          "similarity": 0.4351368546485901
        }
      ]
    }
  ]
}