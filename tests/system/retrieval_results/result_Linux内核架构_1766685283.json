{
  "query": "Linux内核架构",
  "timestamp": "2025-12-26 01:54:43",
  "retrieved_files": [
    {
      "source_file": "mm/list_lru.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:35:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `list_lru.c`\n\n---\n\n# list_lru.c 技术文档\n\n## 1. 文件概述\n\n`list_lru.c` 实现了 Linux 内核中通用的 **List-based LRU（Least Recently Used）基础设施**，用于管理可回收对象的双向链表。该机制支持按 NUMA 节点（node）和内存控制组（memcg）进行细粒度组织，便于内存压力下的高效回收。主要服务于 slab 分配器等子系统，作为 shrinker 框架的一部分，在内存紧张时协助释放非活跃对象。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct list_lru`：顶层 LRU 管理结构，包含 per-node 的 `list_lru_node`\n- `struct list_lru_node`：每个 NUMA 节点对应的 LRU 节点，含自旋锁和总项数\n- `struct list_lru_one`：实际存储对象链表和计数的单元（per-memcg per-node）\n- `struct list_lru_memcg`：当启用 `CONFIG_MEMCG` 时，为每个 memcg 存储 per-node 的 `list_lru_one`\n\n### 主要导出函数\n- `list_lru_add()` / `list_lru_add_obj()`：向 LRU 添加对象\n- `list_lru_del()` / `list_lru_del_obj()`：从 LRU 删除对象\n- `list_lru_isolate()` / `list_lru_isolate_move()`：在回收过程中隔离对象\n- `list_lru_count_one()` / `list_lru_count_node()`：查询 LRU 中对象数量\n- `list_lru_walk_one()` / `list_lru_walk_node()`：遍历并处理 LRU 中的对象（用于 shrinker 回调）\n\n### 内部辅助函数\n- `list_lru_from_memcg_idx()`：根据 memcg ID 获取对应的 `list_lru_one`\n- `__list_lru_walk_one()`：带锁的 LRU 遍历核心逻辑\n- `list_lru_register()` / `list_lru_unregister()`：注册/注销 memcg-aware 的 LRU（用于全局追踪）\n\n## 3. 关键实现\n\n### 内存控制组（memcg）支持\n- 通过 `CONFIG_MEMCG` 条件编译控制 memcg 相关逻辑\n- 使用 XArray (`lru->xa`) 动态存储每个 memcg 对应的 `list_lru_memcg` 结构\n- 每个 memcg 在每个 NUMA 节点上拥有独立的 `list_lru_one`，实现资源隔离\n- 全局 `memcg_list_lrus` 链表和 `list_lrus_mutex` 用于跟踪所有 memcg-aware 的 LRU 实例\n\n### 并发控制\n- 每个 NUMA 节点 (`list_lru_node`) 拥有独立的自旋锁 (`nlru->lock`)\n- 所有对 LRU 链表的操作（增、删、遍历）均在对应节点锁保护下进行\n- 提供 `_irq` 版本的遍历函数（`list_lru_walk_one_irq`）用于中断上下文\n\n### 回收遍历机制\n- `list_lru_walk_*` 函数接受回调函数 `isolate`，由调用者定义回收策略\n- 回调返回值控制遍历行为：\n  - `LRU_REMOVED`：成功移除\n  - `LRU_REMOVED_RETRY`：移除后需重新开始遍历（锁曾被释放）\n  - `LRU_RETRY`：未移除但需重新开始遍历\n  - `LRU_ROTATE`：将对象移到链表尾部（标记为最近使用）\n  - `LRU_SKIP`：跳过当前对象\n  - `LRU_STOP`：立即停止遍历\n- 通过 `nr_to_walk` 限制单次遍历的最大对象数，防止长时间持锁\n\n### Shrinker 集成\n- 当向空的 `list_lru_one` 添加首个对象时，调用 `set_shrinker_bit()` 标记该 memcg/node 需要被 shrinker 处理\n- `lru_shrinker_id()` 返回关联的 shrinker ID，用于通知内存回收子系统\n\n### 对象归属识别\n- `list_lru_add_obj()` / `list_lru_del_obj()` 通过 `mem_cgroup_from_slab_obj()` 自动获取对象所属的 memcg\n- 使用 `page_to_nid(virt_to_page(item))` 确定对象所在的 NUMA 节点\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/list_lru.h>`：定义核心数据结构和 API\n  - `<linux/memcontrol.h>`：memcg 相关接口（如 `memcg_kmem_id`）\n  - `\"slab.h\"` 和 `\"internal.h\"`：slab 分配器内部接口（如 `mem_cgroup_from_slab_obj`）\n- **配置依赖**：\n  - `CONFIG_MEMCG`：决定是否编译 memcg 相关代码\n  - `CONFIG_NUMA`：影响 per-node 数据结构的大小（通过 `nr_node_ids`）\n- **子系统依赖**：\n  - Slab 分配器：作为主要使用者，管理可回收 slab 对象\n  - Memory Control Group (memcg)：提供内存隔离和记账\n  - Shrinker 框架：通过 shrinker 回调触发 LRU 遍历回收\n\n## 5. 使用场景\n\n- **Slab 对象回收**：当系统内存压力大时，shrinker 通过 `list_lru_walk_*` 遍历 inactive slab 对象链表，释放可回收对象\n- **Per-memcg 内存限制**：在 cgroup 内存超限时，仅遍历该 memcg 对应的 LRU 部分，实现精确回收\n- **NUMA 感知管理**：按 NUMA 节点分离 LRU 链表，减少远程内存访问，提升性能\n- **通用 LRU 容器**：任何需要按 LRU 策略管理可回收对象的内核子系统均可使用此基础设施（如 dentry、inode 缓存等）",
      "similarity": 0.6314166784286499,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "mm/list_lru.c",
          "start_line": 425,
          "end_line": 551,
          "content": [
            "static void memcg_reparent_list_lru(struct list_lru *lru,",
            "\t\t\t\t    int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tint i;",
            "",
            "\tfor_each_node(i)",
            "\t\tmemcg_reparent_list_lru_node(lru, i, src_idx, dst_memcg);",
            "",
            "\tmemcg_list_lru_free(lru, src_idx);",
            "}",
            "void memcg_reparent_list_lrus(struct mem_cgroup *memcg, struct mem_cgroup *parent)",
            "{",
            "\tstruct cgroup_subsys_state *css;",
            "\tstruct list_lru *lru;",
            "\tint src_idx = memcg->kmemcg_id;",
            "",
            "\t/*",
            "\t * Change kmemcg_id of this cgroup and all its descendants to the",
            "\t * parent's id, and then move all entries from this cgroup's list_lrus",
            "\t * to ones of the parent.",
            "\t *",
            "\t * After we have finished, all list_lrus corresponding to this cgroup",
            "\t * are guaranteed to remain empty. So we can safely free this cgroup's",
            "\t * list lrus in memcg_list_lru_free().",
            "\t *",
            "\t * Changing ->kmemcg_id to the parent can prevent memcg_list_lru_alloc()",
            "\t * from allocating list lrus for this cgroup after memcg_list_lru_free()",
            "\t * call.",
            "\t */",
            "\trcu_read_lock();",
            "\tcss_for_each_descendant_pre(css, &memcg->css) {",
            "\t\tstruct mem_cgroup *child;",
            "",
            "\t\tchild = mem_cgroup_from_css(css);",
            "\t\tWRITE_ONCE(child->kmemcg_id, parent->kmemcg_id);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_for_each_entry(lru, &memcg_list_lrus, list)",
            "\t\tmemcg_reparent_list_lru(lru, src_idx, parent);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static inline bool memcg_list_lru_allocated(struct mem_cgroup *memcg,",
            "\t\t\t\t\t    struct list_lru *lru)",
            "{",
            "\tint idx = memcg->kmemcg_id;",
            "",
            "\treturn idx < 0 || xa_load(&lru->xa, idx);",
            "}",
            "int memcg_list_lru_alloc(struct mem_cgroup *memcg, struct list_lru *lru,",
            "\t\t\t gfp_t gfp)",
            "{",
            "\tint i;",
            "\tunsigned long flags;",
            "\tstruct list_lru_memcg_table {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tstruct mem_cgroup *memcg;",
            "\t} *table;",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "",
            "\tif (!list_lru_memcg_aware(lru) || memcg_list_lru_allocated(memcg, lru))",
            "\t\treturn 0;",
            "",
            "\tgfp &= GFP_RECLAIM_MASK;",
            "\ttable = kmalloc_array(memcg->css.cgroup->level, sizeof(*table), gfp);",
            "\tif (!table)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * Because the list_lru can be reparented to the parent cgroup's",
            "\t * list_lru, we should make sure that this cgroup and all its",
            "\t * ancestors have allocated list_lru_memcg.",
            "\t */",
            "\tfor (i = 0; memcg; memcg = parent_mem_cgroup(memcg), i++) {",
            "\t\tif (memcg_list_lru_allocated(memcg, lru))",
            "\t\t\tbreak;",
            "",
            "\t\ttable[i].memcg = memcg;",
            "\t\ttable[i].mlru = memcg_init_list_lru_one(gfp);",
            "\t\tif (!table[i].mlru) {",
            "\t\t\twhile (i--)",
            "\t\t\t\tkfree(table[i].mlru);",
            "\t\t\tkfree(table);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t}",
            "",
            "\txas_lock_irqsave(&xas, flags);",
            "\twhile (i--) {",
            "\t\tint index = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\tstruct list_lru_memcg *mlru = table[i].mlru;",
            "",
            "\t\txas_set(&xas, index);",
            "retry:",
            "\t\tif (unlikely(index < 0 || xas_error(&xas) || xas_load(&xas))) {",
            "\t\t\tkfree(mlru);",
            "\t\t} else {",
            "\t\t\txas_store(&xas, mlru);",
            "\t\t\tif (xas_error(&xas) == -ENOMEM) {",
            "\t\t\t\txas_unlock_irqrestore(&xas, flags);",
            "\t\t\t\tif (xas_nomem(&xas, gfp))",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\txas_lock_irqsave(&xas, flags);",
            "\t\t\t\t/*",
            "\t\t\t\t * The xas lock has been released, this memcg",
            "\t\t\t\t * can be reparented before us. So reload",
            "\t\t\t\t * memcg id. More details see the comments",
            "\t\t\t\t * in memcg_reparent_list_lrus().",
            "\t\t\t\t */",
            "\t\t\t\tindex = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\t\t\tif (index < 0)",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\telse if (!xas_error(&xas) && index != xas.xa_index)",
            "\t\t\t\t\txas_set(&xas, index);",
            "\t\t\t\tgoto retry;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\t/* xas_nomem() is used to free memory instead of memory allocation. */",
            "\tif (xas.xa_alloc)",
            "\t\txas_nomem(&xas, gfp);",
            "\txas_unlock_irqrestore(&xas, flags);",
            "\tkfree(table);",
            "",
            "\treturn xas_error(&xas);",
            "}"
          ],
          "function_name": "memcg_reparent_list_lru, memcg_reparent_list_lrus, memcg_list_lru_allocated, memcg_list_lru_alloc",
          "description": "实现内存组层级间的LRU列表迁移与分配机制，包含递归子组处理、动态分配/释放LRU结构体及冲突解决逻辑。",
          "similarity": 0.6103495359420776
        },
        {
          "chunk_id": 1,
          "file_path": "mm/list_lru.c",
          "start_line": 22,
          "end_line": 129,
          "content": [
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn lru->memcg_aware;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_add(&lru->list, &memcg_list_lrus);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_del(&lru->list);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn lru->shrinker_id;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn -1;",
            "}",
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn false;",
            "}",
            "bool list_lru_add(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_add_tail(item, &l->list);",
            "\t\t/* Set shrinker bit if the first element was added */",
            "\t\tif (!l->nr_items++)",
            "\t\t\tset_shrinker_bit(memcg, nid, lru_shrinker_id(lru));",
            "\t\tnlru->nr_items++;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_add_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_add(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_add(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "bool list_lru_del(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (!list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_del_init(item);",
            "\t\tl->nr_items--;",
            "\t\tnlru->nr_items--;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_del_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_del(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_del(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "list_lru_memcg_aware, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_memcg_aware, list_lru_add, list_lru_add_obj, list_lru_del, list_lru_del_obj",
          "description": "实现了LRU列表的添加/删除操作，支持MemCG感知的节点和内存组粒度管理，包含处理多核、内存组切换及RCU安全访问的逻辑。",
          "similarity": 0.561969518661499
        },
        {
          "chunk_id": 3,
          "file_path": "mm/list_lru.c",
          "start_line": 289,
          "end_line": 400,
          "content": [
            "unsigned long",
            "list_lru_walk_one_irq(struct list_lru *lru, int nid, struct mem_cgroup *memcg,",
            "\t\t      list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t      unsigned long *nr_to_walk)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tunsigned long ret;",
            "",
            "\tspin_lock_irq(&nlru->lock);",
            "\tret = __list_lru_walk_one(lru, nid, memcg_kmem_id(memcg), isolate,",
            "\t\t\t\t  cb_arg, nr_to_walk);",
            "\tspin_unlock_irq(&nlru->lock);",
            "\treturn ret;",
            "}",
            "unsigned long list_lru_walk_node(struct list_lru *lru, int nid,",
            "\t\t\t\t list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t\t\t unsigned long *nr_to_walk)",
            "{",
            "\tlong isolated = 0;",
            "",
            "\tisolated += list_lru_walk_one(lru, nid, NULL, isolate, cb_arg,",
            "\t\t\t\t      nr_to_walk);",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (*nr_to_walk > 0 && list_lru_memcg_aware(lru)) {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tunsigned long index;",
            "",
            "\t\txa_for_each(&lru->xa, index, mlru) {",
            "\t\t\tstruct list_lru_node *nlru = &lru->node[nid];",
            "",
            "\t\t\tspin_lock(&nlru->lock);",
            "\t\t\tisolated += __list_lru_walk_one(lru, nid, index,",
            "\t\t\t\t\t\t\tisolate, cb_arg,",
            "\t\t\t\t\t\t\tnr_to_walk);",
            "\t\t\tspin_unlock(&nlru->lock);",
            "",
            "\t\t\tif (*nr_to_walk <= 0)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "#endif",
            "",
            "\treturn isolated;",
            "}",
            "static void init_one_lru(struct list_lru_one *l)",
            "{",
            "\tINIT_LIST_HEAD(&l->list);",
            "\tl->nr_items = 0;",
            "}",
            "static void memcg_list_lru_free(struct list_lru *lru, int src_idx)",
            "{",
            "\tstruct list_lru_memcg *mlru = xa_erase_irq(&lru->xa, src_idx);",
            "",
            "\t/*",
            "\t * The __list_lru_walk_one() can walk the list of this node.",
            "\t * We need kvfree_rcu() here. And the walking of the list",
            "\t * is under lru->node[nid]->lock, which can serve as a RCU",
            "\t * read-side critical section.",
            "\t */",
            "\tif (mlru)",
            "\t\tkvfree_rcu(mlru, rcu);",
            "}",
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "\tif (memcg_aware)",
            "\t\txa_init_flags(&lru->xa, XA_FLAGS_LOCK_IRQ);",
            "\tlru->memcg_aware = memcg_aware;",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "\tstruct list_lru_memcg *mlru;",
            "",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\txas_lock_irq(&xas);",
            "\txas_for_each(&xas, mlru, ULONG_MAX) {",
            "\t\tkfree(mlru);",
            "\t\txas_store(&xas, NULL);",
            "\t}",
            "\txas_unlock_irq(&xas);",
            "}",
            "static void memcg_reparent_list_lru_node(struct list_lru *lru, int nid,",
            "\t\t\t\t\t int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tint dst_idx = dst_memcg->kmemcg_id;",
            "\tstruct list_lru_one *src, *dst;",
            "",
            "\t/*",
            "\t * Since list_lru_{add,del} may be called under an IRQ-safe lock,",
            "\t * we have to use IRQ-safe primitives here to avoid deadlock.",
            "\t */",
            "\tspin_lock_irq(&nlru->lock);",
            "",
            "\tsrc = list_lru_from_memcg_idx(lru, nid, src_idx);",
            "\tif (!src)",
            "\t\tgoto out;",
            "\tdst = list_lru_from_memcg_idx(lru, nid, dst_idx);",
            "",
            "\tlist_splice_init(&src->list, &dst->list);",
            "",
            "\tif (src->nr_items) {",
            "\t\tdst->nr_items += src->nr_items;",
            "\t\tset_shrinker_bit(dst_memcg, nid, lru_shrinker_id(lru));",
            "\t\tsrc->nr_items = 0;",
            "\t}",
            "out:",
            "\tspin_unlock_irq(&nlru->lock);",
            "}"
          ],
          "function_name": "list_lru_walk_one_irq, list_lru_walk_node, init_one_lru, memcg_list_lru_free, memcg_init_list_lru, memcg_destroy_list_lru, memcg_reparent_list_lru_node",
          "description": "包含LRU节点初始化、内存组间列表迁移、资源释放等高级操作，涉及XA表管理、中断安全锁操作及内存组重新归属处理。",
          "similarity": 0.5500266551971436
        },
        {
          "chunk_id": 5,
          "file_path": "mm/list_lru.c",
          "start_line": 556,
          "end_line": 605,
          "content": [
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "}",
            "int __list_lru_init(struct list_lru *lru, bool memcg_aware,",
            "\t\t    struct lock_class_key *key, struct shrinker *shrinker)",
            "{",
            "\tint i;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (shrinker)",
            "\t\tlru->shrinker_id = shrinker->id;",
            "\telse",
            "\t\tlru->shrinker_id = -1;",
            "#endif",
            "",
            "\tlru->node = kcalloc(nr_node_ids, sizeof(*lru->node), GFP_KERNEL);",
            "\tif (!lru->node)",
            "\t\treturn -ENOMEM;",
            "",
            "\tfor_each_node(i) {",
            "\t\tspin_lock_init(&lru->node[i].lock);",
            "\t\tif (key)",
            "\t\t\tlockdep_set_class(&lru->node[i].lock, key);",
            "\t\tinit_one_lru(&lru->node[i].lru);",
            "\t}",
            "",
            "\tmemcg_init_list_lru(lru, memcg_aware);",
            "\tlist_lru_register(lru);",
            "",
            "\treturn 0;",
            "}",
            "void list_lru_destroy(struct list_lru *lru)",
            "{",
            "\t/* Already destroyed or not yet initialized? */",
            "\tif (!lru->node)",
            "\t\treturn;",
            "",
            "\tlist_lru_unregister(lru);",
            "",
            "\tmemcg_destroy_list_lru(lru);",
            "\tkfree(lru->node);",
            "\tlru->node = NULL;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tlru->shrinker_id = -1;",
            "#endif",
            "}"
          ],
          "function_name": "memcg_init_list_lru, memcg_destroy_list_lru, __list_lru_init, list_lru_destroy",
          "description": "该代码段实现了基于内存控制组（MEMCG）的LRU列表管理功能。  \n`__list_lru_init` 初始化 `list_lru` 结构体并注册到系统，其中包含 MEMCG 相关的 shrinker ID 设置及节点锁初始化；`list_lru_destroy` 反向清理资源，但 `memcg_init_list_lru` 和 `memcg_destroy_list_lru` 的具体实现缺失，上下文不完整。",
          "similarity": 0.5441154837608337
        },
        {
          "chunk_id": 0,
          "file_path": "mm/list_lru.c",
          "start_line": 1,
          "end_line": 21,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2013 Red Hat, Inc. and Parallels Inc. All rights reserved.",
            " * Authors: David Chinner and Glauber Costa",
            " *",
            " * Generic LRU infrastructure",
            " */",
            "#include <linux/kernel.h>",
            "#include <linux/module.h>",
            "#include <linux/mm.h>",
            "#include <linux/list_lru.h>",
            "#include <linux/slab.h>",
            "#include <linux/mutex.h>",
            "#include <linux/memcontrol.h>",
            "#include \"slab.h\"",
            "#include \"internal.h\"",
            "",
            "#ifdef CONFIG_MEMCG",
            "static LIST_HEAD(memcg_list_lrus);",
            "static DEFINE_MUTEX(list_lrus_mutex);",
            ""
          ],
          "function_name": null,
          "description": "定义了支持内存控制组（MemCG）的LRU基础设施，声明了全局链表头memcg_list_lrus和互斥锁list_lrus_mutex，用于管理MemCG环境下的LRU列表注册与注销操作。",
          "similarity": 0.531442403793335
        }
      ]
    },
    {
      "source_file": "kernel/fork.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:30:07\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `fork.c`\n\n---\n\n# fork.c 技术文档\n\n## 1. 文件概述\n\n`fork.c` 是 Linux 内核中实现进程创建（fork）系统调用的核心源文件。该文件包含了创建新进程所需的所有辅助例程，负责复制父进程的资源（如内存、文件描述符、信号处理等）以生成子进程。虽然 fork 逻辑本身概念简单，但其涉及的内存管理（尤其是写时复制 COW 机制）极为复杂，实际内存页的复制由 `mm/memory.c` 中的 `copy_page_range()` 等函数处理。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `total_forks`: 累计系统自启动以来创建的进程总数\n- `nr_threads`: 当前系统中的线程总数（不包括 idle 线程）\n- `max_threads`: 可配置的线程数量上限（默认为 `FUTEX_TID_MASK`）\n- `process_counts`: 每 CPU 的进程计数器（per-CPU 变量）\n- `tasklist_lock`: 保护任务链表的读写锁（全局任务列表的同步原语）\n\n### 关键辅助函数\n- `nr_processes()`: 计算系统中所有进程的总数（聚合各 CPU 的 `process_counts`）\n- `arch_release_task_struct()`: 架构相关的 task_struct 释放钩子（弱符号，默认为空）\n- `alloc_task_struct_node()` / `free_task_struct()`: 分配/释放 `task_struct` 结构（基于 slab 分配器）\n- `alloc_thread_stack_node()` / `thread_stack_delayed_free()`: 分配/延迟释放线程内核栈（支持 `CONFIG_VMAP_STACK`）\n\n### 核心数据结构\n- `resident_page_types[]`: 用于内存统计的页面类型名称映射数组\n- `vm_stack`: 用于 RCU 延迟释放的虚拟内存栈封装结构\n- `cached_stacks[NR_CACHED_STACKS]`: 每 CPU 的内核栈缓存（减少频繁 vmalloc/vfree 开销）\n\n## 3. 关键实现\n\n### 进程/线程计数管理\n- 使用 per-CPU 变量 `process_counts` 避免全局锁竞争\n- 全局计数器 `nr_threads` 和 `total_forks` 由 `tasklist_lock` 保护\n- `nr_processes()` 通过遍历所有可能的 CPU 聚合计数\n\n### 内核栈分配策略（`CONFIG_VMAP_STACK`）\n- **缓存机制**：每个 CPU 缓存最多 2 个已释放的栈（`NR_CACHED_STACKS`），减少 TLB 刷新和 vmalloc 开销\n- **内存分配**：\n  - 优先从本地缓存获取栈\n  - 缓存未命中时使用 `__vmalloc_node_range()` 分配连续虚拟地址空间\n  - 显式禁用 `__GFP_ACCOUNT`（因后续手动进行 memcg 计费）\n- **安全清理**：\n  - 重用栈时清零内存（`memset(stack, 0, THREAD_SIZE)`）\n  - KASAN 消毒（`kasan_unpoison_range`）和标签重置\n- **延迟释放**：\n  - 通过 RCU 机制延迟释放栈（`call_rcu`）\n  - 释放时尝试回填缓存，失败则直接 `vfree`\n\n### 内存控制组（memcg）集成\n- 手动对栈的每个物理页进行 memcg 计费（`memcg_kmem_charge_page`）\n- 计费失败时回滚已计费页面（`memcg_kmem_uncharge_page`）\n- 确保内核栈内存纳入 cgroup 内存限制\n\n### 锁与同步\n- `tasklist_lock` 作为全局任务列表的保护锁（读写锁）\n- 提供 `lockdep_tasklist_lock_is_held()` 供 RCU 锁验证使用\n- RCU 用于安全延迟释放内核栈资源\n\n## 4. 依赖关系\n\n### 内核子系统依赖\n- **内存管理 (MM)**：`<linux/mm.h>`, `<linux/vmalloc.h>`, `<linux/memcontrol.h>`\n- **调度器 (Scheduler)**：`<linux/sched/*.h>`, 任务状态和 CPU 绑定\n- **安全模块**：`<linux/security.h>`, `<linux/capability.h>`, `<linux/seccomp.h>`\n- **命名空间**：`<linux/nsproxy.h>`（UTS, IPC, PID, 网络等）\n- **文件系统**：`<linux/fs.h>`, `<linux/fdtable.h>`（文件描述符复制）\n- **跟踪与调试**：`<trace/events/sched.h>`, `<linux/ftrace.h>`, KASAN/KMSAN\n\n### 架构相关依赖\n- `<asm/pgalloc.h>`：页表分配\n- `<asm/mmu_context.h>`：MMU 上下文切换\n- `<asm/tlbflush.h>`：TLB 刷新操作\n- 架构特定的 `THREAD_SIZE` 和栈对齐要求\n\n### 配置选项依赖\n- `CONFIG_VMAP_STACK`：启用虚拟内存分配内核栈\n- `CONFIG_PROVE_RCU`：RCU 锁验证支持\n- `CONFIG_ARCH_TASK_STRUCT_ALLOCATOR`：架构自定义 task_struct 分配器\n- `CONFIG_MEMCG_KMEM`：内核内存 cgroup 支持\n\n## 5. 使用场景\n\n### 进程创建路径\n- **系统调用入口**：`sys_fork()`, `sys_vfork()`, `sys_clone()` 最终调用 `_do_fork()`\n- **内核线程创建**：`kthread_create()` 通过 `kernel_thread()` 触发 fork 逻辑\n- **容器/命名空间初始化**：新 PID/UTS/IPC 命名空间创建时伴随进程 fork\n\n### 资源复制关键点\n- **内存描述符 (mm_struct)**：通过 `dup_mm()` 复制地址空间（COW 页表）\n- **文件描述符表**：`dup_fd()` 复制打开文件表\n- **信号处理**：复制信号掩码和处理函数\n- **POSIX 定时器/异步 I/O**：复制相关上下文（如 `aio`, `posix-timers`）\n\n### 特殊场景处理\n- **写时复制优化**：避免物理内存立即复制，提升 fork 性能\n- **OOM Killer 集成**：在内存不足时参与进程选择\n- **审计与监控**：通过 `audit_alloc()` 和 `proc` 文件系统暴露进程信息\n- **实时性保障**：RT 任务 fork 时保持调度策略和优先级",
      "similarity": 0.6247656345367432,
      "chunks": [
        {
          "chunk_id": 13,
          "file_path": "kernel/fork.c",
          "start_line": 2924,
          "end_line": 3037,
          "content": [
            "pid_t kernel_thread(int (*fn)(void *), void *arg, const char *name,",
            "\t\t    unsigned long flags)",
            "{",
            "\tstruct kernel_clone_args args = {",
            "\t\t.flags\t\t= ((lower_32_bits(flags) | CLONE_VM |",
            "\t\t\t\t    CLONE_UNTRACED) & ~CSIGNAL),",
            "\t\t.exit_signal\t= (lower_32_bits(flags) & CSIGNAL),",
            "\t\t.fn\t\t= fn,",
            "\t\t.fn_arg\t\t= arg,",
            "\t\t.name\t\t= name,",
            "\t\t.kthread\t= 1,",
            "\t};",
            "",
            "\treturn kernel_clone(&args);",
            "}",
            "pid_t user_mode_thread(int (*fn)(void *), void *arg, unsigned long flags)",
            "{",
            "\tstruct kernel_clone_args args = {",
            "\t\t.flags\t\t= ((lower_32_bits(flags) | CLONE_VM |",
            "\t\t\t\t    CLONE_UNTRACED) & ~CSIGNAL),",
            "\t\t.exit_signal\t= (lower_32_bits(flags) & CSIGNAL),",
            "\t\t.fn\t\t= fn,",
            "\t\t.fn_arg\t\t= arg,",
            "\t};",
            "",
            "\treturn kernel_clone(&args);",
            "}",
            "noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,",
            "\t\t\t\t\t      struct clone_args __user *uargs,",
            "\t\t\t\t\t      size_t usize)",
            "{",
            "\tint err;",
            "\tstruct clone_args args;",
            "\tpid_t *kset_tid = kargs->set_tid;",
            "",
            "\tBUILD_BUG_ON(offsetofend(struct clone_args, tls) !=",
            "\t\t     CLONE_ARGS_SIZE_VER0);",
            "\tBUILD_BUG_ON(offsetofend(struct clone_args, set_tid_size) !=",
            "\t\t     CLONE_ARGS_SIZE_VER1);",
            "\tBUILD_BUG_ON(offsetofend(struct clone_args, cgroup) !=",
            "\t\t     CLONE_ARGS_SIZE_VER2);",
            "\tBUILD_BUG_ON(sizeof(struct clone_args) != CLONE_ARGS_SIZE_VER2);",
            "",
            "\tif (unlikely(usize > PAGE_SIZE))",
            "\t\treturn -E2BIG;",
            "\tif (unlikely(usize < CLONE_ARGS_SIZE_VER0))",
            "\t\treturn -EINVAL;",
            "",
            "\terr = copy_struct_from_user(&args, sizeof(args), uargs, usize);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (unlikely(args.set_tid_size > MAX_PID_NS_LEVEL))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (unlikely(!args.set_tid && args.set_tid_size > 0))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (unlikely(args.set_tid && args.set_tid_size == 0))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Verify that higher 32bits of exit_signal are unset and that",
            "\t * it is a valid signal",
            "\t */",
            "\tif (unlikely((args.exit_signal & ~((u64)CSIGNAL)) ||",
            "\t\t     !valid_signal(args.exit_signal)))",
            "\t\treturn -EINVAL;",
            "",
            "\tif ((args.flags & CLONE_INTO_CGROUP) &&",
            "\t    (args.cgroup > INT_MAX || usize < CLONE_ARGS_SIZE_VER2))",
            "\t\treturn -EINVAL;",
            "",
            "\t*kargs = (struct kernel_clone_args){",
            "\t\t.flags\t\t= args.flags,",
            "\t\t.pidfd\t\t= u64_to_user_ptr(args.pidfd),",
            "\t\t.child_tid\t= u64_to_user_ptr(args.child_tid),",
            "\t\t.parent_tid\t= u64_to_user_ptr(args.parent_tid),",
            "\t\t.exit_signal\t= args.exit_signal,",
            "\t\t.stack\t\t= args.stack,",
            "\t\t.stack_size\t= args.stack_size,",
            "\t\t.tls\t\t= args.tls,",
            "\t\t.set_tid_size\t= args.set_tid_size,",
            "\t\t.cgroup\t\t= args.cgroup,",
            "\t};",
            "",
            "\tif (args.set_tid &&",
            "\t\tcopy_from_user(kset_tid, u64_to_user_ptr(args.set_tid),",
            "\t\t\t(kargs->set_tid_size * sizeof(pid_t))))",
            "\t\treturn -EFAULT;",
            "",
            "\tkargs->set_tid = kset_tid;",
            "",
            "\treturn 0;",
            "}",
            "static inline bool clone3_stack_valid(struct kernel_clone_args *kargs)",
            "{",
            "\tif (kargs->stack == 0) {",
            "\t\tif (kargs->stack_size > 0)",
            "\t\t\treturn false;",
            "\t} else {",
            "\t\tif (kargs->stack_size == 0)",
            "\t\t\treturn false;",
            "",
            "\t\tif (!access_ok((void __user *)kargs->stack, kargs->stack_size))",
            "\t\t\treturn false;",
            "",
            "#if !defined(CONFIG_STACK_GROWSUP)",
            "\t\tkargs->stack += kargs->stack_size;",
            "#endif",
            "\t}",
            "",
            "\treturn true;",
            "}"
          ],
          "function_name": "kernel_thread, user_mode_thread, copy_clone_args_from_user, clone3_stack_valid",
          "description": "提供内核线程与用户线程创建接口，解析并验证clone3参数，转换用户空间clone_args到内核结构体，校验栈地址有效性",
          "similarity": 0.5702296495437622
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/fork.c",
          "start_line": 666,
          "end_line": 845,
          "content": [
            "static __latent_entropy int dup_mmap(struct mm_struct *mm,",
            "\t\t\t\t\tstruct mm_struct *oldmm)",
            "{",
            "\tstruct vm_area_struct *mpnt, *tmp;",
            "\tint retval;",
            "\tunsigned long charge = 0;",
            "\tLIST_HEAD(uf);",
            "\tVMA_ITERATOR(vmi, mm, 0);",
            "",
            "\tuprobe_start_dup_mmap();",
            "\tif (mmap_write_lock_killable(oldmm)) {",
            "\t\tretval = -EINTR;",
            "\t\tgoto fail_uprobe_end;",
            "\t}",
            "\tflush_cache_dup_mm(oldmm);",
            "\tuprobe_dup_mmap(oldmm, mm);",
            "\t/*",
            "\t * Not linked in yet - no deadlock potential:",
            "\t */",
            "\tmmap_write_lock_nested(mm, SINGLE_DEPTH_NESTING);",
            "",
            "\t/* No ordering required: file already has been exposed. */",
            "\tdup_mm_exe_file(mm, oldmm);",
            "",
            "\tmm->total_vm = oldmm->total_vm;",
            "\tmm->data_vm = oldmm->data_vm;",
            "\tmm->exec_vm = oldmm->exec_vm;",
            "\tmm->stack_vm = oldmm->stack_vm;",
            "",
            "\t/* Use __mt_dup() to efficiently build an identical maple tree. */",
            "\tretval = __mt_dup(&oldmm->mm_mt, &mm->mm_mt, GFP_KERNEL);",
            "\tif (unlikely(retval))",
            "\t\tgoto out;",
            "",
            "\tmt_clear_in_rcu(vmi.mas.tree);",
            "\tfor_each_vma(vmi, mpnt) {",
            "\t\tstruct file *file;",
            "",
            "\t\tvma_start_write(mpnt);",
            "\t\tif (mpnt->vm_flags & VM_DONTCOPY) {",
            "\t\t\tretval = vma_iter_clear_gfp(&vmi, mpnt->vm_start,",
            "\t\t\t\t\t\t    mpnt->vm_end, GFP_KERNEL);",
            "\t\t\tif (retval)",
            "\t\t\t\tgoto loop_out;",
            "",
            "\t\t\tvm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tcharge = 0;",
            "\t\t/*",
            "\t\t * Don't duplicate many vmas if we've been oom-killed (for",
            "\t\t * example)",
            "\t\t */",
            "\t\tif (fatal_signal_pending(current)) {",
            "\t\t\tretval = -EINTR;",
            "\t\t\tgoto loop_out;",
            "\t\t}",
            "\t\tif (mpnt->vm_flags & VM_ACCOUNT) {",
            "\t\t\tunsigned long len = vma_pages(mpnt);",
            "",
            "\t\t\tif (security_vm_enough_memory_mm(oldmm, len)) /* sic */",
            "\t\t\t\tgoto fail_nomem;",
            "\t\t\tcharge = len;",
            "\t\t}",
            "\t\ttmp = vm_area_dup(mpnt);",
            "\t\tif (!tmp)",
            "\t\t\tgoto fail_nomem;",
            "",
            "\t\t/* track_pfn_copy() will later take care of copying internal state. */",
            "\t\tif (unlikely(tmp->vm_flags & VM_PFNMAP))",
            "\t\t\tuntrack_pfn_clear(tmp);",
            "",
            "\t\tretval = vma_dup_policy(mpnt, tmp);",
            "\t\tif (retval)",
            "\t\t\tgoto fail_nomem_policy;",
            "\t\ttmp->vm_mm = mm;",
            "\t\tretval = dup_userfaultfd(tmp, &uf);",
            "\t\tif (retval)",
            "\t\t\tgoto fail_nomem_anon_vma_fork;",
            "\t\tif (tmp->vm_flags & VM_WIPEONFORK) {",
            "\t\t\t/*",
            "\t\t\t * VM_WIPEONFORK gets a clean slate in the child.",
            "\t\t\t * Don't prepare anon_vma until fault since we don't",
            "\t\t\t * copy page for current vma.",
            "\t\t\t */",
            "\t\t\ttmp->anon_vma = NULL;",
            "\t\t} else if (anon_vma_fork(tmp, mpnt))",
            "\t\t\tgoto fail_nomem_anon_vma_fork;",
            "\t\tvm_flags_clear(tmp, VM_LOCKED_MASK);",
            "\t\tfile = tmp->vm_file;",
            "\t\tif (file) {",
            "\t\t\tstruct address_space *mapping = file->f_mapping;",
            "",
            "\t\t\tget_file(file);",
            "\t\t\ti_mmap_lock_write(mapping);",
            "\t\t\tif (vma_is_shared_maywrite(tmp))",
            "\t\t\t\tmapping_allow_writable(mapping);",
            "\t\t\tflush_dcache_mmap_lock(mapping);",
            "\t\t\t/* insert tmp into the share list, just after mpnt */",
            "\t\t\tvma_interval_tree_insert_after(tmp, mpnt,",
            "\t\t\t\t\t&mapping->i_mmap);",
            "\t\t\tflush_dcache_mmap_unlock(mapping);",
            "\t\t\ti_mmap_unlock_write(mapping);",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Copy/update hugetlb private vma information.",
            "\t\t */",
            "\t\tif (is_vm_hugetlb_page(tmp))",
            "\t\t\thugetlb_dup_vma_private(tmp);",
            "",
            "\t\t/*",
            "\t\t * Link the vma into the MT. After using __mt_dup(), memory",
            "\t\t * allocation is not necessary here, so it cannot fail.",
            "\t\t */",
            "\t\tvma_iter_bulk_store(&vmi, tmp);",
            "",
            "\t\tmm->map_count++;",
            "\t\tif (!(tmp->vm_flags & VM_WIPEONFORK))",
            "\t\t\tretval = copy_page_range(tmp, mpnt);",
            "",
            "\t\tif (tmp->vm_ops && tmp->vm_ops->open)",
            "\t\t\ttmp->vm_ops->open(tmp);",
            "",
            "\t\tif (retval) {",
            "\t\t\tmpnt = vma_next(&vmi);",
            "\t\t\tgoto loop_out;",
            "\t\t}",
            "\t}",
            "\t/* a new mm has just been created */",
            "\tretval = arch_dup_mmap(oldmm, mm);",
            "loop_out:",
            "\tvma_iter_free(&vmi);",
            "\tif (!retval) {",
            "\t\tmt_set_in_rcu(vmi.mas.tree);",
            "\t\tksm_fork(mm, oldmm);",
            "\t\tkhugepaged_fork(mm, oldmm);",
            "\t} else {",
            "",
            "\t\t/*",
            "\t\t * The entire maple tree has already been duplicated. If the",
            "\t\t * mmap duplication fails, mark the failure point with",
            "\t\t * XA_ZERO_ENTRY. In exit_mmap(), if this marker is encountered,",
            "\t\t * stop releasing VMAs that have not been duplicated after this",
            "\t\t * point.",
            "\t\t */",
            "\t\tif (mpnt) {",
            "\t\t\tmas_set_range(&vmi.mas, mpnt->vm_start, mpnt->vm_end - 1);",
            "\t\t\tmas_store(&vmi.mas, XA_ZERO_ENTRY);",
            "\t\t\t/* Avoid OOM iterating a broken tree */",
            "\t\t\tset_bit(MMF_OOM_SKIP, &mm->flags);",
            "\t\t}",
            "\t\t/*",
            "\t\t * The mm_struct is going to exit, but the locks will be dropped",
            "\t\t * first.  Set the mm_struct as unstable is advisable as it is",
            "\t\t * not fully initialised.",
            "\t\t */",
            "\t\tset_bit(MMF_UNSTABLE, &mm->flags);",
            "\t}",
            "out:",
            "\tmmap_write_unlock(mm);",
            "\tflush_tlb_mm(oldmm);",
            "\tmmap_write_unlock(oldmm);",
            "\tif (!retval)",
            "\t\tdup_userfaultfd_complete(&uf);",
            "\telse",
            "\t\tdup_userfaultfd_fail(&uf);",
            "fail_uprobe_end:",
            "\tuprobe_end_dup_mmap();",
            "\treturn retval;",
            "",
            "fail_nomem_anon_vma_fork:",
            "\tmpol_put(vma_policy(tmp));",
            "fail_nomem_policy:",
            "\tvm_area_free(tmp);",
            "fail_nomem:",
            "\tretval = -ENOMEM;",
            "\tvm_unacct_memory(charge);",
            "\tgoto loop_out;",
            "}"
          ],
          "function_name": "dup_mmap",
          "description": "实现进程fork时的内存映射复制逻辑，深度遍历原进程的VMA结构创建副本，处理共享文件映射、hugetlb页等特殊内存类型，并管理OOM异常情况下的失败恢复。",
          "similarity": 0.5456494092941284
        },
        {
          "chunk_id": 12,
          "file_path": "kernel/fork.c",
          "start_line": 2176,
          "end_line": 2282,
          "content": [
            "static void rv_task_fork(struct task_struct *p)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < RV_PER_TASK_MONITORS; i++)",
            "\t\tp->rv[i].da_mon.monitoring = false;",
            "}",
            "static inline void init_idle_pids(struct task_struct *idle)",
            "{",
            "\tenum pid_type type;",
            "",
            "\tfor (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {",
            "\t\tINIT_HLIST_NODE(&idle->pid_links[type]); /* not really needed */",
            "\t\tinit_task_pid(idle, type, &init_struct_pid);",
            "\t}",
            "}",
            "static int idle_dummy(void *dummy)",
            "{",
            "\t/* This function is never called */",
            "\treturn 0;",
            "}",
            "pid_t kernel_clone(struct kernel_clone_args *args)",
            "{",
            "\tu64 clone_flags = args->flags;",
            "\tstruct completion vfork;",
            "\tstruct pid *pid;",
            "\tstruct task_struct *p;",
            "\tint trace = 0;",
            "\tpid_t nr;",
            "",
            "\t/*",
            "\t * For legacy clone() calls, CLONE_PIDFD uses the parent_tid argument",
            "\t * to return the pidfd. Hence, CLONE_PIDFD and CLONE_PARENT_SETTID are",
            "\t * mutually exclusive. With clone3() CLONE_PIDFD has grown a separate",
            "\t * field in struct clone_args and it still doesn't make sense to have",
            "\t * them both point at the same memory location. Performing this check",
            "\t * here has the advantage that we don't need to have a separate helper",
            "\t * to check for legacy clone().",
            "\t */",
            "\tif ((args->flags & CLONE_PIDFD) &&",
            "\t    (args->flags & CLONE_PARENT_SETTID) &&",
            "\t    (args->pidfd == args->parent_tid))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Determine whether and which event to report to ptracer.  When",
            "\t * called from kernel_thread or CLONE_UNTRACED is explicitly",
            "\t * requested, no event is reported; otherwise, report if the event",
            "\t * for the type of forking is enabled.",
            "\t */",
            "\tif (!(clone_flags & CLONE_UNTRACED)) {",
            "\t\tif (clone_flags & CLONE_VFORK)",
            "\t\t\ttrace = PTRACE_EVENT_VFORK;",
            "\t\telse if (args->exit_signal != SIGCHLD)",
            "\t\t\ttrace = PTRACE_EVENT_CLONE;",
            "\t\telse",
            "\t\t\ttrace = PTRACE_EVENT_FORK;",
            "",
            "\t\tif (likely(!ptrace_event_enabled(current, trace)))",
            "\t\t\ttrace = 0;",
            "\t}",
            "",
            "\tp = copy_process(NULL, trace, NUMA_NO_NODE, args);",
            "\tadd_latent_entropy();",
            "",
            "\tif (IS_ERR(p))",
            "\t\treturn PTR_ERR(p);",
            "",
            "\t/*",
            "\t * Do this prior waking up the new thread - the thread pointer",
            "\t * might get invalid after that point, if the thread exits quickly.",
            "\t */",
            "\ttrace_sched_process_fork(current, p);",
            "",
            "\tpid = get_task_pid(p, PIDTYPE_PID);",
            "\tnr = pid_vnr(pid);",
            "",
            "\tif (clone_flags & CLONE_PARENT_SETTID)",
            "\t\tput_user(nr, args->parent_tid);",
            "",
            "\tif (clone_flags & CLONE_VFORK) {",
            "\t\tp->vfork_done = &vfork;",
            "\t\tinit_completion(&vfork);",
            "\t\tget_task_struct(p);",
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_LRU_GEN_WALKS_MMU) && !(clone_flags & CLONE_VM)) {",
            "\t\t/* lock the task to synchronize with memcg migration */",
            "\t\ttask_lock(p);",
            "\t\tlru_gen_add_mm(p->mm);",
            "\t\ttask_unlock(p);",
            "\t}",
            "",
            "\twake_up_new_task(p);",
            "",
            "\t/* forking complete and child started to run, tell ptracer */",
            "\tif (unlikely(trace))",
            "\t\tptrace_event_pid(trace, pid);",
            "",
            "\tif (clone_flags & CLONE_VFORK) {",
            "\t\tif (!wait_for_vfork_done(p, &vfork))",
            "\t\t\tptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);",
            "\t}",
            "",
            "\tput_pid(pid);",
            "\treturn nr;",
            "}"
          ],
          "function_name": "rv_task_fork, init_idle_pids, idle_dummy, kernel_clone",
          "description": "实现kernel_clone核心逻辑，创建新进程并处理克隆标志，管理子进程启动、vfork等待及进程树遍历，包含空闲任务PID初始化与RV监控器重置",
          "similarity": 0.5333591103553772
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/fork.c",
          "start_line": 847,
          "end_line": 965,
          "content": [
            "static inline int mm_alloc_pgd(struct mm_struct *mm)",
            "{",
            "\tmm->pgd = pgd_alloc(mm);",
            "\tif (unlikely(!mm->pgd))",
            "\t\treturn -ENOMEM;",
            "\treturn 0;",
            "}",
            "static inline void mm_free_pgd(struct mm_struct *mm)",
            "{",
            "\tpgd_free(mm, mm->pgd);",
            "}",
            "static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)",
            "{",
            "\tmmap_write_lock(oldmm);",
            "\tdup_mm_exe_file(mm, oldmm);",
            "\tmmap_write_unlock(oldmm);",
            "\treturn 0;",
            "}",
            "static void check_mm(struct mm_struct *mm)",
            "{",
            "\tint i;",
            "",
            "\tBUILD_BUG_ON_MSG(ARRAY_SIZE(resident_page_types) != NR_MM_COUNTERS,",
            "\t\t\t \"Please make sure 'struct resident_page_types[]' is updated as well\");",
            "",
            "\tfor (i = 0; i < NR_MM_COUNTERS; i++) {",
            "\t\tlong x = percpu_counter_sum(&mm->rss_stat[i]);",
            "",
            "\t\tif (unlikely(x))",
            "\t\t\tpr_alert(\"BUG: Bad rss-counter state mm:%p type:%s val:%ld\\n\",",
            "\t\t\t\t mm, resident_page_types[i], x);",
            "\t}",
            "",
            "\tif (mm_pgtables_bytes(mm))",
            "\t\tpr_alert(\"BUG: non-zero pgtables_bytes on freeing mm: %ld\\n\",",
            "\t\t\t\tmm_pgtables_bytes(mm));",
            "",
            "#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !defined(CONFIG_SPLIT_PMD_PTLOCKS)",
            "\tVM_BUG_ON_MM(mm->pmd_huge_pte, mm);",
            "#endif",
            "}",
            "static void do_check_lazy_tlb(void *arg)",
            "{",
            "\tstruct mm_struct *mm = arg;",
            "",
            "\tWARN_ON_ONCE(current->active_mm == mm);",
            "}",
            "static void do_shoot_lazy_tlb(void *arg)",
            "{",
            "\tstruct mm_struct *mm = arg;",
            "",
            "\tif (current->active_mm == mm) {",
            "\t\tWARN_ON_ONCE(current->mm);",
            "\t\tcurrent->active_mm = &init_mm;",
            "\t\tswitch_mm(mm, &init_mm, current);",
            "\t}",
            "}",
            "static void cleanup_lazy_tlbs(struct mm_struct *mm)",
            "{",
            "\tif (!IS_ENABLED(CONFIG_MMU_LAZY_TLB_SHOOTDOWN)) {",
            "\t\t/*",
            "\t\t * In this case, lazy tlb mms are refounted and would not reach",
            "\t\t * __mmdrop until all CPUs have switched away and mmdrop()ed.",
            "\t\t */",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Lazy mm shootdown does not refcount \"lazy tlb mm\" usage, rather it",
            "\t * requires lazy mm users to switch to another mm when the refcount",
            "\t * drops to zero, before the mm is freed. This requires IPIs here to",
            "\t * switch kernel threads to init_mm.",
            "\t *",
            "\t * archs that use IPIs to flush TLBs can piggy-back that lazy tlb mm",
            "\t * switch with the final userspace teardown TLB flush which leaves the",
            "\t * mm lazy on this CPU but no others, reducing the need for additional",
            "\t * IPIs here. There are cases where a final IPI is still required here,",
            "\t * such as the final mmdrop being performed on a different CPU than the",
            "\t * one exiting, or kernel threads using the mm when userspace exits.",
            "\t *",
            "\t * IPI overheads have not found to be expensive, but they could be",
            "\t * reduced in a number of possible ways, for example (roughly",
            "\t * increasing order of complexity):",
            "\t * - The last lazy reference created by exit_mm() could instead switch",
            "\t *   to init_mm, however it's probable this will run on the same CPU",
            "\t *   immediately afterwards, so this may not reduce IPIs much.",
            "\t * - A batch of mms requiring IPIs could be gathered and freed at once.",
            "\t * - CPUs store active_mm where it can be remotely checked without a",
            "\t *   lock, to filter out false-positives in the cpumask.",
            "\t * - After mm_users or mm_count reaches zero, switching away from the",
            "\t *   mm could clear mm_cpumask to reduce some IPIs, perhaps together",
            "\t *   with some batching or delaying of the final IPIs.",
            "\t * - A delayed freeing and RCU-like quiescing sequence based on mm",
            "\t *   switching to avoid IPIs completely.",
            "\t */",
            "\ton_each_cpu_mask(mm_cpumask(mm), do_shoot_lazy_tlb, (void *)mm, 1);",
            "\tif (IS_ENABLED(CONFIG_DEBUG_VM_SHOOT_LAZIES))",
            "\t\ton_each_cpu(do_check_lazy_tlb, (void *)mm, 1);",
            "}",
            "void __mmdrop(struct mm_struct *mm)",
            "{",
            "\tBUG_ON(mm == &init_mm);",
            "\tWARN_ON_ONCE(mm == current->mm);",
            "",
            "\t/* Ensure no CPUs are using this as their lazy tlb mm */",
            "\tcleanup_lazy_tlbs(mm);",
            "",
            "\tWARN_ON_ONCE(mm == current->active_mm);",
            "\tmm_free_pgd(mm);",
            "\tdestroy_context(mm);",
            "\tmmu_notifier_subscriptions_destroy(mm);",
            "\tcheck_mm(mm);",
            "\tput_user_ns(mm->user_ns);",
            "\tmm_pasid_drop(mm);",
            "\tmm_destroy_cid(mm);",
            "\tpercpu_counter_destroy_many(mm->rss_stat, NR_MM_COUNTERS);",
            "",
            "\tfree_mm(mm);",
            "}"
          ],
          "function_name": "mm_alloc_pgd, mm_free_pgd, dup_mmap, check_mm, do_check_lazy_tlb, do_shoot_lazy_tlb, cleanup_lazy_tlbs, __mmdrop",
          "description": "实现mm结构体的页目录表分配与释放，dup_mmap复制mmap信息，check_mm验证内存计数器状态，清理延迟TLB射杀并安全释放mm资源。",
          "similarity": 0.5330517292022705
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/fork.c",
          "start_line": 159,
          "end_line": 303,
          "content": [
            "int lockdep_tasklist_lock_is_held(void)",
            "{",
            "\treturn lockdep_is_held(&tasklist_lock);",
            "}",
            "int nr_processes(void)",
            "{",
            "\tint cpu;",
            "\tint total = 0;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\ttotal += per_cpu(process_counts, cpu);",
            "",
            "\treturn total;",
            "}",
            "void __weak arch_release_task_struct(struct task_struct *tsk)",
            "{",
            "}",
            "static inline void free_task_struct(struct task_struct *tsk)",
            "{",
            "\tkmem_cache_free(task_struct_cachep, tsk);",
            "}",
            "static bool try_release_thread_stack_to_cache(struct vm_struct *vm)",
            "{",
            "\tunsigned int i;",
            "",
            "\tfor (i = 0; i < NR_CACHED_STACKS; i++) {",
            "\t\tif (this_cpu_cmpxchg(cached_stacks[i], NULL, vm) != NULL)",
            "\t\t\tcontinue;",
            "\t\treturn true;",
            "\t}",
            "\treturn false;",
            "}",
            "static void thread_stack_free_rcu(struct rcu_head *rh)",
            "{",
            "\tstruct vm_stack *vm_stack = container_of(rh, struct vm_stack, rcu);",
            "",
            "\tif (try_release_thread_stack_to_cache(vm_stack->stack_vm_area))",
            "\t\treturn;",
            "",
            "\tvfree(vm_stack);",
            "}",
            "static void thread_stack_delayed_free(struct task_struct *tsk)",
            "{",
            "\tstruct vm_stack *vm_stack = tsk->stack;",
            "",
            "\tvm_stack->stack_vm_area = tsk->stack_vm_area;",
            "\tcall_rcu(&vm_stack->rcu, thread_stack_free_rcu);",
            "}",
            "static int free_vm_stack_cache(unsigned int cpu)",
            "{",
            "\tstruct vm_struct **cached_vm_stacks = per_cpu_ptr(cached_stacks, cpu);",
            "\tint i;",
            "",
            "\tfor (i = 0; i < NR_CACHED_STACKS; i++) {",
            "\t\tstruct vm_struct *vm_stack = cached_vm_stacks[i];",
            "",
            "\t\tif (!vm_stack)",
            "\t\t\tcontinue;",
            "",
            "\t\tvfree(vm_stack->addr);",
            "\t\tcached_vm_stacks[i] = NULL;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int memcg_charge_kernel_stack(struct vm_struct *vm)",
            "{",
            "\tint i;",
            "\tint ret;",
            "\tint nr_charged = 0;",
            "",
            "\tBUG_ON(vm->nr_pages != THREAD_SIZE / PAGE_SIZE);",
            "",
            "\tfor (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++) {",
            "\t\tret = memcg_kmem_charge_page(vm->pages[i], GFP_KERNEL, 0);",
            "\t\tif (ret)",
            "\t\t\tgoto err;",
            "\t\tnr_charged++;",
            "\t}",
            "\treturn 0;",
            "err:",
            "\tfor (i = 0; i < nr_charged; i++)",
            "\t\tmemcg_kmem_uncharge_page(vm->pages[i], 0);",
            "\treturn ret;",
            "}",
            "static int alloc_thread_stack_node(struct task_struct *tsk, int node)",
            "{",
            "\tstruct vm_struct *vm;",
            "\tvoid *stack;",
            "\tint i;",
            "",
            "\tfor (i = 0; i < NR_CACHED_STACKS; i++) {",
            "\t\tstruct vm_struct *s;",
            "",
            "\t\ts = this_cpu_xchg(cached_stacks[i], NULL);",
            "",
            "\t\tif (!s)",
            "\t\t\tcontinue;",
            "",
            "\t\t/* Reset stack metadata. */",
            "\t\tkasan_unpoison_range(s->addr, THREAD_SIZE);",
            "",
            "\t\tstack = kasan_reset_tag(s->addr);",
            "",
            "\t\t/* Clear stale pointers from reused stack. */",
            "\t\tmemset(stack, 0, THREAD_SIZE);",
            "",
            "\t\tif (memcg_charge_kernel_stack(s)) {",
            "\t\t\tvfree(s->addr);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "",
            "\t\ttsk->stack_vm_area = s;",
            "\t\ttsk->stack = stack;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\t/*",
            "\t * Allocated stacks are cached and later reused by new threads,",
            "\t * so memcg accounting is performed manually on assigning/releasing",
            "\t * stacks to tasks. Drop __GFP_ACCOUNT.",
            "\t */",
            "\tstack = __vmalloc_node_range(THREAD_SIZE, THREAD_ALIGN,",
            "\t\t\t\t     VMALLOC_START, VMALLOC_END,",
            "\t\t\t\t     THREADINFO_GFP & ~__GFP_ACCOUNT,",
            "\t\t\t\t     PAGE_KERNEL,",
            "\t\t\t\t     0, node, __builtin_return_address(0));",
            "\tif (!stack)",
            "\t\treturn -ENOMEM;",
            "",
            "\tvm = find_vm_area(stack);",
            "\tif (memcg_charge_kernel_stack(vm)) {",
            "\t\tvfree(stack);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "\t/*",
            "\t * We can't call find_vm_area() in interrupt context, and",
            "\t * free_thread_stack() can be called in interrupt context,",
            "\t * so cache the vm_struct.",
            "\t */",
            "\ttsk->stack_vm_area = vm;",
            "\tstack = kasan_reset_tag(stack);",
            "\ttsk->stack = stack;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "lockdep_tasklist_lock_is_held, nr_processes, arch_release_task_struct, free_task_struct, try_release_thread_stack_to_cache, thread_stack_free_rcu, thread_stack_delayed_free, free_vm_stack_cache, memcg_charge_kernel_stack, alloc_thread_stack_node",
          "description": "实现任务列表锁状态检测、进程总数统计及线程栈分配释放逻辑，通过缓存机制优化线程栈复用并利用RCU实现延迟释放，包含栈内存管理和内存会计功能。",
          "similarity": 0.528580367565155
        }
      ]
    },
    {
      "source_file": "kernel/sys_ni.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:31:37\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sys_ni.c`\n\n---\n\n# sys_ni.c 技术文档\n\n## 1. 文件概述\n\n`sys_ni.c` 是 Linux 内核中用于处理**未实现系统调用（Not Implemented Syscall）**的核心文件。当某个系统调用在当前架构或配置下未被实现时，内核会将其重定向到 `sys_ni_syscall()` 函数，该函数统一返回 `-ENOSYS` 错误码（表示“Function not implemented”）。此机制确保了即使某些系统调用未被支持，用户空间程序调用它们时也不会导致内核崩溃，而是获得标准错误响应。\n\n此外，该文件通过宏 `COND_SYSCALL` 和 `COND_SYSCALL_COMPAT` 为大量系统调用提供**弱符号（weak symbol）定义**，使得链接器在找不到具体实现时自动链接到 `sys_ni_syscall`，从而实现“按需启用、默认未实现”的灵活架构。\n\n## 2. 核心功能\n\n### 主要函数\n- **`sys_ni_syscall(void)`**  \n  所有未实现系统调用的默认入口点，返回 `-ENOSYS`。\n\n### 关键宏定义\n- **`COND_SYSCALL(name)`**  \n  展开为 `cond_syscall(sys_##name)`，为指定系统调用名生成弱符号引用。\n- **`COND_SYSCALL_COMPAT(name)`**  \n  展开为 `cond_syscall(compat_sys_##name)`，为 32 位兼容模式下的系统调用生成弱符号引用。\n- **`cond_syscall()`**（由链接器脚本或汇编支持）  \n  实际由链接器处理，将未定义的系统调用符号指向 `sys_ni_syscall`。\n\n## 3. 关键实现\n\n### 未实现系统调用的统一处理\n- 所有未在内核中实际实现的系统调用最终都会跳转到 `sys_ni_syscall()`，该函数仅返回 `-ENOSYS`，实现简洁且安全。\n- 通过 `asmlinkage` 修饰符确保函数使用正确的调用约定（通常为栈传参），与系统调用入口一致。\n\n### 弱符号机制\n- 使用 `COND_SYSCALL(name)` 宏为每个可能未实现的系统调用生成一个弱符号声明。\n- 在链接阶段，若某系统调用（如 `sys_io_setup`）有实际实现，则链接器使用其实现；若无，则自动绑定到 `sys_ni_syscall`。\n- 此机制避免了为每个架构手动维护大量空 stub 函数，提高了代码可维护性。\n\n### 兼容性支持\n- `COND_SYSCALL_COMPAT` 专门处理 32 位兼容层（如 x86_64 上运行 32 位程序）的系统调用，确保兼容模式下未实现的调用同样返回 `-ENOSYS`。\n- 支持架构特定的 syscall wrapper（通过 `CONFIG_ARCH_HAS_SYSCALL_WRAPPER`），允许某些架构自定义 `COND_SYSCALL` 行为。\n\n### 系统调用列表组织\n- 列表严格遵循 `include/uapi/asm-generic/unistd.h` 中的顺序，便于维护一致性。\n- 包含：\n  - 通用系统调用（如 `io_uring_*`, `epoll_*`, `timerfd_*`）\n  - 架构特定调用（如 x86 的 `vm86`、s390 的 `s390_ipc`）\n  - 已废弃但仍被某些架构需要的调用（如 `epoll_create`, `inotify_init`）\n  - 条件编译调用（如 `__ARCH_WANT_SYS_CLONE3` 控制的 `clone3`）\n\n## 4. 依赖关系\n\n### 头文件依赖\n- `<linux/linkage.h>`：提供 `asmlinkage` 宏定义\n- `<linux/errno.h>`：提供 `-ENOSYS` 错误码\n- `<asm/unistd.h>`：包含架构相关的系统调用编号定义\n- `<asm/syscall_wrapper.h>`（条件包含）：允许架构覆盖 `COND_SYSCALL` 宏\n\n### 内核构建系统依赖\n- 依赖链接器脚本（如 `vmlinux.lds`）中的 `__cond_syscall` 段处理弱符号\n- 与 `arch/*/kernel/syscall_table.c` 或等效文件协同工作，后者提供实际系统调用表\n\n### 配置选项依赖\n- `CONFIG_ARCH_HAS_SYSCALL_WRAPPER`：控制是否使用架构自定义 syscall wrapper\n- 各种 `CONFIG_*` 选项（如 `CONFIG_MMU`、`CONFIG_FANOTIFY`）间接影响哪些 `COND_SYSCALL` 条目生效\n\n## 5. 使用场景\n\n### 内核构建时\n- 在编译内核时，若某系统调用未被任何源文件实现（例如因配置选项关闭或架构不支持），链接器自动将其绑定到 `sys_ni_syscall`。\n- 避免链接错误，同时保证系统调用表完整性。\n\n### 用户空间调用未实现 syscall 时\n- 用户程序调用未实现的系统调用（如在不支持 `landlock` 的内核上调用 `landlock_create_ruleset`）。\n- 内核安全返回 `-ENOSYS`，程序可据此进行功能检测或降级处理。\n\n### 架构移植与兼容层\n- 新架构移植时，无需立即实现所有系统调用，未实现部分自动返回 `-ENOSYS`。\n- 32/64 位兼容层（如 x86_64 的 compat 模式）中，未实现的 32 位专用 syscall 同样得到正确处理。\n\n### 废弃 syscall 的平滑过渡\n- 对于已废弃但仍保留在 uAPI 中的系统调用（如 `epoll_create`），通过此机制确保旧程序在新内核上仍能获得明确错误而非崩溃。",
      "similarity": 0.6244513988494873,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/sys_ni.c",
          "start_line": 1,
          "end_line": 19,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "",
            "#include <linux/linkage.h>",
            "#include <linux/errno.h>",
            "",
            "#include <asm/unistd.h>",
            "",
            "#ifdef CONFIG_ARCH_HAS_SYSCALL_WRAPPER",
            "/* Architectures may override COND_SYSCALL and COND_SYSCALL_COMPAT */",
            "#include <asm/syscall_wrapper.h>",
            "#endif /* CONFIG_ARCH_HAS_SYSCALL_WRAPPER */",
            "",
            "/*  we can't #include <linux/syscalls.h> here,",
            "    but tell gcc to not warn with -Wmissing-prototypes  */",
            "asmlinkage long sys_ni_syscall(void);",
            "",
            "/*",
            " * Non-implemented system calls get redirected here.",
            " */"
          ],
          "function_name": null,
          "description": "该代码块声明处理未实现系统调用的入口函数sys_ni_syscall，并包含必要头文件及架构相关兼容性支持，但缺少函数具体实现，上下文不完整",
          "similarity": 0.4812396764755249
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/sys_ni.c",
          "start_line": 20,
          "end_line": 23,
          "content": [
            "asmlinkage long sys_ni_syscall(void)",
            "{",
            "\treturn -ENOSYS;",
            "}"
          ],
          "function_name": "sys_ni_syscall",
          "description": "该函数实现作为默认的未实现系统调用处理程序，始终返回-ENOSYS错误码以指示系统调用不可用",
          "similarity": 0.4105381965637207
        }
      ]
    }
  ]
}