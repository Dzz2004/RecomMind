{
  "query": "操作系统多道处理机制",
  "timestamp": "2025-12-25 23:52:25",
  "retrieved_files": [
    {
      "source_file": "kernel/stop_machine.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:30:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `stop_machine.c`\n\n---\n\n# `stop_machine.c` 技术文档\n\n## 1. 文件概述\n\n`stop_machine.c` 实现了 Linux 内核中用于在所有（或指定）CPU 上同步执行特定函数的机制，即 **stop_machine** 机制。该机制通过为每个 CPU 创建一个高优先级的内核线程（称为 stopper），在需要时唤醒这些线程以执行指定任务，并确保在执行期间其他任务无法抢占，从而实现对整个系统或部分 CPU 的“冻结”式同步操作。此机制常用于需要全局一致状态的关键内核操作，如 CPU 热插拔、模块加载、内核热补丁（livepatch）等。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct cpu_stop_done`**  \n  用于协调多个 CPU 上 stop 任务的完成状态，包含待完成任务计数（`nr_todo`）、返回值（`ret`）和完成信号量（`completion`）。\n\n- **`struct cpu_stopper`**  \n  每个 CPU 对应一个 stopper 实例，包含：\n  - `thread`：stopper 内核线程\n  - `lock`：保护 pending works 链表的自旋锁\n  - `enabled`：该 stopper 是否启用（对应 CPU 是否在线）\n  - `works`：待执行的 `cpu_stop_work` 链表\n  - `stop_work`、`caller`、`fn`：用于 `stop_cpus` 的临时字段\n\n- **`struct multi_stop_data`**  \n  用于多 CPU 同步执行的共享控制结构，包含：\n  - `fn` 和 `data`：要执行的函数及其参数\n  - `num_threads`：参与同步的线程数\n  - `active_cpus`：指定哪些 CPU 需要实际执行函数\n  - `state`：全局状态机（`MULTI_STOP_*` 枚举）\n  - `thread_ack`：用于状态同步的原子计数器\n\n- **`enum multi_stop_state`**  \n  多 CPU 同步执行的状态机，包括：\n  - `MULTI_STOP_NONE`\n  - `MULTI_STOP_PREPARE`\n  - `MULTI_STOP_DISABLE_IRQ`\n  - `MULTI_STOP_RUN`\n  - `MULTI_STOP_EXIT`\n\n### 主要函数\n\n- **`stop_one_cpu(cpu, fn, arg)`**  \n  在指定 CPU 上执行函数 `fn(arg)`，阻塞等待执行完成。若 CPU 离线则返回 `-ENOENT`。\n\n- **`cpu_stop_queue_work(cpu, work)`**  \n  将 stop 任务加入指定 CPU 的 stopper 队列，若 CPU 在线则唤醒其 stopper 线程。\n\n- **`multi_cpu_stop(data)`**  \n  stopper 线程的主函数，实现多 CPU 同步状态机，负责禁用中断、执行函数、状态同步等。\n\n- **`print_stop_info(log_lvl, task)`**  \n  调试辅助函数，若 `task` 是 stopper 线程，则打印其当前执行函数及调用者信息。\n\n- **`set_state()` / `ack_state()`**  \n  控制多 CPU 同步状态机的推进：`set_state` 设置新状态并重置 ack 计数器，`ack_state` 用于线程确认状态，最后一个确认者推进到下一状态。\n\n## 3. 关键实现\n\n### Stopper 线程模型\n- 每个可能的 CPU 都有一个 `cpu_stopper` 实例，其中包含一个专用内核线程。\n- 该线程运行 `multi_cpu_stop` 函数，处于高优先级实时调度策略（由 `smpboot` 框架设置），可抢占普通任务。\n- 当有 stop 任务时，通过 `wake_up_process` 唤醒对应 stopper 线程。\n\n### 多 CPU 同步状态机\n- 使用共享的 `multi_stop_data` 结构协调所有参与 CPU。\n- 状态转换通过 `set_state` 触发，所有线程通过轮询 `msdata->state` 检测状态变化。\n- 每个状态变更需所有线程调用 `ack_state` 确认，最后一个确认者推进到下一状态，确保严格同步。\n- 在 `MULTI_STOP_DISABLE_IRQ` 状态下，所有参与 CPU 禁用本地中断（包括硬中断），ARM64 还会屏蔽 SDEI 事件。\n- 仅 `active_cpus` 中的 CPU 在 `MULTI_STOP_RUN` 状态执行实际函数。\n\n### 中断与 NMI 安全\n- 执行期间禁用本地中断，防止中断处理程序干扰关键操作。\n- 在等待状态循环中调用 `touch_nmi_watchdog()` 防止 NMI watchdog 误报硬锁死。\n- 使用 `rcu_momentary_dyntick_idle()` 通知 RCU 系统当前 CPU 处于空闲状态，避免 RCU stall。\n\n### CPU 热插拔处理\n- `cpu_stopper.enabled` 标志反映 CPU 在线状态。\n- 若 CPU 离线时提交 stop 任务，则立即完成（调用 `cpu_stop_signal_done`），避免阻塞。\n- 支持从非活动 CPU（如 CPU hotplug 的 bringup 路径）调用 `stop_machine`，此时中断可能已禁用，需保存/恢复中断状态。\n\n### 死锁预防\n- `cpu_stop_queue_two_works` 函数通过嵌套锁（`SINGLE_DEPTH_NESTING`）和重试机制，确保两个 stopper 的入队操作原子性，避免与 `stop_cpus` 并发导致的死锁。\n- 使用 `preempt_disable()` 保证唤醒操作在不可抢占上下文中完成，防止唤醒丢失。\n\n## 4. 依赖关系\n\n- **调度子系统**：依赖 `kthread` 创建 stopper 线程，使用 `wake_up_process` 唤醒。\n- **SMP 子系统**：依赖 `smpboot.h` 的 CPU 热插拔通知机制来启用/禁用 stopper。\n- **中断子系统**：调用 `local_irq_disable/restore`、`hard_irq_disable` 控制中断。\n- **RCU 子系统**：通过 `rcu_momentary_dyntick_idle` 与 RCU 交互。\n- **NMI 子系统**：调用 `touch_nmi_watchdog` 避免 watchdog 误报。\n- **ARM64 架构**：条件编译包含 SDEI（Software Delegated Exception Interface）屏蔽/解除屏蔽。\n- **Per-CPU 基础设施**：使用 `DEFINE_PER_CPU` 和 `per_cpu_ptr` 管理 per-CPU stopper 实例。\n\n## 5. 使用场景\n\n- **CPU 热插拔**：在 CPU 上线/下线过程中执行需要全局同步的操作。\n- **内核模块加载/卸载**：某些架构或功能（如 ftrace）需要 stop_machine 来安全修改内核文本。\n- **内核热补丁（Livepatch）**：在应用补丁时冻结所有 CPU 以确保一致性。\n- **动态 tracing（如 ftrace）**：修改函数入口指令时需 stop_machine 保证原子性。\n- **内存热插拔**：某些内存操作需要全局同步。\n- **内核调试与诊断**：通过 `print_stop_info` 辅助分析 stopper 行为。\n- **架构特定操作**：如 ARM64 的 SDEI 事件处理需要在 stop_machine 上下文中屏蔽。",
      "similarity": 0.6145287752151489,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/stop_machine.c",
          "start_line": 401,
          "end_line": 502,
          "content": [
            "static bool queue_stop_cpus_work(const struct cpumask *cpumask,",
            "\t\t\t\t cpu_stop_fn_t fn, void *arg,",
            "\t\t\t\t struct cpu_stop_done *done)",
            "{",
            "\tstruct cpu_stop_work *work;",
            "\tunsigned int cpu;",
            "\tbool queued = false;",
            "",
            "\t/*",
            "\t * Disable preemption while queueing to avoid getting",
            "\t * preempted by a stopper which might wait for other stoppers",
            "\t * to enter @fn which can lead to deadlock.",
            "\t */",
            "\tpreempt_disable();",
            "\tstop_cpus_in_progress = true;",
            "\tbarrier();",
            "\tfor_each_cpu(cpu, cpumask) {",
            "\t\twork = &per_cpu(cpu_stopper.stop_work, cpu);",
            "\t\twork->fn = fn;",
            "\t\twork->arg = arg;",
            "\t\twork->done = done;",
            "\t\twork->caller = _RET_IP_;",
            "\t\tif (cpu_stop_queue_work(cpu, work))",
            "\t\t\tqueued = true;",
            "\t}",
            "\tbarrier();",
            "\tstop_cpus_in_progress = false;",
            "\tpreempt_enable();",
            "",
            "\treturn queued;",
            "}",
            "static int __stop_cpus(const struct cpumask *cpumask,",
            "\t\t       cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tstruct cpu_stop_done done;",
            "",
            "\tcpu_stop_init_done(&done, cpumask_weight(cpumask));",
            "\tif (!queue_stop_cpus_work(cpumask, fn, arg, &done))",
            "\t\treturn -ENOENT;",
            "\twait_for_completion(&done.completion);",
            "\treturn done.ret;",
            "}",
            "static int stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tint ret;",
            "",
            "\t/* static works are used, process one request at a time */",
            "\tmutex_lock(&stop_cpus_mutex);",
            "\tret = __stop_cpus(cpumask, fn, arg);",
            "\tmutex_unlock(&stop_cpus_mutex);",
            "\treturn ret;",
            "}",
            "static int cpu_stop_should_run(unsigned int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\tunsigned long flags;",
            "\tint run;",
            "",
            "\traw_spin_lock_irqsave(&stopper->lock, flags);",
            "\trun = !list_empty(&stopper->works);",
            "\traw_spin_unlock_irqrestore(&stopper->lock, flags);",
            "\treturn run;",
            "}",
            "static void cpu_stopper_thread(unsigned int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\tstruct cpu_stop_work *work;",
            "",
            "repeat:",
            "\twork = NULL;",
            "\traw_spin_lock_irq(&stopper->lock);",
            "\tif (!list_empty(&stopper->works)) {",
            "\t\twork = list_first_entry(&stopper->works,",
            "\t\t\t\t\tstruct cpu_stop_work, list);",
            "\t\tlist_del_init(&work->list);",
            "\t}",
            "\traw_spin_unlock_irq(&stopper->lock);",
            "",
            "\tif (work) {",
            "\t\tcpu_stop_fn_t fn = work->fn;",
            "\t\tvoid *arg = work->arg;",
            "\t\tstruct cpu_stop_done *done = work->done;",
            "\t\tint ret;",
            "",
            "\t\t/* cpu stop callbacks must not sleep, make in_atomic() == T */",
            "\t\tstopper->caller = work->caller;",
            "\t\tstopper->fn = fn;",
            "\t\tpreempt_count_inc();",
            "\t\tret = fn(arg);",
            "\t\tif (done) {",
            "\t\t\tif (ret)",
            "\t\t\t\tdone->ret = ret;",
            "\t\t\tcpu_stop_signal_done(done);",
            "\t\t}",
            "\t\tpreempt_count_dec();",
            "\t\tstopper->fn = NULL;",
            "\t\tstopper->caller = 0;",
            "\t\tWARN_ONCE(preempt_count(),",
            "\t\t\t  \"cpu_stop: %ps(%p) leaked preempt count\\n\", fn, arg);",
            "\t\tgoto repeat;",
            "\t}",
            "}"
          ],
          "function_name": "queue_stop_cpus_work, __stop_cpus, stop_cpus, cpu_stop_should_run, cpu_stopper_thread",
          "description": "实现批量CPU停止的中枢逻辑，通过互斥锁保证串行化处理。包含工作分发、停止执行、状态追踪等功能，支持通用CPU掩码的停止操作，并提供预处理检查和结果收集机制。",
          "similarity": 0.6292138695716858
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/stop_machine.c",
          "start_line": 269,
          "end_line": 369,
          "content": [
            "static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,",
            "\t\t\t\t    int cpu2, struct cpu_stop_work *work2)",
            "{",
            "\tstruct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);",
            "\tstruct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);",
            "\tint err;",
            "",
            "retry:",
            "\t/*",
            "\t * The waking up of stopper threads has to happen in the same",
            "\t * scheduling context as the queueing.  Otherwise, there is a",
            "\t * possibility of one of the above stoppers being woken up by another",
            "\t * CPU, and preempting us. This will cause us to not wake up the other",
            "\t * stopper forever.",
            "\t */",
            "\tpreempt_disable();",
            "\traw_spin_lock_irq(&stopper1->lock);",
            "\traw_spin_lock_nested(&stopper2->lock, SINGLE_DEPTH_NESTING);",
            "",
            "\tif (!stopper1->enabled || !stopper2->enabled) {",
            "\t\terr = -ENOENT;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\t/*",
            "\t * Ensure that if we race with __stop_cpus() the stoppers won't get",
            "\t * queued up in reverse order leading to system deadlock.",
            "\t *",
            "\t * We can't miss stop_cpus_in_progress if queue_stop_cpus_work() has",
            "\t * queued a work on cpu1 but not on cpu2, we hold both locks.",
            "\t *",
            "\t * It can be falsely true but it is safe to spin until it is cleared,",
            "\t * queue_stop_cpus_work() does everything under preempt_disable().",
            "\t */",
            "\tif (unlikely(stop_cpus_in_progress)) {",
            "\t\terr = -EDEADLK;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\terr = 0;",
            "\t__cpu_stop_queue_work(stopper1, work1);",
            "\t__cpu_stop_queue_work(stopper2, work2);",
            "",
            "unlock:",
            "\traw_spin_unlock(&stopper2->lock);",
            "\traw_spin_unlock_irq(&stopper1->lock);",
            "",
            "\tif (unlikely(err == -EDEADLK)) {",
            "\t\tpreempt_enable();",
            "",
            "\t\twhile (stop_cpus_in_progress)",
            "\t\t\tcpu_relax();",
            "",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tif (!err) {",
            "\t\twake_up_process(stopper1->thread);",
            "\t\twake_up_process(stopper2->thread);",
            "\t}",
            "\tpreempt_enable();",
            "",
            "\treturn err;",
            "}",
            "int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tstruct cpu_stop_done done;",
            "\tstruct cpu_stop_work work1, work2;",
            "\tstruct multi_stop_data msdata;",
            "",
            "\tmsdata = (struct multi_stop_data){",
            "\t\t.fn = fn,",
            "\t\t.data = arg,",
            "\t\t.num_threads = 2,",
            "\t\t.active_cpus = cpumask_of(cpu1),",
            "\t};",
            "",
            "\twork1 = work2 = (struct cpu_stop_work){",
            "\t\t.fn = multi_cpu_stop,",
            "\t\t.arg = &msdata,",
            "\t\t.done = &done,",
            "\t\t.caller = _RET_IP_,",
            "\t};",
            "",
            "\tcpu_stop_init_done(&done, 2);",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "",
            "\tif (cpu1 > cpu2)",
            "\t\tswap(cpu1, cpu2);",
            "\tif (cpu_stop_queue_two_works(cpu1, &work1, cpu2, &work2))",
            "\t\treturn -ENOENT;",
            "",
            "\twait_for_completion(&done.completion);",
            "\treturn done.ret;",
            "}",
            "bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,",
            "\t\t\tstruct cpu_stop_work *work_buf)",
            "{",
            "\t*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, .caller = _RET_IP_, };",
            "\treturn cpu_stop_queue_work(cpu, work_buf);",
            "}"
          ],
          "function_name": "cpu_stop_queue_two_works, stop_two_cpus, stop_one_cpu_nowait",
          "description": "实现双CPU停止操作的协同逻辑，通过原子操作防止死锁，确保两个CPU的停止工作被正确排队和唤醒。包含针对两个CPU的停止接口和非阻塞式单CPU停止标记功能。",
          "similarity": 0.6057325005531311
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/stop_machine.c",
          "start_line": 56,
          "end_line": 201,
          "content": [
            "void print_stop_info(const char *log_lvl, struct task_struct *task)",
            "{",
            "\t/*",
            "\t * If @task is a stopper task, it cannot migrate and task_cpu() is",
            "\t * stable.",
            "\t */",
            "\tstruct cpu_stopper *stopper = per_cpu_ptr(&cpu_stopper, task_cpu(task));",
            "",
            "\tif (task != stopper->thread)",
            "\t\treturn;",
            "",
            "\tprintk(\"%sStopper: %pS <- %pS\\n\", log_lvl, stopper->fn, (void *)stopper->caller);",
            "}",
            "static void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)",
            "{",
            "\tmemset(done, 0, sizeof(*done));",
            "\tatomic_set(&done->nr_todo, nr_todo);",
            "\tinit_completion(&done->completion);",
            "}",
            "static void cpu_stop_signal_done(struct cpu_stop_done *done)",
            "{",
            "\tif (atomic_dec_and_test(&done->nr_todo))",
            "\t\tcomplete(&done->completion);",
            "}",
            "static void __cpu_stop_queue_work(struct cpu_stopper *stopper,",
            "\t\t\t\t  struct cpu_stop_work *work)",
            "{",
            "\tlist_add_tail(&work->list, &stopper->works);",
            "}",
            "static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\tunsigned long flags;",
            "\tbool enabled;",
            "",
            "\tpreempt_disable();",
            "\traw_spin_lock_irqsave(&stopper->lock, flags);",
            "\tenabled = stopper->enabled;",
            "\tif (enabled)",
            "\t\t__cpu_stop_queue_work(stopper, work);",
            "\telse if (work->done)",
            "\t\tcpu_stop_signal_done(work->done);",
            "\traw_spin_unlock_irqrestore(&stopper->lock, flags);",
            "",
            "\tif (enabled)",
            "\t\twake_up_process(stopper->thread);",
            "\tpreempt_enable();",
            "",
            "\treturn enabled;",
            "}",
            "int stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tstruct cpu_stop_done done;",
            "\tstruct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done, .caller = _RET_IP_ };",
            "",
            "\tcpu_stop_init_done(&done, 1);",
            "\tif (!cpu_stop_queue_work(cpu, &work))",
            "\t\treturn -ENOENT;",
            "\t/*",
            "\t * In case @cpu == smp_proccessor_id() we can avoid a sleep+wakeup",
            "\t * cycle by doing a preemption:",
            "\t */",
            "\tcond_resched();",
            "\twait_for_completion(&done.completion);",
            "\treturn done.ret;",
            "}",
            "static void set_state(struct multi_stop_data *msdata,",
            "\t\t      enum multi_stop_state newstate)",
            "{",
            "\t/* Reset ack counter. */",
            "\tatomic_set(&msdata->thread_ack, msdata->num_threads);",
            "\tsmp_wmb();",
            "\tWRITE_ONCE(msdata->state, newstate);",
            "}",
            "static void ack_state(struct multi_stop_data *msdata)",
            "{",
            "\tif (atomic_dec_and_test(&msdata->thread_ack))",
            "\t\tset_state(msdata, msdata->state + 1);",
            "}",
            "notrace void __weak stop_machine_yield(const struct cpumask *cpumask)",
            "{",
            "\tcpu_relax();",
            "}",
            "static int multi_cpu_stop(void *data)",
            "{",
            "\tstruct multi_stop_data *msdata = data;",
            "\tenum multi_stop_state newstate, curstate = MULTI_STOP_NONE;",
            "\tint cpu = smp_processor_id(), err = 0;",
            "\tconst struct cpumask *cpumask;",
            "\tunsigned long flags;",
            "\tbool is_active;",
            "",
            "\t/*",
            "\t * When called from stop_machine_from_inactive_cpu(), irq might",
            "\t * already be disabled.  Save the state and restore it on exit.",
            "\t */",
            "\tlocal_save_flags(flags);",
            "",
            "\tif (!msdata->active_cpus) {",
            "\t\tcpumask = cpu_online_mask;",
            "\t\tis_active = cpu == cpumask_first(cpumask);",
            "\t} else {",
            "\t\tcpumask = msdata->active_cpus;",
            "\t\tis_active = cpumask_test_cpu(cpu, cpumask);",
            "\t}",
            "",
            "\t/* Simple state machine */",
            "\tdo {",
            "\t\t/* Chill out and ensure we re-read multi_stop_state. */",
            "\t\tstop_machine_yield(cpumask);",
            "\t\tnewstate = READ_ONCE(msdata->state);",
            "\t\tif (newstate != curstate) {",
            "\t\t\tcurstate = newstate;",
            "\t\t\tswitch (curstate) {",
            "\t\t\tcase MULTI_STOP_DISABLE_IRQ:",
            "\t\t\t\tlocal_irq_disable();",
            "\t\t\t\thard_irq_disable();",
            "#ifdef CONFIG_ARM64",
            "\t\t\t\tsdei_mask_local_cpu();",
            "#endif",
            "\t\t\t\tbreak;",
            "\t\t\tcase MULTI_STOP_RUN:",
            "\t\t\t\tif (is_active)",
            "\t\t\t\t\terr = msdata->fn(msdata->data);",
            "\t\t\t\tbreak;",
            "\t\t\tdefault:",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t\tack_state(msdata);",
            "\t\t} else if (curstate > MULTI_STOP_PREPARE) {",
            "\t\t\t/*",
            "\t\t\t * At this stage all other CPUs we depend on must spin",
            "\t\t\t * in the same loop. Any reason for hard-lockup should",
            "\t\t\t * be detected and reported on their side.",
            "\t\t\t */",
            "\t\t\ttouch_nmi_watchdog();",
            "\t\t}",
            "\t\trcu_momentary_dyntick_idle();",
            "\t} while (curstate != MULTI_STOP_EXIT);",
            "",
            "#ifdef CONFIG_ARM64",
            "\tsdei_unmask_local_cpu();",
            "#endif",
            "\tlocal_irq_restore(flags);",
            "\treturn err;",
            "}"
          ],
          "function_name": "print_stop_info, cpu_stop_init_done, cpu_stop_signal_done, __cpu_stop_queue_work, cpu_stop_queue_work, stop_one_cpu, set_state, ack_state, stop_machine_yield, multi_cpu_stop",
          "description": "实现了停止操作的核心控制逻辑，包括工作队列管理、状态同步、单CPU停止处理及多CPU状态机。提供打印停止信息、初始化完成状态、信号完成、排队工作、单CPU停止、状态切换等辅助函数。",
          "similarity": 0.5884029269218445
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/stop_machine.c",
          "start_line": 687,
          "end_line": 716,
          "content": [
            "int stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,",
            "\t\t\t\t  const struct cpumask *cpus)",
            "{",
            "\tstruct multi_stop_data msdata = { .fn = fn, .data = data,",
            "\t\t\t\t\t    .active_cpus = cpus };",
            "\tstruct cpu_stop_done done;",
            "\tint ret;",
            "",
            "\t/* Local CPU must be inactive and CPU hotplug in progress. */",
            "\tBUG_ON(cpu_active(raw_smp_processor_id()));",
            "\tmsdata.num_threads = num_active_cpus() + 1;\t/* +1 for local */",
            "",
            "\t/* No proper task established and can't sleep - busy wait for lock. */",
            "\twhile (!mutex_trylock(&stop_cpus_mutex))",
            "\t\tcpu_relax();",
            "",
            "\t/* Schedule work on other CPUs and execute directly for local CPU */",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "\tcpu_stop_init_done(&done, num_active_cpus());",
            "\tqueue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,",
            "\t\t\t     &done);",
            "\tret = multi_cpu_stop(&msdata);",
            "",
            "\t/* Busy wait for completion. */",
            "\twhile (!completion_done(&done.completion))",
            "\t\tcpu_relax();",
            "",
            "\tmutex_unlock(&stop_cpus_mutex);",
            "\treturn ret ?: done.ret;",
            "}"
          ],
          "function_name": "stop_machine_from_inactive_cpu",
          "description": "该函数用于在非活跃CPU上协调多CPU的停止操作，确保在CPU热插拔期间正确执行停止回调函数。  \n通过获取互斥锁、异步调度任务并在本地直接执行，最终阻塞等待所有停止操作完成。  \n依赖未显示的辅助函数（如`queue_stop_cpus_work`和`multi_cpu_stop`），上下文可能存在不完整情况。",
          "similarity": 0.5829452276229858
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/stop_machine.c",
          "start_line": 536,
          "end_line": 641,
          "content": [
            "void stop_machine_park(int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\t/*",
            "\t * Lockless. cpu_stopper_thread() will take stopper->lock and flush",
            "\t * the pending works before it parks, until then it is fine to queue",
            "\t * the new works.",
            "\t */",
            "\tstopper->enabled = false;",
            "\tkthread_park(stopper->thread);",
            "}",
            "static void cpu_stop_create(unsigned int cpu)",
            "{",
            "\tsched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));",
            "}",
            "static void cpu_stop_park(unsigned int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "",
            "\tWARN_ON(!list_empty(&stopper->works));",
            "}",
            "void stop_machine_unpark(int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "",
            "\tstopper->enabled = true;",
            "\tkthread_unpark(stopper->thread);",
            "}",
            "static int __init cpu_stop_init(void)",
            "{",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "",
            "\t\traw_spin_lock_init(&stopper->lock);",
            "\t\tINIT_LIST_HEAD(&stopper->works);",
            "\t}",
            "",
            "\tBUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));",
            "\tstop_machine_unpark(raw_smp_processor_id());",
            "\tstop_machine_initialized = true;",
            "\treturn 0;",
            "}",
            "int stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,",
            "\t\t\t    const struct cpumask *cpus)",
            "{",
            "\tstruct multi_stop_data msdata = {",
            "\t\t.fn = fn,",
            "\t\t.data = data,",
            "\t\t.num_threads = num_online_cpus(),",
            "\t\t.active_cpus = cpus,",
            "\t};",
            "",
            "\tlockdep_assert_cpus_held();",
            "",
            "\tif (!stop_machine_initialized) {",
            "\t\t/*",
            "\t\t * Handle the case where stop_machine() is called",
            "\t\t * early in boot before stop_machine() has been",
            "\t\t * initialized.",
            "\t\t */",
            "\t\tunsigned long flags;",
            "\t\tint ret;",
            "",
            "\t\tWARN_ON_ONCE(msdata.num_threads != 1);",
            "",
            "\t\tlocal_irq_save(flags);",
            "\t\thard_irq_disable();",
            "\t\tret = (*fn)(data);",
            "\t\tlocal_irq_restore(flags);",
            "",
            "\t\treturn ret;",
            "\t}",
            "",
            "\t/* Set the initial state and stop all online cpus. */",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "\treturn stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);",
            "}",
            "int stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)",
            "{",
            "\tint ret;",
            "",
            "\t/* No CPUs can come up or down during this. */",
            "\tcpus_read_lock();",
            "\tret = stop_machine_cpuslocked(fn, data, cpus);",
            "\tcpus_read_unlock();",
            "\treturn ret;",
            "}",
            "int stop_core_cpuslocked(unsigned int cpu, cpu_stop_fn_t fn, void *data)",
            "{",
            "\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);",
            "",
            "\tstruct multi_stop_data msdata = {",
            "\t\t.fn = fn,",
            "\t\t.data = data,",
            "\t\t.num_threads = cpumask_weight(smt_mask),",
            "\t\t.active_cpus = smt_mask,",
            "\t};",
            "",
            "\tlockdep_assert_cpus_held();",
            "",
            "\t/* Set the initial state and stop all online cpus. */",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "\treturn stop_cpus(smt_mask, multi_cpu_stop, &msdata);",
            "}"
          ],
          "function_name": "stop_machine_park, cpu_stop_create, cpu_stop_park, stop_machine_unpark, cpu_stop_init, stop_machine_cpuslocked, stop_machine, stop_core_cpuslocked",
          "description": "负责停止器线程的生命周期管理和初始化。包含线程创建、挂起/恢复操作、核心初始化函数，以及带锁的CPU停止接口。提供针对特定CPU核心的停止功能和早期启动阶段的降级处理路径。",
          "similarity": 0.5577624440193176
        }
      ]
    },
    {
      "source_file": "kernel/irq/ipi-mux.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:57:27\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\ipi-mux.c`\n\n---\n\n# `irq/ipi-mux.c` 技术文档\n\n## 1. 文件概述\n\n`ipi-mux.c` 实现了一个虚拟 IPI（Inter-Processor Interrupt，处理器间中断）多路复用机制，允许多个逻辑 IPI 共享一个底层硬件 IPI。该机制通过软件方式在每个 CPU 上维护一个位图，记录哪些虚拟 IPI 处于挂起（pending）或使能（enabled）状态，并在需要时触发底层硬件 IPI。此设计适用于硬件 IPI 资源受限但需要支持多个逻辑 IPI 的系统架构（如某些 RISC-V 或定制 SoC 平台）。\n\n## 2. 核心功能\n\n### 数据结构\n\n- **`struct ipi_mux_cpu`**  \n  每个 CPU 的私有状态结构，包含两个原子变量：\n  - `enable`：表示当前 CPU 上哪些虚拟 IPI 已被使能（unmasked）。\n  - `bits`：表示当前 CPU 上哪些虚拟 IPI 处于挂起状态（pending）。\n\n- **全局变量**\n  - `ipi_mux_pcpu`：指向 per-CPU 的 `ipi_mux_cpu` 实例。\n  - `ipi_mux_domain`：指向虚拟 IPI 的 IRQ domain。\n  - `ipi_mux_send`：回调函数，用于向指定 CPU 发送底层硬件 IPI。\n\n### 主要函数\n\n- **`ipi_mux_mask()` / `ipi_mux_unmask()`**  \n  实现虚拟 IPI 的屏蔽与解除屏蔽逻辑，通过操作 `enable` 字段控制中断使能状态。\n\n- **`ipi_mux_send_mask()`**  \n  实现 `ipi_send_mask` 接口，用于向指定 CPU 集合发送特定虚拟 IPI。通过设置 `bits` 字段标记挂起状态，并在必要时触发底层 IPI。\n\n- **`ipi_mux_process()`**  \n  在底层 IPI 中断处理上下文中调用，读取并清除当前 CPU 的挂起虚拟 IPI 位图，并调用对应的中断处理程序。\n\n- **`ipi_mux_create()`**  \n  初始化整个虚拟 IPI 多路复用系统，包括分配 per-CPU 数据、创建 IRQ domain、分配虚拟 IRQ 号，并注册回调函数。\n\n### IRQ 芯片与 Domain 操作\n\n- **`ipi_mux_chip`**：定义了虚拟 IPI 的 `irq_chip` 操作集。\n- **`ipi_mux_domain_ops`**：定义了 IRQ domain 的分配与释放操作。\n\n## 3. 关键实现\n\n### 虚拟 IPI 状态管理\n\n每个 CPU 维护两个位图：\n- `enable`：记录哪些虚拟 IPI 当前被允许触发（即未被 mask）。\n- `bits`：记录哪些虚拟 IPI 已被请求但尚未处理（pending）。\n\n当调用 `ipi_send_mask()` 时，对应位被置入 `bits`；若该位同时在 `enable` 中置位，则立即触发底层 IPI。\n\n### 内存顺序与同步\n\n- 使用 `atomic_fetch_or_release()` 和 `smp_mb__after_atomic()` 确保：\n  - 对 `bits` 的写入在读取 `enable` 之前完成，避免与 `ipi_mux_unmask()` 竞争。\n  - 虚拟 IPI 标志的设置在触发底层 IPI 前对目标 CPU 可见。\n- 在 `ipi_mux_process()` 中使用 `atomic_fetch_andnot()` 原子地清除已使能且挂起的位，确保中断处理的精确性。\n\n### 中断处理流程\n\n1. 软件调用 `ipi_send_mask()` 发送虚拟 IPI。\n2. 目标 CPU 的 `bits` 对应位置位；若已使能，则调用 `ipi_mux_send()` 触发硬件 IPI。\n3. 硬件 IPI 到达后，调用 `ipi_mux_process()`。\n4. `ipi_mux_process()` 读取 `enable` 与 `bits`，计算需处理的虚拟 IPI 集合，并调用 `generic_handle_domain_irq()` 分发至对应处理函数。\n\n### IRQ Domain 管理\n\n- 使用线性 IRQ domain，虚拟 IPI 的 `hwirq` 编号从 0 开始连续分配。\n- 设置 `IRQ_DOMAIN_FLAG_IPI_SINGLE` 和 `DOMAIN_BUS_IPI` 标志，表明该 domain 专用于 IPI。\n- 每个虚拟 IPI 被配置为 per-CPU 中断（`irq_set_percpu_devid`），使用 `handle_percpu_devid_irq` 处理器。\n\n## 4. 依赖关系\n\n- **`<linux/irq.h>` / `<linux/irqdomain.h>`**：IRQ 子系统核心接口，用于注册 IRQ domain 和管理中断。\n- **`<linux/irqchip/chained_irq.h>`**：提供链式中断处理支持（虽未直接使用，但属于 IPI 架构上下文）。\n- **`<linux/percpu.h>`**：用于分配和访问 per-CPU 数据结构。\n- **`<linux/smp.h>` / `<linux/cpu.h>`**：SMP 相关功能，如 CPU 掩码遍历和处理器 ID 获取。\n- **`<linux/jump_label.h>`**：可能用于优化（当前未显式使用，但包含在头文件中）。\n- **底层平台代码**：必须提供 `mux_send` 回调函数，用于实际触发硬件 IPI。\n\n## 5. 使用场景\n\n- **硬件 IPI 资源受限的 SoC**：当物理 IPI 通道数量少于所需逻辑中断类型时（如仅 1 个硬件 IPI 但需支持 timer、reschedule、call-function 等多种 IPI），使用此机制进行软件复用。\n- **RISC-V 或定制架构平台**：如 Asahi Linux（Apple Silicon）或 Ventana Micro 的 RISC-V 实现，这些平台可能缺乏丰富的硬件 IPI 支持。\n- **内核 SMP 初始化阶段**：在 `ipi_mux_create()` 成功后，其他子系统（如调度器、RCU）可通过分配的虚拟 IPI 实现跨 CPU 通信。\n- **替代传统 IPI 向量机制**：在不支持多向量 IPI 的架构上，提供类似 x86 的多类型 IPI 功能。",
      "similarity": 0.5998804569244385,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/ipi-mux.c",
          "start_line": 29,
          "end_line": 176,
          "content": [
            "static void ipi_mux_mask(struct irq_data *d)",
            "{",
            "\tstruct ipi_mux_cpu *icpu = this_cpu_ptr(ipi_mux_pcpu);",
            "",
            "\tatomic_andnot(BIT(irqd_to_hwirq(d)), &icpu->enable);",
            "}",
            "static void ipi_mux_unmask(struct irq_data *d)",
            "{",
            "\tstruct ipi_mux_cpu *icpu = this_cpu_ptr(ipi_mux_pcpu);",
            "\tu32 ibit = BIT(irqd_to_hwirq(d));",
            "",
            "\tatomic_or(ibit, &icpu->enable);",
            "",
            "\t/*",
            "\t * The atomic_or() above must complete before the atomic_read()",
            "\t * below to avoid racing ipi_mux_send_mask().",
            "\t */",
            "\tsmp_mb__after_atomic();",
            "",
            "\t/* If a pending IPI was unmasked, raise a parent IPI immediately. */",
            "\tif (atomic_read(&icpu->bits) & ibit)",
            "\t\tipi_mux_send(smp_processor_id());",
            "}",
            "static void ipi_mux_send_mask(struct irq_data *d, const struct cpumask *mask)",
            "{",
            "\tstruct ipi_mux_cpu *icpu = this_cpu_ptr(ipi_mux_pcpu);",
            "\tu32 ibit = BIT(irqd_to_hwirq(d));",
            "\tunsigned long pending;",
            "\tint cpu;",
            "",
            "\tfor_each_cpu(cpu, mask) {",
            "\t\ticpu = per_cpu_ptr(ipi_mux_pcpu, cpu);",
            "",
            "\t\t/*",
            "\t\t * This sequence is the mirror of the one in ipi_mux_unmask();",
            "\t\t * see the comment there. Additionally, release semantics",
            "\t\t * ensure that the vIPI flag set is ordered after any shared",
            "\t\t * memory accesses that precede it. This therefore also pairs",
            "\t\t * with the atomic_fetch_andnot in ipi_mux_process().",
            "\t\t */",
            "\t\tpending = atomic_fetch_or_release(ibit, &icpu->bits);",
            "",
            "\t\t/*",
            "\t\t * The atomic_fetch_or_release() above must complete",
            "\t\t * before the atomic_read() below to avoid racing with",
            "\t\t * ipi_mux_unmask().",
            "\t\t */",
            "\t\tsmp_mb__after_atomic();",
            "",
            "\t\t/*",
            "\t\t * The flag writes must complete before the physical IPI is",
            "\t\t * issued to another CPU. This is implied by the control",
            "\t\t * dependency on the result of atomic_read() below, which is",
            "\t\t * itself already ordered after the vIPI flag write.",
            "\t\t */",
            "\t\tif (!(pending & ibit) && (atomic_read(&icpu->enable) & ibit))",
            "\t\t\tipi_mux_send(cpu);",
            "\t}",
            "}",
            "static int ipi_mux_domain_alloc(struct irq_domain *d, unsigned int virq,",
            "\t\t\t\tunsigned int nr_irqs, void *arg)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < nr_irqs; i++) {",
            "\t\tirq_set_percpu_devid(virq + i);",
            "\t\tirq_domain_set_info(d, virq + i, i, &ipi_mux_chip, NULL,",
            "\t\t\t\t    handle_percpu_devid_irq, NULL, NULL);",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "void ipi_mux_process(void)",
            "{",
            "\tstruct ipi_mux_cpu *icpu = this_cpu_ptr(ipi_mux_pcpu);",
            "\tirq_hw_number_t hwirq;",
            "\tunsigned long ipis;",
            "\tunsigned int en;",
            "",
            "\t/*",
            "\t * Reading enable mask does not need to be ordered as long as",
            "\t * this function is called from interrupt handler because only",
            "\t * the CPU itself can change it's own enable mask.",
            "\t */",
            "\ten = atomic_read(&icpu->enable);",
            "",
            "\t/*",
            "\t * Clear the IPIs we are about to handle. This pairs with the",
            "\t * atomic_fetch_or_release() in ipi_mux_send_mask().",
            "\t */",
            "\tipis = atomic_fetch_andnot(en, &icpu->bits) & en;",
            "",
            "\tfor_each_set_bit(hwirq, &ipis, BITS_PER_TYPE(int))",
            "\t\tgeneric_handle_domain_irq(ipi_mux_domain, hwirq);",
            "}",
            "int ipi_mux_create(unsigned int nr_ipi, void (*mux_send)(unsigned int cpu))",
            "{",
            "\tstruct fwnode_handle *fwnode;",
            "\tstruct irq_domain *domain;",
            "\tint rc;",
            "",
            "\tif (ipi_mux_domain)",
            "\t\treturn -EEXIST;",
            "",
            "\tif (BITS_PER_TYPE(int) < nr_ipi || !mux_send)",
            "\t\treturn -EINVAL;",
            "",
            "\tipi_mux_pcpu = alloc_percpu(typeof(*ipi_mux_pcpu));",
            "\tif (!ipi_mux_pcpu)",
            "\t\treturn -ENOMEM;",
            "",
            "\tfwnode = irq_domain_alloc_named_fwnode(\"IPI-Mux\");",
            "\tif (!fwnode) {",
            "\t\tpr_err(\"unable to create IPI Mux fwnode\\n\");",
            "\t\trc = -ENOMEM;",
            "\t\tgoto fail_free_cpu;",
            "\t}",
            "",
            "\tdomain = irq_domain_create_linear(fwnode, nr_ipi,",
            "\t\t\t\t\t  &ipi_mux_domain_ops, NULL);",
            "\tif (!domain) {",
            "\t\tpr_err(\"unable to add IPI Mux domain\\n\");",
            "\t\trc = -ENOMEM;",
            "\t\tgoto fail_free_fwnode;",
            "\t}",
            "",
            "\tdomain->flags |= IRQ_DOMAIN_FLAG_IPI_SINGLE;",
            "\tirq_domain_update_bus_token(domain, DOMAIN_BUS_IPI);",
            "",
            "\trc = irq_domain_alloc_irqs(domain, nr_ipi, NUMA_NO_NODE, NULL);",
            "\tif (rc <= 0) {",
            "\t\tpr_err(\"unable to alloc IRQs from IPI Mux domain\\n\");",
            "\t\tgoto fail_free_domain;",
            "\t}",
            "",
            "\tipi_mux_domain = domain;",
            "\tipi_mux_send = mux_send;",
            "",
            "\treturn rc;",
            "",
            "fail_free_domain:",
            "\tirq_domain_remove(domain);",
            "fail_free_fwnode:",
            "\tirq_domain_free_fwnode(fwnode);",
            "fail_free_cpu:",
            "\tfree_percpu(ipi_mux_pcpu);",
            "\treturn rc;",
            "}"
          ],
          "function_name": "ipi_mux_mask, ipi_mux_unmask, ipi_mux_send_mask, ipi_mux_domain_alloc, ipi_mux_process, ipi_mux_create",
          "description": "实现了IPI多路复用器的核心逻辑，包括中断屏蔽/解除屏蔽操作、批量发送IPI标记、中断域分配、中断处理流程及初始化创建函数，通过原子操作和内存屏障保证跨CPU状态同步与顺序性。",
          "similarity": 0.6509523987770081
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/ipi-mux.c",
          "start_line": 1,
          "end_line": 28,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Multiplex several virtual IPIs over a single HW IPI.",
            " *",
            " * Copyright The Asahi Linux Contributors",
            " * Copyright (c) 2022 Ventana Micro Systems Inc.",
            " */",
            "",
            "#define pr_fmt(fmt) \"ipi-mux: \" fmt",
            "#include <linux/cpu.h>",
            "#include <linux/init.h>",
            "#include <linux/irq.h>",
            "#include <linux/irqchip.h>",
            "#include <linux/irqchip/chained_irq.h>",
            "#include <linux/irqdomain.h>",
            "#include <linux/jump_label.h>",
            "#include <linux/percpu.h>",
            "#include <linux/smp.h>",
            "",
            "struct ipi_mux_cpu {",
            "\tatomic_t\t\t\tenable;",
            "\tatomic_t\t\t\tbits;",
            "};",
            "",
            "static struct ipi_mux_cpu __percpu *ipi_mux_pcpu;",
            "static struct irq_domain *ipi_mux_domain;",
            "static void (*ipi_mux_send)(unsigned int cpu);",
            ""
          ],
          "function_name": null,
          "description": "定义了用于多路复用虚拟IPI的CPU状态结构体ipi_mux_cpu，包含启用位掩码和位计数器。声明了全局的per-CPU指针数组ipi_mux_pcpu，中断域ipi_mux_domain，及发送回调函数指针ipi_mux_send。",
          "similarity": 0.5186734795570374
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.5968435406684875,
      "chunks": [
        {
          "chunk_id": 16,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2977,
          "end_line": 3111,
          "content": [
            "static inline bool",
            "put_cached_bnode(struct kfree_rcu_cpu *krcp,",
            "\tstruct kvfree_rcu_bulk_data *bnode)",
            "{",
            "\t// Check the limit.",
            "\tif (krcp->nr_bkv_objs >= rcu_min_cached_objs)",
            "\t\treturn false;",
            "",
            "\tllist_add((struct llist_node *) bnode, &krcp->bkvcache);",
            "\tWRITE_ONCE(krcp->nr_bkv_objs, krcp->nr_bkv_objs + 1);",
            "\treturn true;",
            "}",
            "static int",
            "drain_page_cache(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tunsigned long flags;",
            "\tstruct llist_node *page_list, *pos, *n;",
            "\tint freed = 0;",
            "",
            "\tif (!rcu_min_cached_objs)",
            "\t\treturn 0;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\tpage_list = llist_del_all(&krcp->bkvcache);",
            "\tWRITE_ONCE(krcp->nr_bkv_objs, 0);",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\tllist_for_each_safe(pos, n, page_list) {",
            "\t\tfree_page((unsigned long)pos);",
            "\t\tfreed++;",
            "\t}",
            "",
            "\treturn freed;",
            "}",
            "static void",
            "kvfree_rcu_bulk(struct kfree_rcu_cpu *krcp,",
            "\tstruct kvfree_rcu_bulk_data *bnode, int idx)",
            "{",
            "\tunsigned long flags;",
            "\tint i;",
            "",
            "\tif (!WARN_ON_ONCE(!poll_state_synchronize_rcu_full(&bnode->gp_snap))) {",
            "\t\tdebug_rcu_bhead_unqueue(bnode);",
            "\t\trcu_lock_acquire(&rcu_callback_map);",
            "\t\tif (idx == 0) { // kmalloc() / kfree().",
            "\t\t\ttrace_rcu_invoke_kfree_bulk_callback(",
            "\t\t\t\trcu_state.name, bnode->nr_records,",
            "\t\t\t\tbnode->records);",
            "",
            "\t\t\tkfree_bulk(bnode->nr_records, bnode->records);",
            "\t\t} else { // vmalloc() / vfree().",
            "\t\t\tfor (i = 0; i < bnode->nr_records; i++) {",
            "\t\t\t\ttrace_rcu_invoke_kvfree_callback(",
            "\t\t\t\t\trcu_state.name, bnode->records[i], 0);",
            "",
            "\t\t\t\tvfree(bnode->records[i]);",
            "\t\t\t}",
            "\t\t}",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "\t}",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\tif (put_cached_bnode(krcp, bnode))",
            "\t\tbnode = NULL;",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\tif (bnode)",
            "\t\tfree_page((unsigned long) bnode);",
            "",
            "\tcond_resched_tasks_rcu_qs();",
            "}",
            "static void",
            "kvfree_rcu_list(struct rcu_head *head)",
            "{",
            "\tstruct rcu_head *next;",
            "",
            "\tfor (; head; head = next) {",
            "\t\tvoid *ptr = (void *) head->func;",
            "\t\tunsigned long offset = (void *) head - ptr;",
            "",
            "\t\tnext = head->next;",
            "\t\tdebug_rcu_head_unqueue((struct rcu_head *)ptr);",
            "\t\trcu_lock_acquire(&rcu_callback_map);",
            "\t\ttrace_rcu_invoke_kvfree_callback(rcu_state.name, head, offset);",
            "",
            "\t\tif (!WARN_ON_ONCE(!__is_kvfree_rcu_offset(offset)))",
            "\t\t\tkvfree(ptr);",
            "",
            "\t\trcu_lock_release(&rcu_callback_map);",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t}",
            "}",
            "static void kfree_rcu_work(struct work_struct *work)",
            "{",
            "\tunsigned long flags;",
            "\tstruct kvfree_rcu_bulk_data *bnode, *n;",
            "\tstruct list_head bulk_head[FREE_N_CHANNELS];",
            "\tstruct rcu_head *head;",
            "\tstruct kfree_rcu_cpu *krcp;",
            "\tstruct kfree_rcu_cpu_work *krwp;",
            "\tstruct rcu_gp_oldstate head_gp_snap;",
            "\tint i;",
            "",
            "\tkrwp = container_of(to_rcu_work(work),",
            "\t\tstruct kfree_rcu_cpu_work, rcu_work);",
            "\tkrcp = krwp->krcp;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\t// Channels 1 and 2.",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tlist_replace_init(&krwp->bulk_head_free[i], &bulk_head[i]);",
            "",
            "\t// Channel 3.",
            "\thead = krwp->head_free;",
            "\tkrwp->head_free = NULL;",
            "\thead_gp_snap = krwp->head_free_gp_snap;",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\t// Handle the first two channels.",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++) {",
            "\t\t// Start from the tail page, so a GP is likely passed for it.",
            "\t\tlist_for_each_entry_safe(bnode, n, &bulk_head[i], list)",
            "\t\t\tkvfree_rcu_bulk(krcp, bnode, i);",
            "\t}",
            "",
            "\t/*",
            "\t * This is used when the \"bulk\" path can not be used for the",
            "\t * double-argument of kvfree_rcu().  This happens when the",
            "\t * page-cache is empty, which means that objects are instead",
            "\t * queued on a linked list through their rcu_head structures.",
            "\t * This list is named \"Channel 3\".",
            "\t */",
            "\tif (head && !WARN_ON_ONCE(!poll_state_synchronize_rcu_full(&head_gp_snap)))",
            "\t\tkvfree_rcu_list(head);",
            "}"
          ],
          "function_name": "put_cached_bnode, drain_page_cache, kvfree_rcu_bulk, kvfree_rcu_list, kfree_rcu_work",
          "description": "处理批量内存释放，通过缓存节点管理、页面缓存填充及多通道分类释放机制实现高效内存回收。",
          "similarity": 0.6384069919586182
        },
        {
          "chunk_id": 22,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4072,
          "end_line": 4226,
          "content": [
            "static void rcu_barrier_trace(const char *s, int cpu, unsigned long done)",
            "{",
            "\ttrace_rcu_barrier(rcu_state.name, s, cpu,",
            "\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);",
            "}",
            "static void rcu_barrier_callback(struct rcu_head *rhp)",
            "{",
            "\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;",
            "",
            "\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {",
            "\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "\t} else {",
            "\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);",
            "\t}",
            "}",
            "static void rcu_barrier_entrain(struct rcu_data *rdp)",
            "{",
            "\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);",
            "\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);",
            "\tbool wake_nocb = false;",
            "\tbool was_alldone = false;",
            "",
            "\tlockdep_assert_held(&rcu_state.barrier_lock);",
            "\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))",
            "\t\treturn;",
            "\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);",
            "\trdp->barrier_head.func = rcu_barrier_callback;",
            "\tdebug_rcu_head_queue(&rdp->barrier_head);",
            "\trcu_nocb_lock(rdp);",
            "\t/*",
            "\t * Flush bypass and wakeup rcuog if we add callbacks to an empty regular",
            "\t * queue. This way we don't wait for bypass timer that can reach seconds",
            "\t * if it's fully lazy.",
            "\t */",
            "\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));",
            "\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {",
            "\t\tatomic_inc(&rcu_state.barrier_cpu_count);",
            "\t} else {",
            "\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);",
            "\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);",
            "\t}",
            "\trcu_nocb_unlock(rdp);",
            "\tif (wake_nocb)",
            "\t\twake_nocb_gp(rdp, false);",
            "\tsmp_store_release(&rdp->barrier_seq_snap, gseq);",
            "}",
            "static void rcu_barrier_handler(void *cpu_in)",
            "{",
            "\tuintptr_t cpu = (uintptr_t)cpu_in;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\tWARN_ON_ONCE(cpu != rdp->cpu);",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\trcu_barrier_entrain(rdp);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "}",
            "void rcu_barrier(void)",
            "{",
            "\tuintptr_t cpu;",
            "\tunsigned long flags;",
            "\tunsigned long gseq;",
            "\tstruct rcu_data *rdp;",
            "\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);",
            "",
            "\trcu_barrier_trace(TPS(\"Begin\"), -1, s);",
            "",
            "\t/* Take mutex to serialize concurrent rcu_barrier() requests. */",
            "\tmutex_lock(&rcu_state.barrier_mutex);",
            "",
            "\t/* Did someone else do our work for us? */",
            "\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {",
            "\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);",
            "\t\tsmp_mb(); /* caller's subsequent code after above check. */",
            "\t\tmutex_unlock(&rcu_state.barrier_mutex);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Mark the start of the barrier operation. */",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\trcu_seq_start(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);",
            "",
            "\t/*",
            "\t * Initialize the count to two rather than to zero in order",
            "\t * to avoid a too-soon return to zero in case of an immediate",
            "\t * invocation of the just-enqueued callback (or preemption of",
            "\t * this task).  Exclude CPU-hotplug operations to ensure that no",
            "\t * offline non-offloaded CPU has callbacks queued.",
            "\t */",
            "\tinit_completion(&rcu_state.barrier_completion);",
            "\tatomic_set(&rcu_state.barrier_cpu_count, 2);",
            "\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "",
            "\t/*",
            "\t * Force each CPU with callbacks to register a new callback.",
            "\t * When that callback is invoked, we will know that all of the",
            "\t * corresponding CPU's preceding callbacks have been invoked.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "retry:",
            "\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)",
            "\t\t\tcontinue;",
            "\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {",
            "\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (!rcu_rdp_cpu_online(rdp)) {",
            "\t\t\trcu_barrier_entrain(rdp);",
            "\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {",
            "\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);",
            "\t}",
            "",
            "\t/*",
            "\t * Now that we have an rcu_barrier_callback() callback on each",
            "\t * CPU, and thus each counted, remove the initial count.",
            "\t */",
            "\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "",
            "\t/* Wait for all rcu_barrier_callback() callbacks to be invoked. */",
            "\twait_for_completion(&rcu_state.barrier_completion);",
            "",
            "\t/* Mark the end of the barrier operation. */",
            "\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);",
            "\trcu_seq_end(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t}",
            "",
            "\t/* Other rcu_barrier() invocations can now safely proceed. */",
            "\tmutex_unlock(&rcu_state.barrier_mutex);",
            "}"
          ],
          "function_name": "rcu_barrier_trace, rcu_barrier_callback, rcu_barrier_entrain, rcu_barrier_handler, rcu_barrier",
          "description": "实现RCU屏障功能，通过分发回调函数强制所有CPU完成当前RCU操作，使用原子计数器跟踪完成状态，通过completion等待所有回调完成",
          "similarity": 0.6104593276977539
        },
        {
          "chunk_id": 17,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 3121,
          "end_line": 3265,
          "content": [
            "static bool",
            "need_offload_krc(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tif (!list_empty(&krcp->bulk_head[i]))",
            "\t\t\treturn true;",
            "",
            "\treturn !!READ_ONCE(krcp->head);",
            "}",
            "static bool",
            "need_wait_for_krwp_work(struct kfree_rcu_cpu_work *krwp)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tif (!list_empty(&krwp->bulk_head_free[i]))",
            "\t\t\treturn true;",
            "",
            "\treturn !!krwp->head_free;",
            "}",
            "static int krc_count(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tint sum = atomic_read(&krcp->head_count);",
            "\tint i;",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++)",
            "\t\tsum += atomic_read(&krcp->bulk_count[i]);",
            "",
            "\treturn sum;",
            "}",
            "static void",
            "__schedule_delayed_monitor_work(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tlong delay, delay_left;",
            "",
            "\tdelay = krc_count(krcp) >= KVFREE_BULK_MAX_ENTR ? 1:KFREE_DRAIN_JIFFIES;",
            "\tif (delayed_work_pending(&krcp->monitor_work)) {",
            "\t\tdelay_left = krcp->monitor_work.timer.expires - jiffies;",
            "\t\tif (delay < delay_left)",
            "\t\t\tmod_delayed_work(system_unbound_wq, &krcp->monitor_work, delay);",
            "\t\treturn;",
            "\t}",
            "\tqueue_delayed_work(system_unbound_wq, &krcp->monitor_work, delay);",
            "}",
            "static void",
            "schedule_delayed_monitor_work(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\t__schedule_delayed_monitor_work(krcp);",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "}",
            "static void",
            "kvfree_rcu_drain_ready(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tstruct list_head bulk_ready[FREE_N_CHANNELS];",
            "\tstruct kvfree_rcu_bulk_data *bnode, *n;",
            "\tstruct rcu_head *head_ready = NULL;",
            "\tunsigned long flags;",
            "\tint i;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++) {",
            "\t\tINIT_LIST_HEAD(&bulk_ready[i]);",
            "",
            "\t\tlist_for_each_entry_safe_reverse(bnode, n, &krcp->bulk_head[i], list) {",
            "\t\t\tif (!poll_state_synchronize_rcu_full(&bnode->gp_snap))",
            "\t\t\t\tbreak;",
            "",
            "\t\t\tatomic_sub(bnode->nr_records, &krcp->bulk_count[i]);",
            "\t\t\tlist_move(&bnode->list, &bulk_ready[i]);",
            "\t\t}",
            "\t}",
            "",
            "\tif (krcp->head && poll_state_synchronize_rcu(krcp->head_gp_snap)) {",
            "\t\thead_ready = krcp->head;",
            "\t\tatomic_set(&krcp->head_count, 0);",
            "\t\tWRITE_ONCE(krcp->head, NULL);",
            "\t}",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "",
            "\tfor (i = 0; i < FREE_N_CHANNELS; i++) {",
            "\t\tlist_for_each_entry_safe(bnode, n, &bulk_ready[i], list)",
            "\t\t\tkvfree_rcu_bulk(krcp, bnode, i);",
            "\t}",
            "",
            "\tif (head_ready)",
            "\t\tkvfree_rcu_list(head_ready);",
            "}",
            "static bool",
            "kvfree_rcu_queue_batch(struct kfree_rcu_cpu *krcp)",
            "{",
            "\tunsigned long flags;",
            "\tbool queued = false;",
            "\tint i, j;",
            "",
            "\traw_spin_lock_irqsave(&krcp->lock, flags);",
            "",
            "\t// Attempt to start a new batch.",
            "\tfor (i = 0; i < KFREE_N_BATCHES; i++) {",
            "\t\tstruct kfree_rcu_cpu_work *krwp = &(krcp->krw_arr[i]);",
            "",
            "\t\t// Try to detach bulk_head or head and attach it, only when",
            "\t\t// all channels are free.  Any channel is not free means at krwp",
            "\t\t// there is on-going rcu work to handle krwp's free business.",
            "\t\tif (need_wait_for_krwp_work(krwp))",
            "\t\t\tcontinue;",
            "",
            "\t\t// kvfree_rcu_drain_ready() might handle this krcp, if so give up.",
            "\t\tif (need_offload_krc(krcp)) {",
            "\t\t\t// Channel 1 corresponds to the SLAB-pointer bulk path.",
            "\t\t\t// Channel 2 corresponds to vmalloc-pointer bulk path.",
            "\t\t\tfor (j = 0; j < FREE_N_CHANNELS; j++) {",
            "\t\t\t\tif (list_empty(&krwp->bulk_head_free[j])) {",
            "\t\t\t\t\tatomic_set(&krcp->bulk_count[j], 0);",
            "\t\t\t\t\tlist_replace_init(&krcp->bulk_head[j],",
            "\t\t\t\t\t\t&krwp->bulk_head_free[j]);",
            "\t\t\t\t}",
            "\t\t\t}",
            "",
            "\t\t\t// Channel 3 corresponds to both SLAB and vmalloc",
            "\t\t\t// objects queued on the linked list.",
            "\t\t\tif (!krwp->head_free) {",
            "\t\t\t\tkrwp->head_free = krcp->head;",
            "\t\t\t\tget_state_synchronize_rcu_full(&krwp->head_free_gp_snap);",
            "\t\t\t\tatomic_set(&krcp->head_count, 0);",
            "\t\t\t\tWRITE_ONCE(krcp->head, NULL);",
            "\t\t\t}",
            "",
            "\t\t\t// One work is per one batch, so there are three",
            "\t\t\t// \"free channels\", the batch can handle. Break",
            "\t\t\t// the loop since it is done with this CPU thus",
            "\t\t\t// queuing an RCU work is _always_ success here.",
            "\t\t\tqueued = queue_rcu_work(system_unbound_wq, &krwp->rcu_work);",
            "\t\t\tWARN_ON_ONCE(!queued);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\traw_spin_unlock_irqrestore(&krcp->lock, flags);",
            "\treturn queued;",
            "}"
          ],
          "function_name": "need_offload_krc, need_wait_for_krwp_work, krc_count, __schedule_delayed_monitor_work, schedule_delayed_monitor_work, kvfree_rcu_drain_ready, kvfree_rcu_queue_batch",
          "description": "调度延迟清理工作，通过统计待处理对象数量动态调整清理时机，支持分批处理以优化系统资源利用。",
          "similarity": 0.6084855794906616
        },
        {
          "chunk_id": 27,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4895,
          "end_line": 5082,
          "content": [
            "static void __init rcu_init_one(void)",
            "{",
            "\tstatic const char * const buf[] = RCU_NODE_NAME_INIT;",
            "\tstatic const char * const fqs[] = RCU_FQS_NAME_INIT;",
            "\tstatic struct lock_class_key rcu_node_class[RCU_NUM_LVLS];",
            "\tstatic struct lock_class_key rcu_fqs_class[RCU_NUM_LVLS];",
            "",
            "\tint levelspread[RCU_NUM_LVLS];\t\t/* kids/node in each level. */",
            "\tint cpustride = 1;",
            "\tint i;",
            "\tint j;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tBUILD_BUG_ON(RCU_NUM_LVLS > ARRAY_SIZE(buf));  /* Fix buf[] init! */",
            "",
            "\t/* Silence gcc 4.8 false positive about array index out of range. */",
            "\tif (rcu_num_lvls <= 0 || rcu_num_lvls > RCU_NUM_LVLS)",
            "\t\tpanic(\"rcu_init_one: rcu_num_lvls out of range\");",
            "",
            "\t/* Initialize the level-tracking arrays. */",
            "",
            "\tfor (i = 1; i < rcu_num_lvls; i++)",
            "\t\trcu_state.level[i] =",
            "\t\t\trcu_state.level[i - 1] + num_rcu_lvl[i - 1];",
            "\trcu_init_levelspread(levelspread, num_rcu_lvl);",
            "",
            "\t/* Initialize the elements themselves, starting from the leaves. */",
            "",
            "\tfor (i = rcu_num_lvls - 1; i >= 0; i--) {",
            "\t\tcpustride *= levelspread[i];",
            "\t\trnp = rcu_state.level[i];",
            "\t\tfor (j = 0; j < num_rcu_lvl[i]; j++, rnp++) {",
            "\t\t\traw_spin_lock_init(&ACCESS_PRIVATE(rnp, lock));",
            "\t\t\tlockdep_set_class_and_name(&ACCESS_PRIVATE(rnp, lock),",
            "\t\t\t\t\t\t   &rcu_node_class[i], buf[i]);",
            "\t\t\traw_spin_lock_init(&rnp->fqslock);",
            "\t\t\tlockdep_set_class_and_name(&rnp->fqslock,",
            "\t\t\t\t\t\t   &rcu_fqs_class[i], fqs[i]);",
            "\t\t\trnp->gp_seq = rcu_state.gp_seq;",
            "\t\t\trnp->gp_seq_needed = rcu_state.gp_seq;",
            "\t\t\trnp->completedqs = rcu_state.gp_seq;",
            "\t\t\trnp->qsmask = 0;",
            "\t\t\trnp->qsmaskinit = 0;",
            "\t\t\trnp->grplo = j * cpustride;",
            "\t\t\trnp->grphi = (j + 1) * cpustride - 1;",
            "\t\t\tif (rnp->grphi >= nr_cpu_ids)",
            "\t\t\t\trnp->grphi = nr_cpu_ids - 1;",
            "\t\t\tif (i == 0) {",
            "\t\t\t\trnp->grpnum = 0;",
            "\t\t\t\trnp->grpmask = 0;",
            "\t\t\t\trnp->parent = NULL;",
            "\t\t\t} else {",
            "\t\t\t\trnp->grpnum = j % levelspread[i - 1];",
            "\t\t\t\trnp->grpmask = BIT(rnp->grpnum);",
            "\t\t\t\trnp->parent = rcu_state.level[i - 1] +",
            "\t\t\t\t\t      j / levelspread[i - 1];",
            "\t\t\t}",
            "\t\t\trnp->level = i;",
            "\t\t\tINIT_LIST_HEAD(&rnp->blkd_tasks);",
            "\t\t\trcu_init_one_nocb(rnp);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[0]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[1]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[2]);",
            "\t\t\tinit_waitqueue_head(&rnp->exp_wq[3]);",
            "\t\t\tspin_lock_init(&rnp->exp_lock);",
            "\t\t\tmutex_init(&rnp->boost_kthread_mutex);",
            "\t\t\traw_spin_lock_init(&rnp->exp_poll_lock);",
            "\t\t\trnp->exp_seq_poll_rq = RCU_GET_STATE_COMPLETED;",
            "\t\t\tINIT_WORK(&rnp->exp_poll_wq, sync_rcu_do_polled_gp);",
            "\t\t}",
            "\t}",
            "",
            "\tinit_swait_queue_head(&rcu_state.gp_wq);",
            "\tinit_swait_queue_head(&rcu_state.expedited_wq);",
            "\trnp = rcu_first_leaf_node();",
            "\tfor_each_possible_cpu(i) {",
            "\t\twhile (i > rnp->grphi)",
            "\t\t\trnp++;",
            "\t\tper_cpu_ptr(&rcu_data, i)->mynode = rnp;",
            "\t\trcu_boot_init_percpu_data(i);",
            "\t}",
            "}",
            "static void __init sanitize_kthread_prio(void)",
            "{",
            "\tint kthread_prio_in = kthread_prio;",
            "",
            "\tif (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 2",
            "\t    && IS_BUILTIN(CONFIG_RCU_TORTURE_TEST))",
            "\t\tkthread_prio = 2;",
            "\telse if (IS_ENABLED(CONFIG_RCU_BOOST) && kthread_prio < 1)",
            "\t\tkthread_prio = 1;",
            "\telse if (kthread_prio < 0)",
            "\t\tkthread_prio = 0;",
            "\telse if (kthread_prio > 99)",
            "\t\tkthread_prio = 99;",
            "",
            "\tif (kthread_prio != kthread_prio_in)",
            "\t\tpr_alert(\"%s: Limited prio to %d from %d\\n\",",
            "\t\t\t __func__, kthread_prio, kthread_prio_in);",
            "}",
            "void rcu_init_geometry(void)",
            "{",
            "\tulong d;",
            "\tint i;",
            "\tstatic unsigned long old_nr_cpu_ids;",
            "\tint rcu_capacity[RCU_NUM_LVLS];",
            "\tstatic bool initialized;",
            "",
            "\tif (initialized) {",
            "\t\t/*",
            "\t\t * Warn if setup_nr_cpu_ids() had not yet been invoked,",
            "\t\t * unless nr_cpus_ids == NR_CPUS, in which case who cares?",
            "\t\t */",
            "\t\tWARN_ON_ONCE(old_nr_cpu_ids != nr_cpu_ids);",
            "\t\treturn;",
            "\t}",
            "",
            "\told_nr_cpu_ids = nr_cpu_ids;",
            "\tinitialized = true;",
            "",
            "\t/*",
            "\t * Initialize any unspecified boot parameters.",
            "\t * The default values of jiffies_till_first_fqs and",
            "\t * jiffies_till_next_fqs are set to the RCU_JIFFIES_TILL_FORCE_QS",
            "\t * value, which is a function of HZ, then adding one for each",
            "\t * RCU_JIFFIES_FQS_DIV CPUs that might be on the system.",
            "\t */",
            "\td = RCU_JIFFIES_TILL_FORCE_QS + nr_cpu_ids / RCU_JIFFIES_FQS_DIV;",
            "\tif (jiffies_till_first_fqs == ULONG_MAX)",
            "\t\tjiffies_till_first_fqs = d;",
            "\tif (jiffies_till_next_fqs == ULONG_MAX)",
            "\t\tjiffies_till_next_fqs = d;",
            "\tadjust_jiffies_till_sched_qs();",
            "",
            "\t/* If the compile-time values are accurate, just leave. */",
            "\tif (rcu_fanout_leaf == RCU_FANOUT_LEAF &&",
            "\t    nr_cpu_ids == NR_CPUS)",
            "\t\treturn;",
            "\tpr_info(\"Adjusting geometry for rcu_fanout_leaf=%d, nr_cpu_ids=%u\\n\",",
            "\t\trcu_fanout_leaf, nr_cpu_ids);",
            "",
            "\t/*",
            "\t * The boot-time rcu_fanout_leaf parameter must be at least two",
            "\t * and cannot exceed the number of bits in the rcu_node masks.",
            "\t * Complain and fall back to the compile-time values if this",
            "\t * limit is exceeded.",
            "\t */",
            "\tif (rcu_fanout_leaf < 2 ||",
            "\t    rcu_fanout_leaf > sizeof(unsigned long) * 8) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Compute number of nodes that can be handled an rcu_node tree",
            "\t * with the given number of levels.",
            "\t */",
            "\trcu_capacity[0] = rcu_fanout_leaf;",
            "\tfor (i = 1; i < RCU_NUM_LVLS; i++)",
            "\t\trcu_capacity[i] = rcu_capacity[i - 1] * RCU_FANOUT;",
            "",
            "\t/*",
            "\t * The tree must be able to accommodate the configured number of CPUs.",
            "\t * If this limit is exceeded, fall back to the compile-time values.",
            "\t */",
            "\tif (nr_cpu_ids > rcu_capacity[RCU_NUM_LVLS - 1]) {",
            "\t\trcu_fanout_leaf = RCU_FANOUT_LEAF;",
            "\t\tWARN_ON(1);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Calculate the number of levels in the tree. */",
            "\tfor (i = 0; nr_cpu_ids > rcu_capacity[i]; i++) {",
            "\t}",
            "\trcu_num_lvls = i + 1;",
            "",
            "\t/* Calculate the number of rcu_nodes at each level of the tree. */",
            "\tfor (i = 0; i < rcu_num_lvls; i++) {",
            "\t\tint cap = rcu_capacity[(rcu_num_lvls - 1) - i];",
            "\t\tnum_rcu_lvl[i] = DIV_ROUND_UP(nr_cpu_ids, cap);",
            "\t}",
            "",
            "\t/* Calculate the total number of rcu_node structures. */",
            "\trcu_num_nodes = 0;",
            "\tfor (i = 0; i < rcu_num_lvls; i++)",
            "\t\trcu_num_nodes += num_rcu_lvl[i];",
            "}"
          ],
          "function_name": "rcu_init_one, sanitize_kthread_prio, rcu_init_geometry",
          "description": "构建多级RCU节点树结构，初始化各层级的锁类和节点属性，动态调整RCU树的几何形态以适配当前CPU数量和层级分布需求。",
          "similarity": 0.5807995796203613
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.5788109302520752
        }
      ]
    }
  ]
}