{
  "query": "virtual to physical address conversion",
  "timestamp": "2025-12-26 00:13:00",
  "retrieved_files": [
    {
      "source_file": "mm/ioremap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:11:43\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `ioremap.c`\n\n---\n\n# ioremap.c 技术文档\n\n## 1. 文件概述\n\n`ioremap.c` 是 Linux 内核中用于将物理 I/O 内存区域重新映射到内核虚拟地址空间的核心实现文件。该机制使得内核能够安全、高效地访问设备寄存器或高地址 PCI I/O 空间（这些区域通常未被直接映射到内核的低地址段，如 PC 架构中的 640KB–1MB 区域之外）。通过 `ioremap` 系列函数，驱动程序可以获取可直接读写的内核虚拟地址，从而操作硬件设备。\n\n## 2. 核心功能\n\n### 主要函数：\n\n- **`generic_ioremap_prot(phys_addr_t phys_addr, size_t size, pgprot_t prot)`**  \n  通用 I/O 内存重映射函数，根据指定的物理地址、大小和页保护属性创建内核虚拟映射。\n\n- **`ioremap_prot(phys_addr_t phys_addr, size_t size, unsigned long prot)`**  \n  对外导出的接口函数，封装 `generic_ioremap_prot`，接受原始的 `prot` 值并转换为 `pgprot_t` 类型。\n\n- **`generic_iounmap(volatile void __iomem *addr)`**  \n  通用 I/O 映射解除函数，释放由 `ioremap` 创建的虚拟地址映射。\n\n- **`iounmap(volatile void __iomem *addr)`**  \n  对外导出的接口函数，调用 `generic_iounmap` 完成实际的解映射操作。\n\n### 关键数据结构：\n\n- **`struct vm_struct`**：用于描述内核虚拟内存区域（vmalloc 区域）的结构体，记录映射的虚拟地址、物理地址、大小及标志等信息。\n- **`pgprot_t`**：页表项保护属性类型，用于控制映射页面的访问权限（如不可缓存、设备内存等）。\n\n## 3. 关键实现\n\n### 地址对齐与边界检查\n- 函数首先校验输入参数：拒绝零长度或地址溢出（wrap-around）的情况。\n- 将物理地址和映射大小按页对齐：提取原始地址在页内的偏移量 `offset`，将 `phys_addr` 下调至页边界，并将 `size` 扩展为包含偏移后的页对齐值。\n\n### 虚拟地址分配\n- 使用 `__get_vm_area_caller()` 在 `IOREMAP_START` 到 `IOREMAP_END` 的专用内核虚拟地址区间中分配一个 `vm_struct` 描述符。\n- 若分配失败，返回 `NULL`。\n\n### 页表建立\n- 调用 `ioremap_page_range()` 建立从分配的虚拟地址 `vaddr` 到对齐后物理地址 `phys_addr` 的页表映射，使用传入的保护属性 `prot`。\n- 若页表建立失败，则通过 `free_vm_area()` 释放已分配的虚拟区域并返回 `NULL`。\n\n### 返回用户可见地址\n- 最终返回的地址为 `vaddr + offset`，即保留原始物理地址的页内偏移，使调用者能精确访问目标 I/O 地址。\n\n### 解映射流程\n- `generic_iounmap()` 首先将传入地址对齐到页边界。\n- 通过 `is_ioremap_addr()` 验证该地址是否属于 ioremap 区域。\n- 若是，则调用 `vunmap()` 释放整个虚拟映射区域。\n\n### 条件编译支持\n- 使用 `#ifndef ioremap_prot` 和 `#ifndef iounmap` 确保在架构未提供自定义实现时，使用本文件提供的通用版本。\n- 通过 `EXPORT_SYMBOL` 导出符号，供内核模块使用。\n\n## 4. 依赖关系\n\n- **`<linux/vmalloc.h>`**：提供 `__get_vm_area_caller()`、`free_vm_area()`、`vunmap()` 等 vmalloc 子系统接口。\n- **`<linux/mm.h>`**：提供内存管理基础定义，如 `PAGE_MASK`、`PAGE_ALIGN`。\n- **`<linux/io.h>`**：定义 `__iomem` 注解及 I/O 访问相关宏。\n- **`<linux/ioremap.h>`**：声明 `ioremap` 相关接口和辅助函数（如 `is_ioremap_addr`）。\n- **`<linux/export.h>`**：提供 `EXPORT_SYMBOL` 宏，用于导出符号给模块使用。\n- 依赖内核 slab 分配器（通过 `slab_is_available()` 检查），确保在早期启动阶段不会因内存子系统未就绪而崩溃。\n\n## 5. 使用场景\n\n- **设备驱动开发**：驱动程序在初始化时调用 `ioremap()` 将设备寄存器的物理地址映射为内核可访问的虚拟地址，后续通过 `readl()`/`writel()` 等 I/O 访问函数操作硬件。\n- **PCI/平台设备资源访问**：当设备 BAR（Base Address Register）指向高物理地址（超出直接映射区）时，必须通过 ioremap 机制访问。\n- **ACPI/固件交互**：访问 ACPI 表或 UEFI 运行时服务所使用的物理内存区域。\n- **体系结构抽象层**：作为通用实现，被未提供特定优化版本的架构（如某些嵌入式平台）所采用。\n- **内核调试与诊断工具**：如 `/dev/mem` 的实现可能间接依赖此机制访问任意物理内存（需配置支持）。",
      "similarity": 0.5177111625671387,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/ioremap.c",
          "start_line": 1,
          "end_line": 59,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Re-map IO memory to kernel address space so that we can access it.",
            " * This is needed for high PCI addresses that aren't mapped in the",
            " * 640k-1MB IO memory area on PC's",
            " *",
            " * (C) Copyright 1995 1996 Linus Torvalds",
            " */",
            "#include <linux/vmalloc.h>",
            "#include <linux/mm.h>",
            "#include <linux/io.h>",
            "#include <linux/export.h>",
            "#include <linux/ioremap.h>",
            "",
            "void __iomem *generic_ioremap_prot(phys_addr_t phys_addr, size_t size,",
            "\t\t\t\t   pgprot_t prot)",
            "{",
            "\tunsigned long offset, vaddr;",
            "\tphys_addr_t last_addr;",
            "\tstruct vm_struct *area;",
            "",
            "\t/* An early platform driver might end up here */",
            "\tif (WARN_ON_ONCE(!slab_is_available()))",
            "\t\treturn NULL;",
            "",
            "\t/* Disallow wrap-around or zero size */",
            "\tlast_addr = phys_addr + size - 1;",
            "\tif (!size || last_addr < phys_addr)",
            "\t\treturn NULL;",
            "",
            "\t/* Page-align mappings */",
            "\toffset = phys_addr & (~PAGE_MASK);",
            "\tphys_addr -= offset;",
            "\tsize = PAGE_ALIGN(size + offset);",
            "",
            "\tarea = __get_vm_area_caller(size, VM_IOREMAP, IOREMAP_START,",
            "\t\t\t\t    IOREMAP_END, __builtin_return_address(0));",
            "\tif (!area)",
            "\t\treturn NULL;",
            "\tvaddr = (unsigned long)area->addr;",
            "\tarea->phys_addr = phys_addr;",
            "",
            "\tif (ioremap_page_range(vaddr, vaddr + size, phys_addr, prot)) {",
            "\t\tfree_vm_area(area);",
            "\t\treturn NULL;",
            "\t}",
            "",
            "\treturn (void __iomem *)(vaddr + offset);",
            "}",
            "",
            "#ifndef ioremap_prot",
            "void __iomem *ioremap_prot(phys_addr_t phys_addr, size_t size,",
            "\t\t\t   unsigned long prot)",
            "{",
            "\treturn generic_ioremap_prot(phys_addr, size, __pgprot(prot));",
            "}",
            "EXPORT_SYMBOL(ioremap_prot);",
            "#endif",
            ""
          ],
          "function_name": null,
          "description": "generic_ioremap_prot 函数实现通用I/O内存映射，接收物理地址、大小和保护属性参数，通过页面对齐计算偏移量，使用__get_vm_area分配虚拟内存区域，并调用ioremap_page_range建立映射，最终返回指向映射后虚拟地址的指针",
          "similarity": 0.5267105102539062
        },
        {
          "chunk_id": 1,
          "file_path": "mm/ioremap.c",
          "start_line": 60,
          "end_line": 70,
          "content": [
            "void generic_iounmap(volatile void __iomem *addr)",
            "{",
            "\tvoid *vaddr = (void *)((unsigned long)addr & PAGE_MASK);",
            "",
            "\tif (is_ioremap_addr(vaddr))",
            "\t\tvunmap(vaddr);",
            "}",
            "void iounmap(volatile void __iomem *addr)",
            "{",
            "\tgeneric_iounmap(addr);",
            "}"
          ],
          "function_name": "generic_iounmap, iounmap",
          "description": "generic_iounmap 函数用于解除I/O内存映射，通过判断地址是否属于ioremap区域，若是则调用vunmap释放对应的虚拟内存区域，iounmap函数作为包装接口调用generic_iounmap执行卸载操作",
          "similarity": 0.44004952907562256
        }
      ]
    },
    {
      "source_file": "kernel/dma/remap.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:16:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `dma\\remap.c`\n\n---\n\n# `dma/remap.c` 技术文档\n\n## 1. 文件概述\n\n`dma/remap.c` 是 Linux 内核中用于 DMA（Direct Memory Access）一致性内存管理的辅助实现文件。该文件提供了一组通用函数，用于将物理页面（`struct page`）重新映射到内核虚拟地址空间中，并标记为 DMA 一致性映射区域（`VM_DMA_COHERENT`）。这些函数主要用于支持架构无关的 DMA 映射操作，特别是在需要将非连续物理页或连续物理内存块映射为连续虚拟地址的场景中。\n\n## 2. 核心功能\n\n### 主要函数：\n\n- **`dma_common_find_pages(void *cpu_addr)`**  \n  根据给定的内核虚拟地址 `cpu_addr`，查找其对应的 `struct page` 数组。该地址必须是由 `dma_common_*_remap` 系列函数创建的、标记为 `VM_DMA_COHERENT` 的 vmalloc 区域。\n\n- **`dma_common_pages_remap(struct page **pages, size_t size, pgprot_t prot, const void *caller)`**  \n  将一组非连续的物理页面（由 `pages` 数组指定）重新映射为一个连续的内核虚拟地址区域，并标记为 `VM_DMA_COHERENT`。该函数不可在原子上下文（如中断处理程序）中调用。\n\n- **`dma_common_contiguous_remap(struct page *page, size_t size, pgprot_t prot, const void *caller)`**  \n  将一段物理上连续的内存区域（起始于 `page`，长度为 `size`）重新映射为连续的内核虚拟地址，并标记为 `VM_DMA_COHERENT`。内部会临时分配一个 `struct page *` 数组来描述每一页。\n\n- **`dma_common_free_remap(void *cpu_addr, size_t size)`**  \n  释放由上述 `remap` 函数创建的虚拟映射区域。会验证该区域是否为有效的 `VM_DMA_COHERENT` 类型，若无效则触发警告。\n\n### 数据结构：\n- 无显式定义新数据结构，但依赖于内核已有的：\n  - `struct page`\n  - `struct vm_struct`\n  - `pgprot_t`\n\n## 3. 关键实现\n\n- **VM 区域标识**：所有通过 `dma_common_*_remap` 创建的映射区域均使用 `VM_DMA_COHERENT` 标志，以便后续可通过 `find_vm_area()` 识别其为 DMA 一致性映射区域。\n  \n- **页面数组管理**：\n  - `dma_common_pages_remap` 直接使用传入的 `pages` 数组，并在成功 `vmap` 后将其保存到 `vm_struct->pages` 字段中，供 `dma_common_find_pages` 查询。\n  - `dma_common_contiguous_remap` 针对连续物理内存，动态构建 `pages` 数组（使用 `kvmalloc_array`），调用 `vmap` 后立即释放该临时数组，但 `vmap` 内部会复制页面指针。\n\n- **内存分配与映射**：\n  - 使用 `vmap()` 将物理页面映射到 vmalloc 区域，确保返回的虚拟地址在内核空间连续。\n  - 使用 `kvmalloc_array`/`kvfree` 进行临时内存分配，兼顾大内存分配的可靠性（可回退到 vmalloc）。\n\n- **错误处理与调试**：\n  - `dma_common_free_remap` 中包含 `WARN(1, ...)`，用于检测非法释放操作，提升调试能力。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/dma-map-ops.h>`：提供 DMA 映射操作相关的类型和接口。\n  - `<linux/slab.h>`：提供 `kvmalloc_array`/`kvfree` 等内存分配接口。\n  - `<linux/vmalloc.h>`：提供 `vmap`、`vunmap`、`find_vm_area` 等 vmalloc 区域管理函数。\n\n- **内核子系统依赖**：\n  - **VMALLOC 子系统**：依赖 `vmap`/`vunmap` 实现虚拟地址映射。\n  - **内存管理子系统（MM）**：依赖 `struct page` 和页面操作函数（如 `nth_page`）。\n  - **DMA 子系统**：作为 `dma_map_ops` 的底层支持，被架构特定的 DMA 实现（如 ARM、ARM64）调用。\n\n## 5. 使用场景\n\n- **DMA 一致性内存分配**：当设备驱动需要分配大块 DMA 一致性内存，且底层无法直接提供连续虚拟地址时，可通过此模块将物理页重新映射为连续虚拟地址。\n  \n- **IOMMU 或非一致性缓存架构支持**：在缓存不一致的系统（如某些 ARM 平台）上，为保证 CPU 与设备对内存视图一致，需使用特殊页表属性（`pgprot_t`）进行映射，本模块提供通用封装。\n\n- **通用 DMA 映射框架后端**：作为 `dma_map_ops` 中 `alloc`/`free` 等操作的辅助实现，被 `dma-direct.c`、`arm_dma_alloc.c` 等架构相关代码调用。\n\n- **调试与验证**：通过 `VM_DMA_COHERENT` 标志和 `WARN` 机制，帮助检测非法的 DMA 内存释放操作，提升系统稳定性。",
      "similarity": 0.4974179267883301,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/dma/remap.c",
          "start_line": 60,
          "end_line": 70,
          "content": [
            "void dma_common_free_remap(void *cpu_addr, size_t size)",
            "{",
            "\tstruct vm_struct *area = find_vm_area(cpu_addr);",
            "",
            "\tif (!area || area->flags != VM_DMA_COHERENT) {",
            "\t\tWARN(1, \"trying to free invalid coherent area: %p\\n\", cpu_addr);",
            "\t\treturn;",
            "\t}",
            "",
            "\tvunmap(cpu_addr);",
            "}"
          ],
          "function_name": "dma_common_free_remap",
          "description": "实现dma_common_free_remap函数，验证虚拟地址所属的vm_area结构体标志后，调用vunmap释放对应DMA一致性区域的映射",
          "similarity": 0.5741220712661743
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/dma/remap.c",
          "start_line": 1,
          "end_line": 59,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (c) 2014 The Linux Foundation",
            " */",
            "#include <linux/dma-map-ops.h>",
            "#include <linux/slab.h>",
            "#include <linux/vmalloc.h>",
            "",
            "struct page **dma_common_find_pages(void *cpu_addr)",
            "{",
            "\tstruct vm_struct *area = find_vm_area(cpu_addr);",
            "",
            "\tif (!area || area->flags != VM_DMA_COHERENT)",
            "\t\treturn NULL;",
            "\treturn area->pages;",
            "}",
            "",
            "/*",
            " * Remaps an array of PAGE_SIZE pages into another vm_area.",
            " * Cannot be used in non-sleeping contexts",
            " */",
            "void *dma_common_pages_remap(struct page **pages, size_t size,",
            "\t\t\t pgprot_t prot, const void *caller)",
            "{",
            "\tvoid *vaddr;",
            "",
            "\tvaddr = vmap(pages, PAGE_ALIGN(size) >> PAGE_SHIFT,",
            "\t\t     VM_DMA_COHERENT, prot);",
            "\tif (vaddr)",
            "\t\tfind_vm_area(vaddr)->pages = pages;",
            "\treturn vaddr;",
            "}",
            "",
            "/*",
            " * Remaps an allocated contiguous region into another vm_area.",
            " * Cannot be used in non-sleeping contexts",
            " */",
            "void *dma_common_contiguous_remap(struct page *page, size_t size,",
            "\t\t\tpgprot_t prot, const void *caller)",
            "{",
            "\tint count = PAGE_ALIGN(size) >> PAGE_SHIFT;",
            "\tstruct page **pages;",
            "\tvoid *vaddr;",
            "\tint i;",
            "",
            "\tpages = kvmalloc_array(count, sizeof(struct page *), GFP_KERNEL);",
            "\tif (!pages)",
            "\t\treturn NULL;",
            "\tfor (i = 0; i < count; i++)",
            "\t\tpages[i] = nth_page(page, i);",
            "\tvaddr = vmap(pages, count, VM_DMA_COHERENT, prot);",
            "\tkvfree(pages);",
            "",
            "\treturn vaddr;",
            "}",
            "",
            "/*",
            " * Unmaps a range previously mapped by dma_common_*_remap",
            " */"
          ],
          "function_name": null,
          "description": "定义dma_common_find_pages函数，通过查找VM_DMA_COHERENT标记的vm_area结构体，返回对应页面数组指针",
          "similarity": 0.4073973596096039
        }
      ]
    },
    {
      "source_file": "mm/vmalloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:32:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `vmalloc.c`\n\n---\n\n# vmalloc.c 技术文档\n\n## 1. 文件概述\n\n`vmalloc.c` 是 Linux 内核中实现虚拟内存分配（vmalloc）机制的核心源文件。该文件提供了在内核虚拟地址空间中非连续物理页映射为连续虚拟地址的功能，主要用于分配大块内存、I/O 映射（如 `ioremap`）以及需要页表特殊属性（如不可执行、缓存控制等）的场景。与 `kmalloc` 不同，`vmalloc` 分配的内存物理上不连续，但虚拟地址连续，适用于大内存分配或硬件寄存器映射。\n\n## 2. 核心功能\n\n### 主要函数\n- `is_vmalloc_addr(const void *x)`：判断给定地址是否位于 vmalloc 区域。\n- `vmap_page_range(unsigned long addr, unsigned long end, phys_addr_t phys_addr, pgprot_t prot)`：将指定物理地址范围映射到内核虚拟地址空间，支持普通页和大页。\n- `ioremap_page_range(...)`：用于 I/O 内存重映射（代码片段未完整展示）。\n- `vmap_range_noflush(...)`：执行实际的页表填充操作，不触发 TLB 刷新。\n- `vmap_pte_range`, `vmap_pmd_range`, `vmap_pud_range`, `vmap_p4d_range`：逐级填充页表项的辅助函数。\n- `vmap_try_huge_*` 系列函数（如 `vmap_try_huge_pmd`）：尝试使用大页（huge page）进行映射以提升性能。\n\n### 主要数据结构\n- `struct vfree_deferred`：用于延迟释放 vmalloc 内存的 per-CPU 工作队列结构。\n- `ioremap_max_page_shift`：控制 I/O 映射时允许的最大页面大小（受 `nohugeiomap` 启动参数影响）。\n- `vmap_allow_huge`：控制 vmalloc 是否允许使用大页（受 `nohugevmalloc` 启动参数影响）。\n\n## 3. 关键实现\n\n### 大页（Huge Page）支持\n- 通过 `CONFIG_HAVE_ARCH_HUGE_VMAP` 和 `CONFIG_HAVE_ARCH_HUGE_VMALLOC` 配置选项启用架构相关的大页映射能力。\n- 在页表填充过程中（如 `vmap_pmd_range`），优先尝试使用 PMD/PUD/P4D 级别的大页映射（通过 `vmap_try_huge_pmd` 等函数），前提是：\n  - 地址和物理地址对齐；\n  - 请求区域大小等于对应层级的大页尺寸；\n  - 架构支持该级别的大页（通过 `arch_vmap_*_supported` 判断）；\n  - 当前页表项未被占用或可安全释放下级页表。\n- 启动参数 `nohugeiomap` 和 `nohugevmalloc` 可分别禁用 I/O 映射和通用 vmalloc 的大页功能。\n\n### 页表操作与跟踪\n- 使用 `_track` 后缀的页表分配函数（如 `pte_alloc_kernel_track`）配合 `pgtbl_mod_mask` 标记修改的页表层级，便于后续同步（如 `arch_sync_kernel_mappings`）。\n- 映射完成后调用 `flush_cache_vmap` 确保缓存一致性，并集成 KMSAN（Kernel Memory Sanitizer）支持。\n\n### 安全与调试\n- 使用 `kasan_reset_tag` 处理 KASAN 的内存标记，确保地址比较正确。\n- 通过 `BUG_ON` 检查页表项是否为空，防止覆盖已有映射。\n- 支持 `kmemleak` 内存泄漏检测。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/highmem.h>`、`<linux/pfn.h>` 等提供基础内存操作。\n- **体系结构相关接口**：通过 `asm/tlbflush.h`、`asm/shmparam.h` 及 `arch_*` 函数（如 `arch_make_huge_pte`）适配不同 CPU 架构。\n- **内核基础设施**：\n  - RCU（`<linux/rcupdate.h>`）用于安全遍历；\n  - Per-CPU 变量（`DEFINE_PER_CPU`）实现无锁延迟释放；\n  - 工作队列（`work_struct`）处理异步释放；\n  - 调试工具（`debugobjects`、`kallsyms`、`trace/events/vmalloc.h`）。\n- **安全特性**：集成 KASAN、KMSAN、`set_memory.h`（页属性设置）等。\n\n## 5. 使用场景\n\n- **内核模块加载**：模块的代码和数据通常通过 `vmalloc` 分配。\n- **大内存分配**：当所需内存超过 `kmalloc` 的限制（通常几 MB）时使用。\n- **设备 I/O 映射**：通过 `ioremap` 将设备寄存器映射到内核地址空间，底层调用 `ioremap_page_range`。\n- **动态内核数据结构**：如网络协议栈的某些缓冲区、文件系统元数据缓存等。\n- **安全隔离**：为敏感数据分配具有特殊页属性（如不可执行 NX）的内存区域。",
      "similarity": 0.49056148529052734,
      "chunks": [
        {
          "chunk_id": 16,
          "file_path": "mm/vmalloc.c",
          "start_line": 3402,
          "end_line": 3534,
          "content": [
            "void vunmap(const void *addr)",
            "{",
            "\tstruct vm_struct *vm;",
            "",
            "\tBUG_ON(in_interrupt());",
            "\tmight_sleep();",
            "",
            "\tif (!addr)",
            "\t\treturn;",
            "\tvm = remove_vm_area(addr);",
            "\tif (unlikely(!vm)) {",
            "\t\tWARN(1, KERN_ERR \"Trying to vunmap() nonexistent vm area (%p)\\n\",",
            "\t\t\t\taddr);",
            "\t\treturn;",
            "\t}",
            "\tkfree(vm);",
            "}",
            "static int vmap_pfn_apply(pte_t *pte, unsigned long addr, void *private)",
            "{",
            "\tstruct vmap_pfn_data *data = private;",
            "\tunsigned long pfn = data->pfns[data->idx];",
            "\tpte_t ptent;",
            "",
            "\tif (WARN_ON_ONCE(pfn_valid(pfn)))",
            "\t\treturn -EINVAL;",
            "",
            "\tptent = pte_mkspecial(pfn_pte(pfn, data->prot));",
            "\tset_pte_at(&init_mm, addr, pte, ptent);",
            "",
            "\tdata->idx++;",
            "\treturn 0;",
            "}",
            "static inline unsigned int",
            "vm_area_alloc_pages(gfp_t gfp, int nid,",
            "\t\tunsigned int order, unsigned int nr_pages, struct page **pages)",
            "{",
            "\tunsigned int nr_allocated = 0;",
            "\tgfp_t alloc_gfp = gfp;",
            "\tbool nofail = gfp & __GFP_NOFAIL;",
            "\tstruct page *page;",
            "\tint i;",
            "",
            "\t/*",
            "\t * For order-0 pages we make use of bulk allocator, if",
            "\t * the page array is partly or not at all populated due",
            "\t * to fails, fallback to a single page allocator that is",
            "\t * more permissive.",
            "\t */",
            "\tif (!order) {",
            "\t\t/* bulk allocator doesn't support nofail req. officially */",
            "\t\tgfp_t bulk_gfp = gfp & ~__GFP_NOFAIL;",
            "",
            "\t\twhile (nr_allocated < nr_pages) {",
            "\t\t\tunsigned int nr, nr_pages_request;",
            "",
            "\t\t\t/*",
            "\t\t\t * A maximum allowed request is hard-coded and is 100",
            "\t\t\t * pages per call. That is done in order to prevent a",
            "\t\t\t * long preemption off scenario in the bulk-allocator",
            "\t\t\t * so the range is [1:100].",
            "\t\t\t */",
            "\t\t\tnr_pages_request = min(100U, nr_pages - nr_allocated);",
            "",
            "\t\t\t/* memory allocation should consider mempolicy, we can't",
            "\t\t\t * wrongly use nearest node when nid == NUMA_NO_NODE,",
            "\t\t\t * otherwise memory may be allocated in only one node,",
            "\t\t\t * but mempolicy wants to alloc memory by interleaving.",
            "\t\t\t */",
            "\t\t\tif (IS_ENABLED(CONFIG_NUMA) && nid == NUMA_NO_NODE)",
            "\t\t\t\tnr = alloc_pages_bulk_array_mempolicy_noprof(bulk_gfp,",
            "\t\t\t\t\t\t\tnr_pages_request,",
            "\t\t\t\t\t\t\tpages + nr_allocated);",
            "",
            "\t\t\telse",
            "\t\t\t\tnr = alloc_pages_bulk_array_node_noprof(bulk_gfp, nid,",
            "\t\t\t\t\t\t\tnr_pages_request,",
            "\t\t\t\t\t\t\tpages + nr_allocated);",
            "",
            "\t\t\tnr_allocated += nr;",
            "\t\t\tcond_resched();",
            "",
            "\t\t\t/*",
            "\t\t\t * If zero or pages were obtained partly,",
            "\t\t\t * fallback to a single page allocator.",
            "\t\t\t */",
            "\t\t\tif (nr != nr_pages_request)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t} else if (gfp & __GFP_NOFAIL) {",
            "\t\t/*",
            "\t\t * Higher order nofail allocations are really expensive and",
            "\t\t * potentially dangerous (pre-mature OOM, disruptive reclaim",
            "\t\t * and compaction etc.",
            "\t\t */",
            "\t\talloc_gfp &= ~__GFP_NOFAIL;",
            "\t}",
            "",
            "\t/* High-order pages or fallback path if \"bulk\" fails. */",
            "\twhile (nr_allocated < nr_pages) {",
            "\t\tif (!nofail && fatal_signal_pending(current))",
            "\t\t\tbreak;",
            "",
            "\t\tif (nid == NUMA_NO_NODE)",
            "\t\t\tpage = alloc_pages_noprof(alloc_gfp, order);",
            "\t\telse",
            "\t\t\tpage = alloc_pages_node_noprof(nid, alloc_gfp, order);",
            "\t\tif (unlikely(!page))",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * Higher order allocations must be able to be treated as",
            "\t\t * indepdenent small pages by callers (as they can with",
            "\t\t * small-page vmallocs). Some drivers do their own refcounting",
            "\t\t * on vmalloc_to_page() pages, some use page->mapping,",
            "\t\t * page->lru, etc.",
            "\t\t */",
            "\t\tif (order)",
            "\t\t\tsplit_page(page, order);",
            "",
            "\t\t/*",
            "\t\t * Careful, we allocate and map page-order pages, but",
            "\t\t * tracking is done per PAGE_SIZE page so as to keep the",
            "\t\t * vm_struct APIs independent of the physical/mapped size.",
            "\t\t */",
            "\t\tfor (i = 0; i < (1U << order); i++)",
            "\t\t\tpages[nr_allocated + i] = page + i;",
            "",
            "\t\tcond_resched();",
            "\t\tnr_allocated += 1U << order;",
            "\t}",
            "",
            "\treturn nr_allocated;",
            "}"
          ],
          "function_name": "vunmap, vmap_pfn_apply, vm_area_alloc_pages",
          "description": "vunmap 解除指定虚拟地址的映射并释放相关结构；vmap_pfn_apply 将物理页框应用到指定地址的页表项；vm_area_alloc_pages 根据参数分配连续物理页面，支持NUMA节点选择和高阶页面拆分",
          "similarity": 0.5716986656188965
        },
        {
          "chunk_id": 6,
          "file_path": "mm/vmalloc.c",
          "start_line": 691,
          "end_line": 822,
          "content": [
            "int vm_area_map_pages(struct vm_struct *area, unsigned long start,",
            "\t\t      unsigned long end, struct page **pages)",
            "{",
            "\tint err;",
            "",
            "\terr = check_sparse_vm_area(area, start, end);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\treturn vmap_pages_range(start, end, PAGE_KERNEL, pages, PAGE_SHIFT);",
            "}",
            "void vm_area_unmap_pages(struct vm_struct *area, unsigned long start,",
            "\t\t\t unsigned long end)",
            "{",
            "\tif (check_sparse_vm_area(area, start, end))",
            "\t\treturn;",
            "",
            "\tvunmap_range(start, end);",
            "}",
            "int is_vmalloc_or_module_addr(const void *x)",
            "{",
            "\t/*",
            "\t * ARM, x86-64 and sparc64 put modules in a special place,",
            "\t * and fall back on vmalloc() if that fails. Others",
            "\t * just put it in the vmalloc space.",
            "\t */",
            "#if defined(CONFIG_MODULES) && defined(MODULES_VADDR)",
            "\tunsigned long addr = (unsigned long)kasan_reset_tag(x);",
            "\tif (addr >= MODULES_VADDR && addr < MODULES_END)",
            "\t\treturn 1;",
            "#endif",
            "\treturn is_vmalloc_addr(x);",
            "}",
            "unsigned long vmalloc_to_pfn(const void *vmalloc_addr)",
            "{",
            "\treturn page_to_pfn(vmalloc_to_page(vmalloc_addr));",
            "}",
            "static inline unsigned int",
            "addr_to_node_id(unsigned long addr)",
            "{",
            "\treturn (addr / vmap_zone_size) % nr_vmap_nodes;",
            "}",
            "static unsigned int",
            "encode_vn_id(unsigned int node_id)",
            "{",
            "\t/* Can store U8_MAX [0:254] nodes. */",
            "\tif (node_id < nr_vmap_nodes)",
            "\t\treturn (node_id + 1) << BITS_PER_BYTE;",
            "",
            "\t/* Warn and no node encoded. */",
            "\tWARN_ONCE(1, \"Encode wrong node id (%u)\\n\", node_id);",
            "\treturn 0;",
            "}",
            "static unsigned int",
            "decode_vn_id(unsigned int val)",
            "{",
            "\tunsigned int node_id = (val >> BITS_PER_BYTE) - 1;",
            "",
            "\t/* Can store U8_MAX [0:254] nodes. */",
            "\tif (node_id < nr_vmap_nodes)",
            "\t\treturn node_id;",
            "",
            "\t/* If it was _not_ zero, warn. */",
            "\tWARN_ONCE(node_id != UINT_MAX,",
            "\t\t\"Decode wrong node id (%d)\\n\", node_id);",
            "",
            "\treturn nr_vmap_nodes;",
            "}",
            "static bool",
            "is_vn_id_valid(unsigned int node_id)",
            "{",
            "\tif (node_id < nr_vmap_nodes)",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "static __always_inline unsigned long",
            "va_size(struct vmap_area *va)",
            "{",
            "\treturn (va->va_end - va->va_start);",
            "}",
            "static __always_inline unsigned long",
            "get_subtree_max_size(struct rb_node *node)",
            "{",
            "\tstruct vmap_area *va;",
            "",
            "\tva = rb_entry_safe(node, struct vmap_area, rb_node);",
            "\treturn va ? va->subtree_max_size : 0;",
            "}",
            "unsigned long vmalloc_nr_pages(void)",
            "{",
            "\treturn atomic_long_read(&nr_vmalloc_pages);",
            "}",
            "static __always_inline void",
            "__link_va(struct vmap_area *va, struct rb_root *root,",
            "\tstruct rb_node *parent, struct rb_node **link,",
            "\tstruct list_head *head, bool augment)",
            "{",
            "\t/*",
            "\t * VA is still not in the list, but we can",
            "\t * identify its future previous list_head node.",
            "\t */",
            "\tif (likely(parent)) {",
            "\t\thead = &rb_entry(parent, struct vmap_area, rb_node)->list;",
            "\t\tif (&parent->rb_right != link)",
            "\t\t\thead = head->prev;",
            "\t}",
            "",
            "\t/* Insert to the rb-tree */",
            "\trb_link_node(&va->rb_node, parent, link);",
            "\tif (augment) {",
            "\t\t/*",
            "\t\t * Some explanation here. Just perform simple insertion",
            "\t\t * to the tree. We do not set va->subtree_max_size to",
            "\t\t * its current size before calling rb_insert_augmented().",
            "\t\t * It is because we populate the tree from the bottom",
            "\t\t * to parent levels when the node _is_ in the tree.",
            "\t\t *",
            "\t\t * Therefore we set subtree_max_size to zero after insertion,",
            "\t\t * to let __augment_tree_propagate_from() puts everything to",
            "\t\t * the correct order later on.",
            "\t\t */",
            "\t\trb_insert_augmented(&va->rb_node,",
            "\t\t\troot, &free_vmap_area_rb_augment_cb);",
            "\t\tva->subtree_max_size = 0;",
            "\t} else {",
            "\t\trb_insert_color(&va->rb_node, root);",
            "\t}",
            "",
            "\t/* Address-sort this list */",
            "\tlist_add(&va->list, head);",
            "}"
          ],
          "function_name": "vm_area_map_pages, vm_area_unmap_pages, is_vmalloc_or_module_addr, vmalloc_to_pfn, addr_to_node_id, encode_vn_id, decode_vn_id, is_vn_id_valid, va_size, get_subtree_max_size, vmalloc_nr_pages, __link_va",
          "description": "提供虚拟内存区域管理接口，包含区域有效性校验、地址转换、节点ID编码解码及基于红黑树的VA链表操作，用于跟踪和管理VMALLOC空间",
          "similarity": 0.5705875158309937
        },
        {
          "chunk_id": 8,
          "file_path": "mm/vmalloc.c",
          "start_line": 1465,
          "end_line": 1624,
          "content": [
            "static __always_inline bool",
            "is_within_this_va(struct vmap_area *va, unsigned long size,",
            "\tunsigned long align, unsigned long vstart)",
            "{",
            "\tunsigned long nva_start_addr;",
            "",
            "\tif (va->va_start > vstart)",
            "\t\tnva_start_addr = ALIGN(va->va_start, align);",
            "\telse",
            "\t\tnva_start_addr = ALIGN(vstart, align);",
            "",
            "\t/* Can be overflowed due to big size or alignment. */",
            "\tif (nva_start_addr + size < nva_start_addr ||",
            "\t\t\tnva_start_addr < vstart)",
            "\t\treturn false;",
            "",
            "\treturn (nva_start_addr + size <= va->va_end);",
            "}",
            "static void",
            "find_vmap_lowest_match_check(struct rb_root *root, struct list_head *head,",
            "\t\t\t     unsigned long size, unsigned long align)",
            "{",
            "\tstruct vmap_area *va_1, *va_2;",
            "\tunsigned long vstart;",
            "\tunsigned int rnd;",
            "",
            "\tget_random_bytes(&rnd, sizeof(rnd));",
            "\tvstart = VMALLOC_START + rnd;",
            "",
            "\tva_1 = find_vmap_lowest_match(root, size, align, vstart, false);",
            "\tva_2 = find_vmap_lowest_linear_match(head, size, align, vstart);",
            "",
            "\tif (va_1 != va_2)",
            "\t\tpr_emerg(\"not lowest: t: 0x%p, l: 0x%p, v: 0x%lx\\n\",",
            "\t\t\tva_1, va_2, vstart);",
            "}",
            "static __always_inline enum fit_type",
            "classify_va_fit_type(struct vmap_area *va,",
            "\tunsigned long nva_start_addr, unsigned long size)",
            "{",
            "\tenum fit_type type;",
            "",
            "\t/* Check if it is within VA. */",
            "\tif (nva_start_addr < va->va_start ||",
            "\t\t\tnva_start_addr + size > va->va_end)",
            "\t\treturn NOTHING_FIT;",
            "",
            "\t/* Now classify. */",
            "\tif (va->va_start == nva_start_addr) {",
            "\t\tif (va->va_end == nva_start_addr + size)",
            "\t\t\ttype = FL_FIT_TYPE;",
            "\t\telse",
            "\t\t\ttype = LE_FIT_TYPE;",
            "\t} else if (va->va_end == nva_start_addr + size) {",
            "\t\ttype = RE_FIT_TYPE;",
            "\t} else {",
            "\t\ttype = NE_FIT_TYPE;",
            "\t}",
            "",
            "\treturn type;",
            "}",
            "static __always_inline int",
            "va_clip(struct rb_root *root, struct list_head *head,",
            "\t\tstruct vmap_area *va, unsigned long nva_start_addr,",
            "\t\tunsigned long size)",
            "{",
            "\tstruct vmap_area *lva = NULL;",
            "\tenum fit_type type = classify_va_fit_type(va, nva_start_addr, size);",
            "",
            "\tif (type == FL_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * No need to split VA, it fully fits.",
            "\t\t *",
            "\t\t * |               |",
            "\t\t * V      NVA      V",
            "\t\t * |---------------|",
            "\t\t */",
            "\t\tunlink_va_augment(va, root);",
            "\t\tkmem_cache_free(vmap_area_cachep, va);",
            "\t} else if (type == LE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split left edge of fit VA.",
            "\t\t *",
            "\t\t * |       |",
            "\t\t * V  NVA  V   R",
            "\t\t * |-------|-------|",
            "\t\t */",
            "\t\tva->va_start += size;",
            "\t} else if (type == RE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split right edge of fit VA.",
            "\t\t *",
            "\t\t *         |       |",
            "\t\t *     L   V  NVA  V",
            "\t\t * |-------|-------|",
            "\t\t */",
            "\t\tva->va_end = nva_start_addr;",
            "\t} else if (type == NE_FIT_TYPE) {",
            "\t\t/*",
            "\t\t * Split no edge of fit VA.",
            "\t\t *",
            "\t\t *     |       |",
            "\t\t *   L V  NVA  V R",
            "\t\t * |---|-------|---|",
            "\t\t */",
            "\t\tlva = __this_cpu_xchg(ne_fit_preload_node, NULL);",
            "\t\tif (unlikely(!lva)) {",
            "\t\t\t/*",
            "\t\t\t * For percpu allocator we do not do any pre-allocation",
            "\t\t\t * and leave it as it is. The reason is it most likely",
            "\t\t\t * never ends up with NE_FIT_TYPE splitting. In case of",
            "\t\t\t * percpu allocations offsets and sizes are aligned to",
            "\t\t\t * fixed align request, i.e. RE_FIT_TYPE and FL_FIT_TYPE",
            "\t\t\t * are its main fitting cases.",
            "\t\t\t *",
            "\t\t\t * There are a few exceptions though, as an example it is",
            "\t\t\t * a first allocation (early boot up) when we have \"one\"",
            "\t\t\t * big free space that has to be split.",
            "\t\t\t *",
            "\t\t\t * Also we can hit this path in case of regular \"vmap\"",
            "\t\t\t * allocations, if \"this\" current CPU was not preloaded.",
            "\t\t\t * See the comment in alloc_vmap_area() why. If so, then",
            "\t\t\t * GFP_NOWAIT is used instead to get an extra object for",
            "\t\t\t * split purpose. That is rare and most time does not",
            "\t\t\t * occur.",
            "\t\t\t *",
            "\t\t\t * What happens if an allocation gets failed. Basically,",
            "\t\t\t * an \"overflow\" path is triggered to purge lazily freed",
            "\t\t\t * areas to free some memory, then, the \"retry\" path is",
            "\t\t\t * triggered to repeat one more time. See more details",
            "\t\t\t * in alloc_vmap_area() function.",
            "\t\t\t */",
            "\t\t\tlva = kmem_cache_alloc(vmap_area_cachep, GFP_NOWAIT);",
            "\t\t\tif (!lva)",
            "\t\t\t\treturn -1;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Build the remainder.",
            "\t\t */",
            "\t\tlva->va_start = va->va_start;",
            "\t\tlva->va_end = nva_start_addr;",
            "",
            "\t\t/*",
            "\t\t * Shrink this VA to remaining size.",
            "\t\t */",
            "\t\tva->va_start = nva_start_addr + size;",
            "\t} else {",
            "\t\treturn -1;",
            "\t}",
            "",
            "\tif (type != FL_FIT_TYPE) {",
            "\t\taugment_tree_propagate_from(va);",
            "",
            "\t\tif (lva)\t/* type == NE_FIT_TYPE */",
            "\t\t\tinsert_vmap_area_augment(lva, &va->rb_node, root, head);",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "is_within_this_va, find_vmap_lowest_match_check, classify_va_fit_type, va_clip",
          "description": "实现虚拟内存区域匹配算法，通过分类判断（FL/LE/RE/NE）进行地址裁剪和区域分割，支持不同场景下的内存分配策略选择",
          "similarity": 0.5553832054138184
        },
        {
          "chunk_id": 18,
          "file_path": "mm/vmalloc.c",
          "start_line": 4366,
          "end_line": 4510,
          "content": [
            "long vread_iter(struct iov_iter *iter, const char *addr, size_t count)",
            "{",
            "\tstruct vmap_node *vn;",
            "\tstruct vmap_area *va;",
            "\tstruct vm_struct *vm;",
            "\tchar *vaddr;",
            "\tsize_t n, size, flags, remains;",
            "\tunsigned long next;",
            "",
            "\taddr = kasan_reset_tag(addr);",
            "",
            "\t/* Don't allow overflow */",
            "\tif ((unsigned long) addr + count < count)",
            "\t\tcount = -(unsigned long) addr;",
            "",
            "\tremains = count;",
            "",
            "\tvn = find_vmap_area_exceed_addr_lock((unsigned long) addr, &va);",
            "\tif (!vn)",
            "\t\tgoto finished_zero;",
            "",
            "\t/* no intersects with alive vmap_area */",
            "\tif ((unsigned long)addr + remains <= va->va_start)",
            "\t\tgoto finished_zero;",
            "",
            "\tdo {",
            "\t\tsize_t copied;",
            "",
            "\t\tif (remains == 0)",
            "\t\t\tgoto finished;",
            "",
            "\t\tvm = va->vm;",
            "\t\tflags = va->flags & VMAP_FLAGS_MASK;",
            "\t\t/*",
            "\t\t * VMAP_BLOCK indicates a sub-type of vm_map_ram area, need",
            "\t\t * be set together with VMAP_RAM.",
            "\t\t */",
            "\t\tWARN_ON(flags == VMAP_BLOCK);",
            "",
            "\t\tif (!vm && !flags)",
            "\t\t\tgoto next_va;",
            "",
            "\t\tif (vm && (vm->flags & VM_UNINITIALIZED))",
            "\t\t\tgoto next_va;",
            "",
            "\t\t/* Pair with smp_wmb() in clear_vm_uninitialized_flag() */",
            "\t\tsmp_rmb();",
            "",
            "\t\tvaddr = (char *) va->va_start;",
            "\t\tsize = vm ? get_vm_area_size(vm) : va_size(va);",
            "",
            "\t\tif (addr >= vaddr + size)",
            "\t\t\tgoto next_va;",
            "",
            "\t\tif (addr < vaddr) {",
            "\t\t\tsize_t to_zero = min_t(size_t, vaddr - addr, remains);",
            "\t\t\tsize_t zeroed = zero_iter(iter, to_zero);",
            "",
            "\t\t\taddr += zeroed;",
            "\t\t\tremains -= zeroed;",
            "",
            "\t\t\tif (remains == 0 || zeroed != to_zero)",
            "\t\t\t\tgoto finished;",
            "\t\t}",
            "",
            "\t\tn = vaddr + size - addr;",
            "\t\tif (n > remains)",
            "\t\t\tn = remains;",
            "",
            "\t\tif (flags & VMAP_RAM)",
            "\t\t\tcopied = vmap_ram_vread_iter(iter, addr, n, flags);",
            "\t\telse if (!(vm && (vm->flags & (VM_IOREMAP | VM_SPARSE))))",
            "\t\t\tcopied = aligned_vread_iter(iter, addr, n);",
            "\t\telse /* IOREMAP | SPARSE area is treated as memory hole */",
            "\t\t\tcopied = zero_iter(iter, n);",
            "",
            "\t\taddr += copied;",
            "\t\tremains -= copied;",
            "",
            "\t\tif (copied != n)",
            "\t\t\tgoto finished;",
            "",
            "\tnext_va:",
            "\t\tnext = va->va_end;",
            "\t\tspin_unlock(&vn->busy.lock);",
            "\t} while ((vn = find_vmap_area_exceed_addr_lock(next, &va)));",
            "",
            "finished_zero:",
            "\tif (vn)",
            "\t\tspin_unlock(&vn->busy.lock);",
            "",
            "\t/* zero-fill memory holes */",
            "\treturn count - remains + zero_iter(iter, remains);",
            "finished:",
            "\t/* Nothing remains, or We couldn't copy/zero everything. */",
            "\tif (vn)",
            "\t\tspin_unlock(&vn->busy.lock);",
            "",
            "\treturn count - remains;",
            "}",
            "int remap_vmalloc_range_partial(struct vm_area_struct *vma, unsigned long uaddr,",
            "\t\t\t\tvoid *kaddr, unsigned long pgoff,",
            "\t\t\t\tunsigned long size)",
            "{",
            "\tstruct vm_struct *area;",
            "\tunsigned long off;",
            "\tunsigned long end_index;",
            "",
            "\tif (check_shl_overflow(pgoff, PAGE_SHIFT, &off))",
            "\t\treturn -EINVAL;",
            "",
            "\tsize = PAGE_ALIGN(size);",
            "",
            "\tif (!PAGE_ALIGNED(uaddr) || !PAGE_ALIGNED(kaddr))",
            "\t\treturn -EINVAL;",
            "",
            "\tarea = find_vm_area(kaddr);",
            "\tif (!area)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (!(area->flags & (VM_USERMAP | VM_DMA_COHERENT)))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (check_add_overflow(size, off, &end_index) ||",
            "\t    end_index > get_vm_area_size(area))",
            "\t\treturn -EINVAL;",
            "\tkaddr += off;",
            "",
            "\tdo {",
            "\t\tstruct page *page = vmalloc_to_page(kaddr);",
            "\t\tint ret;",
            "",
            "\t\tret = vm_insert_page(vma, uaddr, page);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "",
            "\t\tuaddr += PAGE_SIZE;",
            "\t\tkaddr += PAGE_SIZE;",
            "\t\tsize -= PAGE_SIZE;",
            "\t} while (size > 0);",
            "",
            "\tvm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vread_iter, remap_vmalloc_range_partial",
          "description": "vread_iter 实现虚拟内存区域的数据读取，处理零填充、对齐读取和RAM区域特殊读取逻辑；remap_vmalloc_range_partial 部分重新映射虚拟内存区域，将内核空间页面插入用户虚拟地址空间",
          "similarity": 0.533545732498169
        },
        {
          "chunk_id": 5,
          "file_path": "mm/vmalloc.c",
          "start_line": 553,
          "end_line": 656,
          "content": [
            "static int vmap_pages_p4d_range(pgd_t *pgd, unsigned long addr,",
            "\t\tunsigned long end, pgprot_t prot, struct page **pages, int *nr,",
            "\t\tpgtbl_mod_mask *mask)",
            "{",
            "\tp4d_t *p4d;",
            "\tunsigned long next;",
            "",
            "\tp4d = p4d_alloc_track(&init_mm, pgd, addr, mask);",
            "\tif (!p4d)",
            "\t\treturn -ENOMEM;",
            "\tdo {",
            "\t\tnext = p4d_addr_end(addr, end);",
            "\t\tif (vmap_pages_pud_range(p4d, addr, next, prot, pages, nr, mask))",
            "\t\t\treturn -ENOMEM;",
            "\t} while (p4d++, addr = next, addr != end);",
            "\treturn 0;",
            "}",
            "static int vmap_small_pages_range_noflush(unsigned long addr, unsigned long end,",
            "\t\tpgprot_t prot, struct page **pages)",
            "{",
            "\tunsigned long start = addr;",
            "\tpgd_t *pgd;",
            "\tunsigned long next;",
            "\tint err = 0;",
            "\tint nr = 0;",
            "\tpgtbl_mod_mask mask = 0;",
            "",
            "\tBUG_ON(addr >= end);",
            "\tpgd = pgd_offset_k(addr);",
            "\tdo {",
            "\t\tnext = pgd_addr_end(addr, end);",
            "\t\tif (pgd_bad(*pgd))",
            "\t\t\tmask |= PGTBL_PGD_MODIFIED;",
            "\t\terr = vmap_pages_p4d_range(pgd, addr, next, prot, pages, &nr, &mask);",
            "\t\tif (err)",
            "\t\t\tbreak;",
            "\t} while (pgd++, addr = next, addr != end);",
            "",
            "\tif (mask & ARCH_PAGE_TABLE_SYNC_MASK)",
            "\t\tarch_sync_kernel_mappings(start, end);",
            "",
            "\treturn err;",
            "}",
            "int __vmap_pages_range_noflush(unsigned long addr, unsigned long end,",
            "\t\tpgprot_t prot, struct page **pages, unsigned int page_shift)",
            "{",
            "\tunsigned int i, nr = (end - addr) >> PAGE_SHIFT;",
            "",
            "\tWARN_ON(page_shift < PAGE_SHIFT);",
            "",
            "\tif (!IS_ENABLED(CONFIG_HAVE_ARCH_HUGE_VMALLOC) ||",
            "\t\t\tpage_shift == PAGE_SHIFT)",
            "\t\treturn vmap_small_pages_range_noflush(addr, end, prot, pages);",
            "",
            "\tfor (i = 0; i < nr; i += 1U << (page_shift - PAGE_SHIFT)) {",
            "\t\tint err;",
            "",
            "\t\terr = vmap_range_noflush(addr, addr + (1UL << page_shift),",
            "\t\t\t\t\tpage_to_phys(pages[i]), prot,",
            "\t\t\t\t\tpage_shift);",
            "\t\tif (err)",
            "\t\t\treturn err;",
            "",
            "\t\taddr += 1UL << page_shift;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int vmap_pages_range_noflush(unsigned long addr, unsigned long end,",
            "\t\tpgprot_t prot, struct page **pages, unsigned int page_shift)",
            "{",
            "\tint ret = kmsan_vmap_pages_range_noflush(addr, end, prot, pages,",
            "\t\t\t\t\t\t page_shift);",
            "",
            "\tif (ret)",
            "\t\treturn ret;",
            "\treturn __vmap_pages_range_noflush(addr, end, prot, pages, page_shift);",
            "}",
            "static int vmap_pages_range(unsigned long addr, unsigned long end,",
            "\t\tpgprot_t prot, struct page **pages, unsigned int page_shift)",
            "{",
            "\tint err;",
            "",
            "\terr = vmap_pages_range_noflush(addr, end, prot, pages, page_shift);",
            "\tflush_cache_vmap(addr, end);",
            "\treturn err;",
            "}",
            "static int check_sparse_vm_area(struct vm_struct *area, unsigned long start,",
            "\t\t\t\tunsigned long end)",
            "{",
            "\tmight_sleep();",
            "\tif (WARN_ON_ONCE(area->flags & VM_FLUSH_RESET_PERMS))",
            "\t\treturn -EINVAL;",
            "\tif (WARN_ON_ONCE(area->flags & VM_NO_GUARD))",
            "\t\treturn -EINVAL;",
            "\tif (WARN_ON_ONCE(!(area->flags & VM_SPARSE)))",
            "\t\treturn -EINVAL;",
            "\tif ((end - start) >> PAGE_SHIFT > totalram_pages())",
            "\t\treturn -E2BIG;",
            "\tif (start < (unsigned long)area->addr ||",
            "\t    (void *)end > area->addr + get_vm_area_size(area))",
            "\t\treturn -ERANGE;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmap_pages_p4d_range, vmap_small_pages_range_noflush, __vmap_pages_range_noflush, vmap_pages_range_noflush, vmap_pages_range, check_sparse_vm_area",
          "description": "实现将页面映射到虚拟地址空间的递归函数族，包含P4D级分页表构建、小粒度页映射及无刷洗映射逻辑，最终通过flush_cache_vmap触发缓存刷新",
          "similarity": 0.5332979559898376
        }
      ]
    }
  ]
}