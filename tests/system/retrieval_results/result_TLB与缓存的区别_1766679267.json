{
  "query": "TLB与缓存的区别",
  "timestamp": "2025-12-26 00:14:27",
  "retrieved_files": [
    {
      "source_file": "mm/mmu_gather.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:52:52\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mmu_gather.c`\n\n---\n\n# mmu_gather.c 技术文档\n\n## 1. 文件概述\n\n`mmu_gather.c` 是 Linux 内核内存管理子系统中的关键组件，负责在页表项（PTE）或更高层级页表被撤销映射（unmap）后，高效地批量释放对应的物理页面和页表结构。该文件实现了 **MMU gather** 机制，用于延迟并批量处理 TLB（Translation Lookaside Buffer）刷新、反向映射（rmap）清理以及页面回收操作，以减少频繁的 TLB 刷新开销和锁竞争，提升性能。\n\n当内核需要释放大量虚拟内存区域（如进程退出、mmap 区域销毁）时，不会立即释放每个页面，而是先将待释放的页面收集到 `mmu_gather` 结构中，待累积到一定数量或显式调用 flush 操作时，再统一执行 TLB 刷新、rmap 解除和页面释放。\n\n## 2. 核心功能\n\n### 主要函数\n\n- `tlb_next_batch(struct mmu_gather *tlb)`  \n  分配新的批处理批次（batch），用于扩展可收集的页面数量上限。\n\n- `tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)`  \n  （仅在 SMP 下）处理延迟的反向映射（delayed rmap）移除操作，在 TLB 刷新后调用。\n\n- `__tlb_batch_free_encoded_pages(struct mmu_gather_batch *batch)`  \n  批量释放编码后的页面（包括普通页面和 swap 缓存），支持防软锁定（soft lockup）的调度点。\n\n- `tlb_batch_pages_flush(struct mmu_gather *tlb)`  \n  遍历所有批次，释放其中收集的所有页面。\n\n- `tlb_batch_list_free(struct mmu_gather *tlb)`  \n  释放动态分配的批次内存（非本地批次）。\n\n- `__tlb_remove_folio_pages_size(...)` / `__tlb_remove_folio_pages(...)` / `__tlb_remove_page_size(...)`  \n  将页面（单页或多页 folio）加入当前 gather 批次，支持延迟 rmap 和不同页面大小。\n\n- `tlb_remove_table_sync_one(void)`  \n  （RCU 表释放模式下）触发 IPI 同步，确保软件页表遍历安全。\n\n- `tlb_remove_table_rcu(struct rcu_head *head)`  \n  RCU 回调函数，用于异步释放页表结构。\n\n- `tlb_remove_table_free(struct mmu_table_batch *batch)`  \n  将页表批次提交给 RCU 机制进行延迟释放。\n\n### 关键数据结构\n\n- `struct mmu_gather`  \n  核心上下文结构，包含本地批次（`local`）、当前活跃批次（`active`）、批次计数、延迟 rmap 标志等。\n\n- `struct mmu_gather_batch`  \n  页面批次结构，包含指向编码页面指针数组、当前数量（`nr`）、最大容量（`max`）及下一个批次指针。\n\n- `struct mmu_table_batch`  \n  页表结构批次，用于批量收集待释放的页表（如 PMD、PUD 等）。\n\n- `encoded_page` 相关机制  \n  使用指针低位编码额外信息（如是否延迟 rmap、是否后跟 nr_pages 字段），节省内存并提高缓存效率。\n\n## 3. 关键实现\n\n### 批处理与动态扩展\n- 默认使用栈上或局部存储的 `local` 批次（避免内存分配）。\n- 当 `local` 批次满时，通过 `__get_free_page()` 动态分配新批次（最多 `MAX_GATHER_BATCH_COUNT` 个）。\n- `tlb_next_batch()` 在存在延迟 rmap 时限制扩展，确保语义正确性。\n\n### 延迟反向映射（Delayed Rmap）\n- 当页面仍被其他 VMA 引用但当前 VMA 正在 unmap 时，不立即调用 `folio_remove_rmap_ptes()`，而是标记 `ENCODED_PAGE_BIT_DELAY_RMAP`。\n- 在 `tlb_flush_rmaps()` 中统一处理，确保在 TLB 刷新**之后**才解除 rmap，防止 CPU 访问已释放页面。\n\n### 安全释放与防软锁定\n- 页面释放循环中每处理最多 `MAX_NR_FOLIOS_PER_FREE`（512）个 folio 调用 `cond_resched()`，避免在非抢占内核中长时间占用 CPU。\n- 若启用 `page_poisoning` 或 `init_on_free`，则按实际内存大小（而非 folio 数量）限制单次释放量，因初始化开销与内存大小成正比。\n\n### 页表结构的安全释放（RCU 模式）\n- 在支持软件页表遍历（如 `gup_fast`）的架构上，页表释放需与遍历操作同步。\n- 使用 `call_rcu()` 延迟释放页表，配合 `smp_call_function()` 触发 IPI 确保所有 CPU 完成 TLB 刷新后再释放内存。\n- 若 RCU 批次分配失败，则回退到即时释放（代码未完整展示，但注释提及）。\n\n### 编码页面指针\n- 利用页面指针对齐特性（通常低 2~3 位为 0），将标志位（如 `DELAY_RMAP`、`NR_PAGES_NEXT`）存储在指针低位。\n- 支持多页 folio：若 `nr_pages > 1`，则连续两个条目分别存储页面指针（带标志）和页数。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm_types.h>`、`<linux/mm_inline.h>`、`<linux/rmap.h>` 等，与 folio、page、VMA 管理紧密集成。\n- **TLB 管理**：通过 `<asm/tlb.h>` 与架构相关 TLB 刷新接口交互。\n- **RCU 机制**：在 `CONFIG_MMU_GATHER_RCU_TABLE_FREE` 下依赖 `<linux/rcupdate.h>` 实现页表安全释放。\n- **SMP 支持**：`tlb_flush_rmaps` 和页表同步仅在 `CONFIG_SMP` 下编译。\n- **高阶内存与交换**：使用 `<linux/highmem.h>`、`<linux/swap.h>` 处理高端内存和 swap 缓存释放。\n- **内存分配器**：通过 `__get_free_page(GFP_NOWAIT)` 动态分配批次内存。\n\n## 5. 使用场景\n\n- **进程退出（exit_mmap）**：释放整个地址空间时，大量页面通过 mmu_gather 批量回收。\n- **munmap 系统调用**：解除大块内存映射时，避免逐页 TLB 刷新。\n- **内存回收（reclaim）**：在直接回收或 kswapd 中撤销映射时使用。\n- **透明大页（THP）拆分**：拆分大页时需撤销多个 PTE 映射并释放 sub-page。\n- **页表收缩（shrink_page_list）**：在页面回收路径中解除映射。\n- **KSM（Kernel Samepage Merging）**：合并或取消合并页面时更新 rmap。\n- **页表层级释放**：当上层页表（如 PGD/P4D/PUD/PMD）不再被引用时，通过 `tlb_remove_table` 机制安全释放。",
      "similarity": 0.5791828036308289,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "mm/mmu_gather.c",
          "start_line": 292,
          "end_line": 424,
          "content": [
            "static void tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\t__tlb_remove_table_free(batch);",
            "}",
            "static inline void tlb_table_invalidate(struct mmu_gather *tlb)",
            "{",
            "\tif (tlb_needs_table_invalidate()) {",
            "\t\t/*",
            "\t\t * Invalidate page-table caches used by hardware walkers. Then",
            "\t\t * we still need to RCU-sched wait while freeing the pages",
            "\t\t * because software walkers can still be in-flight.",
            "\t\t */",
            "\t\ttlb_flush_mmu_tlbonly(tlb);",
            "\t}",
            "}",
            "static void tlb_remove_table_one(void *table)",
            "{",
            "\ttlb_remove_table_sync_one();",
            "\t__tlb_remove_table(table);",
            "}",
            "static void tlb_table_flush(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_table_batch **batch = &tlb->batch;",
            "",
            "\tif (*batch) {",
            "\t\ttlb_table_invalidate(tlb);",
            "\t\ttlb_remove_table_free(*batch);",
            "\t\t*batch = NULL;",
            "\t}",
            "}",
            "void tlb_remove_table(struct mmu_gather *tlb, void *table)",
            "{",
            "\tstruct mmu_table_batch **batch = &tlb->batch;",
            "",
            "\tif (*batch == NULL) {",
            "\t\t*batch = (struct mmu_table_batch *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);",
            "\t\tif (*batch == NULL) {",
            "\t\t\ttlb_table_invalidate(tlb);",
            "\t\t\ttlb_remove_table_one(table);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\t(*batch)->nr = 0;",
            "\t}",
            "",
            "\t(*batch)->tables[(*batch)->nr++] = table;",
            "\tif ((*batch)->nr == MAX_TABLE_BATCH)",
            "\t\ttlb_table_flush(tlb);",
            "}",
            "static inline void tlb_table_init(struct mmu_gather *tlb)",
            "{",
            "\ttlb->batch = NULL;",
            "}",
            "static inline void tlb_table_flush(struct mmu_gather *tlb) { }",
            "static inline void tlb_table_init(struct mmu_gather *tlb) { }",
            "static void tlb_flush_mmu_free(struct mmu_gather *tlb)",
            "{",
            "\ttlb_table_flush(tlb);",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb_batch_pages_flush(tlb);",
            "#endif",
            "}",
            "void tlb_flush_mmu(struct mmu_gather *tlb)",
            "{",
            "\ttlb_flush_mmu_tlbonly(tlb);",
            "\ttlb_flush_mmu_free(tlb);",
            "}",
            "static void __tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,",
            "\t\t\t     bool fullmm)",
            "{",
            "\ttlb->mm = mm;",
            "\ttlb->fullmm = fullmm;",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb->need_flush_all = 0;",
            "\ttlb->local.next = NULL;",
            "\ttlb->local.nr   = 0;",
            "\ttlb->local.max  = ARRAY_SIZE(tlb->__pages);",
            "\ttlb->active     = &tlb->local;",
            "\ttlb->batch_count = 0;",
            "#endif",
            "\ttlb->delayed_rmap = 0;",
            "",
            "\ttlb_table_init(tlb);",
            "#ifdef CONFIG_MMU_GATHER_PAGE_SIZE",
            "\ttlb->page_size = 0;",
            "#endif",
            "",
            "\t__tlb_reset_range(tlb);",
            "\tinc_tlb_flush_pending(tlb->mm);",
            "}",
            "void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm)",
            "{",
            "\t__tlb_gather_mmu(tlb, mm, false);",
            "}",
            "void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm)",
            "{",
            "\t__tlb_gather_mmu(tlb, mm, true);",
            "}",
            "void tlb_finish_mmu(struct mmu_gather *tlb)",
            "{",
            "\t/*",
            "\t * If there are parallel threads are doing PTE changes on same range",
            "\t * under non-exclusive lock (e.g., mmap_lock read-side) but defer TLB",
            "\t * flush by batching, one thread may end up seeing inconsistent PTEs",
            "\t * and result in having stale TLB entries.  So flush TLB forcefully",
            "\t * if we detect parallel PTE batching threads.",
            "\t *",
            "\t * However, some syscalls, e.g. munmap(), may free page tables, this",
            "\t * needs force flush everything in the given range. Otherwise this",
            "\t * may result in having stale TLB entries for some architectures,",
            "\t * e.g. aarch64, that could specify flush what level TLB.",
            "\t */",
            "\tif (mm_tlb_flush_nested(tlb->mm)) {",
            "\t\t/*",
            "\t\t * The aarch64 yields better performance with fullmm by",
            "\t\t * avoiding multiple CPUs spamming TLBI messages at the",
            "\t\t * same time.",
            "\t\t *",
            "\t\t * On x86 non-fullmm doesn't yield significant difference",
            "\t\t * against fullmm.",
            "\t\t */",
            "\t\ttlb->fullmm = 1;",
            "\t\t__tlb_reset_range(tlb);",
            "\t\ttlb->freed_tables = 1;",
            "\t}",
            "",
            "\ttlb_flush_mmu(tlb);",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            "\ttlb_batch_list_free(tlb);",
            "#endif",
            "\tdec_tlb_flush_pending(tlb->mm);",
            "}"
          ],
          "function_name": "tlb_remove_table_free, tlb_table_invalidate, tlb_remove_table_one, tlb_table_flush, tlb_remove_table, tlb_table_init, tlb_table_flush, tlb_table_init, tlb_flush_mmu_free, tlb_flush_mmu, __tlb_gather_mmu, tlb_gather_mmu, tlb_gather_mmu_fullmm, tlb_finish_mmu",
          "description": "提供 TLB 无效化、页表批量释放及 MMU 收集器初始化/终止接口，包含跨架构的 TLB 同步机制",
          "similarity": 0.5950865745544434
        },
        {
          "chunk_id": 1,
          "file_path": "mm/mmu_gather.c",
          "start_line": 18,
          "end_line": 120,
          "content": [
            "static bool tlb_next_batch(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\t/* Limit batching if we have delayed rmaps pending */",
            "\tif (tlb->delayed_rmap && tlb->active != &tlb->local)",
            "\t\treturn false;",
            "",
            "\tbatch = tlb->active;",
            "\tif (batch->next) {",
            "\t\ttlb->active = batch->next;",
            "\t\treturn true;",
            "\t}",
            "",
            "\tif (tlb->batch_count == MAX_GATHER_BATCH_COUNT)",
            "\t\treturn false;",
            "",
            "\tbatch = (void *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);",
            "\tif (!batch)",
            "\t\treturn false;",
            "",
            "\ttlb->batch_count++;",
            "\tbatch->next = NULL;",
            "\tbatch->nr   = 0;",
            "\tbatch->max  = MAX_GATHER_BATCH;",
            "",
            "\ttlb->active->next = batch;",
            "\ttlb->active = batch;",
            "",
            "\treturn true;",
            "}",
            "static void tlb_flush_rmap_batch(struct mmu_gather_batch *batch, struct vm_area_struct *vma)",
            "{",
            "\tstruct encoded_page **pages = batch->encoded_pages;",
            "",
            "\tfor (int i = 0; i < batch->nr; i++) {",
            "\t\tstruct encoded_page *enc = pages[i];",
            "",
            "\t\tif (encoded_page_flags(enc) & ENCODED_PAGE_BIT_DELAY_RMAP) {",
            "\t\t\tstruct page *page = encoded_page_ptr(enc);",
            "\t\t\tunsigned int nr_pages = 1;",
            "",
            "\t\t\tif (unlikely(encoded_page_flags(enc) &",
            "\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\tnr_pages = encoded_nr_pages(pages[++i]);",
            "",
            "\t\t\tfolio_remove_rmap_ptes(page_folio(page), page, nr_pages,",
            "\t\t\t\t\t       vma);",
            "\t\t}",
            "\t}",
            "}",
            "void tlb_flush_rmaps(struct mmu_gather *tlb, struct vm_area_struct *vma)",
            "{",
            "\tif (!tlb->delayed_rmap)",
            "\t\treturn;",
            "",
            "\ttlb_flush_rmap_batch(&tlb->local, vma);",
            "\tif (tlb->active != &tlb->local)",
            "\t\ttlb_flush_rmap_batch(tlb->active, vma);",
            "\ttlb->delayed_rmap = 0;",
            "}",
            "static void __tlb_batch_free_encoded_pages(struct mmu_gather_batch *batch)",
            "{",
            "\tstruct encoded_page **pages = batch->encoded_pages;",
            "\tunsigned int nr, nr_pages;",
            "",
            "\twhile (batch->nr) {",
            "\t\tif (!page_poisoning_enabled_static() && !want_init_on_free()) {",
            "\t\t\tnr = min(MAX_NR_FOLIOS_PER_FREE, batch->nr);",
            "",
            "\t\t\t/*",
            "\t\t\t * Make sure we cover page + nr_pages, and don't leave",
            "\t\t\t * nr_pages behind when capping the number of entries.",
            "\t\t\t */",
            "\t\t\tif (unlikely(encoded_page_flags(pages[nr - 1]) &",
            "\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\tnr++;",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * With page poisoning and init_on_free, the time it",
            "\t\t\t * takes to free memory grows proportionally with the",
            "\t\t\t * actual memory size. Therefore, limit based on the",
            "\t\t\t * actual memory size and not the number of involved",
            "\t\t\t * folios.",
            "\t\t\t */",
            "\t\t\tfor (nr = 0, nr_pages = 0;",
            "\t\t\t     nr < batch->nr && nr_pages < MAX_NR_FOLIOS_PER_FREE;",
            "\t\t\t     nr++) {",
            "\t\t\t\tif (unlikely(encoded_page_flags(pages[nr]) &",
            "\t\t\t\t\t     ENCODED_PAGE_BIT_NR_PAGES_NEXT))",
            "\t\t\t\t\tnr_pages += encoded_nr_pages(pages[++nr]);",
            "\t\t\t\telse",
            "\t\t\t\t\tnr_pages++;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tfree_pages_and_swap_cache(pages, nr);",
            "\t\tpages += nr;",
            "\t\tbatch->nr -= nr;",
            "",
            "\t\tcond_resched();",
            "\t}",
            "}"
          ],
          "function_name": "tlb_next_batch, tlb_flush_rmap_batch, tlb_flush_rmaps, __tlb_batch_free_encoded_pages",
          "description": "管理 TLB 批量操作的延迟 RMAP 处理逻辑，包括批次链表管理、编码页面释放及 RMAP 标志清除",
          "similarity": 0.48317664861679077
        },
        {
          "chunk_id": 0,
          "file_path": "mm/mmu_gather.c",
          "start_line": 1,
          "end_line": 17,
          "content": [
            "#include <linux/gfp.h>",
            "#include <linux/highmem.h>",
            "#include <linux/kernel.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/mm_types.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/rcupdate.h>",
            "#include <linux/smp.h>",
            "#include <linux/swap.h>",
            "#include <linux/rmap.h>",
            "",
            "#include <asm/pgalloc.h>",
            "#include <asm/tlb.h>",
            "",
            "#ifndef CONFIG_MMU_GATHER_NO_GATHER",
            ""
          ],
          "function_name": null,
          "description": "声明 MMU 聚合功能所需头文件，根据配置条件包含架构相关实现",
          "similarity": 0.41104093194007874
        },
        {
          "chunk_id": 2,
          "file_path": "mm/mmu_gather.c",
          "start_line": 144,
          "end_line": 244,
          "content": [
            "static void tlb_batch_pages_flush(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\tfor (batch = &tlb->local; batch && batch->nr; batch = batch->next)",
            "\t\t__tlb_batch_free_encoded_pages(batch);",
            "\ttlb->active = &tlb->local;",
            "}",
            "static void tlb_batch_list_free(struct mmu_gather *tlb)",
            "{",
            "\tstruct mmu_gather_batch *batch, *next;",
            "",
            "\tfor (batch = tlb->local.next; batch; batch = next) {",
            "\t\tnext = batch->next;",
            "\t\tfree_pages((unsigned long)batch, 0);",
            "\t}",
            "\ttlb->local.next = NULL;",
            "}",
            "static bool __tlb_remove_folio_pages_size(struct mmu_gather *tlb,",
            "\t\tstruct page *page, unsigned int nr_pages, bool delay_rmap,",
            "\t\tint page_size)",
            "{",
            "\tint flags = delay_rmap ? ENCODED_PAGE_BIT_DELAY_RMAP : 0;",
            "\tstruct mmu_gather_batch *batch;",
            "",
            "\tVM_BUG_ON(!tlb->end);",
            "",
            "#ifdef CONFIG_MMU_GATHER_PAGE_SIZE",
            "\tVM_WARN_ON(tlb->page_size != page_size);",
            "\tVM_WARN_ON_ONCE(nr_pages != 1 && page_size != PAGE_SIZE);",
            "\tVM_WARN_ON_ONCE(page_folio(page) != page_folio(page + nr_pages - 1));",
            "#endif",
            "",
            "\tbatch = tlb->active;",
            "\t/*",
            "\t * Add the page and check if we are full. If so",
            "\t * force a flush.",
            "\t */",
            "\tif (likely(nr_pages == 1)) {",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_page(page, flags);",
            "\t} else {",
            "\t\tflags |= ENCODED_PAGE_BIT_NR_PAGES_NEXT;",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_page(page, flags);",
            "\t\tbatch->encoded_pages[batch->nr++] = encode_nr_pages(nr_pages);",
            "\t}",
            "\t/*",
            "\t * Make sure that we can always add another \"page\" + \"nr_pages\",",
            "\t * requiring two entries instead of only a single one.",
            "\t */",
            "\tif (batch->nr >= batch->max - 1) {",
            "\t\tif (!tlb_next_batch(tlb))",
            "\t\t\treturn true;",
            "\t\tbatch = tlb->active;",
            "\t}",
            "\tVM_BUG_ON_PAGE(batch->nr > batch->max - 1, page);",
            "",
            "\treturn false;",
            "}",
            "bool __tlb_remove_folio_pages(struct mmu_gather *tlb, struct page *page,",
            "\t\tunsigned int nr_pages, bool delay_rmap)",
            "{",
            "\treturn __tlb_remove_folio_pages_size(tlb, page, nr_pages, delay_rmap,",
            "\t\t\t\t\t     PAGE_SIZE);",
            "}",
            "bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page,",
            "\t\tbool delay_rmap, int page_size)",
            "{",
            "\treturn __tlb_remove_folio_pages_size(tlb, page, 1, delay_rmap, page_size);",
            "}",
            "static void __tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < batch->nr; i++)",
            "\t\t__tlb_remove_table(batch->tables[i]);",
            "",
            "\tfree_page((unsigned long)batch);",
            "}",
            "static void tlb_remove_table_smp_sync(void *arg)",
            "{",
            "\t/* Simply deliver the interrupt */",
            "}",
            "void tlb_remove_table_sync_one(void)",
            "{",
            "\t/*",
            "\t * This isn't an RCU grace period and hence the page-tables cannot be",
            "\t * assumed to be actually RCU-freed.",
            "\t *",
            "\t * It is however sufficient for software page-table walkers that rely on",
            "\t * IRQ disabling.",
            "\t */",
            "\tsmp_call_function(tlb_remove_table_smp_sync, NULL, 1);",
            "}",
            "static void tlb_remove_table_rcu(struct rcu_head *head)",
            "{",
            "\t__tlb_remove_table_free(container_of(head, struct mmu_table_batch, rcu));",
            "}",
            "static void tlb_remove_table_free(struct mmu_table_batch *batch)",
            "{",
            "\tcall_rcu(&batch->rcu, tlb_remove_table_rcu);",
            "}"
          ],
          "function_name": "tlb_batch_pages_flush, tlb_batch_list_free, __tlb_remove_folio_pages_size, __tlb_remove_folio_pages, __tlb_remove_page_size, __tlb_remove_table_free, tlb_remove_table_smp_sync, tlb_remove_table_sync_one, tlb_remove_table_rcu, tlb_remove_table_free",
          "description": "实现页表条目批量移除和内存表管理，包含多页面处理、NR_PAGES_NEXT 标记解析及 RCU 安全释放",
          "similarity": 0.36221009492874146
        }
      ]
    },
    {
      "source_file": "mm/hugetlb_vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:07:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb_vmemmap.c`\n\n---\n\n# hugetlb_vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb_vmemmap.c` 实现了 **HugeTLB Vmemmap Optimization (HVO)** 功能，旨在优化与 HugeTLB 页面关联的 `vmemmap`（虚拟内存映射）结构所占用的物理内存。在 Linux 内核中，每个物理页都对应一个 `struct page` 结构，这些结构通过 `vmemmap` 虚拟地址空间进行线性映射。当使用大页（如 2MB 或 1GB HugeTLB 页面）时，为整个大页区域分配完整的 `struct page` 数组会造成大量内存浪费（因为大部分尾部页面不会被单独使用）。  \n\n本文件通过 **重映射（remap）** 技术，将大页对应的多个 `vmemmap` 页面中的尾部页面重新映射到同一个物理页（通常是头部页面），从而显著减少 `vmemmap` 所需的物理内存开销，同时保持内核对 `struct page` 的访问语义正确。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct vmemmap_remap_walk`**  \n  用于遍历和操作 `vmemmap` 页表的上下文结构：\n  - `remap_pte`: 回调函数，处理每个 PTE 条目\n  - `nr_walked`: 已遍历的 PTE 数量\n  - `reuse_page`: 用于重用的物理页（通常是头部页）\n  - `reuse_addr`: `reuse_page` 对应的虚拟地址\n  - `vmemmap_pages`: 可释放的 `vmemmap` 页面链表\n  - `flags`: 控制 TLB 刷新行为的标志位（`VMEMMAP_SPLIT_NO_TLB_FLUSH`, `VMEMMAP_REMAP_NO_TLB_FLUSH`）\n\n### 主要函数\n\n- **`vmemmap_split_pmd()`**  \n  将一个 PMD（Page Middle Directory）级别的大页映射拆分为 PTE 级别的细粒度映射，为后续重映射做准备。\n\n- **`vmemmap_pmd_entry()`**  \n  `mm_walk` 回调函数，在遍历到 PMD 条目时触发，负责检查是否需要拆分 PMD 并执行拆分操作。\n\n- **`vmemmap_pte_entry()`**  \n  `mm_walk` 回调函数，在遍历到 PTE 条目时触发，用于识别重用页并执行重映射逻辑。\n\n- **`vmemmap_remap_range()`**  \n  驱动整个重映射流程，使用 `walk_page_range_novma()` 遍历指定的 `vmemmap` 虚拟地址范围。\n\n- **`vmemmap_remap_pte()`**  \n  实际执行 PTE 重映射的核心函数：将尾部 `vmemmap` 页面的 PTE 指向 `reuse_page`，并设置为只读以防止非法写入。\n\n- **`vmemmap_restore_pte()`**  \n  用于恢复原始映射（例如在取消优化时），从可释放列表中取出原页面并恢复其内容。\n\n- **`free_vmemmap_page()` / `free_vmemmap_page_list()`**  \n  安全释放 `vmemmap` 页面，区分来自 `memblock`（启动内存）或 `buddy` 分配器的页面。\n\n- **`reset_struct_pages()`**  \n  重置 `struct page` 结构的关键字段，避免因重映射导致的元数据不一致问题（如“corrupted mapping in tail page”警告）。\n\n## 3. 关键实现\n\n### 重映射机制\n1. **PMD 拆分**：首先将覆盖目标 `vmemmap` 范围的 PMD 大页映射拆分为 PTE 映射，确保可以独立修改每个 `struct page` 对应的物理页。\n2. **重用页识别**：在遍历 PTE 时，第一个遇到的页面被选为 `reuse_page`（即头部页）。\n3. **尾页重映射**：后续所有 PTE 条目均被修改为指向 `reuse_page`，并设置为只读（`PAGE_KERNEL_RO`），防止对尾部 `struct page` 的意外写入。\n4. **元数据清理**：由于尾部 `struct page` 与头部共享物理内存，其元数据（如 `flags`、`mapping`）可能无效。通过 `reset_struct_pages()` 复制有效数据到尾部结构，避免内核校验失败。\n5. **安全释放**：被替换的原始尾部页面被加入 `vmemmap_pages` 链表，可在后续安全释放。\n\n### 自托管检测\n在内存热插拔场景下（`memmap_on_memory`），`vmemmap` 结构可能位于待优化的内存区域内（即“自托管”）。代码通过检查首个 `vmemmap` 页面的 `PageVmemmapSelfHosted()` 标志，若为真则拒绝优化（返回 `-ENOTSUPP`），防止破坏关键元数据。\n\n### 内存屏障与 TLB 刷新\n- 使用 `smp_wmb()` 确保页面内容更新在 PTE 修改前完成。\n- 在 PMD 拆分和 PTE 重映射后，默认执行 `flush_tlb_kernel_range()` 刷新 TLB，可通过标志位跳过以提升性能。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `pgtable.h`、`pagewalk.h`、`mmdebug.h` 等核心 MM 头文件。\n- **架构相关代码**：使用 `asm/pgalloc.h` 和 `asm/tlbflush.h` 提供的页表分配与 TLB 刷新接口。\n- **HugeTLB 子系统**：与 `hugetlb.h` 协同工作，优化 HugeTLB 页面的 `vmemmap` 开销。\n- **内存热插拔**：处理 `memmap_on_memory` 场景下的自托管 `vmemmap` 限制。\n- **启动内存管理**：通过 `bootmem_info.h` 区分 `memblock` 与 `buddy` 分配的页面。\n\n## 5. 使用场景\n\n- **HugeTLB 内存优化**：在系统配置大量 HugeTLB 页面时，显著减少 `vmemmap` 的物理内存占用（例如，2MB HugeTLB 页面可节省约 87.5% 的 `vmemmap` 内存）。\n- **内存受限环境**：在内存资源紧张的系统（如容器、嵌入式设备）中降低内核内存开销。\n- **内存热插拔**：在支持 `memmap_on_memory` 的热插拔场景中，安全地优化新插入内存区域的 `vmemmap`。\n- **内核调试与维护**：通过只读保护捕获对尾部 `struct page` 的非法写入，提升系统稳定性。",
      "similarity": 0.5592788457870483,
      "chunks": [
        {
          "chunk_id": 5,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 619,
          "end_line": 710,
          "content": [
            "void hugetlb_vmemmap_optimize_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\tLIST_HEAD(vmemmap_pages);",
            "",
            "\t/* avoid writes from page_ref_add_unless() while folding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\t__hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages, 0);",
            "\tfree_vmemmap_page_list(&vmemmap_pages);",
            "}",
            "static int hugetlb_vmemmap_split_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tif (!vmemmap_should_optimize_folio(h, folio))",
            "\t\treturn 0;",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * Split PMDs on the vmemmap virtual address range [@vmemmap_start,",
            "\t * @vmemmap_end]",
            "\t */",
            "\treturn vmemmap_remap_split(vmemmap_start, vmemmap_end, vmemmap_reuse);",
            "}",
            "void hugetlb_vmemmap_optimize_folios(struct hstate *h, struct list_head *folio_list)",
            "{",
            "\tstruct folio *folio;",
            "\tLIST_HEAD(vmemmap_pages);",
            "",
            "\tlist_for_each_entry(folio, folio_list, lru) {",
            "\t\tint ret = hugetlb_vmemmap_split_folio(h, folio);",
            "",
            "\t\t/*",
            "\t\t * Spliting the PMD requires allocating a page, thus lets fail",
            "\t\t * early once we encounter the first OOM. No point in retrying",
            "\t\t * as it can be dynamically done on remap with the memory",
            "\t\t * we get back from the vmemmap deduplication.",
            "\t\t */",
            "\t\tif (ret == -ENOMEM)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tflush_tlb_all();",
            "",
            "\t/* avoid writes from page_ref_add_unless() while folding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\tlist_for_each_entry(folio, folio_list, lru) {",
            "\t\tint ret;",
            "",
            "\t\tret = __hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages,",
            "\t\t\t\t\t\t       VMEMMAP_REMAP_NO_TLB_FLUSH);",
            "",
            "\t\t/*",
            "\t\t * Pages to be freed may have been accumulated.  If we",
            "\t\t * encounter an ENOMEM,  free what we have and try again.",
            "\t\t * This can occur in the case that both spliting fails",
            "\t\t * halfway and head page allocation also failed. In this",
            "\t\t * case __hugetlb_vmemmap_optimize_folio() would free memory",
            "\t\t * allowing more vmemmap remaps to occur.",
            "\t\t */",
            "\t\tif (ret == -ENOMEM && !list_empty(&vmemmap_pages)) {",
            "\t\t\tflush_tlb_all();",
            "\t\t\tfree_vmemmap_page_list(&vmemmap_pages);",
            "\t\t\tINIT_LIST_HEAD(&vmemmap_pages);",
            "\t\t\t__hugetlb_vmemmap_optimize_folio(h, folio, &vmemmap_pages,",
            "\t\t\t\t\t\t\t VMEMMAP_REMAP_NO_TLB_FLUSH);",
            "\t\t}",
            "\t}",
            "",
            "\tflush_tlb_all();",
            "\tfree_vmemmap_page_list(&vmemmap_pages);",
            "}",
            "static int __init hugetlb_vmemmap_init(void)",
            "{",
            "\tconst struct hstate *h;",
            "",
            "\t/* HUGETLB_VMEMMAP_RESERVE_SIZE should cover all used struct pages */",
            "\tBUILD_BUG_ON(__NR_USED_SUBPAGE > HUGETLB_VMEMMAP_RESERVE_PAGES);",
            "",
            "\tfor_each_hstate(h) {",
            "\t\tif (hugetlb_vmemmap_optimizable(h)) {",
            "\t\t\tregister_sysctl_init(\"vm\", hugetlb_vmemmap_sysctls);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "hugetlb_vmemmap_optimize_folio, hugetlb_vmemmap_split_folio, hugetlb_vmemmap_optimize_folios, hugetlb_vmemmap_init",
          "description": "该代码段实现了HugeTLB大页面的虚拟内存映射优化机制。  \n`hugetlb_vmemmap_split_folio`负责分割PMDs以优化VMEMMAP区域空间，`hugetlb_vmemmap_optimize_folios`遍历folio列表执行拆分与内存回收操作，`hugetlb_vmemmap_init`注册系统控制接口用于动态调整优化策略。由于`__hugetlb_vmemmap_optimize_folio`等关键函数未完整展示，需结合上下文进一步验证实现细节。",
          "similarity": 0.5145210027694702
        },
        {
          "chunk_id": 0,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 1,
          "end_line": 48,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * HugeTLB Vmemmap Optimization (HVO)",
            " *",
            " * Copyright (c) 2020, ByteDance. All rights reserved.",
            " *",
            " *     Author: Muchun Song <songmuchun@bytedance.com>",
            " *",
            " * See Documentation/mm/vmemmap_dedup.rst",
            " */",
            "#define pr_fmt(fmt)\t\"HugeTLB: \" fmt",
            "",
            "#include <linux/pgtable.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/bootmem_info.h>",
            "#include <linux/mmdebug.h>",
            "#include <linux/pagewalk.h>",
            "#include <asm/pgalloc.h>",
            "#include <asm/tlbflush.h>",
            "#include \"hugetlb_vmemmap.h\"",
            "",
            "/**",
            " * struct vmemmap_remap_walk - walk vmemmap page table",
            " *",
            " * @remap_pte:\t\tcalled for each lowest-level entry (PTE).",
            " * @nr_walked:\t\tthe number of walked pte.",
            " * @reuse_page:\t\tthe page which is reused for the tail vmemmap pages.",
            " * @reuse_addr:\t\tthe virtual address of the @reuse_page page.",
            " * @vmemmap_pages:\tthe list head of the vmemmap pages that can be freed",
            " *\t\t\tor is mapped from.",
            " * @flags:\t\tused to modify behavior in vmemmap page table walking",
            " *\t\t\toperations.",
            " */",
            "struct vmemmap_remap_walk {",
            "\tvoid\t\t\t(*remap_pte)(pte_t *pte, unsigned long addr,",
            "\t\t\t\t\t     struct vmemmap_remap_walk *walk);",
            "\tunsigned long\t\tnr_walked;",
            "\tstruct page\t\t*reuse_page;",
            "\tunsigned long\t\treuse_addr;",
            "\tstruct list_head\t*vmemmap_pages;",
            "",
            "/* Skip the TLB flush when we split the PMD */",
            "#define VMEMMAP_SPLIT_NO_TLB_FLUSH\tBIT(0)",
            "/* Skip the TLB flush when we remap the PTE */",
            "#define VMEMMAP_REMAP_NO_TLB_FLUSH\tBIT(1)",
            "\tunsigned long\t\tflags;",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义了用于HugeTLB vmemmap优化的结构体vmemmap_remap_walk，包含回调函数指针、统计计数器、复用页面信息及控制标志位，用于跟踪页表遍历过程中的重映射操作。",
          "similarity": 0.5053563117980957
        },
        {
          "chunk_id": 4,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 450,
          "end_line": 579,
          "content": [
            "static int __hugetlb_vmemmap_restore_folio(const struct hstate *h,",
            "\t\t\t\t\t   struct folio *folio, unsigned long flags)",
            "{",
            "\tint ret;",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tVM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);",
            "\tVM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);",
            "",
            "\tif (!folio_test_hugetlb_vmemmap_optimized(folio))",
            "\t\treturn 0;",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * The pages which the vmemmap virtual address range [@vmemmap_start,",
            "\t * @vmemmap_end) are mapped to are freed to the buddy allocator, and",
            "\t * the range is mapped to the page which @vmemmap_reuse is mapped to.",
            "\t * When a HugeTLB page is freed to the buddy allocator, previously",
            "\t * discarded vmemmap pages must be allocated and remapping.",
            "\t */",
            "\tret = vmemmap_remap_alloc(vmemmap_start, vmemmap_end, vmemmap_reuse, flags);",
            "\tif (!ret) {",
            "\t\tfolio_clear_hugetlb_vmemmap_optimized(folio);",
            "\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "int hugetlb_vmemmap_restore_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\t/* avoid writes from page_ref_add_unless() while unfolding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\treturn __hugetlb_vmemmap_restore_folio(h, folio, 0);",
            "}",
            "long hugetlb_vmemmap_restore_folios(const struct hstate *h,",
            "\t\t\t\t\tstruct list_head *folio_list,",
            "\t\t\t\t\tstruct list_head *non_hvo_folios)",
            "{",
            "\tstruct folio *folio, *t_folio;",
            "\tlong restored = 0;",
            "\tlong ret = 0;",
            "",
            "\t/* avoid writes from page_ref_add_unless() while unfolding vmemmap */",
            "\tsynchronize_rcu();",
            "",
            "\tlist_for_each_entry_safe(folio, t_folio, folio_list, lru) {",
            "\t\tif (folio_test_hugetlb_vmemmap_optimized(folio)) {",
            "\t\t\tret = __hugetlb_vmemmap_restore_folio(h, folio,",
            "\t\t\t\t\t\t\t      VMEMMAP_REMAP_NO_TLB_FLUSH);",
            "\t\t\tif (ret)",
            "\t\t\t\tbreak;",
            "\t\t\trestored++;",
            "\t\t}",
            "",
            "\t\t/* Add non-optimized folios to output list */",
            "\t\tlist_move(&folio->lru, non_hvo_folios);",
            "\t}",
            "",
            "\tif (restored)",
            "\t\tflush_tlb_all();",
            "\tif (!ret)",
            "\t\tret = restored;",
            "\treturn ret;",
            "}",
            "static bool vmemmap_should_optimize_folio(const struct hstate *h, struct folio *folio)",
            "{",
            "\tif (folio_test_hugetlb_vmemmap_optimized(folio))",
            "\t\treturn false;",
            "",
            "\tif (!READ_ONCE(vmemmap_optimize_enabled))",
            "\t\treturn false;",
            "",
            "\tif (!hugetlb_vmemmap_optimizable(h))",
            "\t\treturn false;",
            "",
            "\treturn true;",
            "}",
            "static int __hugetlb_vmemmap_optimize_folio(const struct hstate *h,",
            "\t\t\t\t\t    struct folio *folio,",
            "\t\t\t\t\t    struct list_head *vmemmap_pages,",
            "\t\t\t\t\t    unsigned long flags)",
            "{",
            "\tint ret = 0;",
            "\tunsigned long vmemmap_start = (unsigned long)&folio->page, vmemmap_end;",
            "\tunsigned long vmemmap_reuse;",
            "",
            "\tVM_WARN_ON_ONCE_FOLIO(!folio_test_hugetlb(folio), folio);",
            "\tVM_WARN_ON_ONCE_FOLIO(folio_ref_count(folio), folio);",
            "",
            "\tif (!vmemmap_should_optimize_folio(h, folio))",
            "\t\treturn ret;",
            "",
            "\tstatic_branch_inc(&hugetlb_optimize_vmemmap_key);",
            "\t/*",
            "\t * Very Subtle",
            "\t * If VMEMMAP_REMAP_NO_TLB_FLUSH is set, TLB flushing is not performed",
            "\t * immediately after remapping.  As a result, subsequent accesses",
            "\t * and modifications to struct pages associated with the hugetlb",
            "\t * page could be to the OLD struct pages.  Set the vmemmap optimized",
            "\t * flag here so that it is copied to the new head page.  This keeps",
            "\t * the old and new struct pages in sync.",
            "\t * If there is an error during optimization, we will immediately FLUSH",
            "\t * the TLB and clear the flag below.",
            "\t */",
            "\tfolio_set_hugetlb_vmemmap_optimized(folio);",
            "",
            "\tvmemmap_end\t= vmemmap_start + hugetlb_vmemmap_size(h);",
            "\tvmemmap_reuse\t= vmemmap_start;",
            "\tvmemmap_start\t+= HUGETLB_VMEMMAP_RESERVE_SIZE;",
            "",
            "\t/*",
            "\t * Remap the vmemmap virtual address range [@vmemmap_start, @vmemmap_end)",
            "\t * to the page which @vmemmap_reuse is mapped to.  Add pages previously",
            "\t * mapping the range to vmemmap_pages list so that they can be freed by",
            "\t * the caller.",
            "\t */",
            "\tret = vmemmap_remap_free(vmemmap_start, vmemmap_end, vmemmap_reuse,",
            "\t\t\t\t vmemmap_pages, flags);",
            "\tif (ret) {",
            "\t\tstatic_branch_dec(&hugetlb_optimize_vmemmap_key);",
            "\t\tfolio_clear_hugetlb_vmemmap_optimized(folio);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "__hugetlb_vmemmap_restore_folio, hugetlb_vmemmap_restore_folio, hugetlb_vmemmap_restore_folios, vmemmap_should_optimize_folio, __hugetlb_vmemmap_optimize_folio",
          "description": "实现hugeTLB页的vmemmap优化控制逻辑，包含__hugetlb_vmemmap_restore_folio页回收恢复、hugetlb_vmemmap_restore_folios批量处理、vmemmap_should_optimize_folio优化判定及__hugetlb_vmemmap_optimize_folio优化执行路径。",
          "similarity": 0.47735050320625305
        },
        {
          "chunk_id": 3,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 316,
          "end_line": 427,
          "content": [
            "static int vmemmap_remap_free(unsigned long start, unsigned long end,",
            "\t\t\t      unsigned long reuse,",
            "\t\t\t      struct list_head *vmemmap_pages,",
            "\t\t\t      unsigned long flags)",
            "{",
            "\tint ret;",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= vmemmap_remap_pte,",
            "\t\t.reuse_addr\t= reuse,",
            "\t\t.vmemmap_pages\t= vmemmap_pages,",
            "\t\t.flags\t\t= flags,",
            "\t};",
            "\tint nid = page_to_nid((struct page *)reuse);",
            "\tgfp_t gfp_mask = GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN;",
            "",
            "\t/*",
            "\t * Allocate a new head vmemmap page to avoid breaking a contiguous",
            "\t * block of struct page memory when freeing it back to page allocator",
            "\t * in free_vmemmap_page_list(). This will allow the likely contiguous",
            "\t * struct page backing memory to be kept contiguous and allowing for",
            "\t * more allocations of hugepages. Fallback to the currently",
            "\t * mapped head page in case should it fail to allocate.",
            "\t */",
            "\twalk.reuse_page = alloc_pages_node(nid, gfp_mask, 0);",
            "\tif (walk.reuse_page) {",
            "\t\tcopy_page(page_to_virt(walk.reuse_page),",
            "\t\t\t  (void *)walk.reuse_addr);",
            "\t\tlist_add(&walk.reuse_page->lru, vmemmap_pages);",
            "\t}",
            "",
            "\t/*",
            "\t * In order to make remapping routine most efficient for the huge pages,",
            "\t * the routine of vmemmap page table walking has the following rules",
            "\t * (see more details from the vmemmap_pte_range()):",
            "\t *",
            "\t * - The range [@start, @end) and the range [@reuse, @reuse + PAGE_SIZE)",
            "\t *   should be continuous.",
            "\t * - The @reuse address is part of the range [@reuse, @end) that we are",
            "\t *   walking which is passed to vmemmap_remap_range().",
            "\t * - The @reuse address is the first in the complete range.",
            "\t *",
            "\t * So we need to make sure that @start and @reuse meet the above rules.",
            "\t */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tret = vmemmap_remap_range(reuse, end, &walk);",
            "\tif (ret && walk.nr_walked) {",
            "\t\tend = reuse + walk.nr_walked * PAGE_SIZE;",
            "\t\t/*",
            "\t\t * vmemmap_pages contains pages from the previous",
            "\t\t * vmemmap_remap_range call which failed.  These",
            "\t\t * are pages which were removed from the vmemmap.",
            "\t\t * They will be restored in the following call.",
            "\t\t */",
            "\t\twalk = (struct vmemmap_remap_walk) {",
            "\t\t\t.remap_pte\t= vmemmap_restore_pte,",
            "\t\t\t.reuse_addr\t= reuse,",
            "\t\t\t.vmemmap_pages\t= vmemmap_pages,",
            "\t\t\t.flags\t\t= 0,",
            "\t\t};",
            "",
            "\t\tvmemmap_remap_range(reuse, end, &walk);",
            "\t}",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn ret;",
            "}",
            "static int alloc_vmemmap_page_list(unsigned long start, unsigned long end,",
            "\t\t\t\t   struct list_head *list)",
            "{",
            "\tgfp_t gfp_mask = GFP_KERNEL | __GFP_RETRY_MAYFAIL;",
            "\tunsigned long nr_pages = (end - start) >> PAGE_SHIFT;",
            "\tint nid = page_to_nid((struct page *)start);",
            "\tstruct page *page, *next;",
            "",
            "\twhile (nr_pages--) {",
            "\t\tpage = alloc_pages_node(nid, gfp_mask, 0);",
            "\t\tif (!page)",
            "\t\t\tgoto out;",
            "\t\tlist_add(&page->lru, list);",
            "\t}",
            "",
            "\treturn 0;",
            "out:",
            "\tlist_for_each_entry_safe(page, next, list, lru)",
            "\t\t__free_page(page);",
            "\treturn -ENOMEM;",
            "}",
            "static int vmemmap_remap_alloc(unsigned long start, unsigned long end,",
            "\t\t\t       unsigned long reuse, unsigned long flags)",
            "{",
            "\tLIST_HEAD(vmemmap_pages);",
            "\tstruct vmemmap_remap_walk walk = {",
            "\t\t.remap_pte\t= vmemmap_restore_pte,",
            "\t\t.reuse_addr\t= reuse,",
            "\t\t.vmemmap_pages\t= &vmemmap_pages,",
            "\t\t.flags\t\t= flags,",
            "\t};",
            "",
            "\t/* See the comment in the vmemmap_remap_free(). */",
            "\tBUG_ON(start - reuse != PAGE_SIZE);",
            "",
            "\tif (alloc_vmemmap_page_list(start, end, &vmemmap_pages))",
            "\t\treturn -ENOMEM;",
            "",
            "\tmmap_read_lock(&init_mm);",
            "\tvmemmap_remap_range(reuse, end, &walk);",
            "\tmmap_read_unlock(&init_mm);",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmemmap_remap_free, alloc_vmemmap_page_list, vmemmap_remap_alloc",
          "description": "包含vmemmap_remap_free释放大页时的映射回收逻辑、alloc_vmemmap_page_list页面列表分配函数及vmemmap_remap_alloc预分配页面的接口，实现高效的资源管理。",
          "similarity": 0.39348581433296204
        },
        {
          "chunk_id": 1,
          "file_path": "mm/hugetlb_vmemmap.c",
          "start_line": 49,
          "end_line": 150,
          "content": [
            "static int vmemmap_split_pmd(pmd_t *pmd, struct page *head, unsigned long start,",
            "\t\t\t     struct vmemmap_remap_walk *walk)",
            "{",
            "\tpmd_t __pmd;",
            "\tint i;",
            "\tunsigned long addr = start;",
            "\tpte_t *pgtable;",
            "",
            "\tpgtable = pte_alloc_one_kernel(&init_mm);",
            "\tif (!pgtable)",
            "\t\treturn -ENOMEM;",
            "",
            "\tpmd_populate_kernel(&init_mm, &__pmd, pgtable);",
            "",
            "\tfor (i = 0; i < PTRS_PER_PTE; i++, addr += PAGE_SIZE) {",
            "\t\tpte_t entry, *pte;",
            "\t\tpgprot_t pgprot = PAGE_KERNEL;",
            "",
            "\t\tentry = mk_pte(head + i, pgprot);",
            "\t\tpte = pte_offset_kernel(&__pmd, addr);",
            "\t\tset_pte_at(&init_mm, addr, pte, entry);",
            "\t}",
            "",
            "\tspin_lock(&init_mm.page_table_lock);",
            "\tif (likely(pmd_leaf(*pmd))) {",
            "\t\t/*",
            "\t\t * Higher order allocations from buddy allocator must be able to",
            "\t\t * be treated as indepdenent small pages (as they can be freed",
            "\t\t * individually).",
            "\t\t */",
            "\t\tif (!PageReserved(head))",
            "\t\t\tsplit_page(head, get_order(PMD_SIZE));",
            "",
            "\t\t/* Make pte visible before pmd. See comment in pmd_install(). */",
            "\t\tsmp_wmb();",
            "\t\tpmd_populate_kernel(&init_mm, pmd, pgtable);",
            "\t\tif (!(walk->flags & VMEMMAP_SPLIT_NO_TLB_FLUSH))",
            "\t\t\tflush_tlb_kernel_range(start, start + PMD_SIZE);",
            "\t} else {",
            "\t\tpte_free_kernel(&init_mm, pgtable);",
            "\t}",
            "\tspin_unlock(&init_mm.page_table_lock);",
            "",
            "\treturn 0;",
            "}",
            "static int vmemmap_pmd_entry(pmd_t *pmd, unsigned long addr,",
            "\t\t\t     unsigned long next, struct mm_walk *walk)",
            "{",
            "\tint ret = 0;",
            "\tstruct page *head;",
            "\tstruct vmemmap_remap_walk *vmemmap_walk = walk->private;",
            "",
            "\t/* Only splitting, not remapping the vmemmap pages. */",
            "\tif (!vmemmap_walk->remap_pte)",
            "\t\twalk->action = ACTION_CONTINUE;",
            "",
            "\tspin_lock(&init_mm.page_table_lock);",
            "\thead = pmd_leaf(*pmd) ? pmd_page(*pmd) : NULL;",
            "\t/*",
            "\t * Due to HugeTLB alignment requirements and the vmemmap",
            "\t * pages being at the start of the hotplugged memory",
            "\t * region in memory_hotplug.memmap_on_memory case. Checking",
            "\t * the vmemmap page associated with the first vmemmap page",
            "\t * if it is self-hosted is sufficient.",
            "\t *",
            "\t * [                  hotplugged memory                  ]",
            "\t * [        section        ][...][        section        ]",
            "\t * [ vmemmap ][              usable memory               ]",
            "\t *   ^  | ^                        |",
            "\t *   +--+ |                        |",
            "\t *        +------------------------+",
            "\t */",
            "\tif (unlikely(!vmemmap_walk->nr_walked)) {",
            "\t\tstruct page *page = head ? head + pte_index(addr) :",
            "\t\t\t\t    pte_page(ptep_get(pte_offset_kernel(pmd, addr)));",
            "",
            "\t\tif (PageVmemmapSelfHosted(page))",
            "\t\t\tret = -ENOTSUPP;",
            "\t}",
            "\tspin_unlock(&init_mm.page_table_lock);",
            "\tif (!head || ret)",
            "\t\treturn ret;",
            "",
            "\treturn vmemmap_split_pmd(pmd, head, addr & PMD_MASK, vmemmap_walk);",
            "}",
            "static int vmemmap_pte_entry(pte_t *pte, unsigned long addr,",
            "\t\t\t     unsigned long next, struct mm_walk *walk)",
            "{",
            "\tstruct vmemmap_remap_walk *vmemmap_walk = walk->private;",
            "",
            "\t/*",
            "\t * The reuse_page is found 'first' in page table walking before",
            "\t * starting remapping.",
            "\t */",
            "\tif (!vmemmap_walk->reuse_page)",
            "\t\tvmemmap_walk->reuse_page = pte_page(ptep_get(pte));",
            "\telse",
            "\t\tvmemmap_walk->remap_pte(pte, addr, vmemmap_walk);",
            "\tvmemmap_walk->nr_walked++;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmemmap_split_pmd, vmemmap_pmd_entry, vmemmap_pte_entry",
          "description": "实现了vmeammap_split_pmd分割PMD页表项、vmemmap_pmd_entry检查页表项有效性、vmemmap_pte_entry处理页目录项的逻辑，支持大页拆分与页表项初始化。",
          "similarity": 0.37362685799598694
        }
      ]
    },
    {
      "source_file": "mm/hugetlb.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:06:09\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb.c`\n\n---\n\n# hugetlb.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb.c` 是 Linux 内核中实现通用大页（HugeTLB）内存管理的核心文件。该文件提供了对 HugeTLB 页面池的初始化、分配、释放、预留（reservation）、子池（subpool）管理以及与虚拟内存区域（VMA）相关的同步机制等关键功能。它支持多种 HugeTLB 页面大小，并通过灵活的配额和预留策略，满足不同应用场景对大页内存的需求，同时确保系统稳定性。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `hugetlb_max_hstate`: 系统中已注册的大页状态（hstate）数量。\n- `default_hstate_idx`: 默认使用的大页状态索引。\n- `hstates[HUGE_MAX_HSTATE]`: 存储所有已配置的大页状态（如页面大小、页面池信息等）的数组。\n- `huge_boot_pages`: 链表，用于在启动阶段收集通过 `memblock` 分配的大页。\n- `hugetlb_lock`: 自旋锁，保护大页池的关键数据结构（如空闲/活跃列表、页面计数等）。\n- `hugetlb_fault_mutex_table`: 故障互斥锁表，用于序列化同一逻辑大页上的缺页异常，防止因竞争导致的虚假 OOM。\n\n### 主要数据结构\n- `struct hstate`: 描述一种特定大小的大页配置，包括页面大小、节点分布、页面池统计等。\n- `struct hugepage_subpool`: 大页子池，用于实现基于挂载点或 inode 的配额控制（最大/最小页面数限制）。\n- `struct resv_map`: 预留映射，用于跟踪 VMA 中哪些大页已被预留但尚未分配。\n\n### 主要函数\n- **子池管理**:\n  - `hugepage_new_subpool()`: 创建新的大页子池。\n  - `hugepage_put_subpool()`: 释放子池引用，若无引用则销毁子池。\n  - `hugepage_subpool_get_pages()`: 从子池中获取页面，处理最大/最小配额逻辑。\n  - `hugepage_subpool_put_pages()`: 向子池归还页面，处理配额恢复逻辑。\n- **VMA 锁机制**:\n  - `hugetlb_vma_lock_read()/unlock_read()`: 对 VMA 进行读锁定。\n  - `hugetlb_vma_lock_write()/unlock_write()`: 对 VMA 进行写锁定。\n- **内存释放**:\n  - `hugetlb_free_folio()`: 释放大页 folio，优先尝试通过 CMA 释放。\n- **辅助函数**:\n  - `subpool_is_free()`: 判断子池是否可被安全释放。\n  - `unlock_or_release_subpool()`: 解锁并根据条件释放子池。\n\n## 3. 关键实现\n\n### 大页子池（Subpool）配额机制\n子池机制允许为不同的 hugetlbfs 挂载点设置独立的页面配额：\n- **最大配额 (`max_hpages`)**: 限制子池可使用的最大页面数。\n- **最小配额 (`min_hpages`)**: 启动时预占资源，确保最低可用页面数。\n- 获取页面时 (`hugepage_subpool_get_pages`)，先检查最大配额，再处理最小配额的预留抵扣。\n- 归还页面时 (`hugepage_subpool_put_pages`)，若使用量低于最小配额，则恢复预留计数。\n- 当子池引用计数归零且无活跃页面时，自动释放其最小配额并销毁子池。\n\n### VMA 同步锁设计\n为避免多个进程同时处理同一逻辑大页的缺页异常导致资源竞争或 OOM，内核采用两种锁策略：\n- **共享锁**: 多个 VMA 映射同一文件区域时，共享一个 `hugetlb_vma_lock` 结构。\n- **私有锁**: 私有映射使用 `resv_map` 中的读写信号量。\n- 通过 `__vma_shareable_lock()` 和 `__vma_private_lock()` 宏判断 VMA 类型，动态选择锁对象。\n\n### CMA 集成\n当启用 `CONFIG_CMA` 时，大页可从 CMA（Contiguous Memory Allocator）区域分配：\n- 每个 NUMA 节点维护独立的 `hugetlb_cma` 区域。\n- 释放大页时优先调用 `cma_free_folio()`，失败后才走通用路径 `folio_put()`。\n\n### 启动阶段大页分配\n通过 `huge_boot_pages` 链表在内核早期启动阶段收集大页，后续在 `hugetlb_init()` 中将其整合到各 `hstate` 的空闲列表中，确保大页池初始化完成前即可分配页面。\n\n## 4. 依赖关系\n\n- **内存管理子系统**: 依赖 `<linux/mm.h>`, `<linux/page-flags.h>`, `<linux/gfp.h>` 等基础内存管理接口。\n- **NUMA 支持**: 通过 `<linux/numa.h>`, `<linux/nodemask.h>` 实现节点感知的大页分配。\n- **hugetlbfs 文件系统**: 与 `fs/hugetlbfs/` 模块紧密耦合，通过 `HUGETLBFS_SB()` 获取子池信息。\n- **CMA 子系统**: 条件编译依赖 `<linux/cma.h>`，用于连续物理内存分配。\n- **内存控制组 (cgroup)**: 通过 `<linux/hugetlb_cgroup.h>` 集成资源限制。\n- **体系结构相关代码**: 依赖 `<asm/pgalloc.h>`, `<asm/tlb.h>` 处理页表操作和 TLB 刷新。\n\n## 5. 使用场景\n\n- **高性能计算 (HPC)**: 应用程序通过 `mmap(MAP_HUGETLB)` 或挂载 hugetlbfs 使用大页，减少 TLB 缺失开销。\n- **数据库系统**: 如 Oracle、MySQL 利用大页提升内存访问性能。\n- **虚拟化环境**: KVM/QEMU 为虚拟机分配大页作为后端内存，提高 I/O 性能。\n- **实时系统**: 通过预留大页确保关键任务的内存确定性。\n- **容器资源隔离**: 结合 cgroup v2 的 hugetlb 控制器，限制容器的大页使用量。\n- **内核启动参数配置**: 通过 `hugepagesz=`, `hugepages=` 等参数在启动时预分配大页池。",
      "similarity": 0.5547850131988525,
      "chunks": [
        {
          "chunk_id": 35,
          "file_path": "mm/hugetlb.c",
          "start_line": 5879,
          "end_line": 6166,
          "content": [
            "bool hugetlbfs_pagecache_present(struct hstate *h,",
            "\t\t\t\t struct vm_area_struct *vma, unsigned long address)",
            "{",
            "\tstruct address_space *mapping = vma->vm_file->f_mapping;",
            "\tpgoff_t idx = linear_page_index(vma, address);",
            "\tstruct folio *folio;",
            "",
            "\tfolio = filemap_get_folio(mapping, idx);",
            "\tif (IS_ERR(folio))",
            "\t\treturn false;",
            "\tfolio_put(folio);",
            "\treturn true;",
            "}",
            "int hugetlb_add_to_page_cache(struct folio *folio, struct address_space *mapping,",
            "\t\t\t   pgoff_t idx)",
            "{",
            "\tstruct inode *inode = mapping->host;",
            "\tstruct hstate *h = hstate_inode(inode);",
            "\tint err;",
            "",
            "\tidx <<= huge_page_order(h);",
            "\t__folio_set_locked(folio);",
            "\terr = __filemap_add_folio(mapping, folio, idx, GFP_KERNEL, NULL);",
            "",
            "\tif (unlikely(err)) {",
            "\t\t__folio_clear_locked(folio);",
            "\t\treturn err;",
            "\t}",
            "\tfolio_clear_hugetlb_restore_reserve(folio);",
            "",
            "\t/*",
            "\t * mark folio dirty so that it will not be removed from cache/file",
            "\t * by non-hugetlbfs specific code paths.",
            "\t */",
            "\tfolio_mark_dirty(folio);",
            "",
            "\tspin_lock(&inode->i_lock);",
            "\tinode->i_blocks += blocks_per_huge_page(h);",
            "\tspin_unlock(&inode->i_lock);",
            "\treturn 0;",
            "}",
            "static inline vm_fault_t hugetlb_handle_userfault(struct vm_fault *vmf,",
            "\t\t\t\t\t\t  struct address_space *mapping,",
            "\t\t\t\t\t\t  unsigned long reason)",
            "{",
            "\tu32 hash;",
            "",
            "\t/*",
            "\t * vma_lock and hugetlb_fault_mutex must be dropped before handling",
            "\t * userfault. Also mmap_lock could be dropped due to handling",
            "\t * userfault, any vma operation should be careful from here.",
            "\t */",
            "\thugetlb_vma_unlock_read(vmf->vma);",
            "\thash = hugetlb_fault_mutex_hash(mapping, vmf->pgoff);",
            "\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);",
            "\treturn handle_userfault(vmf, reason);",
            "}",
            "static bool hugetlb_pte_stable(struct hstate *h, struct mm_struct *mm,",
            "\t\t\t       pte_t *ptep, pte_t old_pte)",
            "{",
            "\tspinlock_t *ptl;",
            "\tbool same;",
            "",
            "\tptl = huge_pte_lock(h, mm, ptep);",
            "\tsame = pte_same(huge_ptep_get(ptep), old_pte);",
            "\tspin_unlock(ptl);",
            "",
            "\treturn same;",
            "}",
            "static vm_fault_t hugetlb_no_page(struct mm_struct *mm,",
            "\t\t\tstruct vm_area_struct *vma,",
            "\t\t\tstruct address_space *mapping, pgoff_t idx,",
            "\t\t\tunsigned long address, pte_t *ptep,",
            "\t\t\tpte_t old_pte, unsigned int flags,",
            "\t\t\tstruct vm_fault *vmf)",
            "{",
            "\tstruct hstate *h = hstate_vma(vma);",
            "\tvm_fault_t ret = VM_FAULT_SIGBUS;",
            "\tint anon_rmap = 0;",
            "\tunsigned long size;",
            "\tstruct folio *folio;",
            "\tpte_t new_pte;",
            "\tspinlock_t *ptl;",
            "\tunsigned long haddr = address & huge_page_mask(h);",
            "\tbool new_folio, new_pagecache_folio = false;",
            "\tu32 hash = hugetlb_fault_mutex_hash(mapping, idx);",
            "",
            "\t/*",
            "\t * Currently, we are forced to kill the process in the event the",
            "\t * original mapper has unmapped pages from the child due to a failed",
            "\t * COW/unsharing. Warn that such a situation has occurred as it may not",
            "\t * be obvious.",
            "\t */",
            "\tif (is_vma_resv_set(vma, HPAGE_RESV_UNMAPPED)) {",
            "\t\tpr_warn_ratelimited(\"PID %d killed due to inadequate hugepage pool\\n\",",
            "\t\t\t   current->pid);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/*",
            "\t * Use page lock to guard against racing truncation",
            "\t * before we get page_table_lock.",
            "\t */",
            "\tnew_folio = false;",
            "\tfolio = filemap_lock_hugetlb_folio(h, mapping, idx);",
            "\tif (IS_ERR(folio)) {",
            "\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);",
            "\t\tif (idx >= size)",
            "\t\t\tgoto out;",
            "\t\t/* Check for page in userfault range */",
            "\t\tif (userfaultfd_missing(vma)) {",
            "\t\t\t/*",
            "\t\t\t * Since hugetlb_no_page() was examining pte",
            "\t\t\t * without pgtable lock, we need to re-test under",
            "\t\t\t * lock because the pte may not be stable and could",
            "\t\t\t * have changed from under us.  Try to detect",
            "\t\t\t * either changed or during-changing ptes and retry",
            "\t\t\t * properly when needed.",
            "\t\t\t *",
            "\t\t\t * Note that userfaultfd is actually fine with",
            "\t\t\t * false positives (e.g. caused by pte changed),",
            "\t\t\t * but not wrong logical events (e.g. caused by",
            "\t\t\t * reading a pte during changing).  The latter can",
            "\t\t\t * confuse the userspace, so the strictness is very",
            "\t\t\t * much preferred.  E.g., MISSING event should",
            "\t\t\t * never happen on the page after UFFDIO_COPY has",
            "\t\t\t * correctly installed the page and returned.",
            "\t\t\t */",
            "\t\t\tif (!hugetlb_pte_stable(h, mm, ptep, old_pte)) {",
            "\t\t\t\tret = 0;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "",
            "\t\t\treturn hugetlb_handle_userfault(vmf, mapping,",
            "\t\t\t\t\t\t\tVM_UFFD_MISSING);",
            "\t\t}",
            "",
            "\t\tif (!(vma->vm_flags & VM_MAYSHARE)) {",
            "\t\t\tret = __vmf_anon_prepare(vmf);",
            "\t\t\tif (unlikely(ret))",
            "\t\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tfolio = alloc_hugetlb_folio(vma, haddr, 0);",
            "\t\tif (IS_ERR(folio)) {",
            "\t\t\t/*",
            "\t\t\t * Returning error will result in faulting task being",
            "\t\t\t * sent SIGBUS.  The hugetlb fault mutex prevents two",
            "\t\t\t * tasks from racing to fault in the same page which",
            "\t\t\t * could result in false unable to allocate errors.",
            "\t\t\t * Page migration does not take the fault mutex, but",
            "\t\t\t * does a clear then write of pte's under page table",
            "\t\t\t * lock.  Page fault code could race with migration,",
            "\t\t\t * notice the clear pte and try to allocate a page",
            "\t\t\t * here.  Before returning error, get ptl and make",
            "\t\t\t * sure there really is no pte entry.",
            "\t\t\t */",
            "\t\t\tif (hugetlb_pte_stable(h, mm, ptep, old_pte))",
            "\t\t\t\tret = vmf_error(PTR_ERR(folio));",
            "\t\t\telse",
            "\t\t\t\tret = 0;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tclear_huge_page(&folio->page, address, pages_per_huge_page(h));",
            "\t\t__folio_mark_uptodate(folio);",
            "\t\tnew_folio = true;",
            "",
            "\t\tif (vma->vm_flags & VM_MAYSHARE) {",
            "\t\t\tint err = hugetlb_add_to_page_cache(folio, mapping, idx);",
            "\t\t\tif (err) {",
            "\t\t\t\t/*",
            "\t\t\t\t * err can't be -EEXIST which implies someone",
            "\t\t\t\t * else consumed the reservation since hugetlb",
            "\t\t\t\t * fault mutex is held when add a hugetlb page",
            "\t\t\t\t * to the page cache. So it's safe to call",
            "\t\t\t\t * restore_reserve_on_error() here.",
            "\t\t\t\t */",
            "\t\t\t\trestore_reserve_on_error(h, vma, haddr, folio);",
            "\t\t\t\tfolio_put(folio);",
            "\t\t\t\tret = VM_FAULT_SIGBUS;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t\tnew_pagecache_folio = true;",
            "\t\t} else {",
            "\t\t\tfolio_lock(folio);",
            "\t\t\tanon_rmap = 1;",
            "\t\t}",
            "\t} else {",
            "\t\t/*",
            "\t\t * If memory error occurs between mmap() and fault, some process",
            "\t\t * don't have hwpoisoned swap entry for errored virtual address.",
            "\t\t * So we need to block hugepage fault by PG_hwpoison bit check.",
            "\t\t */",
            "\t\tif (unlikely(folio_test_hwpoison(folio))) {",
            "\t\t\tret = VM_FAULT_HWPOISON_LARGE |",
            "\t\t\t\tVM_FAULT_SET_HINDEX(hstate_index(h));",
            "\t\t\tgoto backout_unlocked;",
            "\t\t}",
            "",
            "\t\t/* Check for page in userfault range. */",
            "\t\tif (userfaultfd_minor(vma)) {",
            "\t\t\tfolio_unlock(folio);",
            "\t\t\tfolio_put(folio);",
            "\t\t\t/* See comment in userfaultfd_missing() block above */",
            "\t\t\tif (!hugetlb_pte_stable(h, mm, ptep, old_pte)) {",
            "\t\t\t\tret = 0;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t\treturn hugetlb_handle_userfault(vmf, mapping,",
            "\t\t\t\t\t\t\tVM_UFFD_MINOR);",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If we are going to COW a private mapping later, we examine the",
            "\t * pending reservations for this page now. This will ensure that",
            "\t * any allocations necessary to record that reservation occur outside",
            "\t * the spinlock.",
            "\t */",
            "\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {",
            "\t\tif (vma_needs_reservation(h, vma, haddr) < 0) {",
            "\t\t\tret = VM_FAULT_OOM;",
            "\t\t\tgoto backout_unlocked;",
            "\t\t}",
            "\t\t/* Just decrements count, does not deallocate */",
            "\t\tvma_end_reservation(h, vma, haddr);",
            "\t}",
            "",
            "\tptl = huge_pte_lock(h, mm, ptep);",
            "\tret = 0;",
            "\t/* If pte changed from under us, retry */",
            "\tif (!pte_same(huge_ptep_get(ptep), old_pte))",
            "\t\tgoto backout;",
            "",
            "\tif (anon_rmap)",
            "\t\thugetlb_add_new_anon_rmap(folio, vma, haddr);",
            "\telse",
            "\t\thugetlb_add_file_rmap(folio);",
            "\tnew_pte = make_huge_pte(vma, &folio->page, ((vma->vm_flags & VM_WRITE)",
            "\t\t\t\t&& (vma->vm_flags & VM_SHARED)));",
            "\t/*",
            "\t * If this pte was previously wr-protected, keep it wr-protected even",
            "\t * if populated.",
            "\t */",
            "\tif (unlikely(pte_marker_uffd_wp(old_pte)))",
            "\t\tnew_pte = huge_pte_mkuffd_wp(new_pte);",
            "\tset_huge_pte_at(mm, haddr, ptep, new_pte, huge_page_size(h));",
            "",
            "\thugetlb_count_add(pages_per_huge_page(h), mm);",
            "\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {",
            "\t\t/* Optimization, do the COW without a second fault */",
            "\t\tret = hugetlb_wp(mm, vma, address, ptep, flags, folio, ptl, vmf);",
            "\t}",
            "",
            "\tspin_unlock(ptl);",
            "",
            "\t/*",
            "\t * Only set hugetlb_migratable in newly allocated pages.  Existing pages",
            "\t * found in the pagecache may not have hugetlb_migratable if they have",
            "\t * been isolated for migration.",
            "\t */",
            "\tif (new_folio)",
            "\t\tfolio_set_hugetlb_migratable(folio);",
            "",
            "\tfolio_unlock(folio);",
            "out:",
            "\thugetlb_vma_unlock_read(vma);",
            "",
            "\t/*",
            "\t * We must check to release the per-VMA lock. __vmf_anon_prepare() is",
            "\t * the only way ret can be set to VM_FAULT_RETRY.",
            "\t */",
            "\tif (unlikely(ret & VM_FAULT_RETRY))",
            "\t\tvma_end_read(vma);",
            "",
            "\tmutex_unlock(&hugetlb_fault_mutex_table[hash]);",
            "\treturn ret;",
            "",
            "backout:",
            "\tspin_unlock(ptl);",
            "backout_unlocked:",
            "\tif (new_folio && !new_pagecache_folio)",
            "\t\trestore_reserve_on_error(h, vma, haddr, folio);",
            "",
            "\tfolio_unlock(folio);",
            "\tfolio_put(folio);",
            "\tgoto out;",
            "}"
          ],
          "function_name": "hugetlbfs_pagecache_present, hugetlb_add_to_page_cache, hugetlb_handle_userfault, hugetlb_pte_stable, hugetlb_no_page",
          "description": "检查并确认指定地址的HugeTLB页面是否存在于页缓存中，若不存在则返回false；若存在则返回true。该函数用于验证目标地址对应的页面是否已在内核页缓存中注册。",
          "similarity": 0.5734388828277588
        },
        {
          "chunk_id": 39,
          "file_path": "mm/hugetlb.c",
          "start_line": 6797,
          "end_line": 6955,
          "content": [
            "bool hugetlb_reserve_pages(struct inode *inode,",
            "\t\t\t\t\tlong from, long to,",
            "\t\t\t\t\tstruct vm_area_struct *vma,",
            "\t\t\t\t\tvm_flags_t vm_flags)",
            "{",
            "\tlong chg = -1, add = -1;",
            "\tstruct hstate *h = hstate_inode(inode);",
            "\tstruct hugepage_subpool *spool = subpool_inode(inode);",
            "\tstruct resv_map *resv_map;",
            "\tstruct hugetlb_cgroup *h_cg = NULL;",
            "\tlong gbl_reserve, regions_needed = 0;",
            "",
            "\t/* This should never happen */",
            "\tif (from > to) {",
            "\t\tVM_WARN(1, \"%s called with a negative range\\n\", __func__);",
            "\t\treturn false;",
            "\t}",
            "",
            "\t/*",
            "\t * vma specific semaphore used for pmd sharing and fault/truncation",
            "\t * synchronization",
            "\t */",
            "\thugetlb_vma_lock_alloc(vma);",
            "",
            "\t/*",
            "\t * Only apply hugepage reservation if asked. At fault time, an",
            "\t * attempt will be made for VM_NORESERVE to allocate a page",
            "\t * without using reserves",
            "\t */",
            "\tif (vm_flags & VM_NORESERVE)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * Shared mappings base their reservation on the number of pages that",
            "\t * are already allocated on behalf of the file. Private mappings need",
            "\t * to reserve the full area even if read-only as mprotect() may be",
            "\t * called to make the mapping read-write. Assume !vma is a shm mapping",
            "\t */",
            "\tif (!vma || vma->vm_flags & VM_MAYSHARE) {",
            "\t\t/*",
            "\t\t * resv_map can not be NULL as hugetlb_reserve_pages is only",
            "\t\t * called for inodes for which resv_maps were created (see",
            "\t\t * hugetlbfs_get_inode).",
            "\t\t */",
            "\t\tresv_map = inode_resv_map(inode);",
            "",
            "\t\tchg = region_chg(resv_map, from, to, &regions_needed);",
            "\t} else {",
            "\t\t/* Private mapping. */",
            "\t\tresv_map = resv_map_alloc();",
            "\t\tif (!resv_map)",
            "\t\t\tgoto out_err;",
            "",
            "\t\tchg = to - from;",
            "",
            "\t\tset_vma_resv_map(vma, resv_map);",
            "\t\tset_vma_resv_flags(vma, HPAGE_RESV_OWNER);",
            "\t}",
            "",
            "\tif (chg < 0)",
            "\t\tgoto out_err;",
            "",
            "\tif (hugetlb_cgroup_charge_cgroup_rsvd(hstate_index(h),",
            "\t\t\t\tchg * pages_per_huge_page(h), &h_cg) < 0)",
            "\t\tgoto out_err;",
            "",
            "\tif (vma && !(vma->vm_flags & VM_MAYSHARE) && h_cg) {",
            "\t\t/* For private mappings, the hugetlb_cgroup uncharge info hangs",
            "\t\t * of the resv_map.",
            "\t\t */",
            "\t\tresv_map_set_hugetlb_cgroup_uncharge_info(resv_map, h_cg, h);",
            "\t}",
            "",
            "\t/*",
            "\t * There must be enough pages in the subpool for the mapping. If",
            "\t * the subpool has a minimum size, there may be some global",
            "\t * reservations already in place (gbl_reserve).",
            "\t */",
            "\tgbl_reserve = hugepage_subpool_get_pages(spool, chg);",
            "\tif (gbl_reserve < 0)",
            "\t\tgoto out_uncharge_cgroup;",
            "",
            "\t/*",
            "\t * Check enough hugepages are available for the reservation.",
            "\t * Hand the pages back to the subpool if there are not",
            "\t */",
            "\tif (hugetlb_acct_memory(h, gbl_reserve) < 0)",
            "\t\tgoto out_put_pages;",
            "",
            "\t/*",
            "\t * Account for the reservations made. Shared mappings record regions",
            "\t * that have reservations as they are shared by multiple VMAs.",
            "\t * When the last VMA disappears, the region map says how much",
            "\t * the reservation was and the page cache tells how much of",
            "\t * the reservation was consumed. Private mappings are per-VMA and",
            "\t * only the consumed reservations are tracked. When the VMA",
            "\t * disappears, the original reservation is the VMA size and the",
            "\t * consumed reservations are stored in the map. Hence, nothing",
            "\t * else has to be done for private mappings here",
            "\t */",
            "\tif (!vma || vma->vm_flags & VM_MAYSHARE) {",
            "\t\tadd = region_add(resv_map, from, to, regions_needed, h, h_cg);",
            "",
            "\t\tif (unlikely(add < 0)) {",
            "\t\t\thugetlb_acct_memory(h, -gbl_reserve);",
            "\t\t\tgoto out_put_pages;",
            "\t\t} else if (unlikely(chg > add)) {",
            "\t\t\t/*",
            "\t\t\t * pages in this range were added to the reserve",
            "\t\t\t * map between region_chg and region_add.  This",
            "\t\t\t * indicates a race with alloc_hugetlb_folio.  Adjust",
            "\t\t\t * the subpool and reserve counts modified above",
            "\t\t\t * based on the difference.",
            "\t\t\t */",
            "\t\t\tlong rsv_adjust;",
            "",
            "\t\t\t/*",
            "\t\t\t * hugetlb_cgroup_uncharge_cgroup_rsvd() will put the",
            "\t\t\t * reference to h_cg->css. See comment below for detail.",
            "\t\t\t */",
            "\t\t\thugetlb_cgroup_uncharge_cgroup_rsvd(",
            "\t\t\t\thstate_index(h),",
            "\t\t\t\t(chg - add) * pages_per_huge_page(h), h_cg);",
            "",
            "\t\t\trsv_adjust = hugepage_subpool_put_pages(spool,",
            "\t\t\t\t\t\t\t\tchg - add);",
            "\t\t\thugetlb_acct_memory(h, -rsv_adjust);",
            "\t\t} else if (h_cg) {",
            "\t\t\t/*",
            "\t\t\t * The file_regions will hold their own reference to",
            "\t\t\t * h_cg->css. So we should release the reference held",
            "\t\t\t * via hugetlb_cgroup_charge_cgroup_rsvd() when we are",
            "\t\t\t * done.",
            "\t\t\t */",
            "\t\t\thugetlb_cgroup_put_rsvd_cgroup(h_cg);",
            "\t\t}",
            "\t}",
            "\treturn true;",
            "",
            "out_put_pages:",
            "\t/* put back original number of pages, chg */",
            "\t(void)hugepage_subpool_put_pages(spool, chg);",
            "out_uncharge_cgroup:",
            "\thugetlb_cgroup_uncharge_cgroup_rsvd(hstate_index(h),",
            "\t\t\t\t\t    chg * pages_per_huge_page(h), h_cg);",
            "out_err:",
            "\thugetlb_vma_lock_free(vma);",
            "\tif (!vma || vma->vm_flags & VM_MAYSHARE)",
            "\t\t/* Only call region_abort if the region_chg succeeded but the",
            "\t\t * region_add failed or didn't run.",
            "\t\t */",
            "\t\tif (chg >= 0 && add < 0)",
            "\t\t\tregion_abort(resv_map, from, to, regions_needed);",
            "\tif (vma && is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {",
            "\t\tkref_put(&resv_map->refs, resv_map_release);",
            "\t\tset_vma_resv_map(vma, NULL);",
            "\t}",
            "\treturn false;",
            "}"
          ],
          "function_name": "hugetlb_reserve_pages",
          "description": "为文件映射区域预留HugeTLB页面资源，区分共享与私有映射类型进行全局和局部资源检查，处理资源不足场景并通过引用计数管理预留状态。",
          "similarity": 0.5489602088928223
        },
        {
          "chunk_id": 38,
          "file_path": "mm/hugetlb.c",
          "start_line": 6649,
          "end_line": 6794,
          "content": [
            "long hugetlb_change_protection(struct vm_area_struct *vma,",
            "\t\tunsigned long address, unsigned long end,",
            "\t\tpgprot_t newprot, unsigned long cp_flags)",
            "{",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tunsigned long start = address;",
            "\tpte_t *ptep;",
            "\tpte_t pte;",
            "\tstruct hstate *h = hstate_vma(vma);",
            "\tlong pages = 0, psize = huge_page_size(h);",
            "\tbool shared_pmd = false;",
            "\tstruct mmu_notifier_range range;",
            "\tunsigned long last_addr_mask;",
            "\tbool uffd_wp = cp_flags & MM_CP_UFFD_WP;",
            "\tbool uffd_wp_resolve = cp_flags & MM_CP_UFFD_WP_RESOLVE;",
            "",
            "\t/*",
            "\t * In the case of shared PMDs, the area to flush could be beyond",
            "\t * start/end.  Set range.start/range.end to cover the maximum possible",
            "\t * range if PMD sharing is possible.",
            "\t */",
            "\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_VMA,",
            "\t\t\t\t0, mm, start, end);",
            "\tadjust_range_if_pmd_sharing_possible(vma, &range.start, &range.end);",
            "",
            "\tBUG_ON(address >= end);",
            "\tflush_cache_range(vma, range.start, range.end);",
            "",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "\thugetlb_vma_lock_write(vma);",
            "\ti_mmap_lock_write(vma->vm_file->f_mapping);",
            "\tlast_addr_mask = hugetlb_mask_last_page(h);",
            "\tfor (; address < end; address += psize) {",
            "\t\tspinlock_t *ptl;",
            "\t\tptep = hugetlb_walk(vma, address, psize);",
            "\t\tif (!ptep) {",
            "\t\t\tif (!uffd_wp) {",
            "\t\t\t\taddress |= last_addr_mask;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\t/*",
            "\t\t\t * Userfaultfd wr-protect requires pgtable",
            "\t\t\t * pre-allocations to install pte markers.",
            "\t\t\t */",
            "\t\t\tptep = huge_pte_alloc(mm, vma, address, psize);",
            "\t\t\tif (!ptep) {",
            "\t\t\t\tpages = -ENOMEM;",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tptl = huge_pte_lock(h, mm, ptep);",
            "\t\tif (huge_pmd_unshare(mm, vma, address, ptep)) {",
            "\t\t\t/*",
            "\t\t\t * When uffd-wp is enabled on the vma, unshare",
            "\t\t\t * shouldn't happen at all.  Warn about it if it",
            "\t\t\t * happened due to some reason.",
            "\t\t\t */",
            "\t\t\tWARN_ON_ONCE(uffd_wp || uffd_wp_resolve);",
            "\t\t\tpages++;",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\tshared_pmd = true;",
            "\t\t\taddress |= last_addr_mask;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tpte = huge_ptep_get(ptep);",
            "\t\tif (unlikely(is_hugetlb_entry_hwpoisoned(pte))) {",
            "\t\t\t/* Nothing to do. */",
            "\t\t} else if (unlikely(is_hugetlb_entry_migration(pte))) {",
            "\t\t\tswp_entry_t entry = pte_to_swp_entry(pte);",
            "\t\t\tstruct page *page = pfn_swap_entry_to_page(entry);",
            "\t\t\tpte_t newpte = pte;",
            "",
            "\t\t\tif (is_writable_migration_entry(entry)) {",
            "\t\t\t\tif (PageAnon(page))",
            "\t\t\t\t\tentry = make_readable_exclusive_migration_entry(",
            "\t\t\t\t\t\t\t\tswp_offset(entry));",
            "\t\t\t\telse",
            "\t\t\t\t\tentry = make_readable_migration_entry(",
            "\t\t\t\t\t\t\t\tswp_offset(entry));",
            "\t\t\t\tnewpte = swp_entry_to_pte(entry);",
            "\t\t\t\tpages++;",
            "\t\t\t}",
            "",
            "\t\t\tif (uffd_wp)",
            "\t\t\t\tnewpte = pte_swp_mkuffd_wp(newpte);",
            "\t\t\telse if (uffd_wp_resolve)",
            "\t\t\t\tnewpte = pte_swp_clear_uffd_wp(newpte);",
            "\t\t\tif (!pte_same(pte, newpte))",
            "\t\t\t\tset_huge_pte_at(mm, address, ptep, newpte, psize);",
            "\t\t} else if (unlikely(is_pte_marker(pte))) {",
            "\t\t\t/*",
            "\t\t\t * Do nothing on a poison marker; page is",
            "\t\t\t * corrupted, permissons do not apply.  Here",
            "\t\t\t * pte_marker_uffd_wp()==true implies !poison",
            "\t\t\t * because they're mutual exclusive.",
            "\t\t\t */",
            "\t\t\tif (pte_marker_uffd_wp(pte) && uffd_wp_resolve)",
            "\t\t\t\t/* Safe to modify directly (non-present->none). */",
            "\t\t\t\thuge_pte_clear(mm, address, ptep, psize);",
            "\t\t} else if (!huge_pte_none(pte)) {",
            "\t\t\tpte_t old_pte;",
            "\t\t\tunsigned int shift = huge_page_shift(hstate_vma(vma));",
            "",
            "\t\t\told_pte = huge_ptep_modify_prot_start(vma, address, ptep);",
            "\t\t\tpte = huge_pte_modify(old_pte, newprot);",
            "\t\t\tpte = arch_make_huge_pte(pte, shift, vma->vm_flags);",
            "\t\t\tif (uffd_wp)",
            "\t\t\t\tpte = huge_pte_mkuffd_wp(pte);",
            "\t\t\telse if (uffd_wp_resolve)",
            "\t\t\t\tpte = huge_pte_clear_uffd_wp(pte);",
            "\t\t\thuge_ptep_modify_prot_commit(vma, address, ptep, old_pte, pte);",
            "\t\t\tpages++;",
            "\t\t} else {",
            "\t\t\t/* None pte */",
            "\t\t\tif (unlikely(uffd_wp))",
            "\t\t\t\t/* Safe to modify directly (none->non-present). */",
            "\t\t\t\tset_huge_pte_at(mm, address, ptep,",
            "\t\t\t\t\t\tmake_pte_marker(PTE_MARKER_UFFD_WP),",
            "\t\t\t\t\t\tpsize);",
            "\t\t}",
            "\t\tspin_unlock(ptl);",
            "\t}",
            "\t/*",
            "\t * Must flush TLB before releasing i_mmap_rwsem: x86's huge_pmd_unshare",
            "\t * may have cleared our pud entry and done put_page on the page table:",
            "\t * once we release i_mmap_rwsem, another task can do the final put_page",
            "\t * and that page table be reused and filled with junk.  If we actually",
            "\t * did unshare a page of pmds, flush the range corresponding to the pud.",
            "\t */",
            "\tif (shared_pmd)",
            "\t\tflush_hugetlb_tlb_range(vma, range.start, range.end);",
            "\telse",
            "\t\tflush_hugetlb_tlb_range(vma, start, end);",
            "\t/*",
            "\t * No need to call mmu_notifier_arch_invalidate_secondary_tlbs() we are",
            "\t * downgrading page table protection not changing it to point to a new",
            "\t * page.",
            "\t *",
            "\t * See Documentation/mm/mmu_notifier.rst",
            "\t */",
            "\ti_mmap_unlock_write(vma->vm_file->f_mapping);",
            "\thugetlb_vma_unlock_write(vma);",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "",
            "\treturn pages > 0 ? (pages << h->order) : pages;",
            "}"
          ],
          "function_name": "hugetlb_change_protection",
          "description": "修改HugeTLB页面的访问保护属性（如只读/可写），处理迁移入口、硬件污染标记等特殊情形，同步更新页表项并触发TLB刷新以保证可见性。",
          "similarity": 0.5322356224060059
        },
        {
          "chunk_id": 37,
          "file_path": "mm/hugetlb.c",
          "start_line": 6438,
          "end_line": 6646,
          "content": [
            "int hugetlb_mfill_atomic_pte(pte_t *dst_pte,",
            "\t\t\t     struct vm_area_struct *dst_vma,",
            "\t\t\t     unsigned long dst_addr,",
            "\t\t\t     unsigned long src_addr,",
            "\t\t\t     uffd_flags_t flags,",
            "\t\t\t     struct folio **foliop)",
            "{",
            "\tstruct mm_struct *dst_mm = dst_vma->vm_mm;",
            "\tbool is_continue = uffd_flags_mode_is(flags, MFILL_ATOMIC_CONTINUE);",
            "\tbool wp_enabled = (flags & MFILL_ATOMIC_WP);",
            "\tstruct hstate *h = hstate_vma(dst_vma);",
            "\tstruct address_space *mapping = dst_vma->vm_file->f_mapping;",
            "\tpgoff_t idx = vma_hugecache_offset(h, dst_vma, dst_addr);",
            "\tunsigned long size;",
            "\tint vm_shared = dst_vma->vm_flags & VM_SHARED;",
            "\tpte_t _dst_pte;",
            "\tspinlock_t *ptl;",
            "\tint ret = -ENOMEM;",
            "\tstruct folio *folio;",
            "\tint writable;",
            "\tbool folio_in_pagecache = false;",
            "",
            "\tif (uffd_flags_mode_is(flags, MFILL_ATOMIC_POISON)) {",
            "\t\tptl = huge_pte_lock(h, dst_mm, dst_pte);",
            "",
            "\t\t/* Don't overwrite any existing PTEs (even markers) */",
            "\t\tif (!huge_pte_none(huge_ptep_get(dst_pte))) {",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\treturn -EEXIST;",
            "\t\t}",
            "",
            "\t\t_dst_pte = make_pte_marker(PTE_MARKER_POISONED);",
            "\t\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte,",
            "\t\t\t\thuge_page_size(h));",
            "",
            "\t\t/* No need to invalidate - it was non-present before */",
            "\t\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);",
            "",
            "\t\tspin_unlock(ptl);",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tif (is_continue) {",
            "\t\tret = -EFAULT;",
            "\t\tfolio = filemap_lock_hugetlb_folio(h, mapping, idx);",
            "\t\tif (IS_ERR(folio))",
            "\t\t\tgoto out;",
            "\t\tfolio_in_pagecache = true;",
            "\t} else if (!*foliop) {",
            "\t\t/* If a folio already exists, then it's UFFDIO_COPY for",
            "\t\t * a non-missing case. Return -EEXIST.",
            "\t\t */",
            "\t\tif (vm_shared &&",
            "\t\t    hugetlbfs_pagecache_present(h, dst_vma, dst_addr)) {",
            "\t\t\tret = -EEXIST;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tfolio = alloc_hugetlb_folio(dst_vma, dst_addr, 0);",
            "\t\tif (IS_ERR(folio)) {",
            "\t\t\tret = -ENOMEM;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tret = copy_folio_from_user(folio, (const void __user *) src_addr,",
            "\t\t\t\t\t   false);",
            "",
            "\t\t/* fallback to copy_from_user outside mmap_lock */",
            "\t\tif (unlikely(ret)) {",
            "\t\t\tret = -ENOENT;",
            "\t\t\t/* Free the allocated folio which may have",
            "\t\t\t * consumed a reservation.",
            "\t\t\t */",
            "\t\t\trestore_reserve_on_error(h, dst_vma, dst_addr, folio);",
            "\t\t\tfolio_put(folio);",
            "",
            "\t\t\t/* Allocate a temporary folio to hold the copied",
            "\t\t\t * contents.",
            "\t\t\t */",
            "\t\t\tfolio = alloc_hugetlb_folio_vma(h, dst_vma, dst_addr);",
            "\t\t\tif (!folio) {",
            "\t\t\t\tret = -ENOMEM;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t\t*foliop = folio;",
            "\t\t\t/* Set the outparam foliop and return to the caller to",
            "\t\t\t * copy the contents outside the lock. Don't free the",
            "\t\t\t * folio.",
            "\t\t\t */",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t} else {",
            "\t\tif (vm_shared &&",
            "\t\t    hugetlbfs_pagecache_present(h, dst_vma, dst_addr)) {",
            "\t\t\tfolio_put(*foliop);",
            "\t\t\tret = -EEXIST;",
            "\t\t\t*foliop = NULL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tfolio = alloc_hugetlb_folio(dst_vma, dst_addr, 0);",
            "\t\tif (IS_ERR(folio)) {",
            "\t\t\tfolio_put(*foliop);",
            "\t\t\tret = -ENOMEM;",
            "\t\t\t*foliop = NULL;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\tret = copy_user_large_folio(folio, *foliop, dst_addr, dst_vma);",
            "\t\tfolio_put(*foliop);",
            "\t\t*foliop = NULL;",
            "\t\tif (ret) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * The memory barrier inside __folio_mark_uptodate makes sure that",
            "\t * preceding stores to the page contents become visible before",
            "\t * the set_pte_at() write.",
            "\t */",
            "\t__folio_mark_uptodate(folio);",
            "",
            "\t/* Add shared, newly allocated pages to the page cache. */",
            "\tif (vm_shared && !is_continue) {",
            "\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);",
            "\t\tret = -EFAULT;",
            "\t\tif (idx >= size)",
            "\t\t\tgoto out_release_nounlock;",
            "",
            "\t\t/*",
            "\t\t * Serialization between remove_inode_hugepages() and",
            "\t\t * hugetlb_add_to_page_cache() below happens through the",
            "\t\t * hugetlb_fault_mutex_table that here must be hold by",
            "\t\t * the caller.",
            "\t\t */",
            "\t\tret = hugetlb_add_to_page_cache(folio, mapping, idx);",
            "\t\tif (ret)",
            "\t\t\tgoto out_release_nounlock;",
            "\t\tfolio_in_pagecache = true;",
            "\t}",
            "",
            "\tptl = huge_pte_lock(h, dst_mm, dst_pte);",
            "",
            "\tret = -EIO;",
            "\tif (folio_test_hwpoison(folio))",
            "\t\tgoto out_release_unlock;",
            "",
            "\t/*",
            "\t * We allow to overwrite a pte marker: consider when both MISSING|WP",
            "\t * registered, we firstly wr-protect a none pte which has no page cache",
            "\t * page backing it, then access the page.",
            "\t */",
            "\tret = -EEXIST;",
            "\tif (!huge_pte_none_mostly(huge_ptep_get(dst_pte)))",
            "\t\tgoto out_release_unlock;",
            "",
            "\tif (folio_in_pagecache)",
            "\t\thugetlb_add_file_rmap(folio);",
            "\telse",
            "\t\thugetlb_add_new_anon_rmap(folio, dst_vma, dst_addr);",
            "",
            "\t/*",
            "\t * For either: (1) CONTINUE on a non-shared VMA, or (2) UFFDIO_COPY",
            "\t * with wp flag set, don't set pte write bit.",
            "\t */",
            "\tif (wp_enabled || (is_continue && !vm_shared))",
            "\t\twritable = 0;",
            "\telse",
            "\t\twritable = dst_vma->vm_flags & VM_WRITE;",
            "",
            "\t_dst_pte = make_huge_pte(dst_vma, &folio->page, writable);",
            "\t/*",
            "\t * Always mark UFFDIO_COPY page dirty; note that this may not be",
            "\t * extremely important for hugetlbfs for now since swapping is not",
            "\t * supported, but we should still be clear in that this page cannot be",
            "\t * thrown away at will, even if write bit not set.",
            "\t */",
            "\t_dst_pte = huge_pte_mkdirty(_dst_pte);",
            "\t_dst_pte = pte_mkyoung(_dst_pte);",
            "",
            "\tif (wp_enabled)",
            "\t\t_dst_pte = huge_pte_mkuffd_wp(_dst_pte);",
            "",
            "\tset_huge_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte, huge_page_size(h));",
            "",
            "\thugetlb_count_add(pages_per_huge_page(h), dst_mm);",
            "",
            "\t/* No need to invalidate - it was non-present before */",
            "\tupdate_mmu_cache(dst_vma, dst_addr, dst_pte);",
            "",
            "\tspin_unlock(ptl);",
            "\tif (!is_continue)",
            "\t\tfolio_set_hugetlb_migratable(folio);",
            "\tif (vm_shared || is_continue)",
            "\t\tfolio_unlock(folio);",
            "\tret = 0;",
            "out:",
            "\treturn ret;",
            "out_release_unlock:",
            "\tspin_unlock(ptl);",
            "\tif (vm_shared || is_continue)",
            "\t\tfolio_unlock(folio);",
            "out_release_nounlock:",
            "\tif (!folio_in_pagecache)",
            "\t\trestore_reserve_on_error(h, dst_vma, dst_addr, folio);",
            "\tfolio_put(folio);",
            "\tgoto out;",
            "}"
          ],
          "function_name": "hugetlb_mfill_atomic_pte",
          "description": "原子方式将用户空间数据填充至新分配的HugeTLB页面中，支持连续操作模式及用户故障描述符标志，处理页面拷贝过程中的异常情况并释放资源。",
          "similarity": 0.5300599336624146
        },
        {
          "chunk_id": 15,
          "file_path": "mm/hugetlb.c",
          "start_line": 2726,
          "end_line": 2901,
          "content": [
            "static long vma_needs_reservation(struct hstate *h,",
            "\t\t\tstruct vm_area_struct *vma, unsigned long addr)",
            "{",
            "\treturn __vma_reservation_common(h, vma, addr, VMA_NEEDS_RESV);",
            "}",
            "static long vma_commit_reservation(struct hstate *h,",
            "\t\t\tstruct vm_area_struct *vma, unsigned long addr)",
            "{",
            "\treturn __vma_reservation_common(h, vma, addr, VMA_COMMIT_RESV);",
            "}",
            "static void vma_end_reservation(struct hstate *h,",
            "\t\t\tstruct vm_area_struct *vma, unsigned long addr)",
            "{",
            "\t(void)__vma_reservation_common(h, vma, addr, VMA_END_RESV);",
            "}",
            "static long vma_add_reservation(struct hstate *h,",
            "\t\t\tstruct vm_area_struct *vma, unsigned long addr)",
            "{",
            "\treturn __vma_reservation_common(h, vma, addr, VMA_ADD_RESV);",
            "}",
            "static long vma_del_reservation(struct hstate *h,",
            "\t\t\tstruct vm_area_struct *vma, unsigned long addr)",
            "{",
            "\treturn __vma_reservation_common(h, vma, addr, VMA_DEL_RESV);",
            "}",
            "void restore_reserve_on_error(struct hstate *h, struct vm_area_struct *vma,",
            "\t\t\tunsigned long address, struct folio *folio)",
            "{",
            "\tlong rc = vma_needs_reservation(h, vma, address);",
            "",
            "\tif (folio_test_hugetlb_restore_reserve(folio)) {",
            "\t\tif (unlikely(rc < 0))",
            "\t\t\t/*",
            "\t\t\t * Rare out of memory condition in reserve map",
            "\t\t\t * manipulation.  Clear hugetlb_restore_reserve so",
            "\t\t\t * that global reserve count will not be incremented",
            "\t\t\t * by free_huge_folio.  This will make it appear",
            "\t\t\t * as though the reservation for this folio was",
            "\t\t\t * consumed.  This may prevent the task from",
            "\t\t\t * faulting in the folio at a later time.  This",
            "\t\t\t * is better than inconsistent global huge page",
            "\t\t\t * accounting of reserve counts.",
            "\t\t\t */",
            "\t\t\tfolio_clear_hugetlb_restore_reserve(folio);",
            "\t\telse if (rc)",
            "\t\t\t(void)vma_add_reservation(h, vma, address);",
            "\t\telse",
            "\t\t\tvma_end_reservation(h, vma, address);",
            "\t} else {",
            "\t\tif (!rc) {",
            "\t\t\t/*",
            "\t\t\t * This indicates there is an entry in the reserve map",
            "\t\t\t * not added by alloc_hugetlb_folio.  We know it was added",
            "\t\t\t * before the alloc_hugetlb_folio call, otherwise",
            "\t\t\t * hugetlb_restore_reserve would be set on the folio.",
            "\t\t\t * Remove the entry so that a subsequent allocation",
            "\t\t\t * does not consume a reservation.",
            "\t\t\t */",
            "\t\t\trc = vma_del_reservation(h, vma, address);",
            "\t\t\tif (rc < 0)",
            "\t\t\t\t/*",
            "\t\t\t\t * VERY rare out of memory condition.  Since",
            "\t\t\t\t * we can not delete the entry, set",
            "\t\t\t\t * hugetlb_restore_reserve so that the reserve",
            "\t\t\t\t * count will be incremented when the folio",
            "\t\t\t\t * is freed.  This reserve will be consumed",
            "\t\t\t\t * on a subsequent allocation.",
            "\t\t\t\t */",
            "\t\t\t\tfolio_set_hugetlb_restore_reserve(folio);",
            "\t\t} else if (rc < 0) {",
            "\t\t\t/*",
            "\t\t\t * Rare out of memory condition from",
            "\t\t\t * vma_needs_reservation call.  Memory allocation is",
            "\t\t\t * only attempted if a new entry is needed.  Therefore,",
            "\t\t\t * this implies there is not an entry in the",
            "\t\t\t * reserve map.",
            "\t\t\t *",
            "\t\t\t * For shared mappings, no entry in the map indicates",
            "\t\t\t * no reservation.  We are done.",
            "\t\t\t */",
            "\t\t\tif (!(vma->vm_flags & VM_MAYSHARE))",
            "\t\t\t\t/*",
            "\t\t\t\t * For private mappings, no entry indicates",
            "\t\t\t\t * a reservation is present.  Since we can",
            "\t\t\t\t * not add an entry, set hugetlb_restore_reserve",
            "\t\t\t\t * on the folio so reserve count will be",
            "\t\t\t\t * incremented when freed.  This reserve will",
            "\t\t\t\t * be consumed on a subsequent allocation.",
            "\t\t\t\t */",
            "\t\t\t\tfolio_set_hugetlb_restore_reserve(folio);",
            "\t\t} else",
            "\t\t\t/*",
            "\t\t\t * No reservation present, do nothing",
            "\t\t\t */",
            "\t\t\t vma_end_reservation(h, vma, address);",
            "\t}",
            "}",
            "static int alloc_and_dissolve_hugetlb_folio(struct hstate *h,",
            "\t\t\tstruct folio *old_folio, struct list_head *list)",
            "{",
            "\tgfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;",
            "\tint nid = folio_nid(old_folio);",
            "\tstruct folio *new_folio = NULL;",
            "\tint ret = 0;",
            "",
            "retry:",
            "\tspin_lock_irq(&hugetlb_lock);",
            "\tif (!folio_test_hugetlb(old_folio)) {",
            "\t\t/*",
            "\t\t * Freed from under us. Drop new_folio too.",
            "\t\t */",
            "\t\tgoto free_new;",
            "\t} else if (folio_ref_count(old_folio)) {",
            "\t\tbool isolated;",
            "",
            "\t\t/*",
            "\t\t * Someone has grabbed the folio, try to isolate it here.",
            "\t\t * Fail with -EBUSY if not possible.",
            "\t\t */",
            "\t\tspin_unlock_irq(&hugetlb_lock);",
            "\t\tisolated = isolate_hugetlb(old_folio, list);",
            "\t\tret = isolated ? 0 : -EBUSY;",
            "\t\tspin_lock_irq(&hugetlb_lock);",
            "\t\tgoto free_new;",
            "\t} else if (!folio_test_hugetlb_freed(old_folio)) {",
            "\t\t/*",
            "\t\t * Folio's refcount is 0 but it has not been enqueued in the",
            "\t\t * freelist yet. Race window is small, so we can succeed here if",
            "\t\t * we retry.",
            "\t\t */",
            "\t\tspin_unlock_irq(&hugetlb_lock);",
            "\t\tcond_resched();",
            "\t\tgoto retry;",
            "\t} else {",
            "\t\tif (!new_folio) {",
            "\t\t\tspin_unlock_irq(&hugetlb_lock);",
            "\t\t\tnew_folio = alloc_buddy_hugetlb_folio(h, gfp_mask, nid,",
            "\t\t\t\t\t\t\t      NULL, NULL);",
            "\t\t\tif (!new_folio)",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t__prep_new_hugetlb_folio(h, new_folio);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Ok, old_folio is still a genuine free hugepage. Remove it from",
            "\t\t * the freelist and decrease the counters. These will be",
            "\t\t * incremented again when calling __prep_account_new_huge_page()",
            "\t\t * and enqueue_hugetlb_folio() for new_folio. The counters will",
            "\t\t * remain stable since this happens under the lock.",
            "\t\t */",
            "\t\tremove_hugetlb_folio(h, old_folio, false);",
            "",
            "\t\t/*",
            "\t\t * Ref count on new_folio is already zero as it was dropped",
            "\t\t * earlier.  It can be directly added to the pool free list.",
            "\t\t */",
            "\t\t__prep_account_new_huge_page(h, nid);",
            "\t\tenqueue_hugetlb_folio(h, new_folio);",
            "",
            "\t\t/*",
            "\t\t * Folio has been replaced, we can safely free the old one.",
            "\t\t */",
            "\t\tspin_unlock_irq(&hugetlb_lock);",
            "\t\tupdate_and_free_hugetlb_folio(h, old_folio, false);",
            "\t}",
            "",
            "\treturn ret;",
            "",
            "free_new:",
            "\tspin_unlock_irq(&hugetlb_lock);",
            "\tif (new_folio)",
            "\t\tupdate_and_free_hugetlb_folio(h, new_folio, false);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "vma_needs_reservation, vma_commit_reservation, vma_end_reservation, vma_add_reservation, vma_del_reservation, restore_reserve_on_error, alloc_and_dissolve_hugetlb_folio",
          "description": "实现Hugetlb预留管理函数，用于处理虚拟内存区域的保留操作，包括检测、提交、结束和添加/删除预留。restore_reserve_on_error负责错误恢复时调整保留状态，alloc_and_dissolve_hugetlb_folio用于替换旧的页帧并进行内存分配回收。",
          "similarity": 0.5126953125
        }
      ]
    }
  ]
}