{
  "query": "分段和分页的核心区别",
  "timestamp": "2025-12-26 00:11:12",
  "retrieved_files": [
    {
      "source_file": "mm/khugepaged.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:26:37\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `khugepaged.c`\n\n---\n\n# khugepaged.c 技术文档\n\n## 1. 文件概述\n\n`khugepaged.c` 是 Linux 内核中透明大页（Transparent Huge Page, THP）子系统的核心组件之一，负责在后台异步地将符合条件的小页（4KB）合并为大页（通常为 2MB 的 PMD 级别大页）。该文件实现了名为 `khugepaged` 的内核线程及其相关扫描、合并逻辑，旨在提升内存访问性能并减少 TLB 压力。通过周期性扫描进程地址空间，识别可合并区域，并尝试分配和填充大页，从而优化系统整体内存效率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`enum scan_result`**  \n  定义了页面扫描过程中可能返回的各种结果状态码，用于控制合并流程的决策（如失败原因、成功条件等）。\n\n- **`struct collapse_control`**  \n  控制页面折叠（collapse）过程的上下文信息，包括是否由 `khugepaged` 发起、各 NUMA 节点的负载统计及分配回退掩码。\n\n- **`struct khugepaged_mm_slot`**  \n  表示正在被 `khugepaged` 扫描的每个 `mm_struct`（进程地址空间）的元数据槽位，继承自通用 `mm_slot` 结构。\n\n- **`struct khugepaged_scan`**  \n  全局扫描游标，记录当前扫描的 `mm` 列表头、当前 `mm_slot` 及下一次扫描的虚拟地址。\n\n### 全局变量\n\n- `khugepaged_thread`：指向后台 `khugepaged` 内核线程的 `task_struct`。\n- `khugepaged_pages_to_scan`：每次扫描迭代处理的 PTE 或 VMA 数量。\n- `khugepaged_scan_sleep_millisecs` / `khugepaged_alloc_sleep_millisecs`：控制扫描与内存分配的休眠间隔。\n- `khugepaged_max_ptes_none/swap/shared`：限制在合并过程中允许存在的未映射、交换或共享 PTE 的最大数量。\n- `mm_slots_hash`：哈希表，用于快速查找正在被扫描的 `mm_struct`。\n- `khugepaged_scan`：全局唯一的扫描状态结构体。\n\n### Sysfs 接口（CONFIG_SYSFS）\n\n提供用户空间可配置参数：\n- `scan_sleep_millisecs`：扫描间隔\n- `alloc_sleep_millisecs`：分配失败后的重试间隔\n- `pages_to_scan`：每次扫描的页数\n- `pages_collapsed` / `full_scans`：只读统计信息\n- `defrag`：是否启用内存碎片整理\n- `max_ptes_none` / `max_ptes_swap`：控制合并容忍度\n\n## 3. 关键实现\n\n### 后台扫描机制\n- 使用单一线程 `khugepaged` 循环遍历所有注册到 `mm_slots_hash` 中的进程地址空间。\n- 每次从 `khugepaged_scan.mm_head` 列表中取出一个 `mm_slot`，按虚拟地址顺序扫描其 VMA 区域。\n- 扫描粒度由 `khugepaged_pages_to_scan` 控制，默认为 4096 页（8×512），每轮扫描后休眠 `khugepaged_scan_sleep_millisecs` 毫秒。\n\n### 大页合并条件\n- 仅对支持透明大页的 VMA（如匿名私有映射）进行处理。\n- 检查目标 2MB 区域内：\n  - 已映射的小页数量足够多；\n  - 未映射（none）、交换（swap）或共享（shared）的 PTE 数量不超过 `khugepaged_max_ptes_*` 阈值；\n  - 所有页面满足可合并条件（如非 KSM、非 compound、已加入 LRU、引用计数合适等）。\n- 若满足条件，则分配一个新大页，复制小页内容，并更新页表。\n\n### 内存分配与回退策略\n- 优先在本地 NUMA 节点分配大页。\n- 若分配失败且启用了 `defrag`，则尝试内存压缩（compaction）。\n- 支持基于 `alloc_nmask` 的跨节点分配回退。\n\n### 并发与同步\n- 使用 `khugepaged_mutex` 保护关键操作（如添加/移除 mm slot）。\n- 通过 `mm_slot` 机制确保同一 `mm` 不被重复扫描。\n- 利用 RCU 和页锁（`trylock_page()`）避免与用户态访问或其它内核路径冲突。\n\n### 统计与追踪\n- 更新 `khugepaged_pages_collapsed` 和 `khugepaged_full_scans` 等统计计数器。\n- 集成 `trace/events/huge_memory.h` 提供详细的合并事件追踪点。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/rmap.h>`、`<linux/swap.h>` 等，进行页表遍历、反向映射、页面迁移等操作。\n- **透明大页框架**：与 `huge_memory.c` 协同工作，共享 THP 配置标志（如 `TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG`）。\n- **KSM（Kernel Samepage Merging）**：检查页面是否已被 KSM 标记，避免合并 KSM 页面。\n- **Userfaultfd**：检测 `UFFD_WP`（用户态写保护）标记，防止非法合并。\n- **NUMA 与内存策略**：使用 `nodemask_t` 和 NUMA 感知分配。\n- **内核线程与调度**：基于 `kthread` 框架实现后台任务，支持 freezer（挂起/恢复）。\n- **Sysfs**：通过 sysfs 向用户空间暴露 tunable 参数（需 `CONFIG_SYSFS`）。\n\n## 5. 使用场景\n\n- **通用服务器负载**：在数据库、虚拟化、大数据处理等内存密集型应用中，自动提升 TLB 覆盖率，降低缺页开销。\n- **延迟敏感型应用**：通过后台预合并，避免运行时同步分配大页导致的延迟毛刺。\n- **内存碎片整理**：配合 `defrag` 选项，在内存紧张时主动整理碎片以促进大页分配。\n- **动态调优**：管理员可通过 sysfs 实时调整扫描频率、合并激进程度等参数，平衡性能与内存开销。\n- **NUMA 系统优化**：在多节点系统中，结合本地分配策略提升内存访问局部性。",
      "similarity": 0.5758796334266663,
      "chunks": [
        {
          "chunk_id": 8,
          "file_path": "mm/khugepaged.c",
          "start_line": 1101,
          "end_line": 1261,
          "content": [
            "static int collapse_huge_page(struct mm_struct *mm, unsigned long address,",
            "\t\t\t      int referenced, int unmapped,",
            "\t\t\t      struct collapse_control *cc)",
            "{",
            "\tLIST_HEAD(compound_pagelist);",
            "\tpmd_t *pmd, _pmd;",
            "\tpte_t *pte;",
            "\tpgtable_t pgtable;",
            "\tstruct folio *folio;",
            "\tspinlock_t *pmd_ptl, *pte_ptl;",
            "\tint result = SCAN_FAIL;",
            "\tstruct vm_area_struct *vma;",
            "\tstruct mmu_notifier_range range;",
            "",
            "\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);",
            "",
            "\t/*",
            "\t * Before allocating the hugepage, release the mmap_lock read lock.",
            "\t * The allocation can take potentially a long time if it involves",
            "\t * sync compaction, and we do not need to hold the mmap_lock during",
            "\t * that. We will recheck the vma after taking it again in write mode.",
            "\t */",
            "\tmmap_read_unlock(mm);",
            "",
            "\tresult = alloc_charge_folio(&folio, mm, cc);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_nolock;",
            "",
            "\tmmap_read_lock(mm);",
            "\tresult = hugepage_vma_revalidate(mm, address, true, &vma, cc);",
            "\tif (result != SCAN_SUCCEED) {",
            "\t\tmmap_read_unlock(mm);",
            "\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tresult = find_pmd_or_thp_or_none(mm, address, &pmd);",
            "\tif (result != SCAN_SUCCEED) {",
            "\t\tmmap_read_unlock(mm);",
            "\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tif (unmapped) {",
            "\t\t/*",
            "\t\t * __collapse_huge_page_swapin will return with mmap_lock",
            "\t\t * released when it fails. So we jump out_nolock directly in",
            "\t\t * that case.  Continuing to collapse causes inconsistency.",
            "\t\t */",
            "\t\tresult = __collapse_huge_page_swapin(mm, vma, address, pmd,",
            "\t\t\t\t\t\t     referenced);",
            "\t\tif (result != SCAN_SUCCEED)",
            "\t\t\tgoto out_nolock;",
            "\t}",
            "",
            "\tmmap_read_unlock(mm);",
            "\t/*",
            "\t * Prevent all access to pagetables with the exception of",
            "\t * gup_fast later handled by the ptep_clear_flush and the VM",
            "\t * handled by the anon_vma lock + PG_lock.",
            "\t *",
            "\t * UFFDIO_MOVE is prevented to race as well thanks to the",
            "\t * mmap_lock.",
            "\t */",
            "\tmmap_write_lock(mm);",
            "\tresult = hugepage_vma_revalidate(mm, address, true, &vma, cc);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_up_write;",
            "\t/* check if the pmd is still valid */",
            "\tresult = check_pmd_still_valid(mm, address, pmd);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out_up_write;",
            "",
            "\tvma_start_write(vma);",
            "\tanon_vma_lock_write(vma->anon_vma);",
            "",
            "\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm, address,",
            "\t\t\t\taddress + HPAGE_PMD_SIZE);",
            "\tmmu_notifier_invalidate_range_start(&range);",
            "",
            "\tpmd_ptl = pmd_lock(mm, pmd); /* probably unnecessary */",
            "\t/*",
            "\t * This removes any huge TLB entry from the CPU so we won't allow",
            "\t * huge and small TLB entries for the same virtual address to",
            "\t * avoid the risk of CPU bugs in that area.",
            "\t *",
            "\t * Parallel GUP-fast is fine since GUP-fast will back off when",
            "\t * it detects PMD is changed.",
            "\t */",
            "\t_pmd = pmdp_collapse_flush(vma, address, pmd);",
            "\tspin_unlock(pmd_ptl);",
            "\tmmu_notifier_invalidate_range_end(&range);",
            "\ttlb_remove_table_sync_one();",
            "",
            "\tpte = pte_offset_map_lock(mm, &_pmd, address, &pte_ptl);",
            "\tif (pte) {",
            "\t\tresult = __collapse_huge_page_isolate(vma, address, pte, cc,",
            "\t\t\t\t\t\t      &compound_pagelist);",
            "\t\tspin_unlock(pte_ptl);",
            "\t} else {",
            "\t\tresult = SCAN_PMD_NULL;",
            "\t}",
            "",
            "\tif (unlikely(result != SCAN_SUCCEED)) {",
            "\t\tif (pte)",
            "\t\t\tpte_unmap(pte);",
            "\t\tspin_lock(pmd_ptl);",
            "\t\tBUG_ON(!pmd_none(*pmd));",
            "\t\t/*",
            "\t\t * We can only use set_pmd_at when establishing",
            "\t\t * hugepmds and never for establishing regular pmds that",
            "\t\t * points to regular pagetables. Use pmd_populate for that",
            "\t\t */",
            "\t\tpmd_populate(mm, pmd, pmd_pgtable(_pmd));",
            "\t\tspin_unlock(pmd_ptl);",
            "\t\tanon_vma_unlock_write(vma->anon_vma);",
            "\t\tgoto out_up_write;",
            "\t}",
            "",
            "\t/*",
            "\t * All pages are isolated and locked so anon_vma rmap",
            "\t * can't run anymore.",
            "\t */",
            "\tanon_vma_unlock_write(vma->anon_vma);",
            "",
            "\tresult = __collapse_huge_page_copy(pte, folio, pmd, _pmd,",
            "\t\t\t\t\t   vma, address, pte_ptl,",
            "\t\t\t\t\t   &compound_pagelist);",
            "\tpte_unmap(pte);",
            "\tif (unlikely(result != SCAN_SUCCEED))",
            "\t\tgoto out_up_write;",
            "",
            "\t/*",
            "\t * The smp_wmb() inside __folio_mark_uptodate() ensures the",
            "\t * copy_huge_page writes become visible before the set_pmd_at()",
            "\t * write.",
            "\t */",
            "\t__folio_mark_uptodate(folio);",
            "\tpgtable = pmd_pgtable(_pmd);",
            "",
            "\t_pmd = mk_huge_pmd(&folio->page, vma->vm_page_prot);",
            "\t_pmd = maybe_pmd_mkwrite(pmd_mkdirty(_pmd), vma);",
            "",
            "\tspin_lock(pmd_ptl);",
            "\tBUG_ON(!pmd_none(*pmd));",
            "\tfolio_add_new_anon_rmap(folio, vma, address, RMAP_EXCLUSIVE);",
            "\tfolio_add_lru_vma(folio, vma);",
            "\tpgtable_trans_huge_deposit(mm, pmd, pgtable);",
            "\tset_pmd_at(mm, address, pmd, _pmd);",
            "\tupdate_mmu_cache_pmd(vma, address, pmd);",
            "\tspin_unlock(pmd_ptl);",
            "",
            "\tfolio = NULL;",
            "",
            "\tresult = SCAN_SUCCEED;",
            "out_up_write:",
            "\tmmap_write_unlock(mm);",
            "out_nolock:",
            "\tif (folio)",
            "\t\tfolio_put(folio);",
            "\ttrace_mm_collapse_huge_page(mm, result == SCAN_SUCCEED, result);",
            "\treturn result;",
            "}"
          ],
          "function_name": "collapse_huge_page",
          "description": "执行大页合并主流程，包括页表隔离、数据复制及新PMD设置",
          "similarity": 0.5750422477722168
        },
        {
          "chunk_id": 12,
          "file_path": "mm/khugepaged.c",
          "start_line": 2235,
          "end_line": 2480,
          "content": [
            "static int hpage_collapse_scan_file(struct mm_struct *mm, unsigned long addr,",
            "\t\t\t\t    struct file *file, pgoff_t start,",
            "\t\t\t\t    struct collapse_control *cc)",
            "{",
            "\tstruct folio *folio = NULL;",
            "\tstruct address_space *mapping = file->f_mapping;",
            "\tXA_STATE(xas, &mapping->i_pages, start);",
            "\tint present, swap;",
            "\tint node = NUMA_NO_NODE;",
            "\tint result = SCAN_SUCCEED;",
            "",
            "\tpresent = 0;",
            "\tswap = 0;",
            "\tmemset(cc->node_load, 0, sizeof(cc->node_load));",
            "\tnodes_clear(cc->alloc_nmask);",
            "\trcu_read_lock();",
            "\txas_for_each(&xas, folio, start + HPAGE_PMD_NR - 1) {",
            "\t\tif (xas_retry(&xas, folio))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (xa_is_value(folio)) {",
            "\t\t\tswap += 1 << xas_get_order(&xas);",
            "\t\t\tif (cc->is_khugepaged &&",
            "\t\t\t    swap > khugepaged_max_ptes_swap) {",
            "\t\t\t\tresult = SCAN_EXCEED_SWAP_PTE;",
            "\t\t\t\tcount_vm_event(THP_SCAN_EXCEED_SWAP_PTE);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (folio_order(folio) == HPAGE_PMD_ORDER &&",
            "\t\t    folio->index == start) {",
            "\t\t\t/* Maybe PMD-mapped */",
            "\t\t\tresult = SCAN_PTE_MAPPED_HUGEPAGE;",
            "\t\t\t/*",
            "\t\t\t * For SCAN_PTE_MAPPED_HUGEPAGE, further processing",
            "\t\t\t * by the caller won't touch the page cache, and so",
            "\t\t\t * it's safe to skip LRU and refcount checks before",
            "\t\t\t * returning.",
            "\t\t\t */",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tnode = folio_nid(folio);",
            "\t\tif (hpage_collapse_scan_abort(node, cc)) {",
            "\t\t\tresult = SCAN_SCAN_ABORT;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tcc->node_load[node]++;",
            "",
            "\t\tif (!folio_test_lru(folio)) {",
            "\t\t\tresult = SCAN_PAGE_LRU;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tif (!is_refcount_suitable(folio)) {",
            "\t\t\tresult = SCAN_PAGE_COUNT;",
            "\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * We probably should check if the folio is referenced",
            "\t\t * here, but nobody would transfer pte_young() to",
            "\t\t * folio_test_referenced() for us.  And rmap walk here",
            "\t\t * is just too costly...",
            "\t\t */",
            "",
            "\t\tpresent += folio_nr_pages(folio);",
            "",
            "\t\tif (need_resched()) {",
            "\t\t\txas_pause(&xas);",
            "\t\t\tcond_resched_rcu();",
            "\t\t}",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tif (result == SCAN_SUCCEED) {",
            "\t\tif (cc->is_khugepaged &&",
            "\t\t    present < HPAGE_PMD_NR - khugepaged_max_ptes_none) {",
            "\t\t\tresult = SCAN_EXCEED_NONE_PTE;",
            "\t\t\tcount_vm_event(THP_SCAN_EXCEED_NONE_PTE);",
            "\t\t} else {",
            "\t\t\tresult = collapse_file(mm, addr, file, start, cc);",
            "\t\t}",
            "\t}",
            "",
            "\ttrace_mm_khugepaged_scan_file(mm, folio, file, present, swap, result);",
            "\treturn result;",
            "}",
            "static int hpage_collapse_scan_file(struct mm_struct *mm, unsigned long addr,",
            "\t\t\t\t    struct file *file, pgoff_t start,",
            "\t\t\t\t    struct collapse_control *cc)",
            "{",
            "\tBUILD_BUG();",
            "}",
            "static unsigned int khugepaged_scan_mm_slot(unsigned int pages, int *result,",
            "\t\t\t\t\t    struct collapse_control *cc)",
            "\t__releases(&khugepaged_mm_lock)",
            "\t__acquires(&khugepaged_mm_lock)",
            "{",
            "\tstruct vma_iterator vmi;",
            "\tstruct khugepaged_mm_slot *mm_slot;",
            "\tstruct mm_slot *slot;",
            "\tstruct mm_struct *mm;",
            "\tstruct vm_area_struct *vma;",
            "\tint progress = 0;",
            "",
            "\tVM_BUG_ON(!pages);",
            "\tlockdep_assert_held(&khugepaged_mm_lock);",
            "\t*result = SCAN_FAIL;",
            "",
            "\tif (khugepaged_scan.mm_slot) {",
            "\t\tmm_slot = khugepaged_scan.mm_slot;",
            "\t\tslot = &mm_slot->slot;",
            "\t} else {",
            "\t\tslot = list_entry(khugepaged_scan.mm_head.next,",
            "\t\t\t\t     struct mm_slot, mm_node);",
            "\t\tmm_slot = mm_slot_entry(slot, struct khugepaged_mm_slot, slot);",
            "\t\tkhugepaged_scan.address = 0;",
            "\t\tkhugepaged_scan.mm_slot = mm_slot;",
            "\t}",
            "\tspin_unlock(&khugepaged_mm_lock);",
            "",
            "\tmm = slot->mm;",
            "\t/*",
            "\t * Don't wait for semaphore (to avoid long wait times).  Just move to",
            "\t * the next mm on the list.",
            "\t */",
            "\tvma = NULL;",
            "\tif (unlikely(!mmap_read_trylock(mm)))",
            "\t\tgoto breakouterloop_mmap_lock;",
            "",
            "\tprogress++;",
            "\tif (unlikely(hpage_collapse_test_exit_or_disable(mm)))",
            "\t\tgoto breakouterloop;",
            "",
            "\tvma_iter_init(&vmi, mm, khugepaged_scan.address);",
            "\tfor_each_vma(vmi, vma) {",
            "\t\tunsigned long hstart, hend;",
            "",
            "\t\tcond_resched();",
            "\t\tif (unlikely(hpage_collapse_test_exit_or_disable(mm))) {",
            "\t\t\tprogress++;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tif (!thp_vma_allowable_order(vma, vma->vm_flags,",
            "\t\t\t\t\tTVA_ENFORCE_SYSFS, PMD_ORDER)) {",
            "skip:",
            "\t\t\tprogress++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\thstart = round_up(vma->vm_start, HPAGE_PMD_SIZE);",
            "\t\thend = round_down(vma->vm_end, HPAGE_PMD_SIZE);",
            "\t\tif (khugepaged_scan.address > hend)",
            "\t\t\tgoto skip;",
            "\t\tif (khugepaged_scan.address < hstart)",
            "\t\t\tkhugepaged_scan.address = hstart;",
            "\t\tVM_BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);",
            "",
            "\t\twhile (khugepaged_scan.address < hend) {",
            "\t\t\tbool mmap_locked = true;",
            "",
            "\t\t\tcond_resched();",
            "\t\t\tif (unlikely(hpage_collapse_test_exit_or_disable(mm)))",
            "\t\t\t\tgoto breakouterloop;",
            "",
            "\t\t\tVM_BUG_ON(khugepaged_scan.address < hstart ||",
            "\t\t\t\t  khugepaged_scan.address + HPAGE_PMD_SIZE >",
            "\t\t\t\t  hend);",
            "\t\t\tif (IS_ENABLED(CONFIG_SHMEM) && !vma_is_anonymous(vma)) {",
            "\t\t\t\tstruct file *file = get_file(vma->vm_file);",
            "\t\t\t\tpgoff_t pgoff = linear_page_index(vma,",
            "\t\t\t\t\t\tkhugepaged_scan.address);",
            "",
            "\t\t\t\tmmap_read_unlock(mm);",
            "\t\t\t\tmmap_locked = false;",
            "\t\t\t\t*result = hpage_collapse_scan_file(mm,",
            "\t\t\t\t\tkhugepaged_scan.address, file, pgoff, cc);",
            "\t\t\t\tfput(file);",
            "\t\t\t\tif (*result == SCAN_PTE_MAPPED_HUGEPAGE) {",
            "\t\t\t\t\tmmap_read_lock(mm);",
            "\t\t\t\t\tif (hpage_collapse_test_exit_or_disable(mm))",
            "\t\t\t\t\t\tgoto breakouterloop;",
            "\t\t\t\t\t*result = collapse_pte_mapped_thp(mm,",
            "\t\t\t\t\t\tkhugepaged_scan.address, false);",
            "\t\t\t\t\tif (*result == SCAN_PMD_MAPPED)",
            "\t\t\t\t\t\t*result = SCAN_SUCCEED;",
            "\t\t\t\t\tmmap_read_unlock(mm);",
            "\t\t\t\t}",
            "\t\t\t} else {",
            "\t\t\t\t*result = hpage_collapse_scan_pmd(mm, vma,",
            "\t\t\t\t\tkhugepaged_scan.address, &mmap_locked, cc);",
            "\t\t\t}",
            "",
            "\t\t\tif (*result == SCAN_SUCCEED)",
            "\t\t\t\t++khugepaged_pages_collapsed;",
            "",
            "\t\t\t/* move to next address */",
            "\t\t\tkhugepaged_scan.address += HPAGE_PMD_SIZE;",
            "\t\t\tprogress += HPAGE_PMD_NR;",
            "\t\t\tif (!mmap_locked)",
            "\t\t\t\t/*",
            "\t\t\t\t * We released mmap_lock so break loop.  Note",
            "\t\t\t\t * that we drop mmap_lock before all hugepage",
            "\t\t\t\t * allocations, so if allocation fails, we are",
            "\t\t\t\t * guaranteed to break here and report the",
            "\t\t\t\t * correct result back to caller.",
            "\t\t\t\t */",
            "\t\t\t\tgoto breakouterloop_mmap_lock;",
            "\t\t\tif (progress >= pages)",
            "\t\t\t\tgoto breakouterloop;",
            "\t\t}",
            "\t}",
            "breakouterloop:",
            "\tmmap_read_unlock(mm); /* exit_mmap will destroy ptes after this */",
            "breakouterloop_mmap_lock:",
            "",
            "\tspin_lock(&khugepaged_mm_lock);",
            "\tVM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);",
            "\t/*",
            "\t * Release the current mm_slot if this mm is about to die, or",
            "\t * if we scanned all vmas of this mm.",
            "\t */",
            "\tif (hpage_collapse_test_exit(mm) || !vma) {",
            "\t\t/*",
            "\t\t * Make sure that if mm_users is reaching zero while",
            "\t\t * khugepaged runs here, khugepaged_exit will find",
            "\t\t * mm_slot not pointing to the exiting mm.",
            "\t\t */",
            "\t\tif (slot->mm_node.next != &khugepaged_scan.mm_head) {",
            "\t\t\tslot = list_entry(slot->mm_node.next,",
            "\t\t\t\t\t  struct mm_slot, mm_node);",
            "\t\t\tkhugepaged_scan.mm_slot =",
            "\t\t\t\tmm_slot_entry(slot, struct khugepaged_mm_slot, slot);",
            "\t\t\tkhugepaged_scan.address = 0;",
            "\t\t} else {",
            "\t\t\tkhugepaged_scan.mm_slot = NULL;",
            "\t\t\tkhugepaged_full_scans++;",
            "\t\t}",
            "",
            "\t\tcollect_mm_slot(mm_slot);",
            "\t}",
            "",
            "\treturn progress;",
            "}"
          ],
          "function_name": "hpage_collapse_scan_file, hpage_collapse_scan_file, khugepaged_scan_mm_slot",
          "description": "hpage_collapse_scan_file 扫描文件映射的页面，统计符合条件的页面数量并决定是否执行合并操作，若存在大页已映射则提前返回。khugepaged_scan_mm_slot 遍历 mm 槽位，对每个 VMA 区间执行大页合并，根据扫描结果更新全局进度并管理 mm 槽位列表。",
          "similarity": 0.5219368934631348
        },
        {
          "chunk_id": 11,
          "file_path": "mm/khugepaged.c",
          "start_line": 1697,
          "end_line": 2207,
          "content": [
            "static void retract_page_tables(struct address_space *mapping, pgoff_t pgoff)",
            "{",
            "\tstruct vm_area_struct *vma;",
            "",
            "\ti_mmap_lock_read(mapping);",
            "\tvma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {",
            "\t\tstruct mmu_notifier_range range;",
            "\t\tstruct mm_struct *mm;",
            "\t\tunsigned long addr;",
            "\t\tpmd_t *pmd, pgt_pmd;",
            "\t\tspinlock_t *pml;",
            "\t\tspinlock_t *ptl;",
            "\t\tbool skipped_uffd = false;",
            "",
            "\t\t/*",
            "\t\t * Check vma->anon_vma to exclude MAP_PRIVATE mappings that",
            "\t\t * got written to. These VMAs are likely not worth removing",
            "\t\t * page tables from, as PMD-mapping is likely to be split later.",
            "\t\t */",
            "\t\tif (READ_ONCE(vma->anon_vma))",
            "\t\t\tcontinue;",
            "",
            "\t\taddr = vma->vm_start + ((pgoff - vma->vm_pgoff) << PAGE_SHIFT);",
            "\t\tif (addr & ~HPAGE_PMD_MASK ||",
            "\t\t    vma->vm_end < addr + HPAGE_PMD_SIZE)",
            "\t\t\tcontinue;",
            "",
            "\t\tmm = vma->vm_mm;",
            "\t\tif (find_pmd_or_thp_or_none(mm, addr, &pmd) != SCAN_SUCCEED)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (hpage_collapse_test_exit(mm))",
            "\t\t\tcontinue;",
            "\t\t/*",
            "\t\t * When a vma is registered with uffd-wp, we cannot recycle",
            "\t\t * the page table because there may be pte markers installed.",
            "\t\t * Other vmas can still have the same file mapped hugely, but",
            "\t\t * skip this one: it will always be mapped in small page size",
            "\t\t * for uffd-wp registered ranges.",
            "\t\t */",
            "\t\tif (userfaultfd_wp(vma))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* PTEs were notified when unmapped; but now for the PMD? */",
            "\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, mm,",
            "\t\t\t\t\taddr, addr + HPAGE_PMD_SIZE);",
            "\t\tmmu_notifier_invalidate_range_start(&range);",
            "",
            "\t\tpml = pmd_lock(mm, pmd);",
            "\t\tptl = pte_lockptr(mm, pmd);",
            "\t\tif (ptl != pml)",
            "\t\t\tspin_lock_nested(ptl, SINGLE_DEPTH_NESTING);",
            "",
            "\t\t/*",
            "\t\t * Huge page lock is still held, so normally the page table",
            "\t\t * must remain empty; and we have already skipped anon_vma",
            "\t\t * and userfaultfd_wp() vmas.  But since the mmap_lock is not",
            "\t\t * held, it is still possible for a racing userfaultfd_ioctl()",
            "\t\t * to have inserted ptes or markers.  Now that we hold ptlock,",
            "\t\t * repeating the anon_vma check protects from one category,",
            "\t\t * and repeating the userfaultfd_wp() check from another.",
            "\t\t */",
            "\t\tif (unlikely(vma->anon_vma || userfaultfd_wp(vma))) {",
            "\t\t\tskipped_uffd = true;",
            "\t\t} else {",
            "\t\t\tpgt_pmd = pmdp_collapse_flush(vma, addr, pmd);",
            "\t\t\tpmdp_get_lockless_sync();",
            "\t\t}",
            "",
            "\t\tif (ptl != pml)",
            "\t\t\tspin_unlock(ptl);",
            "\t\tspin_unlock(pml);",
            "",
            "\t\tmmu_notifier_invalidate_range_end(&range);",
            "",
            "\t\tif (!skipped_uffd) {",
            "\t\t\tmm_dec_nr_ptes(mm);",
            "\t\t\tpage_table_check_pte_clear_range(mm, addr, pgt_pmd);",
            "\t\t\tpte_free_defer(mm, pmd_pgtable(pgt_pmd));",
            "\t\t}",
            "\t}",
            "\ti_mmap_unlock_read(mapping);",
            "}",
            "static int collapse_file(struct mm_struct *mm, unsigned long addr,",
            "\t\t\t struct file *file, pgoff_t start,",
            "\t\t\t struct collapse_control *cc)",
            "{",
            "\tstruct address_space *mapping = file->f_mapping;",
            "\tstruct page *dst;",
            "\tstruct folio *folio, *tmp, *new_folio;",
            "\tpgoff_t index = 0, end = start + HPAGE_PMD_NR;",
            "\tLIST_HEAD(pagelist);",
            "\tXA_STATE_ORDER(xas, &mapping->i_pages, start, HPAGE_PMD_ORDER);",
            "\tint nr_none = 0, result = SCAN_SUCCEED;",
            "\tbool is_shmem = shmem_file(file);",
            "",
            "\tVM_BUG_ON(!IS_ENABLED(CONFIG_READ_ONLY_THP_FOR_FS) && !is_shmem);",
            "\tVM_BUG_ON(start & (HPAGE_PMD_NR - 1));",
            "",
            "\tresult = alloc_charge_folio(&new_folio, mm, cc);",
            "\tif (result != SCAN_SUCCEED)",
            "\t\tgoto out;",
            "",
            "\t__folio_set_locked(new_folio);",
            "\tif (is_shmem)",
            "\t\t__folio_set_swapbacked(new_folio);",
            "\tnew_folio->index = start;",
            "\tnew_folio->mapping = mapping;",
            "",
            "\t/*",
            "\t * Ensure we have slots for all the pages in the range.  This is",
            "\t * almost certainly a no-op because most of the pages must be present",
            "\t */",
            "\tdo {",
            "\t\txas_lock_irq(&xas);",
            "\t\txas_create_range(&xas);",
            "\t\tif (!xas_error(&xas))",
            "\t\t\tbreak;",
            "\t\txas_unlock_irq(&xas);",
            "\t\tif (!xas_nomem(&xas, GFP_KERNEL)) {",
            "\t\t\tresult = SCAN_FAIL;",
            "\t\t\tgoto rollback;",
            "\t\t}",
            "\t} while (1);",
            "",
            "\tfor (index = start; index < end;) {",
            "\t\txas_set(&xas, index);",
            "\t\tfolio = xas_load(&xas);",
            "",
            "\t\tVM_BUG_ON(index != xas.xa_index);",
            "\t\tif (is_shmem) {",
            "\t\t\tif (!folio) {",
            "\t\t\t\t/*",
            "\t\t\t\t * Stop if extent has been truncated or",
            "\t\t\t\t * hole-punched, and is now completely",
            "\t\t\t\t * empty.",
            "\t\t\t\t */",
            "\t\t\t\tif (index == start) {",
            "\t\t\t\t\tif (!xas_next_entry(&xas, end - 1)) {",
            "\t\t\t\t\t\tresult = SCAN_TRUNCATED;",
            "\t\t\t\t\t\tgoto xa_locked;",
            "\t\t\t\t\t}",
            "\t\t\t\t}",
            "\t\t\t\tnr_none++;",
            "\t\t\t\tindex++;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tif (xa_is_value(folio) || !folio_test_uptodate(folio)) {",
            "\t\t\t\txas_unlock_irq(&xas);",
            "\t\t\t\t/* swap in or instantiate fallocated page */",
            "\t\t\t\tif (shmem_get_folio(mapping->host, index, 0,",
            "\t\t\t\t\t\t&folio, SGP_NOALLOC)) {",
            "\t\t\t\t\tresult = SCAN_FAIL;",
            "\t\t\t\t\tgoto xa_unlocked;",
            "\t\t\t\t}",
            "\t\t\t\t/* drain lru cache to help isolate_lru_page() */",
            "\t\t\t\tlru_add_drain();",
            "\t\t\t} else if (folio_trylock(folio)) {",
            "\t\t\t\tfolio_get(folio);",
            "\t\t\t\txas_unlock_irq(&xas);",
            "\t\t\t} else {",
            "\t\t\t\tresult = SCAN_PAGE_LOCK;",
            "\t\t\t\tgoto xa_locked;",
            "\t\t\t}",
            "\t\t} else {\t/* !is_shmem */",
            "\t\t\tif (!folio || xa_is_value(folio)) {",
            "\t\t\t\txas_unlock_irq(&xas);",
            "\t\t\t\tpage_cache_sync_readahead(mapping, &file->f_ra,",
            "\t\t\t\t\t\t\t  file, index,",
            "\t\t\t\t\t\t\t  end - index);",
            "\t\t\t\t/* drain lru cache to help isolate_lru_page() */",
            "\t\t\t\tlru_add_drain();",
            "\t\t\t\tfolio = filemap_lock_folio(mapping, index);",
            "\t\t\t\tif (IS_ERR(folio)) {",
            "\t\t\t\t\tresult = SCAN_FAIL;",
            "\t\t\t\t\tgoto xa_unlocked;",
            "\t\t\t\t}",
            "\t\t\t} else if (folio_test_dirty(folio)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * khugepaged only works on read-only fd,",
            "\t\t\t\t * so this page is dirty because it hasn't",
            "\t\t\t\t * been flushed since first write. There",
            "\t\t\t\t * won't be new dirty pages.",
            "\t\t\t\t *",
            "\t\t\t\t * Trigger async flush here and hope the",
            "\t\t\t\t * writeback is done when khugepaged",
            "\t\t\t\t * revisits this page.",
            "\t\t\t\t *",
            "\t\t\t\t * This is a one-off situation. We are not",
            "\t\t\t\t * forcing writeback in loop.",
            "\t\t\t\t */",
            "\t\t\t\txas_unlock_irq(&xas);",
            "\t\t\t\tfilemap_flush(mapping);",
            "\t\t\t\tresult = SCAN_FAIL;",
            "\t\t\t\tgoto xa_unlocked;",
            "\t\t\t} else if (folio_test_writeback(folio)) {",
            "\t\t\t\txas_unlock_irq(&xas);",
            "\t\t\t\tresult = SCAN_FAIL;",
            "\t\t\t\tgoto xa_unlocked;",
            "\t\t\t} else if (folio_trylock(folio)) {",
            "\t\t\t\tfolio_get(folio);",
            "\t\t\t\txas_unlock_irq(&xas);",
            "\t\t\t} else {",
            "\t\t\t\tresult = SCAN_PAGE_LOCK;",
            "\t\t\t\tgoto xa_locked;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * The folio must be locked, so we can drop the i_pages lock",
            "\t\t * without racing with truncate.",
            "\t\t */",
            "\t\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "",
            "\t\t/* make sure the folio is up to date */",
            "\t\tif (unlikely(!folio_test_uptodate(folio))) {",
            "\t\t\tresult = SCAN_FAIL;",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If file was truncated then extended, or hole-punched, before",
            "\t\t * we locked the first folio, then a THP might be there already.",
            "\t\t * This will be discovered on the first iteration.",
            "\t\t */",
            "\t\tif (folio_order(folio) == HPAGE_PMD_ORDER &&",
            "\t\t    folio->index == start) {",
            "\t\t\t/* Maybe PMD-mapped */",
            "\t\t\tresult = SCAN_PTE_MAPPED_HUGEPAGE;",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "",
            "\t\tif (folio_mapping(folio) != mapping) {",
            "\t\t\tresult = SCAN_TRUNCATED;",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "",
            "\t\tif (!is_shmem && (folio_test_dirty(folio) ||",
            "\t\t\t\t  folio_test_writeback(folio))) {",
            "\t\t\t/*",
            "\t\t\t * khugepaged only works on read-only fd, so this",
            "\t\t\t * folio is dirty because it hasn't been flushed",
            "\t\t\t * since first write.",
            "\t\t\t */",
            "\t\t\tresult = SCAN_FAIL;",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "",
            "\t\tif (!folio_isolate_lru(folio)) {",
            "\t\t\tresult = SCAN_DEL_PAGE_LRU;",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "",
            "\t\tif (!filemap_release_folio(folio, GFP_KERNEL)) {",
            "\t\t\tresult = SCAN_PAGE_HAS_PRIVATE;",
            "\t\t\tfolio_putback_lru(folio);",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "",
            "\t\tif (folio_mapped(folio))",
            "\t\t\ttry_to_unmap(folio,",
            "\t\t\t\t\tTTU_IGNORE_MLOCK | TTU_BATCH_FLUSH);",
            "",
            "\t\txas_lock_irq(&xas);",
            "",
            "\t\tVM_BUG_ON_FOLIO(folio != xa_load(xas.xa, index), folio);",
            "",
            "\t\t/*",
            "\t\t * We control 2 + nr_pages references to the folio:",
            "\t\t *  - we hold a pin on it;",
            "\t\t *  - nr_pages reference from page cache;",
            "\t\t *  - one from lru_isolate_folio;",
            "\t\t * If those are the only references, then any new usage",
            "\t\t * of the folio will have to fetch it from the page",
            "\t\t * cache. That requires locking the folio to handle",
            "\t\t * truncate, so any new usage will be blocked until we",
            "\t\t * unlock folio after collapse/during rollback.",
            "\t\t */",
            "\t\tif (folio_ref_count(folio) != 2 + folio_nr_pages(folio)) {",
            "\t\t\tresult = SCAN_PAGE_COUNT;",
            "\t\t\txas_unlock_irq(&xas);",
            "\t\t\tfolio_putback_lru(folio);",
            "\t\t\tgoto out_unlock;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Accumulate the folios that are being collapsed.",
            "\t\t */",
            "\t\tlist_add_tail(&folio->lru, &pagelist);",
            "\t\tindex += folio_nr_pages(folio);",
            "\t\tcontinue;",
            "out_unlock:",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_put(folio);",
            "\t\tgoto xa_unlocked;",
            "\t}",
            "",
            "\tif (!is_shmem) {",
            "\t\tfilemap_nr_thps_inc(mapping);",
            "\t\t/*",
            "\t\t * Paired with smp_mb() in do_dentry_open() to ensure",
            "\t\t * i_writecount is up to date and the update to nr_thps is",
            "\t\t * visible. Ensures the page cache will be truncated if the",
            "\t\t * file is opened writable.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t\tif (inode_is_open_for_write(mapping->host)) {",
            "\t\t\tresult = SCAN_FAIL;",
            "\t\t\tfilemap_nr_thps_dec(mapping);",
            "\t\t}",
            "\t}",
            "",
            "xa_locked:",
            "\txas_unlock_irq(&xas);",
            "xa_unlocked:",
            "",
            "\t/*",
            "\t * If collapse is successful, flush must be done now before copying.",
            "\t * If collapse is unsuccessful, does flush actually need to be done?",
            "\t * Do it anyway, to clear the state.",
            "\t */",
            "\ttry_to_unmap_flush();",
            "",
            "\tif (result == SCAN_SUCCEED && nr_none &&",
            "\t    !shmem_charge(mapping->host, nr_none))",
            "\t\tresult = SCAN_FAIL;",
            "\tif (result != SCAN_SUCCEED) {",
            "\t\tnr_none = 0;",
            "\t\tgoto rollback;",
            "\t}",
            "",
            "\t/*",
            "\t * The old folios are locked, so they won't change anymore.",
            "\t */",
            "\tindex = start;",
            "\tdst = folio_page(new_folio, 0);",
            "\tlist_for_each_entry(folio, &pagelist, lru) {",
            "\t\tint i, nr_pages = folio_nr_pages(folio);",
            "",
            "\t\twhile (index < folio->index) {",
            "\t\t\tclear_highpage(dst);",
            "\t\t\tindex++;",
            "\t\t\tdst++;",
            "\t\t}",
            "",
            "\t\tfor (i = 0; i < nr_pages; i++) {",
            "\t\t\tif (copy_mc_highpage(dst, folio_page(folio, i))) {",
            "\t\t\t\tresult = SCAN_COPY_MC;",
            "\t\t\t\tgoto rollback;",
            "\t\t\t}",
            "\t\t\tindex++;",
            "\t\t\tdst++;",
            "\t\t}",
            "\t}",
            "\twhile (index < end) {",
            "\t\tclear_highpage(dst);",
            "\t\tindex++;",
            "\t\tdst++;",
            "\t}",
            "",
            "\tif (nr_none) {",
            "\t\tstruct vm_area_struct *vma;",
            "\t\tint nr_none_check = 0;",
            "",
            "\t\ti_mmap_lock_read(mapping);",
            "\t\txas_lock_irq(&xas);",
            "",
            "\t\txas_set(&xas, start);",
            "\t\tfor (index = start; index < end; index++) {",
            "\t\t\tif (!xas_next(&xas)) {",
            "\t\t\t\txas_store(&xas, XA_RETRY_ENTRY);",
            "\t\t\t\tif (xas_error(&xas)) {",
            "\t\t\t\t\tresult = SCAN_STORE_FAILED;",
            "\t\t\t\t\tgoto immap_locked;",
            "\t\t\t\t}",
            "\t\t\t\tnr_none_check++;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tif (nr_none != nr_none_check) {",
            "\t\t\tresult = SCAN_PAGE_FILLED;",
            "\t\t\tgoto immap_locked;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * If userspace observed a missing page in a VMA with",
            "\t\t * a MODE_MISSING userfaultfd, then it might expect a",
            "\t\t * UFFD_EVENT_PAGEFAULT for that page. If so, we need to",
            "\t\t * roll back to avoid suppressing such an event. Since",
            "\t\t * wp/minor userfaultfds don't give userspace any",
            "\t\t * guarantees that the kernel doesn't fill a missing",
            "\t\t * page with a zero page, so they don't matter here.",
            "\t\t *",
            "\t\t * Any userfaultfds registered after this point will",
            "\t\t * not be able to observe any missing pages due to the",
            "\t\t * previously inserted retry entries.",
            "\t\t */",
            "\t\tvma_interval_tree_foreach(vma, &mapping->i_mmap, start, end) {",
            "\t\t\tif (userfaultfd_missing(vma)) {",
            "\t\t\t\tresult = SCAN_EXCEED_NONE_PTE;",
            "\t\t\t\tgoto immap_locked;",
            "\t\t\t}",
            "\t\t}",
            "",
            "immap_locked:",
            "\t\ti_mmap_unlock_read(mapping);",
            "\t\tif (result != SCAN_SUCCEED) {",
            "\t\t\txas_set(&xas, start);",
            "\t\t\tfor (index = start; index < end; index++) {",
            "\t\t\t\tif (xas_next(&xas) == XA_RETRY_ENTRY)",
            "\t\t\t\t\txas_store(&xas, NULL);",
            "\t\t\t}",
            "",
            "\t\t\txas_unlock_irq(&xas);",
            "\t\t\tgoto rollback;",
            "\t\t}",
            "\t} else {",
            "\t\txas_lock_irq(&xas);",
            "\t}",
            "",
            "\tif (is_shmem)",
            "\t\t__lruvec_stat_mod_folio(new_folio, NR_SHMEM_THPS, HPAGE_PMD_NR);",
            "\telse",
            "\t\t__lruvec_stat_mod_folio(new_folio, NR_FILE_THPS, HPAGE_PMD_NR);",
            "",
            "\tif (nr_none) {",
            "\t\t__lruvec_stat_mod_folio(new_folio, NR_FILE_PAGES, nr_none);",
            "\t\t/* nr_none is always 0 for non-shmem. */",
            "\t\t__lruvec_stat_mod_folio(new_folio, NR_SHMEM, nr_none);",
            "\t}",
            "",
            "\t/*",
            "\t * Mark new_folio as uptodate before inserting it into the",
            "\t * page cache so that it isn't mistaken for an fallocated but",
            "\t * unwritten page.",
            "\t */",
            "\tfolio_mark_uptodate(new_folio);",
            "\tfolio_ref_add(new_folio, HPAGE_PMD_NR - 1);",
            "",
            "\tif (is_shmem)",
            "\t\tfolio_mark_dirty(new_folio);",
            "\tfolio_add_lru(new_folio);",
            "",
            "\t/* Join all the small entries into a single multi-index entry. */",
            "\txas_set_order(&xas, start, HPAGE_PMD_ORDER);",
            "\txas_store(&xas, new_folio);",
            "\tWARN_ON_ONCE(xas_error(&xas));",
            "\txas_unlock_irq(&xas);",
            "",
            "\t/*",
            "\t * Remove pte page tables, so we can re-fault the page as huge.",
            "\t * If MADV_COLLAPSE, adjust result to call collapse_pte_mapped_thp().",
            "\t */",
            "\tretract_page_tables(mapping, start);",
            "\tif (cc && !cc->is_khugepaged)",
            "\t\tresult = SCAN_PTE_MAPPED_HUGEPAGE;",
            "\tfolio_unlock(new_folio);",
            "",
            "\t/*",
            "\t * The collapse has succeeded, so free the old folios.",
            "\t */",
            "\tlist_for_each_entry_safe(folio, tmp, &pagelist, lru) {",
            "\t\tlist_del(&folio->lru);",
            "\t\tfolio->mapping = NULL;",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_clear_unevictable(folio);",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_put_refs(folio, 2 + folio_nr_pages(folio));",
            "\t}",
            "",
            "\tgoto out;",
            "",
            "rollback:",
            "\t/* Something went wrong: roll back page cache changes */",
            "\tif (nr_none) {",
            "\t\txas_lock_irq(&xas);",
            "\t\tmapping->nrpages -= nr_none;",
            "\t\txas_unlock_irq(&xas);",
            "\t\tshmem_uncharge(mapping->host, nr_none);",
            "\t}",
            "",
            "\tlist_for_each_entry_safe(folio, tmp, &pagelist, lru) {",
            "\t\tlist_del(&folio->lru);",
            "\t\tfolio_unlock(folio);",
            "\t\tfolio_putback_lru(folio);",
            "\t\tfolio_put(folio);",
            "\t}",
            "\t/*",
            "\t * Undo the updates of filemap_nr_thps_inc for non-SHMEM",
            "\t * file only. This undo is not needed unless failure is",
            "\t * due to SCAN_COPY_MC.",
            "\t */",
            "\tif (!is_shmem && result == SCAN_COPY_MC) {",
            "\t\tfilemap_nr_thps_dec(mapping);",
            "\t\t/*",
            "\t\t * Paired with smp_mb() in do_dentry_open() to",
            "\t\t * ensure the update to nr_thps is visible.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t}",
            "",
            "\tnew_folio->mapping = NULL;",
            "",
            "\tfolio_unlock(new_folio);",
            "\tfolio_put(new_folio);",
            "out:",
            "\tVM_BUG_ON(!list_empty(&pagelist));",
            "\ttrace_mm_khugepaged_collapse_file(mm, new_folio, index, addr, is_shmem, file, HPAGE_PMD_NR, result);",
            "\treturn result;",
            "}"
          ],
          "function_name": "retract_page_tables, collapse_file",
          "description": "retract_page_tables 函数回收指定文件映射的页表项，跳过匿名和用户故障页的情况，清空页表并释放资源。collapse_file 函数将文件中的一组小页合并为大页，包括加载页面、校验数据完整性、复制内容、调整统计计数器等，处理文件截断、脏页等异常情况。",
          "similarity": 0.5178864598274231
        },
        {
          "chunk_id": 0,
          "file_path": "mm/khugepaged.c",
          "start_line": 1,
          "end_line": 139,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt",
            "",
            "#include <linux/mm.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/mm.h>",
            "#include <linux/sched/coredump.h>",
            "#include <linux/mmu_notifier.h>",
            "#include <linux/rmap.h>",
            "#include <linux/swap.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/kthread.h>",
            "#include <linux/khugepaged.h>",
            "#include <linux/freezer.h>",
            "#include <linux/mman.h>",
            "#include <linux/hashtable.h>",
            "#include <linux/userfaultfd_k.h>",
            "#include <linux/page_idle.h>",
            "#include <linux/page_table_check.h>",
            "#include <linux/rcupdate_wait.h>",
            "#include <linux/swapops.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/ksm.h>",
            "",
            "#include <asm/tlb.h>",
            "#include <asm/pgalloc.h>",
            "#include \"internal.h\"",
            "#include \"mm_slot.h\"",
            "",
            "enum scan_result {",
            "\tSCAN_FAIL,",
            "\tSCAN_SUCCEED,",
            "\tSCAN_PMD_NULL,",
            "\tSCAN_PMD_NONE,",
            "\tSCAN_PMD_MAPPED,",
            "\tSCAN_EXCEED_NONE_PTE,",
            "\tSCAN_EXCEED_SWAP_PTE,",
            "\tSCAN_EXCEED_SHARED_PTE,",
            "\tSCAN_PTE_NON_PRESENT,",
            "\tSCAN_PTE_UFFD_WP,",
            "\tSCAN_PTE_MAPPED_HUGEPAGE,",
            "\tSCAN_PAGE_RO,",
            "\tSCAN_LACK_REFERENCED_PAGE,",
            "\tSCAN_PAGE_NULL,",
            "\tSCAN_SCAN_ABORT,",
            "\tSCAN_PAGE_COUNT,",
            "\tSCAN_PAGE_LRU,",
            "\tSCAN_PAGE_LOCK,",
            "\tSCAN_PAGE_ANON,",
            "\tSCAN_PAGE_COMPOUND,",
            "\tSCAN_ANY_PROCESS,",
            "\tSCAN_VMA_NULL,",
            "\tSCAN_VMA_CHECK,",
            "\tSCAN_ADDRESS_RANGE,",
            "\tSCAN_DEL_PAGE_LRU,",
            "\tSCAN_ALLOC_HUGE_PAGE_FAIL,",
            "\tSCAN_CGROUP_CHARGE_FAIL,",
            "\tSCAN_TRUNCATED,",
            "\tSCAN_PAGE_HAS_PRIVATE,",
            "\tSCAN_STORE_FAILED,",
            "\tSCAN_COPY_MC,",
            "\tSCAN_PAGE_FILLED,",
            "};",
            "",
            "#define CREATE_TRACE_POINTS",
            "#include <trace/events/huge_memory.h>",
            "",
            "static struct task_struct *khugepaged_thread __read_mostly;",
            "static DEFINE_MUTEX(khugepaged_mutex);",
            "",
            "/* default scan 8*512 pte (or vmas) every 30 second */",
            "static unsigned int khugepaged_pages_to_scan __read_mostly;",
            "static unsigned int khugepaged_pages_collapsed;",
            "static unsigned int khugepaged_full_scans;",
            "static unsigned int khugepaged_scan_sleep_millisecs __read_mostly = 10000;",
            "/* during fragmentation poll the hugepage allocator once every minute */",
            "static unsigned int khugepaged_alloc_sleep_millisecs __read_mostly = 60000;",
            "static unsigned long khugepaged_sleep_expire;",
            "static DEFINE_SPINLOCK(khugepaged_mm_lock);",
            "static DECLARE_WAIT_QUEUE_HEAD(khugepaged_wait);",
            "/*",
            " * default collapse hugepages if there is at least one pte mapped like",
            " * it would have happened if the vma was large enough during page",
            " * fault.",
            " *",
            " * Note that these are only respected if collapse was initiated by khugepaged.",
            " */",
            "static unsigned int khugepaged_max_ptes_none __read_mostly;",
            "static unsigned int khugepaged_max_ptes_swap __read_mostly;",
            "static unsigned int khugepaged_max_ptes_shared __read_mostly;",
            "",
            "#define MM_SLOTS_HASH_BITS 10",
            "static DEFINE_READ_MOSTLY_HASHTABLE(mm_slots_hash, MM_SLOTS_HASH_BITS);",
            "",
            "static struct kmem_cache *mm_slot_cache __ro_after_init;",
            "",
            "struct collapse_control {",
            "\tbool is_khugepaged;",
            "",
            "\t/* Num pages scanned per node */",
            "\tu32 node_load[MAX_NUMNODES];",
            "",
            "\t/* nodemask for allocation fallback */",
            "\tnodemask_t alloc_nmask;",
            "};",
            "",
            "/**",
            " * struct khugepaged_mm_slot - khugepaged information per mm that is being scanned",
            " * @slot: hash lookup from mm to mm_slot",
            " */",
            "struct khugepaged_mm_slot {",
            "\tstruct mm_slot slot;",
            "};",
            "",
            "/**",
            " * struct khugepaged_scan - cursor for scanning",
            " * @mm_head: the head of the mm list to scan",
            " * @mm_slot: the current mm_slot we are scanning",
            " * @address: the next address inside that to be scanned",
            " *",
            " * There is only the one khugepaged_scan instance of this cursor structure.",
            " */",
            "struct khugepaged_scan {",
            "\tstruct list_head mm_head;",
            "\tstruct khugepaged_mm_slot *mm_slot;",
            "\tunsigned long address;",
            "};",
            "",
            "static struct khugepaged_scan khugepaged_scan = {",
            "\t.mm_head = LIST_HEAD_INIT(khugepaged_scan.mm_head),",
            "};",
            "",
            "/* khugepaged should scan or not, trying to be thp, default as false */",
            "static unsigned int khugepaged_thp_scan_state;",
            "/* khugepaged adopt feature default set as false */",
            "static unsigned int khugepaged_adapt_enable;",
            "static void set_recommended_min_free_kbytes(void);",
            "",
            "#ifdef CONFIG_SYSFS"
          ],
          "function_name": null,
          "description": "定义khugepaged模块的头文件和全局变量，包括枚举常量、睡眠时间参数、哈希表及用于管理内存槽的结构体和缓存，为后续的大页合并逻辑提供基础配置和数据结构支持。",
          "similarity": 0.5139815807342529
        },
        {
          "chunk_id": 5,
          "file_path": "mm/khugepaged.c",
          "start_line": 715,
          "end_line": 826,
          "content": [
            "static void __collapse_huge_page_copy_succeeded(pte_t *pte,",
            "\t\t\t\t\t\tstruct vm_area_struct *vma,",
            "\t\t\t\t\t\tunsigned long address,",
            "\t\t\t\t\t\tspinlock_t *ptl,",
            "\t\t\t\t\t\tstruct list_head *compound_pagelist)",
            "{",
            "\tstruct folio *src, *tmp;",
            "\tpte_t *_pte;",
            "\tpte_t pteval;",
            "",
            "\tfor (_pte = pte; _pte < pte + HPAGE_PMD_NR;",
            "\t     _pte++, address += PAGE_SIZE) {",
            "\t\tpteval = ptep_get(_pte);",
            "\t\tif (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {",
            "\t\t\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, 1);",
            "\t\t\tif (is_zero_pfn(pte_pfn(pteval))) {",
            "\t\t\t\t/*",
            "\t\t\t\t * ptl mostly unnecessary.",
            "\t\t\t\t */",
            "\t\t\t\tspin_lock(ptl);",
            "\t\t\t\tptep_clear(vma->vm_mm, address, _pte);",
            "\t\t\t\tspin_unlock(ptl);",
            "\t\t\t\tksm_might_unmap_zero_page(vma->vm_mm, pteval);",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tstruct page *src_page = pte_page(pteval);",
            "",
            "\t\t\tsrc = page_folio(src_page);",
            "\t\t\tif (!folio_test_large(src))",
            "\t\t\t\trelease_pte_folio(src);",
            "\t\t\t/*",
            "\t\t\t * ptl mostly unnecessary, but preempt has to",
            "\t\t\t * be disabled to update the per-cpu stats",
            "\t\t\t * inside folio_remove_rmap_pte().",
            "\t\t\t */",
            "\t\t\tspin_lock(ptl);",
            "\t\t\tptep_clear(vma->vm_mm, address, _pte);",
            "\t\t\tfolio_remove_rmap_pte(src, src_page, vma);",
            "\t\t\tspin_unlock(ptl);",
            "\t\t\tfree_page_and_swap_cache(src_page);",
            "\t\t}",
            "\t}",
            "",
            "\tlist_for_each_entry_safe(src, tmp, compound_pagelist, lru) {",
            "\t\tlist_del(&src->lru);",
            "\t\tnode_stat_sub_folio(src, NR_ISOLATED_ANON +",
            "\t\t\t\tfolio_is_file_lru(src));",
            "\t\tfolio_unlock(src);",
            "\t\tfree_swap_cache(src);",
            "\t\tfolio_putback_lru(src);",
            "\t}",
            "}",
            "static void __collapse_huge_page_copy_failed(pte_t *pte,",
            "\t\t\t\t\t     pmd_t *pmd,",
            "\t\t\t\t\t     pmd_t orig_pmd,",
            "\t\t\t\t\t     struct vm_area_struct *vma,",
            "\t\t\t\t\t     struct list_head *compound_pagelist)",
            "{",
            "\tspinlock_t *pmd_ptl;",
            "",
            "\t/*",
            "\t * Re-establish the PMD to point to the original page table",
            "\t * entry. Restoring PMD needs to be done prior to releasing",
            "\t * pages. Since pages are still isolated and locked here,",
            "\t * acquiring anon_vma_lock_write is unnecessary.",
            "\t */",
            "\tpmd_ptl = pmd_lock(vma->vm_mm, pmd);",
            "\tpmd_populate(vma->vm_mm, pmd, pmd_pgtable(orig_pmd));",
            "\tspin_unlock(pmd_ptl);",
            "\t/*",
            "\t * Release both raw and compound pages isolated",
            "\t * in __collapse_huge_page_isolate.",
            "\t */",
            "\trelease_pte_pages(pte, pte + HPAGE_PMD_NR, compound_pagelist);",
            "}",
            "static int __collapse_huge_page_copy(pte_t *pte, struct folio *folio,",
            "\t\tpmd_t *pmd, pmd_t orig_pmd, struct vm_area_struct *vma,",
            "\t\tunsigned long address, spinlock_t *ptl,",
            "\t\tstruct list_head *compound_pagelist)",
            "{",
            "\tunsigned int i;",
            "\tint result = SCAN_SUCCEED;",
            "",
            "\t/*",
            "\t * Copying pages' contents is subject to memory poison at any iteration.",
            "\t */",
            "\tfor (i = 0; i < HPAGE_PMD_NR; i++) {",
            "\t\tpte_t pteval = ptep_get(pte + i);",
            "\t\tstruct page *page = folio_page(folio, i);",
            "\t\tunsigned long src_addr = address + i * PAGE_SIZE;",
            "\t\tstruct page *src_page;",
            "",
            "\t\tif (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {",
            "\t\t\tclear_user_highpage(page, src_addr);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tsrc_page = pte_page(pteval);",
            "\t\tif (copy_mc_user_highpage(page, src_page, src_addr, vma)) {",
            "\t\t\tresult = SCAN_COPY_MC;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tif (likely(result == SCAN_SUCCEED))",
            "\t\t__collapse_huge_page_copy_succeeded(pte, vma, address, ptl,",
            "\t\t\t\t\t\t    compound_pagelist);",
            "\telse",
            "\t\t__collapse_huge_page_copy_failed(pte, pmd, orig_pmd, vma,",
            "\t\t\t\t\t\t compound_pagelist);",
            "",
            "\treturn result;",
            "}"
          ],
          "function_name": "__collapse_huge_page_copy_succeeded, __collapse_huge_page_copy_failed, __collapse_huge_page_copy",
          "description": "处理将多个小页合并为大页后的成功与失败路径，成功时释放源页面并更新统计，失败时恢复原页表项",
          "similarity": 0.5130655765533447
        }
      ]
    },
    {
      "source_file": "mm/page_alloc.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:59:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `page_alloc.c`\n\n---\n\n# page_alloc.c 技术文档\n\n## 1. 文件概述\n\n`page_alloc.c` 是 Linux 内核内存管理子系统的核心文件之一，负责物理页面的分配与释放。该文件实现了基于区域（zone）和迁移类型（migratetype）的伙伴系统（Buddy System）内存分配器，管理系统的空闲页链表，并提供高效的页面分配/回收机制。它不处理小对象分配（由 slab/slub/slob 子系统负责），而是专注于以页为单位的大块物理内存管理。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct per_cpu_pages`**：每个 CPU 的每区（per-zone）页面缓存，用于减少锁竞争，提升分配性能。\n- **`node_states[NR_NODE_STATES]`**：全局节点状态掩码数组，跟踪各 NUMA 节点的状态（如在线、有内存等）。\n- **`sysctl_lowmem_reserve_ratio[MAX_NR_ZONES]`**：各内存区域的低内存保留比例，防止高优先级区域耗尽低优先级区域的内存。\n- **`zone_names[]` 和 `migratetype_names[]`**：内存区域和页面迁移类型的名称字符串，用于调试和日志。\n- **`gfp_allowed_mask`**：全局 GFP（Get Free Page）标志掩码，控制启动早期可使用的分配标志。\n\n### 主要函数（部分声明）\n- **`__free_pages_ok()`**：内部页面释放函数，执行实际的伙伴系统合并与链表插入逻辑。\n- 各种页面分配函数（如 `alloc_pages()`、`__alloc_pages()` 等，定义在其他位置但在此文件中实现核心逻辑）。\n- 每 CPU 页面列表操作辅助宏（如 `pcp_spin_lock()`、`pcp_spin_trylock()`）。\n\n### 关键常量与标志\n- **`fpi_t` 类型及标志**：\n  - `FPI_NONE`：无特殊要求。\n  - `FPI_SKIP_REPORT_NOTIFY`：跳过空闲页报告通知。\n  - `FPI_TO_TAIL`：将页面放回空闲链表尾部（用于优化场景如内存热插拔）。\n- **`min_free_kbytes`**：系统保留的最小空闲内存（KB），影响水位线计算。\n\n## 3. 关键实现\n\n### 每 CPU 页面缓存（Per-CPU Page Caching）\n- 通过 `struct per_cpu_pages` 为每个 CPU 维护热/冷页列表，避免频繁访问全局 zone 锁。\n- 使用 `pcpu_spin_lock` 宏族安全地访问每 CPU 数据，结合 `preempt_disable()`（非 RT）或 `migrate_disable()`（RT）防止任务迁移导致访问错误 CPU 的数据。\n- 在 UP 系统上，使用 IRQ 关闭防止重入；在 SMP/RT 系统上依赖自旋锁语义。\n\n### 内存区域（Zone）与 NUMA 支持\n- 支持多种内存区域（DMA、DMA32、Normal、HighMem、Movable、Device），通过 `zone_names` 标识。\n- 实现 `lowmem_reserve_ratio` 机制，确保高区域分配不会耗尽低区域的保留内存（如 ZONE_DMA 为设备保留）。\n- 通过 `node_states` 和 per-CPU 变量（如 `numa_node`、`_numa_mem_`）支持 NUMA 和无内存节点架构。\n\n### 空闲页管理优化\n- **`FPI_TO_TAIL` 标志**：允许将页面放回空闲链表尾部，配合内存打乱（shuffle）或热插拔时批量初始化。\n- **`FPI_SKIP_REPORT_NOTIFY` 标志**：在临时取出并归还页面时不触发空闲页报告机制，减少开销。\n- **水位线与保留内存**：`min_free_kbytes` 控制最低水位，影响 OOM（Out-Of-Memory）决策和内存回收行为。\n\n### 实时内核（PREEMPT_RT）适配\n- 在 RT 内核中使用 `migrate_disable()` 替代 `preempt_disable()`，避免干扰 RT 自旋锁的优先级继承机制。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/highmem.h>`, `\"internal.h\"`\n- **同步机制**：`<linux/spinlock.h>`（隐含）、`<linux/mutex.h>`\n- **NUMA 与拓扑**：`<linux/topology.h>`, `<linux/nodemask.h>`\n- **调试与追踪**：`<linux/kasan.h>`, `<trace/events/kmem.h>`, `<linux/page_owner.h>`\n- **高级特性**：`<linux/compaction.h>`, `<linux/migrate.h>`, `<linux/memcontrol.h>`\n\n### 子系统交互\n- **Slab 分配器**：本文件不处理 kmalloc，由 `slab.c` 等负责。\n- **内存回收**：与 `vmscan.c` 协同，通过水位线触发 reclaim。\n- **内存热插拔**：通过 `memory_hotplug.h` 接口管理动态内存。\n- **OOM Killer**：通过 `oom.h` 和水位线机制触发 OOM。\n- **透明大页（THP）**：与 `khugepaged` 协同进行大页分配。\n\n## 5. 使用场景\n\n- **内核内存分配**：所有以页为单位的内核内存请求（如 `alloc_pages()`）最终由本文件处理。\n- **用户空间缺页处理**：匿名页、文件页的物理页分配。\n- **内存映射（mmap）**：大块物理内存的分配与管理。\n- **内存回收与迁移**：页面回收、压缩（compaction）、迁移（migration）过程中涉及的页面释放与重新分配。\n- **系统启动与热插拔**：初始化内存区域、处理动态添加/移除内存。\n- **实时系统**：在 PREEMPT_RT 内核中提供低延迟的页面分配路径。\n- **调试与监控**：通过 page owner、KASAN、tracepoint 等机制提供内存使用追踪。",
      "similarity": 0.572314977645874,
      "chunks": [
        {
          "chunk_id": 16,
          "file_path": "mm/page_alloc.c",
          "start_line": 2768,
          "end_line": 2902,
          "content": [
            "void split_page(struct page *page, unsigned int order)",
            "{",
            "\tint i;",
            "",
            "\tVM_BUG_ON_PAGE(PageCompound(page), page);",
            "\tVM_BUG_ON_PAGE(!page_count(page), page);",
            "",
            "\tfor (i = 1; i < (1 << order); i++)",
            "\t\tset_page_refcounted(page + i);",
            "\tsplit_page_owner(page, order, 0);",
            "\tpgalloc_tag_split(page_folio(page), order, 0);",
            "\tsplit_page_memcg(page, order, 0);",
            "}",
            "int __isolate_free_page(struct page *page, unsigned int order)",
            "{",
            "\tstruct zone *zone = page_zone(page);",
            "\tint mt = get_pageblock_migratetype(page);",
            "",
            "\tif (!is_migrate_isolate(mt)) {",
            "\t\tunsigned long watermark;",
            "\t\t/*",
            "\t\t * Obey watermarks as if the page was being allocated. We can",
            "\t\t * emulate a high-order watermark check with a raised order-0",
            "\t\t * watermark, because we already know our high-order page",
            "\t\t * exists.",
            "\t\t */",
            "\t\twatermark = zone->_watermark[WMARK_MIN] + (1UL << order);",
            "\t\tif (!zone_watermark_ok(zone, 0, watermark, 0, ALLOC_CMA))",
            "\t\t\treturn 0;",
            "\t}",
            "",
            "\tdel_page_from_free_list(page, zone, order, mt);",
            "",
            "\t/*",
            "\t * Set the pageblock if the isolated page is at least half of a",
            "\t * pageblock",
            "\t */",
            "\tif (order >= pageblock_order - 1) {",
            "\t\tstruct page *endpage = page + (1 << order) - 1;",
            "\t\tfor (; page < endpage; page += pageblock_nr_pages) {",
            "\t\t\tint mt = get_pageblock_migratetype(page);",
            "\t\t\t/*",
            "\t\t\t * Only change normal pageblocks (i.e., they can merge",
            "\t\t\t * with others)",
            "\t\t\t */",
            "\t\t\tif (migratetype_is_mergeable(mt))",
            "\t\t\t\tmove_freepages_block(zone, page, mt,",
            "\t\t\t\t\t\t     MIGRATE_MOVABLE);",
            "\t\t}",
            "\t}",
            "",
            "\treturn 1UL << order;",
            "}",
            "void __putback_isolated_page(struct page *page, unsigned int order, int mt)",
            "{",
            "\tstruct zone *zone = page_zone(page);",
            "",
            "\t/* zone lock should be held when this function is called */",
            "\tlockdep_assert_held(&zone->lock);",
            "",
            "\t/* Return isolated page to tail of freelist. */",
            "\t__free_one_page(page, page_to_pfn(page), zone, order, mt,",
            "\t\t\tFPI_SKIP_REPORT_NOTIFY | FPI_TO_TAIL);",
            "}",
            "static inline void zone_statistics(struct zone *preferred_zone, struct zone *z,",
            "\t\t\t\t   long nr_account)",
            "{",
            "#ifdef CONFIG_NUMA",
            "\tenum numa_stat_item local_stat = NUMA_LOCAL;",
            "",
            "\t/* skip numa counters update if numa stats is disabled */",
            "\tif (!static_branch_likely(&vm_numa_stat_key))",
            "\t\treturn;",
            "",
            "\tif (zone_to_nid(z) != numa_node_id())",
            "\t\tlocal_stat = NUMA_OTHER;",
            "",
            "\tif (zone_to_nid(z) == zone_to_nid(preferred_zone))",
            "\t\t__count_numa_events(z, NUMA_HIT, nr_account);",
            "\telse {",
            "\t\t__count_numa_events(z, NUMA_MISS, nr_account);",
            "\t\t__count_numa_events(preferred_zone, NUMA_FOREIGN, nr_account);",
            "\t}",
            "\t__count_numa_events(z, local_stat, nr_account);",
            "#endif",
            "}",
            "static int nr_pcp_alloc(struct per_cpu_pages *pcp, struct zone *zone, int order)",
            "{",
            "\tint high, base_batch, batch, max_nr_alloc;",
            "\tint high_max, high_min;",
            "",
            "\tbase_batch = READ_ONCE(pcp->batch);",
            "\thigh_min = READ_ONCE(pcp->high_min);",
            "\thigh_max = READ_ONCE(pcp->high_max);",
            "\thigh = pcp->high = clamp(pcp->high, high_min, high_max);",
            "",
            "\t/* Check for PCP disabled or boot pageset */",
            "\tif (unlikely(high < base_batch))",
            "\t\treturn 1;",
            "",
            "\tif (order)",
            "\t\tbatch = base_batch;",
            "\telse",
            "\t\tbatch = (base_batch << pcp->alloc_factor);",
            "",
            "\t/*",
            "\t * If we had larger pcp->high, we could avoid to allocate from",
            "\t * zone.",
            "\t */",
            "\tif (high_min != high_max && !test_bit(ZONE_BELOW_HIGH, &zone->flags))",
            "\t\thigh = pcp->high = min(high + batch, high_max);",
            "",
            "\tif (!order) {",
            "\t\tmax_nr_alloc = max(high - pcp->count - base_batch, base_batch);",
            "\t\t/*",
            "\t\t * Double the number of pages allocated each time there is",
            "\t\t * subsequent allocation of order-0 pages without any freeing.",
            "\t\t */",
            "\t\tif (batch <= max_nr_alloc &&",
            "\t\t    pcp->alloc_factor < CONFIG_PCP_BATCH_SCALE_MAX)",
            "\t\t\tpcp->alloc_factor++;",
            "\t\tbatch = min(batch, max_nr_alloc);",
            "\t}",
            "",
            "\t/*",
            "\t * Scale batch relative to order if batch implies free pages",
            "\t * can be stored on the PCP. Batch can be 1 for small zones or",
            "\t * for boot pagesets which should never store free pages as",
            "\t * the pages may belong to arbitrary zones.",
            "\t */",
            "\tif (batch > 1)",
            "\t\tbatch = max(batch >> order, 2);",
            "",
            "\treturn batch;",
            "}"
          ],
          "function_name": "split_page, __isolate_free_page, __putback_isolated_page, zone_statistics, nr_pcp_alloc",
          "description": "split_page 分裂复合页面为独立页；__isolate_free_page 将隔离页从自由列表移除并调整页块类型；__putback_isolated_page 将隔离页重新放入自由列表尾部；nr_pcp_alloc 计算 PCP 列表可容纳的页面数量，动态调整批次大小以优化内存分配效率。",
          "similarity": 0.5734192132949829
        },
        {
          "chunk_id": 23,
          "file_path": "mm/page_alloc.c",
          "start_line": 4471,
          "end_line": 4660,
          "content": [
            "static inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order,",
            "\t\tint preferred_nid, nodemask_t *nodemask,",
            "\t\tstruct alloc_context *ac, gfp_t *alloc_gfp,",
            "\t\tunsigned int *alloc_flags)",
            "{",
            "\tac->highest_zoneidx = gfp_zone(gfp_mask);",
            "\tac->zonelist = node_zonelist(preferred_nid, gfp_mask);",
            "\tac->nodemask = nodemask;",
            "\tac->migratetype = gfp_migratetype(gfp_mask);",
            "",
            "\tif (cpusets_enabled()) {",
            "\t\t*alloc_gfp |= __GFP_HARDWALL;",
            "\t\t/*",
            "\t\t * When we are in the interrupt context, it is irrelevant",
            "\t\t * to the current task context. It means that any node ok.",
            "\t\t */",
            "\t\tif (in_task() && !ac->nodemask)",
            "\t\t\tac->nodemask = &cpuset_current_mems_allowed;",
            "\t\telse",
            "\t\t\t*alloc_flags |= ALLOC_CPUSET;",
            "\t}",
            "",
            "\tmight_alloc(gfp_mask);",
            "",
            "\tif (should_fail_alloc_page(gfp_mask, order))",
            "\t\treturn false;",
            "",
            "\t*alloc_flags = gfp_to_alloc_flags_cma(gfp_mask, *alloc_flags);",
            "",
            "\t/* Dirty zone balancing only done in the fast path */",
            "\tac->spread_dirty_pages = (gfp_mask & __GFP_WRITE);",
            "",
            "\t/*",
            "\t * The preferred zone is used for statistics but crucially it is",
            "\t * also used as the starting point for the zonelist iterator. It",
            "\t * may get reset for allocations that ignore memory policies.",
            "\t */",
            "\tac->preferred_zoneref = first_zones_zonelist(ac->zonelist,",
            "\t\t\t\t\tac->highest_zoneidx, ac->nodemask);",
            "",
            "\treturn true;",
            "}",
            "unsigned long alloc_pages_bulk_noprof(gfp_t gfp, int preferred_nid,",
            "\t\t\tnodemask_t *nodemask, int nr_pages,",
            "\t\t\tstruct list_head *page_list,",
            "\t\t\tstruct page **page_array)",
            "{",
            "\tstruct page *page;",
            "\tunsigned long __maybe_unused UP_flags;",
            "\tstruct zone *zone;",
            "\tstruct zoneref *z;",
            "\tstruct per_cpu_pages *pcp;",
            "\tstruct list_head *pcp_list;",
            "\tstruct alloc_context ac;",
            "\tgfp_t alloc_gfp;",
            "\tunsigned int alloc_flags = ALLOC_WMARK_LOW;",
            "\tint nr_populated = 0, nr_account = 0;",
            "",
            "\t/*",
            "\t * Skip populated array elements to determine if any pages need",
            "\t * to be allocated before disabling IRQs.",
            "\t */",
            "\twhile (page_array && nr_populated < nr_pages && page_array[nr_populated])",
            "\t\tnr_populated++;",
            "",
            "\t/* No pages requested? */",
            "\tif (unlikely(nr_pages <= 0))",
            "\t\tgoto out;",
            "",
            "\t/* Already populated array? */",
            "\tif (unlikely(page_array && nr_pages - nr_populated == 0))",
            "\t\tgoto out;",
            "",
            "\t/* Bulk allocator does not support memcg accounting. */",
            "\tif (memcg_kmem_online() && (gfp & __GFP_ACCOUNT))",
            "\t\tgoto failed;",
            "",
            "\t/* Use the single page allocator for one page. */",
            "\tif (nr_pages - nr_populated == 1)",
            "\t\tgoto failed;",
            "",
            "#ifdef CONFIG_PAGE_OWNER",
            "\t/*",
            "\t * PAGE_OWNER may recurse into the allocator to allocate space to",
            "\t * save the stack with pagesets.lock held. Releasing/reacquiring",
            "\t * removes much of the performance benefit of bulk allocation so",
            "\t * force the caller to allocate one page at a time as it'll have",
            "\t * similar performance to added complexity to the bulk allocator.",
            "\t */",
            "\tif (static_branch_unlikely(&page_owner_inited))",
            "\t\tgoto failed;",
            "#endif",
            "",
            "\t/* May set ALLOC_NOFRAGMENT, fragmentation will return 1 page. */",
            "\tgfp &= gfp_allowed_mask;",
            "\talloc_gfp = gfp;",
            "\tif (!prepare_alloc_pages(gfp, 0, preferred_nid, nodemask, &ac, &alloc_gfp, &alloc_flags))",
            "\t\tgoto out;",
            "\tgfp = alloc_gfp;",
            "",
            "\t/* Find an allowed local zone that meets the low watermark. */",
            "\tz = ac.preferred_zoneref;",
            "\tfor_next_zone_zonelist_nodemask(zone, z, ac.highest_zoneidx, ac.nodemask) {",
            "\t\tunsigned long mark;",
            "",
            "\t\tif (cpusets_enabled() && (alloc_flags & ALLOC_CPUSET) &&",
            "\t\t    !__cpuset_zone_allowed(zone, gfp)) {",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tif (nr_online_nodes > 1 && zone != ac.preferred_zoneref->zone &&",
            "\t\t    zone_to_nid(zone) != zone_to_nid(ac.preferred_zoneref->zone)) {",
            "\t\t\tgoto failed;",
            "\t\t}",
            "",
            "\t\tmark = wmark_pages(zone, alloc_flags & ALLOC_WMARK_MASK) + nr_pages;",
            "\t\tif (zone_watermark_fast(zone, 0,  mark,",
            "\t\t\t\tzonelist_zone_idx(ac.preferred_zoneref),",
            "\t\t\t\talloc_flags, gfp)) {",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If there are no allowed local zones that meets the watermarks then",
            "\t * try to allocate a single page and reclaim if necessary.",
            "\t */",
            "\tif (unlikely(!zone))",
            "\t\tgoto failed;",
            "",
            "\t/* spin_trylock may fail due to a parallel drain or IRQ reentrancy. */",
            "\tpcp_trylock_prepare(UP_flags);",
            "\tpcp = pcp_spin_trylock(zone->per_cpu_pageset);",
            "\tif (!pcp)",
            "\t\tgoto failed_irq;",
            "",
            "\t/* Attempt the batch allocation */",
            "\tpcp_list = &pcp->lists[order_to_pindex(ac.migratetype, 0)];",
            "\twhile (nr_populated < nr_pages) {",
            "",
            "\t\t/* Skip existing pages */",
            "\t\tif (page_array && page_array[nr_populated]) {",
            "\t\t\tnr_populated++;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tpage = __rmqueue_pcplist(zone, 0, ac.migratetype, alloc_flags,",
            "\t\t\t\t\t\t\t\tpcp, pcp_list);",
            "\t\tif (unlikely(!page)) {",
            "\t\t\t/* Try and allocate at least one page */",
            "\t\t\tif (!nr_account) {",
            "\t\t\t\tpcp_spin_unlock(pcp);",
            "\t\t\t\tgoto failed_irq;",
            "\t\t\t}",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tnr_account++;",
            "",
            "\t\tprep_new_page(page, 0, gfp, 0);",
            "\t\tif (page_list)",
            "\t\t\tlist_add(&page->lru, page_list);",
            "\t\telse",
            "\t\t\tpage_array[nr_populated] = page;",
            "\t\tnr_populated++;",
            "\t}",
            "",
            "\tpcp_spin_unlock(pcp);",
            "\tpcp_trylock_finish(UP_flags);",
            "",
            "\t__count_zid_vm_events(PGALLOC, zone_idx(zone), nr_account);",
            "\tzone_statistics(ac.preferred_zoneref->zone, zone, nr_account);",
            "",
            "out:",
            "\treturn nr_populated;",
            "",
            "failed_irq:",
            "\tpcp_trylock_finish(UP_flags);",
            "",
            "failed:",
            "\tpage = __alloc_pages_noprof(gfp, 0, preferred_nid, nodemask);",
            "\tif (page) {",
            "\t\tif (page_list)",
            "\t\t\tlist_add(&page->lru, page_list);",
            "\t\telse",
            "\t\t\tpage_array[nr_populated] = page;",
            "\t\tnr_populated++;",
            "\t}",
            "",
            "\tgoto out;",
            "}"
          ],
          "function_name": "prepare_alloc_pages, alloc_pages_bulk_noprof",
          "description": "该代码段实现了内存页面的批量分配逻辑，其中`prepare_alloc_pages`用于初始化分配上下文参数并配置内存策略，`alloc_pages_bulk_noprof`则通过遍历内存区域尝试批量分配连续页面，优先使用本地节点且支持CPU集约束。  \n`prepare_alloc_pages`构建分配上下文，设置Zone列表、迁移类型及节点掩码，处理CPU集隔离与水位线检查；`alloc_pages_bulk_noprof`在满足水位线前提下批量分配页面，失败时回退至单页分配。  \n上下文完整，未引入未展示的API或机制。",
          "similarity": 0.5634069442749023
        },
        {
          "chunk_id": 24,
          "file_path": "mm/page_alloc.c",
          "start_line": 4766,
          "end_line": 4869,
          "content": [
            "unsigned long get_free_pages_noprof(gfp_t gfp_mask, unsigned int order)",
            "{",
            "\tstruct page *page;",
            "",
            "\tpage = alloc_pages_noprof(gfp_mask & ~__GFP_HIGHMEM, order);",
            "\tif (!page)",
            "\t\treturn 0;",
            "\treturn (unsigned long) page_address(page);",
            "}",
            "unsigned long get_zeroed_page_noprof(gfp_t gfp_mask)",
            "{",
            "\treturn get_free_pages_noprof(gfp_mask | __GFP_ZERO, 0);",
            "}",
            "void __free_pages(struct page *page, unsigned int order)",
            "{",
            "\t/* get PageHead before we drop reference */",
            "\tint head = PageHead(page);",
            "\t/* get alloc tag in case the page is released by others */",
            "\tstruct alloc_tag *tag = pgalloc_tag_get(page);",
            "",
            "\tif (put_page_testzero(page))",
            "\t\tfree_unref_page(page, order);",
            "\telse if (!head) {",
            "\t\tpgalloc_tag_sub_pages(tag, (1 << order) - 1);",
            "\t\twhile (order-- > 0)",
            "\t\t\tfree_unref_page(page + (1 << order), order);",
            "\t}",
            "}",
            "void free_pages(unsigned long addr, unsigned int order)",
            "{",
            "\tif (addr != 0) {",
            "\t\tVM_BUG_ON(!virt_addr_valid((void *)addr));",
            "\t\t__free_pages(virt_to_page((void *)addr), order);",
            "\t}",
            "}",
            "void __page_frag_cache_drain(struct page *page, unsigned int count)",
            "{",
            "\tVM_BUG_ON_PAGE(page_ref_count(page) == 0, page);",
            "",
            "\tif (page_ref_sub_and_test(page, count))",
            "\t\tfree_unref_page(page, compound_order(page));",
            "}",
            "void page_frag_free(void *addr)",
            "{",
            "\tstruct page *page = virt_to_head_page(addr);",
            "",
            "\tif (unlikely(put_page_testzero(page)))",
            "\t\tfree_unref_page(page, compound_order(page));",
            "}",
            "void free_pages_exact(void *virt, size_t size)",
            "{",
            "\tunsigned long addr = (unsigned long)virt;",
            "\tunsigned long end = addr + PAGE_ALIGN(size);",
            "",
            "\twhile (addr < end) {",
            "\t\tfree_page(addr);",
            "\t\taddr += PAGE_SIZE;",
            "\t}",
            "}",
            "static unsigned long nr_free_zone_pages(int offset)",
            "{",
            "\tstruct zoneref *z;",
            "\tstruct zone *zone;",
            "",
            "\t/* Just pick one node, since fallback list is circular */",
            "\tunsigned long sum = 0;",
            "",
            "\tstruct zonelist *zonelist = node_zonelist(numa_node_id(), GFP_KERNEL);",
            "",
            "\tfor_each_zone_zonelist(zone, z, zonelist, offset) {",
            "\t\tunsigned long size = zone_managed_pages(zone);",
            "\t\tunsigned long high = high_wmark_pages(zone);",
            "\t\tif (size > high)",
            "\t\t\tsum += size - high;",
            "\t}",
            "",
            "\treturn sum;",
            "}",
            "unsigned long nr_free_buffer_pages(void)",
            "{",
            "\treturn nr_free_zone_pages(gfp_zone(GFP_USER));",
            "}",
            "static void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref)",
            "{",
            "\tzoneref->zone = zone;",
            "\tzoneref->zone_idx = zone_idx(zone);",
            "}",
            "static int build_zonerefs_node(pg_data_t *pgdat, struct zoneref *zonerefs)",
            "{",
            "\tstruct zone *zone;",
            "\tenum zone_type zone_type = MAX_NR_ZONES;",
            "\tint nr_zones = 0;",
            "",
            "\tdo {",
            "\t\tzone_type--;",
            "\t\tzone = pgdat->node_zones + zone_type;",
            "\t\tif (populated_zone(zone)) {",
            "\t\t\tzoneref_set_zone(zone, &zonerefs[nr_zones++]);",
            "\t\t\tcheck_highest_zone(zone_type);",
            "\t\t}",
            "\t} while (zone_type);",
            "",
            "\treturn nr_zones;",
            "}"
          ],
          "function_name": "get_free_pages_noprof, get_zeroed_page_noprof, __free_pages, free_pages, __page_frag_cache_drain, page_frag_free, free_pages_exact, nr_free_zone_pages, nr_free_buffer_pages, zoneref_set_zone, build_zonerefs_node",
          "description": "该代码段实现非profile模式下的页面分配与释放逻辑，包含get_free_pages_noprof等函数用于分配带零填充的连续页框，__free_pages及其辅助函数负责处理复杂引用计数和碎片化页面回收，同时通过nr_free_zone_pages等接口统计ZONE层级的空闲页数量。部分函数依赖未展示的上下文（如alloc_pages_noprof、compound_order等）。",
          "similarity": 0.5476675033569336
        },
        {
          "chunk_id": 29,
          "file_path": "mm/page_alloc.c",
          "start_line": 5701,
          "end_line": 5807,
          "content": [
            "void __meminit setup_zone_pageset(struct zone *zone)",
            "{",
            "\tint cpu;",
            "",
            "\t/* Size may be 0 on !SMP && !NUMA */",
            "\tif (sizeof(struct per_cpu_zonestat) > 0)",
            "\t\tzone->per_cpu_zonestats = alloc_percpu(struct per_cpu_zonestat);",
            "",
            "\tzone->per_cpu_pageset = alloc_percpu(struct per_cpu_pages);",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct per_cpu_pages *pcp;",
            "\t\tstruct per_cpu_zonestat *pzstats;",
            "",
            "\t\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);",
            "\t\tpzstats = per_cpu_ptr(zone->per_cpu_zonestats, cpu);",
            "\t\tper_cpu_pages_init(pcp, pzstats);",
            "\t}",
            "",
            "\tzone_set_pageset_high_and_batch(zone, 0);",
            "}",
            "static void zone_pcp_update(struct zone *zone, int cpu_online)",
            "{",
            "\tmutex_lock(&pcp_batch_high_lock);",
            "\tzone_set_pageset_high_and_batch(zone, cpu_online);",
            "\tmutex_unlock(&pcp_batch_high_lock);",
            "}",
            "static void zone_pcp_update_cacheinfo(struct zone *zone, unsigned int cpu)",
            "{",
            "\tstruct per_cpu_pages *pcp;",
            "\tstruct cpu_cacheinfo *cci;",
            "",
            "\tpcp = per_cpu_ptr(zone->per_cpu_pageset, cpu);",
            "\tcci = get_cpu_cacheinfo(cpu);",
            "\t/*",
            "\t * If data cache slice of CPU is large enough, \"pcp->batch\"",
            "\t * pages can be preserved in PCP before draining PCP for",
            "\t * consecutive high-order pages freeing without allocation.",
            "\t * This can reduce zone lock contention without hurting",
            "\t * cache-hot pages sharing.",
            "\t */",
            "\tspin_lock(&pcp->lock);",
            "\tif ((cci->per_cpu_data_slice_size >> PAGE_SHIFT) > 3 * pcp->batch)",
            "\t\tpcp->flags |= PCPF_FREE_HIGH_BATCH;",
            "\telse",
            "\t\tpcp->flags &= ~PCPF_FREE_HIGH_BATCH;",
            "\tspin_unlock(&pcp->lock);",
            "}",
            "void setup_pcp_cacheinfo(unsigned int cpu)",
            "{",
            "\tstruct zone *zone;",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tzone_pcp_update_cacheinfo(zone, cpu);",
            "}",
            "void __init setup_per_cpu_pageset(void)",
            "{",
            "\tstruct pglist_data *pgdat;",
            "\tstruct zone *zone;",
            "\tint __maybe_unused cpu;",
            "",
            "\tfor_each_populated_zone(zone)",
            "\t\tsetup_zone_pageset(zone);",
            "",
            "#ifdef CONFIG_NUMA",
            "\t/*",
            "\t * Unpopulated zones continue using the boot pagesets.",
            "\t * The numa stats for these pagesets need to be reset.",
            "\t * Otherwise, they will end up skewing the stats of",
            "\t * the nodes these zones are associated with.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct per_cpu_zonestat *pzstats = &per_cpu(boot_zonestats, cpu);",
            "\t\tmemset(pzstats->vm_numa_event, 0,",
            "\t\t       sizeof(pzstats->vm_numa_event));",
            "\t}",
            "#endif",
            "",
            "\tfor_each_online_pgdat(pgdat)",
            "\t\tpgdat->per_cpu_nodestats =",
            "\t\t\talloc_percpu(struct per_cpu_nodestat);",
            "}",
            "__meminit void zone_pcp_init(struct zone *zone)",
            "{",
            "\t/*",
            "\t * per cpu subsystem is not up at this point. The following code",
            "\t * relies on the ability of the linker to provide the",
            "\t * offset of a (static) per cpu variable into the per cpu area.",
            "\t */",
            "\tzone->per_cpu_pageset = &boot_pageset;",
            "\tzone->per_cpu_zonestats = &boot_zonestats;",
            "\tzone->pageset_high_min = BOOT_PAGESET_HIGH;",
            "\tzone->pageset_high_max = BOOT_PAGESET_HIGH;",
            "\tzone->pageset_batch = BOOT_PAGESET_BATCH;",
            "",
            "\tif (populated_zone(zone))",
            "\t\tpr_debug(\"  %s zone: %lu pages, LIFO batch:%u\\n\", zone->name,",
            "\t\t\t zone->present_pages, zone_batchsize(zone));",
            "}",
            "void adjust_managed_page_count(struct page *page, long count)",
            "{",
            "\tatomic_long_add(count, &page_zone(page)->managed_pages);",
            "\ttotalram_pages_add(count);",
            "#ifdef CONFIG_HIGHMEM",
            "\tif (PageHighMem(page))",
            "\t\ttotalhigh_pages_add(count);",
            "#endif",
            "}"
          ],
          "function_name": "setup_zone_pageset, zone_pcp_update, zone_pcp_update_cacheinfo, setup_pcp_cacheinfo, setup_per_cpu_pageset, zone_pcp_init, adjust_managed_page_count",
          "description": "分配并初始化每个区的页集结构，设置初始参数，根据CPU状态更新页集配置，维护管理页面计数统计信息。",
          "similarity": 0.5373843908309937
        },
        {
          "chunk_id": 27,
          "file_path": "mm/page_alloc.c",
          "start_line": 5441,
          "end_line": 5545,
          "content": [
            "static noinline void __init",
            "build_all_zonelists_init(void)",
            "{",
            "\tint cpu;",
            "",
            "\t__build_all_zonelists(NULL);",
            "",
            "\t/*",
            "\t * Initialize the boot_pagesets that are going to be used",
            "\t * for bootstrapping processors. The real pagesets for",
            "\t * each zone will be allocated later when the per cpu",
            "\t * allocator is available.",
            "\t *",
            "\t * boot_pagesets are used also for bootstrapping offline",
            "\t * cpus if the system is already booted because the pagesets",
            "\t * are needed to initialize allocators on a specific cpu too.",
            "\t * F.e. the percpu allocator needs the page allocator which",
            "\t * needs the percpu allocator in order to allocate its pagesets",
            "\t * (a chicken-egg dilemma).",
            "\t */",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu_pages_init(&per_cpu(boot_pageset, cpu), &per_cpu(boot_zonestats, cpu));",
            "",
            "\tmminit_verify_zonelist();",
            "\tcpuset_init_current_mems_allowed();",
            "}",
            "void __ref build_all_zonelists(pg_data_t *pgdat)",
            "{",
            "\tunsigned long vm_total_pages;",
            "",
            "\tif (system_state == SYSTEM_BOOTING) {",
            "\t\tbuild_all_zonelists_init();",
            "\t} else {",
            "\t\t__build_all_zonelists(pgdat);",
            "\t\t/* cpuset refresh routine should be here */",
            "\t}",
            "\t/* Get the number of free pages beyond high watermark in all zones. */",
            "\tvm_total_pages = nr_free_zone_pages(gfp_zone(GFP_HIGHUSER_MOVABLE));",
            "\t/*",
            "\t * Disable grouping by mobility if the number of pages in the",
            "\t * system is too low to allow the mechanism to work. It would be",
            "\t * more accurate, but expensive to check per-zone. This check is",
            "\t * made on memory-hotadd so a system can start with mobility",
            "\t * disabled and enable it later",
            "\t */",
            "\tif (vm_total_pages < (pageblock_nr_pages * MIGRATE_TYPES))",
            "\t\tpage_group_by_mobility_disabled = 1;",
            "\telse",
            "\t\tpage_group_by_mobility_disabled = 0;",
            "",
            "\tpr_info(\"Built %u zonelists, mobility grouping %s.  Total pages: %ld\\n\",",
            "\t\tnr_online_nodes,",
            "\t\tpage_group_by_mobility_disabled ? \"off\" : \"on\",",
            "\t\tvm_total_pages);",
            "#ifdef CONFIG_NUMA",
            "\tpr_info(\"Policy zone: %s\\n\", zone_names[policy_zone]);",
            "#endif",
            "}",
            "static int zone_batchsize(struct zone *zone)",
            "{",
            "#ifdef CONFIG_MMU",
            "\tint batch;",
            "",
            "\t/*",
            "\t * The number of pages to batch allocate is either ~0.1%",
            "\t * of the zone or 1MB, whichever is smaller. The batch",
            "\t * size is striking a balance between allocation latency",
            "\t * and zone lock contention.",
            "\t */",
            "\tbatch = min(zone_managed_pages(zone) >> 10, SZ_1M / PAGE_SIZE);",
            "\tbatch /= 4;\t\t/* We effectively *= 4 below */",
            "\tif (batch < 1)",
            "\t\tbatch = 1;",
            "",
            "\t/*",
            "\t * Clamp the batch to a 2^n - 1 value. Having a power",
            "\t * of 2 value was found to be more likely to have",
            "\t * suboptimal cache aliasing properties in some cases.",
            "\t *",
            "\t * For example if 2 tasks are alternately allocating",
            "\t * batches of pages, one task can end up with a lot",
            "\t * of pages of one half of the possible page colors",
            "\t * and the other with pages of the other colors.",
            "\t */",
            "\tbatch = rounddown_pow_of_two(batch + batch/2) - 1;",
            "",
            "\treturn batch;",
            "",
            "#else",
            "\t/* The deferral and batching of frees should be suppressed under NOMMU",
            "\t * conditions.",
            "\t *",
            "\t * The problem is that NOMMU needs to be able to allocate large chunks",
            "\t * of contiguous memory as there's no hardware page translation to",
            "\t * assemble apparent contiguous memory from discontiguous pages.",
            "\t *",
            "\t * Queueing large contiguous runs of pages for batching, however,",
            "\t * causes the pages to actually be freed in smaller chunks.  As there",
            "\t * can be a significant delay between the individual batches being",
            "\t * recycled, this leads to the once large chunks of space being",
            "\t * fragmented and becoming unavailable for high-order allocations.",
            "\t */",
            "\treturn 0;",
            "#endif",
            "}"
          ],
          "function_name": "build_all_zonelists_init, build_all_zonelists, zone_batchsize",
          "description": "初始化页集结构，计算并设置每个区的页面批量大小，根据系统状态决定是否启用移动性分组，打印zonelist构建信息。",
          "similarity": 0.5336756706237793
        }
      ]
    },
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.5640525817871094,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.46434837579727173
        },
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.44423598051071167
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.43688663840293884
        }
      ]
    }
  ]
}