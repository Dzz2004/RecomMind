{
  "query": "irq priority handling",
  "timestamp": "2025-12-26 02:13:33",
  "retrieved_files": [
    {
      "source_file": "kernel/irq/spurious.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:09:47\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\spurious.c`\n\n---\n\n# `irq/spurious.c` 技术文档\n\n## 1. 文件概述\n\n`irq/spurious.c` 是 Linux 内核中断子系统中的一个关键组件，负责处理**伪中断**（spurious interrupts）和**错误路由中断**（misrouted interrupts）。当硬件中断未被任何中断处理程序正确处理（返回 `IRQ_NONE`）时，内核会怀疑该中断是伪中断或被错误路由到当前 IRQ 线。该文件实现了检测、诊断和恢复机制，包括：\n\n- 统计未处理中断次数并判断是否为“卡住”的 IRQ\n- 在启用 `irqfixup` 选项时尝试在其他 IRQ 线上查找真正的中断源（中断错位恢复）\n- 定期轮询被禁用的伪中断线以尝试恢复共享中断设备\n- 提供诊断信息（如调用栈和注册的处理函数列表）\n\n该机制对于提高系统在硬件或固件存在缺陷时的鲁棒性至关重要。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `irqfixup`：模块参数，控制伪中断修复行为（0=禁用，1=仅对未处理中断尝试修复，2=对标记为 `IRQF_IRQPOLL` 的中断也尝试修复）\n- `poll_spurious_irq_timer`：定时器，用于定期轮询被标记为 `IRQS_SPURIOUS_DISABLED` 的中断线\n- `irq_poll_cpu`：记录当前正在执行轮询任务的 CPU ID\n- `irq_poll_active`：原子变量，防止多个 CPU 同时执行轮询\n\n### 主要函数\n- `irq_wait_for_poll(struct irq_desc *desc)`  \n  等待轮询操作完成，避免与轮询线程竞争。在 SMP 系统中自旋等待 `IRQS_POLL_INPROGRESS` 标志清除。\n  \n- `try_one_irq(struct irq_desc *desc, bool force)`  \n  尝试在指定中断描述符上执行中断处理。跳过 PER_CPU、嵌套线程和显式标记为轮询的中断。若中断被禁用，则仅在 `force=true` 时处理。支持共享中断的 `IRQS_PENDING` 重试机制。\n\n- `misrouted_irq(int irq)`  \n  遍历所有 IRQ（除 0 和当前 IRQ），调用 `try_one_irq()` 尝试在其他线上找到真正的中断源。用于中断错位恢复。\n\n- `poll_spurious_irqs(struct timer_list *unused)`  \n  定时器回调函数，轮询所有被标记为 `IRQS_SPURIOUS_DISABLED` 的中断线，强制尝试处理（`force=true`）。\n\n- `__report_bad_irq()` / `report_bad_irq()`  \n  打印伪中断诊断信息，包括中断号、错误返回值、调用栈及所有注册的处理函数。\n\n- `try_misrouted_irq()`  \n  根据 `irqfixup` 级别判断是否应尝试中断错位恢复。\n\n- `note_interrupt(struct irq_desc *desc, irqreturn_t action_ret)`  \n  中断处理结果分析入口。统计未处理中断，触发伪中断检测、诊断和恢复逻辑。\n\n## 3. 关键实现\n\n### 伪中断检测机制\n- 当 `note_interrupt()` 收到 `IRQ_NONE` 时，会递增中断描述符的未处理计数。\n- 若在 100,000 次中断中有 99,900 次未处理，则判定该 IRQ “卡住”，打印诊断信息并建议使用 `irqpoll` 启动参数。\n- 诊断信息包含所有注册的处理函数地址及符号名，便于调试。\n\n### 中断错位恢复（Misrouted IRQ Recovery）\n- 通过 `irqfixup` 内核参数启用（启动时传入 `irqfixup=1` 或 `2`）。\n- 当当前 IRQ 未被处理时，遍历其他所有 IRQ 线，尝试调用其处理函数（`try_one_irq()`）。\n- 仅适用于共享中断（`IRQF_SHARED`）且非 PER_CPU/嵌套线程类型。\n- 使用 `IRQS_POLL_INPROGRESS` 标志防止与正常中断处理冲突。\n\n### 轮询恢复机制\n- 被判定为伪中断的 IRQ 会被标记 `IRQS_SPURIOUS_DISABLED` 并禁用。\n- 启用 `irqfixup` 时，启动定时器 `poll_spurious_irq_timer`（间隔 100ms）。\n- 定时器回调 `poll_spurious_irqs()` 遍历所有 `IRQS_SPURIOUS_DISABLED` 的 IRQ，强制尝试处理（即使已禁用）。\n- 通过 `local_irq_disable/enable()` 保证轮询期间本地中断关闭，避免嵌套。\n\n### SMP 安全性\n- 使用 `irq_poll_active` 原子变量确保同一时间仅一个 CPU 执行轮询。\n- `irq_wait_for_poll()` 在 SMP 下自旋等待轮询完成，防止死锁。\n- 所有关键操作均在 `desc->lock` 保护下进行。\n\n### 线程化中断处理支持\n- 若主处理函数返回 `IRQ_WAKE_THREAD`，则延迟伪中断判断至下一次硬件中断，以等待线程处理结果。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/irq.h>`、`<linux/interrupt.h>`：中断核心数据结构和 API\n  - `<linux/timer.h>`：定时器支持（用于轮询）\n  - `\"internals.h\"`：中断子系统内部接口\n\n- **内核配置依赖**：\n  - `CONFIG_SMP`：影响 `irq_wait_for_poll()` 的实现\n  - `irqfixup` 模块参数：控制恢复行为\n\n- **与其他模块交互**：\n  - 被通用中断处理流程（如 `handle_irq_event()`）调用\n  - 与中断描述符管理（`irq_desc`）紧密集成\n  - 依赖内核打印和栈回溯机制（`dump_stack()`）\n\n## 5. 使用场景\n\n1. **硬件/固件缺陷处理**：  \n   当 BIOS 或硬件错误地将设备中断路由到错误的 IRQ 线时，通过 `irqfixup` 机制尝试在其他线上找到真正的处理函数。\n\n2. **共享中断线故障恢复**：  \n   在多个设备共享同一 IRQ 线时，若其中一个设备故障产生持续中断但无处理函数响应，内核可禁用该线并定期轮询，避免系统被中断风暴拖垮。\n\n3. **系统调试与诊断**：  \n   当出现“nobody cared”中断错误时，自动打印详细的处理函数列表和调用栈，帮助开发者定位问题设备或驱动。\n\n4. **高可用性系统**：  \n   在无法立即修复硬件问题的生产环境中，通过 `irqpoll` 启动参数启用轮询机制，维持系统基本运行。\n\n5. **传统 PC 兼容性**：  \n   特别处理 IRQ 0（系统定时器），因其在传统 PC 架构中的特殊地位，即使在 `irqfixup=2` 模式下也始终尝试恢复。",
      "similarity": 0.5509437918663025,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 36,
          "end_line": 136,
          "content": [
            "bool irq_wait_for_poll(struct irq_desc *desc)",
            "\t__must_hold(&desc->lock)",
            "{",
            "\tif (WARN_ONCE(irq_poll_cpu == smp_processor_id(),",
            "\t\t      \"irq poll in progress on cpu %d for irq %d\\n\",",
            "\t\t      smp_processor_id(), desc->irq_data.irq))",
            "\t\treturn false;",
            "",
            "#ifdef CONFIG_SMP",
            "\tdo {",
            "\t\traw_spin_unlock(&desc->lock);",
            "\t\twhile (irqd_irq_inprogress(&desc->irq_data))",
            "\t\t\tcpu_relax();",
            "\t\traw_spin_lock(&desc->lock);",
            "\t} while (irqd_irq_inprogress(&desc->irq_data));",
            "\t/* Might have been disabled in meantime */",
            "\treturn !irqd_irq_disabled(&desc->irq_data) && desc->action;",
            "#else",
            "\treturn false;",
            "#endif",
            "}",
            "static int try_one_irq(struct irq_desc *desc, bool force)",
            "{",
            "\tirqreturn_t ret = IRQ_NONE;",
            "\tstruct irqaction *action;",
            "",
            "\traw_spin_lock(&desc->lock);",
            "",
            "\t/*",
            "\t * PER_CPU, nested thread interrupts and interrupts explicitly",
            "\t * marked polled are excluded from polling.",
            "\t */",
            "\tif (irq_settings_is_per_cpu(desc) ||",
            "\t    irq_settings_is_nested_thread(desc) ||",
            "\t    irq_settings_is_polled(desc))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Do not poll disabled interrupts unless the spurious",
            "\t * disabled poller asks explicitly.",
            "\t */",
            "\tif (irqd_irq_disabled(&desc->irq_data) && !force)",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * All handlers must agree on IRQF_SHARED, so we test just the",
            "\t * first.",
            "\t */",
            "\taction = desc->action;",
            "\tif (!action || !(action->flags & IRQF_SHARED) ||",
            "\t    (action->flags & __IRQF_TIMER))",
            "\t\tgoto out;",
            "",
            "\t/* Already running on another processor */",
            "\tif (irqd_irq_inprogress(&desc->irq_data)) {",
            "\t\t/*",
            "\t\t * Already running: If it is shared get the other",
            "\t\t * CPU to go looking for our mystery interrupt too",
            "\t\t */",
            "\t\tdesc->istate |= IRQS_PENDING;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* Mark it poll in progress */",
            "\tdesc->istate |= IRQS_POLL_INPROGRESS;",
            "\tdo {",
            "\t\tif (handle_irq_event(desc) == IRQ_HANDLED)",
            "\t\t\tret = IRQ_HANDLED;",
            "\t\t/* Make sure that there is still a valid action */",
            "\t\taction = desc->action;",
            "\t} while ((desc->istate & IRQS_PENDING) && action);",
            "\tdesc->istate &= ~IRQS_POLL_INPROGRESS;",
            "out:",
            "\traw_spin_unlock(&desc->lock);",
            "\treturn ret == IRQ_HANDLED;",
            "}",
            "static int misrouted_irq(int irq)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint i, ok = 0;",
            "",
            "\tif (atomic_inc_return(&irq_poll_active) != 1)",
            "\t\tgoto out;",
            "",
            "\tirq_poll_cpu = smp_processor_id();",
            "",
            "\tfor_each_irq_desc(i, desc) {",
            "\t\tif (!i)",
            "\t\t\t continue;",
            "",
            "\t\tif (i == irq)\t/* Already tried */",
            "\t\t\tcontinue;",
            "",
            "\t\tif (try_one_irq(desc, false))",
            "\t\t\tok = 1;",
            "\t}",
            "out:",
            "\tatomic_dec(&irq_poll_active);",
            "\t/* So the caller can adjust the irq error counts */",
            "\treturn ok;",
            "}"
          ],
          "function_name": "irq_wait_for_poll, try_one_irq, misrouted_irq",
          "description": "实现了irq_wait_for_poll用于等待轮询完成，try_one_irq尝试处理单个中断，misrouted_irq尝试修复误路由中断。",
          "similarity": 0.5818134546279907
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 144,
          "end_line": 256,
          "content": [
            "static void poll_spurious_irqs(struct timer_list *unused)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint i;",
            "",
            "\tif (atomic_inc_return(&irq_poll_active) != 1)",
            "\t\tgoto out;",
            "\tirq_poll_cpu = smp_processor_id();",
            "",
            "\tfor_each_irq_desc(i, desc) {",
            "\t\tunsigned int state;",
            "",
            "\t\tif (!i)",
            "\t\t\t continue;",
            "",
            "\t\t/* Racy but it doesn't matter */",
            "\t\tstate = desc->istate;",
            "\t\tbarrier();",
            "\t\tif (!(state & IRQS_SPURIOUS_DISABLED))",
            "\t\t\tcontinue;",
            "",
            "\t\tlocal_irq_disable();",
            "\t\ttry_one_irq(desc, true);",
            "\t\tlocal_irq_enable();",
            "\t}",
            "out:",
            "\tatomic_dec(&irq_poll_active);",
            "\tmod_timer(&poll_spurious_irq_timer,",
            "\t\t  jiffies + POLL_SPURIOUS_IRQ_INTERVAL);",
            "}",
            "static inline int bad_action_ret(irqreturn_t action_ret)",
            "{",
            "\tunsigned int r = action_ret;",
            "",
            "\tif (likely(r <= (IRQ_HANDLED | IRQ_WAKE_THREAD)))",
            "\t\treturn 0;",
            "\treturn 1;",
            "}",
            "static void __report_bad_irq(struct irq_desc *desc, irqreturn_t action_ret)",
            "{",
            "\tunsigned int irq = irq_desc_get_irq(desc);",
            "\tstruct irqaction *action;",
            "\tunsigned long flags;",
            "",
            "\tif (bad_action_ret(action_ret)) {",
            "\t\tprintk(KERN_ERR \"irq event %d: bogus return value %x\\n\",",
            "\t\t\t\tirq, action_ret);",
            "\t} else {",
            "\t\tprintk(KERN_ERR \"irq %d: nobody cared (try booting with \"",
            "\t\t\t\t\"the \\\"irqpoll\\\" option)\\n\", irq);",
            "\t}",
            "\tdump_stack();",
            "\tprintk(KERN_ERR \"handlers:\\n\");",
            "",
            "\t/*",
            "\t * We need to take desc->lock here. note_interrupt() is called",
            "\t * w/o desc->lock held, but IRQ_PROGRESS set. We might race",
            "\t * with something else removing an action. It's ok to take",
            "\t * desc->lock here. See synchronize_irq().",
            "\t */",
            "\traw_spin_lock_irqsave(&desc->lock, flags);",
            "\tfor_each_action_of_desc(desc, action) {",
            "\t\tprintk(KERN_ERR \"[<%p>] %ps\", action->handler, action->handler);",
            "\t\tif (action->thread_fn)",
            "\t\t\tprintk(KERN_CONT \" threaded [<%p>] %ps\",",
            "\t\t\t\t\taction->thread_fn, action->thread_fn);",
            "\t\tprintk(KERN_CONT \"\\n\");",
            "\t}",
            "\traw_spin_unlock_irqrestore(&desc->lock, flags);",
            "}",
            "static void report_bad_irq(struct irq_desc *desc, irqreturn_t action_ret)",
            "{",
            "\tstatic int count = 100;",
            "",
            "\tif (count > 0) {",
            "\t\tcount--;",
            "\t\t__report_bad_irq(desc, action_ret);",
            "\t}",
            "}",
            "static inline int",
            "try_misrouted_irq(unsigned int irq, struct irq_desc *desc,",
            "\t\t  irqreturn_t action_ret)",
            "{",
            "\tstruct irqaction *action;",
            "",
            "\tif (!irqfixup)",
            "\t\treturn 0;",
            "",
            "\t/* We didn't actually handle the IRQ - see if it was misrouted? */",
            "\tif (action_ret == IRQ_NONE)",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * But for 'irqfixup == 2' we also do it for handled interrupts if",
            "\t * they are marked as IRQF_IRQPOLL (or for irq zero, which is the",
            "\t * traditional PC timer interrupt.. Legacy)",
            "\t */",
            "\tif (irqfixup < 2)",
            "\t\treturn 0;",
            "",
            "\tif (!irq)",
            "\t\treturn 1;",
            "",
            "\t/*",
            "\t * Since we don't get the descriptor lock, \"action\" can",
            "\t * change under us.  We don't really care, but we don't",
            "\t * want to follow a NULL pointer. So tell the compiler to",
            "\t * just load it once by using a barrier.",
            "\t */",
            "\taction = desc->action;",
            "\tbarrier();",
            "\treturn action && (action->flags & IRQF_IRQPOLL);",
            "}"
          ],
          "function_name": "poll_spurious_irqs, bad_action_ret, __report_bad_irq, report_bad_irq, try_misrouted_irq",
          "description": "poll_spurious_irqs定时扫描中断描述符并处理疑似虚假中断，包含错误报告辅助函数和误路由检测逻辑。",
          "similarity": 0.5091271996498108
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 436,
          "end_line": 467,
          "content": [
            "int noirqdebug_setup(char *str)",
            "{",
            "\tnoirqdebug = 1;",
            "\tprintk(KERN_INFO \"IRQ lockup detection disabled\\n\");",
            "",
            "\treturn 1;",
            "}",
            "static int __init irqfixup_setup(char *str)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {",
            "\t\tpr_warn(\"irqfixup boot option not supported with PREEMPT_RT\\n\");",
            "\t\treturn 1;",
            "\t}",
            "\tirqfixup = 1;",
            "\tprintk(KERN_WARNING \"Misrouted IRQ fixup support enabled.\\n\");",
            "\tprintk(KERN_WARNING \"This may impact system performance.\\n\");",
            "",
            "\treturn 1;",
            "}",
            "static int __init irqpoll_setup(char *str)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {",
            "\t\tpr_warn(\"irqpoll boot option not supported with PREEMPT_RT\\n\");",
            "\t\treturn 1;",
            "\t}",
            "\tirqfixup = 2;",
            "\tprintk(KERN_WARNING \"Misrouted IRQ fixup and polling support \"",
            "\t\t\t\t\"enabled\\n\");",
            "\tprintk(KERN_WARNING \"This may significantly impact system \"",
            "\t\t\t\t\"performance\\n\");",
            "\treturn 1;",
            "}"
          ],
          "function_name": "noirqdebug_setup, irqfixup_setup, irqpoll_setup",
          "description": "提供启动参数配置接口，用于启用或禁用irqfixup和irqpoll功能，并输出相应警告信息。",
          "similarity": 0.48801013827323914
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 272,
          "end_line": 432,
          "content": [
            "void note_interrupt(struct irq_desc *desc, irqreturn_t action_ret)",
            "{",
            "\tunsigned int irq;",
            "",
            "\tif (desc->istate & IRQS_POLL_INPROGRESS ||",
            "\t    irq_settings_is_polled(desc))",
            "\t\treturn;",
            "",
            "\tif (bad_action_ret(action_ret)) {",
            "\t\treport_bad_irq(desc, action_ret);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * We cannot call note_interrupt from the threaded handler",
            "\t * because we need to look at the compound of all handlers",
            "\t * (primary and threaded). Aside of that in the threaded",
            "\t * shared case we have no serialization against an incoming",
            "\t * hardware interrupt while we are dealing with a threaded",
            "\t * result.",
            "\t *",
            "\t * So in case a thread is woken, we just note the fact and",
            "\t * defer the analysis to the next hardware interrupt.",
            "\t *",
            "\t * The threaded handlers store whether they successfully",
            "\t * handled an interrupt and we check whether that number",
            "\t * changed versus the last invocation.",
            "\t *",
            "\t * We could handle all interrupts with the delayed by one",
            "\t * mechanism, but for the non forced threaded case we'd just",
            "\t * add pointless overhead to the straight hardirq interrupts",
            "\t * for the sake of a few lines less code.",
            "\t */",
            "\tif (action_ret & IRQ_WAKE_THREAD) {",
            "\t\t/*",
            "\t\t * There is a thread woken. Check whether one of the",
            "\t\t * shared primary handlers returned IRQ_HANDLED. If",
            "\t\t * not we defer the spurious detection to the next",
            "\t\t * interrupt.",
            "\t\t */",
            "\t\tif (action_ret == IRQ_WAKE_THREAD) {",
            "\t\t\tint handled;",
            "\t\t\t/*",
            "\t\t\t * We use bit 31 of thread_handled_last to",
            "\t\t\t * denote the deferred spurious detection",
            "\t\t\t * active. No locking necessary as",
            "\t\t\t * thread_handled_last is only accessed here",
            "\t\t\t * and we have the guarantee that hard",
            "\t\t\t * interrupts are not reentrant.",
            "\t\t\t */",
            "\t\t\tif (!(desc->threads_handled_last & SPURIOUS_DEFERRED)) {",
            "\t\t\t\tdesc->threads_handled_last |= SPURIOUS_DEFERRED;",
            "\t\t\t\treturn;",
            "\t\t\t}",
            "\t\t\t/*",
            "\t\t\t * Check whether one of the threaded handlers",
            "\t\t\t * returned IRQ_HANDLED since the last",
            "\t\t\t * interrupt happened.",
            "\t\t\t *",
            "\t\t\t * For simplicity we just set bit 31, as it is",
            "\t\t\t * set in threads_handled_last as well. So we",
            "\t\t\t * avoid extra masking. And we really do not",
            "\t\t\t * care about the high bits of the handled",
            "\t\t\t * count. We just care about the count being",
            "\t\t\t * different than the one we saw before.",
            "\t\t\t */",
            "\t\t\thandled = atomic_read(&desc->threads_handled);",
            "\t\t\thandled |= SPURIOUS_DEFERRED;",
            "\t\t\tif (handled != desc->threads_handled_last) {",
            "\t\t\t\taction_ret = IRQ_HANDLED;",
            "\t\t\t\t/*",
            "\t\t\t\t * Note: We keep the SPURIOUS_DEFERRED",
            "\t\t\t\t * bit set. We are handling the",
            "\t\t\t\t * previous invocation right now.",
            "\t\t\t\t * Keep it for the current one, so the",
            "\t\t\t\t * next hardware interrupt will",
            "\t\t\t\t * account for it.",
            "\t\t\t\t */",
            "\t\t\t\tdesc->threads_handled_last = handled;",
            "\t\t\t} else {",
            "\t\t\t\t/*",
            "\t\t\t\t * None of the threaded handlers felt",
            "\t\t\t\t * responsible for the last interrupt",
            "\t\t\t\t *",
            "\t\t\t\t * We keep the SPURIOUS_DEFERRED bit",
            "\t\t\t\t * set in threads_handled_last as we",
            "\t\t\t\t * need to account for the current",
            "\t\t\t\t * interrupt as well.",
            "\t\t\t\t */",
            "\t\t\t\taction_ret = IRQ_NONE;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\t/*",
            "\t\t\t * One of the primary handlers returned",
            "\t\t\t * IRQ_HANDLED. So we don't care about the",
            "\t\t\t * threaded handlers on the same line. Clear",
            "\t\t\t * the deferred detection bit.",
            "\t\t\t *",
            "\t\t\t * In theory we could/should check whether the",
            "\t\t\t * deferred bit is set and take the result of",
            "\t\t\t * the previous run into account here as",
            "\t\t\t * well. But it's really not worth the",
            "\t\t\t * trouble. If every other interrupt is",
            "\t\t\t * handled we never trigger the spurious",
            "\t\t\t * detector. And if this is just the one out",
            "\t\t\t * of 100k unhandled ones which is handled",
            "\t\t\t * then we merily delay the spurious detection",
            "\t\t\t * by one hard interrupt. Not a real problem.",
            "\t\t\t */",
            "\t\t\tdesc->threads_handled_last &= ~SPURIOUS_DEFERRED;",
            "\t\t}",
            "\t}",
            "",
            "\tif (unlikely(action_ret == IRQ_NONE)) {",
            "\t\t/*",
            "\t\t * If we are seeing only the odd spurious IRQ caused by",
            "\t\t * bus asynchronicity then don't eventually trigger an error,",
            "\t\t * otherwise the counter becomes a doomsday timer for otherwise",
            "\t\t * working systems",
            "\t\t */",
            "\t\tif (time_after(jiffies, desc->last_unhandled + HZ/10))",
            "\t\t\tdesc->irqs_unhandled = 1;",
            "\t\telse",
            "\t\t\tdesc->irqs_unhandled++;",
            "\t\tdesc->last_unhandled = jiffies;",
            "\t}",
            "",
            "\tirq = irq_desc_get_irq(desc);",
            "\tif (unlikely(try_misrouted_irq(irq, desc, action_ret))) {",
            "\t\tint ok = misrouted_irq(irq);",
            "\t\tif (action_ret == IRQ_NONE)",
            "\t\t\tdesc->irqs_unhandled -= ok;",
            "\t}",
            "",
            "\tif (likely(!desc->irqs_unhandled))",
            "\t\treturn;",
            "",
            "\t/* Now getting into unhandled irq detection */",
            "\tdesc->irq_count++;",
            "\tif (likely(desc->irq_count < 100000))",
            "\t\treturn;",
            "",
            "\tdesc->irq_count = 0;",
            "\tif (unlikely(desc->irqs_unhandled > 99900)) {",
            "\t\t/*",
            "\t\t * The interrupt is stuck",
            "\t\t */",
            "\t\t__report_bad_irq(desc, action_ret);",
            "\t\t/*",
            "\t\t * Now kill the IRQ",
            "\t\t */",
            "\t\tprintk(KERN_EMERG \"Disabling IRQ #%d\\n\", irq);",
            "\t\tdesc->istate |= IRQS_SPURIOUS_DISABLED;",
            "\t\tdesc->depth++;",
            "\t\tirq_disable(desc);",
            "",
            "\t\tmod_timer(&poll_spurious_irq_timer,",
            "\t\t\t  jiffies + POLL_SPURIOUS_IRQ_INTERVAL);",
            "\t}",
            "\tdesc->irqs_unhandled = 0;",
            "}"
          ],
          "function_name": "note_interrupt",
          "description": "note_interrupt记录中断事件，检测未处理中断并触发报告，处理线程唤醒场景下的特殊逻辑。",
          "similarity": 0.46472710371017456
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq/spurious.c",
          "start_line": 1,
          "end_line": 35,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Copyright (C) 1992, 1998-2004 Linus Torvalds, Ingo Molnar",
            " *",
            " * This file contains spurious interrupt handling.",
            " */",
            "",
            "#include <linux/jiffies.h>",
            "#include <linux/irq.h>",
            "#include <linux/module.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/moduleparam.h>",
            "#include <linux/timer.h>",
            "",
            "#include \"internals.h\"",
            "",
            "static int irqfixup __read_mostly;",
            "",
            "#define POLL_SPURIOUS_IRQ_INTERVAL (HZ/10)",
            "static void poll_spurious_irqs(struct timer_list *unused);",
            "static DEFINE_TIMER(poll_spurious_irq_timer, poll_spurious_irqs);",
            "static int irq_poll_cpu;",
            "static atomic_t irq_poll_active;",
            "",
            "/*",
            " * We wait here for a poller to finish.",
            " *",
            " * If the poll runs on this CPU, then we yell loudly and return",
            " * false. That will leave the interrupt line disabled in the worst",
            " * case, but it should never happen.",
            " *",
            " * We wait until the poller is done and then recheck disabled and",
            " * action (about to be disabled). Only if it's still active, we return",
            " * true and let the handler run.",
            " */"
          ],
          "function_name": null,
          "description": "定义了处理虚假中断的相关变量和定时器，用于周期性地扫描和处理可能存在的虚假中断。",
          "similarity": 0.38225826621055603
        }
      ]
    },
    {
      "source_file": "kernel/irq_work.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:11:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq_work.c`\n\n---\n\n# `irq_work.c` 技术文档\n\n## 1. 文件概述\n\n`irq_work.c` 实现了一个轻量级的中断上下文工作队列机制，允许在硬中断（hardirq）或 NMI（不可屏蔽中断）上下文中安全地调度回调函数，并在稍后的硬中断上下文或专用内核线程中执行。该机制的核心目标是提供一种 **NMI 安全** 的方式来延迟执行某些不能在 NMI 或硬中断中直接完成的操作。\n\n该框架特别适用于需要从 NMI 或硬中断中触发后续处理（如 perf 事件、ftrace、RCU 等子系统）但又不能阻塞或执行复杂逻辑的场景。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `struct irq_work`：表示一个中断工作项，包含回调函数 `func` 和状态标志（如 `IRQ_WORK_PENDING`、`IRQ_WORK_CLAIMED`、`IRQ_WORK_BUSY`、`IRQ_WORK_LAZY`、`IRQ_WORK_HARD_IRQ`）。\n- 每 CPU 变量：\n  - `raised_list`：存放需在硬中断上下文中立即处理的工作项。\n  - `lazy_list`：存放“惰性”工作项，在非硬中断上下文（如 tick 或专用线程）中处理。\n  - `irq_workd`：指向每 CPU 的 `irq_work` 内核线程（仅在 `CONFIG_PREEMPT_RT` 下使用）。\n\n### 主要函数\n\n| 函数 | 功能 |\n|------|------|\n| `irq_work_queue(struct irq_work *work)` | 在当前 CPU 上排队一个 `irq_work`，若未被声明则声明并入队。 |\n| `irq_work_queue_on(struct irq_work *work, int cpu)` | 将 `irq_work` 排队到指定 CPU（支持跨 CPU 调度）。 |\n| `irq_work_run(void)` | 在当前 CPU 上执行所有 `raised_list` 和（非 RT 下的）`lazy_list` 中的工作项。 |\n| `irq_work_tick(void)` | 由时钟 tick 调用，处理未被硬中断处理的 `raised_list` 和 `lazy_list`。 |\n| `irq_work_sync(struct irq_work *work)` | 同步等待指定 `irq_work` 执行完毕。 |\n| `irq_work_single(void *arg)` | 执行单个工作项的回调函数，并清理状态。 |\n| `arch_irq_work_raise(void)` | 架构相关函数，用于触发 IPI 或中断以唤醒处理逻辑（弱符号，默认为空）。 |\n\n## 3. 关键实现\n\n### 状态管理与原子操作\n\n- 每个 `irq_work` 通过 `atomic_t node.a_flags` 管理状态：\n  - `IRQ_WORK_PENDING`：表示工作项已入队但尚未执行。\n  - `IRQ_WORK_CLAIMED`：表示已被声明，防止重复入队。\n  - `IRQ_WORK_BUSY`：表示正在执行中。\n- `irq_work_claim()` 使用 `atomic_fetch_or()` 原子地设置 `CLAIMED` 和 `PENDING` 标志，并检查是否已存在，避免重复入队。\n\n### 双队列设计\n\n- **`raised_list`**：用于需要尽快在硬中断上下文执行的工作（如标记为 `IRQ_WORK_HARD_IRQ` 的项）。\n- **`lazy_list`**：\n  - 在非 RT 内核中，由 `irq_work_tick()` 或 `irq_work_run()` 在软中断或进程上下文中处理。\n  - 在 `CONFIG_PREEMPT_RT` 下，由每 CPU 的 `irq_work/%u` 内核线程处理（以避免在硬中断中执行非硬实时代码）。\n\n### NMI 安全性\n\n- 入队操作（如 `irq_work_queue`）仅使用原子操作和每 CPU 链表（`llist`），不涉及锁或内存分配，因此可在 NMI 上下文中安全调用。\n- 跨 CPU 入队时（`irq_work_queue_on`）会检查 `in_nmi()`，防止在 NMI 中调用非 NMI 安全的 IPI 发送函数。\n\n### PREEMPT_RT 支持\n\n- 在 RT 内核中，非 `IRQ_WORK_HARD_IRQ` 的工作项被放入 `lazy_list`，并通过专用内核线程执行，以避免在硬中断中运行可能阻塞或延迟高的代码。\n- 使用 `rcuwait` 机制实现 `irq_work_sync()` 的睡眠等待。\n\n### IPI 触发机制\n\n- 若架构支持（通过 `arch_irq_work_has_interrupt()`），调用 `arch_irq_work_raise()` 触发本地中断处理。\n- 否则依赖时钟 tick（`irq_work_tick`）或显式调用 `irq_work_run` 来处理队列。\n\n## 4. 依赖关系\n\n- **架构依赖**：\n  - `arch_irq_work_raise()` 和 `arch_irq_work_has_interrupt()` 需由具体架构实现（如 x86 提供）。\n- **内核子系统**：\n  - `llist`（无锁链表）：用于高效、无锁的每 CPU 队列管理。\n  - `smpboot`：用于注册每 CPU 内核线程（RT 模式）。\n  - `rcu`：`rcuwait` 用于同步等待（RT 模式）。\n  - `tick`：`tick_nohz_tick_stopped()` 用于判断是否需要立即触发处理。\n  - `trace_events`：IPI 跟踪点 `trace_ipi_send_cpu`。\n- **配置选项**：\n  - `CONFIG_SMP`：启用跨 CPU 调度和 IPI 支持。\n  - `CONFIG_PREEMPT_RT`：启用 RT 模式下的线程化处理。\n\n## 5. 使用场景\n\n- **性能监控（perf）**：从 NMI 中记录采样后，通过 `irq_work` 安全地将数据传递到常规上下文处理。\n- **ftrace / tracing**：在中断上下文中触发延迟的跟踪事件处理。\n- **RCU**：某些 RCU 实现使用 `irq_work` 来触发宽限期处理。\n- **热插拔 CPU**：在 CPU 离线前通过 `flush_smp_call_function_queue()` 调用 `irq_work_run()` 确保工作项被清空。\n- **中断负载均衡或延迟处理**：将非关键中断处理逻辑延迟到更安全的上下文执行。\n\n该机制为内核提供了一种高效、安全且可扩展的中断后处理框架，尤其适用于实时性和可靠性要求高的子系统。",
      "similarity": 0.5460106134414673,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/irq_work.c",
          "start_line": 31,
          "end_line": 157,
          "content": [
            "static void wake_irq_workd(void)",
            "{",
            "\tstruct task_struct *tsk = __this_cpu_read(irq_workd);",
            "",
            "\tif (!llist_empty(this_cpu_ptr(&lazy_list)) && tsk)",
            "\t\twake_up_process(tsk);",
            "}",
            "static void irq_work_wake(struct irq_work *entry)",
            "{",
            "\twake_irq_workd();",
            "}",
            "static int irq_workd_should_run(unsigned int cpu)",
            "{",
            "\treturn !llist_empty(this_cpu_ptr(&lazy_list));",
            "}",
            "static bool irq_work_claim(struct irq_work *work)",
            "{",
            "\tint oflags;",
            "",
            "\toflags = atomic_fetch_or(IRQ_WORK_CLAIMED | CSD_TYPE_IRQ_WORK, &work->node.a_flags);",
            "\t/*",
            "\t * If the work is already pending, no need to raise the IPI.",
            "\t * The pairing smp_mb() in irq_work_single() makes sure",
            "\t * everything we did before is visible.",
            "\t */",
            "\tif (oflags & IRQ_WORK_PENDING)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "void __weak arch_irq_work_raise(void)",
            "{",
            "\t/*",
            "\t * Lame architectures will get the timer tick callback",
            "\t */",
            "}",
            "static __always_inline void irq_work_raise(struct irq_work *work)",
            "{",
            "\tif (trace_ipi_send_cpu_enabled() && arch_irq_work_has_interrupt())",
            "\t\ttrace_ipi_send_cpu(smp_processor_id(), _RET_IP_, work->func);",
            "",
            "\tarch_irq_work_raise();",
            "}",
            "static void __irq_work_queue_local(struct irq_work *work)",
            "{",
            "\tstruct llist_head *list;",
            "\tbool rt_lazy_work = false;",
            "\tbool lazy_work = false;",
            "\tint work_flags;",
            "",
            "\twork_flags = atomic_read(&work->node.a_flags);",
            "\tif (work_flags & IRQ_WORK_LAZY)",
            "\t\tlazy_work = true;",
            "\telse if (IS_ENABLED(CONFIG_PREEMPT_RT) &&",
            "\t\t !(work_flags & IRQ_WORK_HARD_IRQ))",
            "\t\trt_lazy_work = true;",
            "",
            "\tif (lazy_work || rt_lazy_work)",
            "\t\tlist = this_cpu_ptr(&lazy_list);",
            "\telse",
            "\t\tlist = this_cpu_ptr(&raised_list);",
            "",
            "\tif (!llist_add(&work->node.llist, list))",
            "\t\treturn;",
            "",
            "\t/* If the work is \"lazy\", handle it from next tick if any */",
            "\tif (!lazy_work || tick_nohz_tick_stopped())",
            "\t\tirq_work_raise(work);",
            "}",
            "bool irq_work_queue(struct irq_work *work)",
            "{",
            "\t/* Only queue if not already pending */",
            "\tif (!irq_work_claim(work))",
            "\t\treturn false;",
            "",
            "\t/* Queue the entry and raise the IPI if needed. */",
            "\tpreempt_disable();",
            "\t__irq_work_queue_local(work);",
            "\tpreempt_enable();",
            "",
            "\treturn true;",
            "}",
            "bool irq_work_queue_on(struct irq_work *work, int cpu)",
            "{",
            "#ifndef CONFIG_SMP",
            "\treturn irq_work_queue(work);",
            "",
            "#else /* CONFIG_SMP: */",
            "\t/* All work should have been flushed before going offline */",
            "\tWARN_ON_ONCE(cpu_is_offline(cpu));",
            "",
            "\t/* Only queue if not already pending */",
            "\tif (!irq_work_claim(work))",
            "\t\treturn false;",
            "",
            "\tkasan_record_aux_stack_noalloc(work);",
            "",
            "\tpreempt_disable();",
            "\tif (cpu != smp_processor_id()) {",
            "\t\t/* Arch remote IPI send/receive backend aren't NMI safe */",
            "\t\tWARN_ON_ONCE(in_nmi());",
            "",
            "\t\t/*",
            "\t\t * On PREEMPT_RT the items which are not marked as",
            "\t\t * IRQ_WORK_HARD_IRQ are added to the lazy list and a HARD work",
            "\t\t * item is used on the remote CPU to wake the thread.",
            "\t\t */",
            "\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT) &&",
            "\t\t    !(atomic_read(&work->node.a_flags) & IRQ_WORK_HARD_IRQ)) {",
            "",
            "\t\t\tif (!llist_add(&work->node.llist, &per_cpu(lazy_list, cpu)))",
            "\t\t\t\tgoto out;",
            "",
            "\t\t\twork = &per_cpu(irq_work_wakeup, cpu);",
            "\t\t\tif (!irq_work_claim(work))",
            "\t\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\t__smp_call_single_queue(cpu, &work->node.llist);",
            "\t} else {",
            "\t\t__irq_work_queue_local(work);",
            "\t}",
            "out:",
            "\tpreempt_enable();",
            "",
            "\treturn true;",
            "#endif /* CONFIG_SMP */",
            "}"
          ],
          "function_name": "wake_irq_workd, irq_work_wake, irq_workd_should_run, irq_work_claim, arch_irq_work_raise, irq_work_raise, __irq_work_queue_local, irq_work_queue, irq_work_queue_on",
          "description": "实现了中断工作项的排队逻辑，区分硬中断与延迟工作项，通过IPI或线程唤醒机制确保跨CPU执行，支持PREEMPT_RT配置下的延迟处理",
          "similarity": 0.5263946056365967
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/irq_work.c",
          "start_line": 1,
          "end_line": 30,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (C) 2010 Red Hat, Inc., Peter Zijlstra",
            " *",
            " * Provides a framework for enqueueing and running callbacks from hardirq",
            " * context. The enqueueing is NMI-safe.",
            " */",
            "",
            "#include <linux/bug.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/percpu.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/irqflags.h>",
            "#include <linux/sched.h>",
            "#include <linux/tick.h>",
            "#include <linux/cpu.h>",
            "#include <linux/notifier.h>",
            "#include <linux/smp.h>",
            "#include <linux/smpboot.h>",
            "#include <asm/processor.h>",
            "#include <linux/kasan.h>",
            "",
            "#include <trace/events/ipi.h>",
            "",
            "static DEFINE_PER_CPU(struct llist_head, raised_list);",
            "static DEFINE_PER_CPU(struct llist_head, lazy_list);",
            "static DEFINE_PER_CPU(struct task_struct *, irq_workd);",
            ""
          ],
          "function_name": null,
          "description": "定义了用于管理中断工作队列的per-CPU链表（raised_list/lazy_list）和irq_workd线程指针，提供NMI安全的enqueue框架",
          "similarity": 0.5256078243255615
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/irq_work.c",
          "start_line": 184,
          "end_line": 286,
          "content": [
            "bool irq_work_needs_cpu(void)",
            "{",
            "\tstruct llist_head *raised, *lazy;",
            "",
            "\traised = this_cpu_ptr(&raised_list);",
            "\tlazy = this_cpu_ptr(&lazy_list);",
            "",
            "\tif (llist_empty(raised) || arch_irq_work_has_interrupt())",
            "\t\tif (llist_empty(lazy))",
            "\t\t\treturn false;",
            "",
            "\t/* All work should have been flushed before going offline */",
            "\tWARN_ON_ONCE(cpu_is_offline(smp_processor_id()));",
            "",
            "\treturn true;",
            "}",
            "void irq_work_single(void *arg)",
            "{",
            "\tstruct irq_work *work = arg;",
            "\tint flags;",
            "",
            "\t/*",
            "\t * Clear the PENDING bit, after this point the @work can be re-used.",
            "\t * The PENDING bit acts as a lock, and we own it, so we can clear it",
            "\t * without atomic ops.",
            "\t */",
            "\tflags = atomic_read(&work->node.a_flags);",
            "\tflags &= ~IRQ_WORK_PENDING;",
            "\tatomic_set(&work->node.a_flags, flags);",
            "",
            "\t/*",
            "\t * See irq_work_claim().",
            "\t */",
            "\tsmp_mb();",
            "",
            "\tlockdep_irq_work_enter(flags);",
            "\twork->func(work);",
            "\tlockdep_irq_work_exit(flags);",
            "",
            "\t/*",
            "\t * Clear the BUSY bit, if set, and return to the free state if no-one",
            "\t * else claimed it meanwhile.",
            "\t */",
            "\t(void)atomic_cmpxchg(&work->node.a_flags, flags, flags & ~IRQ_WORK_BUSY);",
            "",
            "\tif ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||",
            "\t    !arch_irq_work_has_interrupt())",
            "\t\trcuwait_wake_up(&work->irqwait);",
            "}",
            "static void irq_work_run_list(struct llist_head *list)",
            "{",
            "\tstruct irq_work *work, *tmp;",
            "\tstruct llist_node *llnode;",
            "",
            "\t/*",
            "\t * On PREEMPT_RT IRQ-work which is not marked as HARD will be processed",
            "\t * in a per-CPU thread in preemptible context. Only the items which are",
            "\t * marked as IRQ_WORK_HARD_IRQ will be processed in hardirq context.",
            "\t */",
            "\tBUG_ON(!irqs_disabled() && !IS_ENABLED(CONFIG_PREEMPT_RT));",
            "",
            "\tif (llist_empty(list))",
            "\t\treturn;",
            "",
            "\tllnode = llist_del_all(list);",
            "\tllist_for_each_entry_safe(work, tmp, llnode, node.llist)",
            "\t\tirq_work_single(work);",
            "}",
            "void irq_work_run(void)",
            "{",
            "\tirq_work_run_list(this_cpu_ptr(&raised_list));",
            "\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tirq_work_run_list(this_cpu_ptr(&lazy_list));",
            "\telse",
            "\t\twake_irq_workd();",
            "}",
            "void irq_work_tick(void)",
            "{",
            "\tstruct llist_head *raised = this_cpu_ptr(&raised_list);",
            "",
            "\tif (!llist_empty(raised) && !arch_irq_work_has_interrupt())",
            "\t\tirq_work_run_list(raised);",
            "",
            "\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tirq_work_run_list(this_cpu_ptr(&lazy_list));",
            "\telse",
            "\t\twake_irq_workd();",
            "}",
            "void irq_work_sync(struct irq_work *work)",
            "{",
            "\tlockdep_assert_irqs_enabled();",
            "\tmight_sleep();",
            "",
            "\tif ((IS_ENABLED(CONFIG_PREEMPT_RT) && !irq_work_is_hard(work)) ||",
            "\t    !arch_irq_work_has_interrupt()) {",
            "\t\trcuwait_wait_event(&work->irqwait, !irq_work_is_busy(work),",
            "\t\t\t\t   TASK_UNINTERRUPTIBLE);",
            "\t\treturn;",
            "\t}",
            "",
            "\twhile (irq_work_is_busy(work))",
            "\t\tcpu_relax();",
            "}"
          ],
          "function_name": "irq_work_needs_cpu, irq_work_single, irq_work_run_list, irq_work_run, irq_work_tick, irq_work_sync",
          "description": "处理工作项的实际执行流程，包含单次执行逻辑、链表遍历运行及同步等待机制，区分硬中断上下文与RCU等待状态的处理",
          "similarity": 0.5138089060783386
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/irq_work.c",
          "start_line": 303,
          "end_line": 316,
          "content": [
            "static void run_irq_workd(unsigned int cpu)",
            "{",
            "\tirq_work_run_list(this_cpu_ptr(&lazy_list));",
            "}",
            "static void irq_workd_setup(unsigned int cpu)",
            "{",
            "\tsched_set_fifo_low(current);",
            "}",
            "static __init int irq_work_init_threads(void)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tBUG_ON(smpboot_register_percpu_thread(&irqwork_threads));",
            "\treturn 0;",
            "}"
          ],
          "function_name": "run_irq_workd, irq_workd_setup, irq_work_init_threads",
          "description": "初始化PREEMPT_RT环境下的per-CPU工作线程，注册并启动处理延迟工作项的专用线程，通过smpboot接口创建线程实体",
          "similarity": 0.4745962619781494
        }
      ]
    },
    {
      "source_file": "kernel/irq/irqdesc.c",
      "md_summary": "> 自动生成时间: 2025-10-25 13:59:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `irq\\irqdesc.c`\n\n---\n\n# `irq/irqdesc.c` 技术文档\n\n## 1. 文件概述\n\n`irq/irqdesc.c` 是 Linux 内核通用中断子系统（Generic IRQ）的核心实现文件之一，负责中断描述符（`struct irq_desc`）的分配、初始化、管理和释放。该文件实现了中断描述符的生命周期管理，包括在稀疏 IRQ（`CONFIG_SPARSE_IRQ`）配置下的动态分配机制，以及与 SMP（对称多处理）相关的中断亲和性（affinity）管理。它为上层中断处理（如设备驱动注册中断处理函数）和底层硬件中断控制器（通过 `irq_chip`）之间提供了统一的抽象层。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **`struct irq_desc`**：中断描述符，代表一个逻辑中断号（IRQ number），包含中断状态、处理函数、统计信息、锁、亲和性掩码等。\n- **`struct irq_data`**：嵌入在 `irq_desc` 中，包含与硬件中断控制器相关的数据（如 `irq_chip`、`hwirq`、`irq_domain` 等）。\n- **`struct irq_common_data`**：`irq_desc` 和 `irq_data` 共享的数据，如 MSI 描述符、亲和性掩码等。\n- **`sparse_irqs`**：基于 Maple Tree 的稀疏 IRQ 描述符存储结构，用于动态分配 IRQ 号。\n\n### 主要函数\n- **`init_desc()`**：初始化一个 `irq_desc` 实例，包括分配 per-CPU 统计结构、SMP 掩码、初始化锁和默认值。\n- **`desc_set_defaults()`**：设置 `irq_desc` 的默认初始状态（如禁用、屏蔽、默认处理函数为 `handle_bad_irq`）。\n- **`alloc_masks()` / `free_masks()` / `desc_smp_init()`**：SMP 相关的亲和性掩码（affinity、effective_affinity、pending_mask）的分配、释放和初始化。\n- **`irq_find_free_area()` / `irq_find_at_or_after()`**：在稀疏 IRQ 模式下查找可用的 IRQ 号范围或下一个可用 IRQ。\n- **`irq_insert_desc()` / `delete_irq_desc()`**：将 `irq_desc` 插入或从稀疏 IRQ 的 Maple Tree 中删除。\n- **`init_irq_default_affinity()`**：初始化默认的中断亲和性掩码（通常为所有 CPU）。\n- **`irq_kobj_release()` 及相关 sysfs 属性函数**：实现 IRQ 描述符的 sysfs 接口（如 `per_cpu_count`、`chip_name`、`hwirq` 等）。\n\n### 全局变量\n- **`nr_irqs`**：系统支持的最大 IRQ 数量，可被平台代码覆盖。\n- **`irq_default_affinity`**：默认的中断亲和性 CPU 掩码（SMP 模式下）。\n- **`irq_desc_lock_class`**：用于 lockdep 的 IRQ 描述符自旋锁的统一锁类。\n\n## 3. 关键实现\n\n### 稀疏 IRQ 管理（`CONFIG_SPARSE_IRQ`）\n- 使用 **Maple Tree** 数据结构（`sparse_irqs`）替代传统的静态数组，支持动态分配 IRQ 描述符。\n- `irq_find_free_area()` 利用 Maple Tree 的空闲区间查找功能，高效分配连续的 IRQ 号。\n- `irq_insert_desc()` 和 `delete_irq_desc()` 通过 RCU 安全地插入/删除描述符，支持运行时 IRQ 的动态增删。\n- 每个 `irq_desc` 作为独立的 kobject，通过 sysfs 暴露属性（如中断计数、芯片名称等）。\n\n### SMP 中断亲和性\n- **亲和性掩码**：每个 IRQ 可配置其允许运行的 CPU 集合（`affinity`），支持负载均衡和局部性优化。\n- **有效亲和性**（`effective_affinity`）：实际生效的亲和性（可能受中断迁移或 pending 状态影响）。\n- **Pending 掩码**（`pending_mask`）：用于在中断迁移过程中暂存中断事件。\n- 启动参数 `irqaffinity=` 可设置全局默认亲和性，但至少包含引导 CPU 以防配置错误。\n\n### 描述符初始化\n- `init_desc()` 完成描述符的完整初始化：\n  - 分配 per-CPU 中断统计结构（`kstat_irqs`）。\n  - 初始化 SMP 相关掩码（若启用）。\n  - 设置自旋锁（带 lockdep 类）和互斥锁（`request_mutex`）。\n  - 调用 `desc_set_defaults()` 设置默认状态（禁用、屏蔽、无效处理函数）。\n  - 初始化 RCU 回调（用于稀疏 IRQ 的延迟释放）。\n\n### 锁与并发控制\n- **`desc->lock`**：raw spinlock，保护描述符关键字段（如状态、处理函数），在中断上下文中使用。\n- **`desc->request_mutex`**：mutex，用于串行化中断请求/释放操作（如 `request_irq()`）。\n- **Maple Tree 操作**：通过外部互斥锁（`sparse_irq_lock`）和 RCU 保证并发安全。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/irq.h>`、`<linux/interrupt.h>`：IRQ 子系统核心 API 和数据结构。\n  - `<linux/irqdomain.h>`：硬件中断号（hwirq）到逻辑 IRQ 号的映射。\n  - `<linux/maple_tree.h>`：稀疏 IRQ 的底层存储实现。\n  - `<linux/sysfs.h>`：sysfs 属性支持。\n  - `\"internals.h\"`：IRQ 子系统内部函数和宏。\n- **配置依赖**：\n  - `CONFIG_SMP`：启用多处理器支持（亲和性掩码管理）。\n  - `CONFIG_SPARSE_IRQ`：启用动态 IRQ 分配（替代静态数组）。\n  - `CONFIG_GENERIC_PENDING_IRQ` / `CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK`：扩展的 SMP 中断管理功能。\n- **模块交互**：\n  - **中断控制器驱动**：通过 `irq_chip` 操作硬件，依赖 `irq_desc` 提供的抽象。\n  - **设备驱动**：通过 `request_irq()` 等接口注册中断处理函数，操作 `irq_desc`。\n  - **电源管理**：通过 `wakeup` 属性控制中断的唤醒能力。\n\n## 5. 使用场景\n\n- **系统启动阶段**：\n  - 初始化默认中断亲和性（`init_irq_default_affinity()`）。\n  - 预分配或动态创建平台所需的 IRQ 描述符（通过 `alloc_descs()` 等）。\n- **设备驱动加载/卸载**：\n  - 动态分配 IRQ 描述符（稀疏 IRQ 模式下通过 `irq_alloc_desc()`）。\n  - 注册/注销中断处理函数（修改 `handle_irq` 和 action 链表）。\n- **运行时中断管理**：\n  - 修改中断亲和性（`/proc/irq/<n>/smp_affinity`）。\n  - 查询中断统计信息（`/proc/interrupts`，通过 per-CPU 计数）。\n  - 通过 sysfs 查看 IRQ 属性（芯片名称、硬件 IRQ 号、触发类型等）。\n- **中断迁移**（SMP）：\n  - 在 CPU 热插拔或负载均衡时，更新 `affinity` 和 `pending_mask`。\n- **错误处理**：\n  - 未处理的中断由 `handle_bad_irq` 处理，记录到 `irqs_unhandled`。",
      "similarity": 0.5451322197914124,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 705,
          "end_line": 824,
          "content": [
            "int generic_handle_irq(unsigned int irq)",
            "{",
            "\treturn handle_irq_desc(irq_to_desc(irq));",
            "}",
            "int generic_handle_irq_safe(unsigned int irq)",
            "{",
            "\tunsigned long flags;",
            "\tint ret;",
            "",
            "\tlocal_irq_save(flags);",
            "\tret = handle_irq_desc(irq_to_desc(irq));",
            "\tlocal_irq_restore(flags);",
            "\treturn ret;",
            "}",
            "int generic_handle_domain_irq(struct irq_domain *domain, unsigned int hwirq)",
            "{",
            "\treturn handle_irq_desc(irq_resolve_mapping(domain, hwirq));",
            "}",
            "int generic_handle_domain_irq_safe(struct irq_domain *domain, unsigned int hwirq)",
            "{",
            "\tunsigned long flags;",
            "\tint ret;",
            "",
            "\tlocal_irq_save(flags);",
            "\tret = handle_irq_desc(irq_resolve_mapping(domain, hwirq));",
            "\tlocal_irq_restore(flags);",
            "\treturn ret;",
            "}",
            "int generic_handle_domain_nmi(struct irq_domain *domain, unsigned int hwirq)",
            "{",
            "\tWARN_ON_ONCE(!in_nmi());",
            "\treturn handle_irq_desc(irq_resolve_mapping(domain, hwirq));",
            "}",
            "void irq_free_descs(unsigned int from, unsigned int cnt)",
            "{",
            "\tint i;",
            "",
            "\tif (from >= nr_irqs || (from + cnt) > nr_irqs)",
            "\t\treturn;",
            "",
            "\tmutex_lock(&sparse_irq_lock);",
            "\tfor (i = 0; i < cnt; i++)",
            "\t\tfree_desc(from + i);",
            "",
            "\tmutex_unlock(&sparse_irq_lock);",
            "}",
            "int __ref",
            "__irq_alloc_descs(int irq, unsigned int from, unsigned int cnt, int node,",
            "\t\t  struct module *owner, const struct irq_affinity_desc *affinity)",
            "{",
            "\tint start, ret;",
            "",
            "\tif (!cnt)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (irq >= 0) {",
            "\t\tif (from > irq)",
            "\t\t\treturn -EINVAL;",
            "\t\tfrom = irq;",
            "\t} else {",
            "\t\t/*",
            "\t\t * For interrupts which are freely allocated the",
            "\t\t * architecture can force a lower bound to the @from",
            "\t\t * argument. x86 uses this to exclude the GSI space.",
            "\t\t */",
            "\t\tfrom = arch_dynirq_lower_bound(from);",
            "\t}",
            "",
            "\tmutex_lock(&sparse_irq_lock);",
            "",
            "\tstart = irq_find_free_area(from, cnt);",
            "\tret = -EEXIST;",
            "\tif (irq >=0 && start != irq)",
            "\t\tgoto unlock;",
            "",
            "\tif (start + cnt > nr_irqs) {",
            "\t\tret = irq_expand_nr_irqs(start + cnt);",
            "\t\tif (ret)",
            "\t\t\tgoto unlock;",
            "\t}",
            "\tret = alloc_descs(start, cnt, node, affinity, owner);",
            "unlock:",
            "\tmutex_unlock(&sparse_irq_lock);",
            "\treturn ret;",
            "}",
            "unsigned int irq_get_next_irq(unsigned int offset)",
            "{",
            "\treturn irq_find_at_or_after(offset);",
            "}",
            "void __irq_put_desc_unlock(struct irq_desc *desc, unsigned long flags, bool bus)",
            "\t__releases(&desc->lock)",
            "{",
            "\traw_spin_unlock_irqrestore(&desc->lock, flags);",
            "\tif (bus)",
            "\t\tchip_bus_sync_unlock(desc);",
            "}",
            "int irq_set_percpu_devid_partition(unsigned int irq,",
            "\t\t\t\t   const struct cpumask *affinity)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (!desc)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (desc->percpu_enabled)",
            "\t\treturn -EINVAL;",
            "",
            "\tdesc->percpu_enabled = kzalloc(sizeof(*desc->percpu_enabled), GFP_KERNEL);",
            "",
            "\tif (!desc->percpu_enabled)",
            "\t\treturn -ENOMEM;",
            "",
            "\tif (affinity)",
            "\t\tdesc->percpu_affinity = affinity;",
            "\telse",
            "\t\tdesc->percpu_affinity = cpu_possible_mask;",
            "",
            "\tirq_set_percpu_devid_flags(irq);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "generic_handle_irq, generic_handle_irq_safe, generic_handle_domain_irq, generic_handle_domain_irq_safe, generic_handle_domain_nmi, irq_free_descs, __irq_alloc_descs, irq_get_next_irq, __irq_put_desc_unlock, irq_set_percpu_devid_partition",
          "description": "generic_handle_irq 安全处理通用中断，调用handle_irq_desc。generic_handle_domain_irq 处理IRQ domain映射的硬件中断。irq_free_descs 释放指定范围的中断描述符。__irq_alloc_descs 动态分配连续中断号并初始化描述符。irq_get_next_irq 获取下一个可用中断号。__irq_put_desc_unlock 解锁中断描述符并同步总线。irq_set_percpu_devid_partition 设置中断亲和性分区。",
          "similarity": 0.579975962638855
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 562,
          "end_line": 667,
          "content": [
            "int __init early_irq_init(void)",
            "{",
            "\tint i, initcnt, node = first_online_node;",
            "\tstruct irq_desc *desc;",
            "",
            "\tinit_irq_default_affinity();",
            "",
            "\t/* Let arch update nr_irqs and return the nr of preallocated irqs */",
            "\tinitcnt = arch_probe_nr_irqs();",
            "\tprintk(KERN_INFO \"NR_IRQS: %d, nr_irqs: %d, preallocated irqs: %d\\n\",",
            "\t       NR_IRQS, nr_irqs, initcnt);",
            "",
            "\tif (WARN_ON(nr_irqs > MAX_SPARSE_IRQS))",
            "\t\tnr_irqs = MAX_SPARSE_IRQS;",
            "",
            "\tif (WARN_ON(initcnt > MAX_SPARSE_IRQS))",
            "\t\tinitcnt = MAX_SPARSE_IRQS;",
            "",
            "\tif (initcnt > nr_irqs)",
            "\t\tnr_irqs = initcnt;",
            "",
            "\tfor (i = 0; i < initcnt; i++) {",
            "\t\tdesc = alloc_desc(i, node, 0, NULL, NULL);",
            "\t\tirq_insert_desc(i, desc);",
            "\t}",
            "\treturn arch_early_irq_init();",
            "}",
            "int __init early_irq_init(void)",
            "{",
            "\tint count, i, node = first_online_node;",
            "\tint ret;",
            "",
            "\tinit_irq_default_affinity();",
            "",
            "\tprintk(KERN_INFO \"NR_IRQS: %d\\n\", NR_IRQS);",
            "",
            "\tcount = ARRAY_SIZE(irq_desc);",
            "",
            "\tfor (i = 0; i < count; i++) {",
            "\t\tret = init_desc(irq_desc + i, i, node, 0, NULL, NULL);",
            "\t\tif (unlikely(ret))",
            "\t\t\tgoto __free_desc_res;",
            "\t}",
            "",
            "\treturn arch_early_irq_init();",
            "",
            "__free_desc_res:",
            "\twhile (--i >= 0) {",
            "\t\tfree_masks(irq_desc + i);",
            "\t\tfree_percpu(irq_desc[i].kstat_irqs);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "static void free_desc(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "\tunsigned long flags;",
            "",
            "\traw_spin_lock_irqsave(&desc->lock, flags);",
            "\tdesc_set_defaults(irq, desc, irq_desc_get_node(desc), NULL, NULL);",
            "\traw_spin_unlock_irqrestore(&desc->lock, flags);",
            "\tdelete_irq_desc(irq);",
            "}",
            "static inline int alloc_descs(unsigned int start, unsigned int cnt, int node,",
            "\t\t\t      const struct irq_affinity_desc *affinity,",
            "\t\t\t      struct module *owner)",
            "{",
            "\tu32 i;",
            "",
            "\tfor (i = 0; i < cnt; i++) {",
            "\t\tstruct irq_desc *desc = irq_to_desc(start + i);",
            "",
            "\t\tdesc->owner = owner;",
            "\t\tirq_insert_desc(start + i, desc);",
            "\t}",
            "\treturn start;",
            "}",
            "static int irq_expand_nr_irqs(unsigned int nr)",
            "{",
            "\treturn -ENOMEM;",
            "}",
            "void irq_mark_irq(unsigned int irq)",
            "{",
            "\tmutex_lock(&sparse_irq_lock);",
            "\tirq_insert_desc(irq, irq_desc + irq);",
            "\tmutex_unlock(&sparse_irq_lock);",
            "}",
            "void irq_init_desc(unsigned int irq)",
            "{",
            "\tfree_desc(irq);",
            "}",
            "int handle_irq_desc(struct irq_desc *desc)",
            "{",
            "\tstruct irq_data *data;",
            "",
            "\tif (!desc)",
            "\t\treturn -EINVAL;",
            "",
            "\tdata = irq_desc_get_irq_data(desc);",
            "\tif (WARN_ON_ONCE(!in_hardirq() && handle_enforce_irqctx(data)))",
            "\t\treturn -EPERM;",
            "",
            "\tgeneric_handle_irq_desc(desc);",
            "\treturn 0;",
            "}"
          ],
          "function_name": "early_irq_init, early_irq_init, free_desc, alloc_descs, irq_expand_nr_irqs, irq_mark_irq, irq_init_desc, handle_irq_desc",
          "description": "early_irq_init 初始化早期中断描述符，调用架构特定函数确定中断数量并分配初始中断描述符。free_desc 释放中断描述符并重置为其默认状态。alloc_descs 批量分配中断描述符到指定范围。irq_mark_irq 将中断标记为已初始化。irq_init_desc 初始化指定中断描述符。handle_irq_desc 处理中断描述符，检查硬中断上下文并调用通用处理函数。",
          "similarity": 0.5561732053756714
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 938,
          "end_line": 1027,
          "content": [
            "int irq_set_percpu_devid(unsigned int irq)",
            "{",
            "\treturn irq_set_percpu_devid_partition(irq, NULL);",
            "}",
            "int irq_get_percpu_devid_partition(unsigned int irq, struct cpumask *affinity)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (!desc || !desc->percpu_enabled)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (affinity)",
            "\t\tcpumask_copy(affinity, desc->percpu_affinity);",
            "",
            "\treturn 0;",
            "}",
            "void kstat_incr_irq_this_cpu(unsigned int irq)",
            "{",
            "\tkstat_incr_irqs_this_cpu(irq_to_desc(irq));",
            "}",
            "unsigned int kstat_irqs_cpu(unsigned int irq, int cpu)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\treturn desc && desc->kstat_irqs ? per_cpu(desc->kstat_irqs->cnt, cpu) : 0;",
            "}",
            "static bool irq_is_nmi(struct irq_desc *desc)",
            "{",
            "\treturn desc->istate & IRQS_NMI;",
            "}",
            "unsigned int kstat_irqs_desc(struct irq_desc *desc, const struct cpumask *cpumask)",
            "{",
            "\tunsigned int sum = 0;",
            "\tint cpu;",
            "",
            "\tif (!irq_settings_is_per_cpu_devid(desc) &&",
            "\t    !irq_settings_is_per_cpu(desc) &&",
            "\t    !irq_is_nmi(desc))",
            "\t\treturn data_race(desc->tot_count);",
            "",
            "\tfor_each_cpu(cpu, cpumask)",
            "\t\tsum += data_race(per_cpu(desc->kstat_irqs->cnt, cpu));",
            "\treturn sum;",
            "}",
            "static unsigned int kstat_irqs(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (!desc || !desc->kstat_irqs)",
            "\t\treturn 0;",
            "\treturn kstat_irqs_desc(desc, cpu_possible_mask);",
            "}",
            "void kstat_snapshot_irqs(void)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tunsigned int irq;",
            "",
            "\tfor_each_irq_desc(irq, desc) {",
            "\t\tif (!desc->kstat_irqs)",
            "\t\t\tcontinue;",
            "\t\tthis_cpu_write(desc->kstat_irqs->ref, this_cpu_read(desc->kstat_irqs->cnt));",
            "\t}",
            "}",
            "unsigned int kstat_get_irq_since_snapshot(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (!desc || !desc->kstat_irqs)",
            "\t\treturn 0;",
            "\treturn this_cpu_read(desc->kstat_irqs->cnt) - this_cpu_read(desc->kstat_irqs->ref);",
            "}",
            "unsigned int kstat_irqs_usr(unsigned int irq)",
            "{",
            "\tunsigned int sum;",
            "",
            "\trcu_read_lock();",
            "\tsum = kstat_irqs(irq);",
            "\trcu_read_unlock();",
            "\treturn sum;",
            "}",
            "void __irq_set_lockdep_class(unsigned int irq, struct lock_class_key *lock_class,",
            "\t\t\t     struct lock_class_key *request_class)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tif (desc) {",
            "\t\tlockdep_set_class(&desc->lock, lock_class);",
            "\t\tlockdep_set_class(&desc->request_mutex, request_class);",
            "\t}",
            "}"
          ],
          "function_name": "irq_set_percpu_devid, irq_get_percpu_devid_partition, kstat_incr_irq_this_cpu, kstat_irqs_cpu, irq_is_nmi, kstat_irqs_desc, kstat_irqs, kstat_snapshot_irqs, kstat_get_irq_since_snapshot, kstat_irqs_usr, __irq_set_lockdep_class",
          "description": "irq_set_percpu_devid 设置中断为每个CPU专用模式。irq_get_percpu_devid_partition 获取中断的亲和性掩码。kstat_incr_irq_this_cpu 增加当前CPU的中断统计计数。kstat_irqs_cpu 查询指定CPU的中断次数。irq_is_nmi 判断中断是否为NMI。kstat_irqs_desc 计算指定CPU掩码下的中断总数。kstat_snapshot_irqs 快照中断统计数据。kstat_get_irq_since_snapshot 获取自快照后的中断次数。__irq_set_lockdep_class 设置中断描述符锁的锁跟踪类别。",
          "similarity": 0.5195629596710205
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 27,
          "end_line": 127,
          "content": [
            "static int __init irq_affinity_setup(char *str)",
            "{",
            "\talloc_bootmem_cpumask_var(&irq_default_affinity);",
            "\tcpulist_parse(str, irq_default_affinity);",
            "\t/*",
            "\t * Set at least the boot cpu. We don't want to end up with",
            "\t * bugreports caused by random commandline masks",
            "\t */",
            "\tcpumask_set_cpu(smp_processor_id(), irq_default_affinity);",
            "\treturn 1;",
            "}",
            "static void __init init_irq_default_affinity(void)",
            "{",
            "\tif (!cpumask_available(irq_default_affinity))",
            "\t\tzalloc_cpumask_var(&irq_default_affinity, GFP_NOWAIT);",
            "\tif (cpumask_empty(irq_default_affinity))",
            "\t\tcpumask_setall(irq_default_affinity);",
            "}",
            "static void __init init_irq_default_affinity(void)",
            "{",
            "}",
            "static int alloc_masks(struct irq_desc *desc, int node)",
            "{",
            "\tif (!zalloc_cpumask_var_node(&desc->irq_common_data.affinity,",
            "\t\t\t\t     GFP_KERNEL, node))",
            "\t\treturn -ENOMEM;",
            "",
            "#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK",
            "\tif (!zalloc_cpumask_var_node(&desc->irq_common_data.effective_affinity,",
            "\t\t\t\t     GFP_KERNEL, node)) {",
            "\t\tfree_cpumask_var(desc->irq_common_data.affinity);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "#endif",
            "",
            "#ifdef CONFIG_GENERIC_PENDING_IRQ",
            "\tif (!zalloc_cpumask_var_node(&desc->pending_mask, GFP_KERNEL, node)) {",
            "#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK",
            "\t\tfree_cpumask_var(desc->irq_common_data.effective_affinity);",
            "#endif",
            "\t\tfree_cpumask_var(desc->irq_common_data.affinity);",
            "\t\treturn -ENOMEM;",
            "\t}",
            "#endif",
            "\treturn 0;",
            "}",
            "static void desc_smp_init(struct irq_desc *desc, int node,",
            "\t\t\t  const struct cpumask *affinity)",
            "{",
            "\tif (!affinity)",
            "\t\taffinity = irq_default_affinity;",
            "\tcpumask_copy(desc->irq_common_data.affinity, affinity);",
            "",
            "#ifdef CONFIG_GENERIC_PENDING_IRQ",
            "\tcpumask_clear(desc->pending_mask);",
            "#endif",
            "#ifdef CONFIG_NUMA",
            "\tdesc->irq_common_data.node = node;",
            "#endif",
            "}",
            "static void free_masks(struct irq_desc *desc)",
            "{",
            "#ifdef CONFIG_GENERIC_PENDING_IRQ",
            "\tfree_cpumask_var(desc->pending_mask);",
            "#endif",
            "\tfree_cpumask_var(desc->irq_common_data.affinity);",
            "#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK",
            "\tfree_cpumask_var(desc->irq_common_data.effective_affinity);",
            "#endif",
            "}",
            "static inline int",
            "alloc_masks(struct irq_desc *desc, int node) { return 0; }",
            "static inline void",
            "desc_smp_init(struct irq_desc *desc, int node, const struct cpumask *affinity) { }",
            "static inline void free_masks(struct irq_desc *desc) { }",
            "static void desc_set_defaults(unsigned int irq, struct irq_desc *desc, int node,",
            "\t\t\t      const struct cpumask *affinity, struct module *owner)",
            "{",
            "\tint cpu;",
            "",
            "\tdesc->irq_common_data.handler_data = NULL;",
            "\tdesc->irq_common_data.msi_desc = NULL;",
            "",
            "\tdesc->irq_data.common = &desc->irq_common_data;",
            "\tdesc->irq_data.irq = irq;",
            "\tdesc->irq_data.chip = &no_irq_chip;",
            "\tdesc->irq_data.chip_data = NULL;",
            "\tirq_settings_clr_and_set(desc, ~0, _IRQ_DEFAULT_INIT_FLAGS);",
            "\tirqd_set(&desc->irq_data, IRQD_IRQ_DISABLED);",
            "\tirqd_set(&desc->irq_data, IRQD_IRQ_MASKED);",
            "\tdesc->handle_irq = handle_bad_irq;",
            "\tdesc->depth = 1;",
            "\tdesc->irq_count = 0;",
            "\tdesc->irqs_unhandled = 0;",
            "\tdesc->tot_count = 0;",
            "\tdesc->name = NULL;",
            "\tdesc->owner = owner;",
            "\tfor_each_possible_cpu(cpu)",
            "\t\t*per_cpu_ptr(desc->kstat_irqs, cpu) = (struct irqstat) { };",
            "\tdesc_smp_init(desc, node, affinity);",
            "}"
          ],
          "function_name": "irq_affinity_setup, init_irq_default_affinity, init_irq_default_affinity, alloc_masks, desc_smp_init, free_masks, alloc_masks, desc_smp_init, free_masks, desc_set_defaults",
          "description": "包含中断亲和性初始化与内存分配相关函数，负责设置默认CPU亲和掩码、分配irq_desc结构体的affinity字段、初始化SMP相关信息及释放相关资源。存在多处函数重载实现，体现不同配置条件下的差异化处理。",
          "similarity": 0.5106658339500427
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/irq/irqdesc.c",
          "start_line": 422,
          "end_line": 523,
          "content": [
            "static void irq_sysfs_add(int irq, struct irq_desc *desc) {}",
            "static void irq_sysfs_del(struct irq_desc *desc) {}",
            "void irq_lock_sparse(void)",
            "{",
            "\tmutex_lock(&sparse_irq_lock);",
            "}",
            "void irq_unlock_sparse(void)",
            "{",
            "\tmutex_unlock(&sparse_irq_lock);",
            "}",
            "static void irq_kobj_release(struct kobject *kobj)",
            "{",
            "\tstruct irq_desc *desc = container_of(kobj, struct irq_desc, kobj);",
            "",
            "\tfree_masks(desc);",
            "\tfree_percpu(desc->kstat_irqs);",
            "\tkfree(desc);",
            "}",
            "static void delayed_free_desc(struct rcu_head *rhp)",
            "{",
            "\tstruct irq_desc *desc = container_of(rhp, struct irq_desc, rcu);",
            "",
            "\tkobject_put(&desc->kobj);",
            "}",
            "static void free_desc(unsigned int irq)",
            "{",
            "\tstruct irq_desc *desc = irq_to_desc(irq);",
            "",
            "\tirq_remove_debugfs_entry(desc);",
            "\tunregister_irq_proc(irq, desc);",
            "",
            "\t/*",
            "\t * sparse_irq_lock protects also show_interrupts() and",
            "\t * kstat_irq_usr(). Once we deleted the descriptor from the",
            "\t * sparse tree we can free it. Access in proc will fail to",
            "\t * lookup the descriptor.",
            "\t *",
            "\t * The sysfs entry must be serialized against a concurrent",
            "\t * irq_sysfs_init() as well.",
            "\t */",
            "\tirq_sysfs_del(desc);",
            "\tdelete_irq_desc(irq);",
            "",
            "\t/*",
            "\t * We free the descriptor, masks and stat fields via RCU. That",
            "\t * allows demultiplex interrupts to do rcu based management of",
            "\t * the child interrupts.",
            "\t * This also allows us to use rcu in kstat_irqs_usr().",
            "\t */",
            "\tcall_rcu(&desc->rcu, delayed_free_desc);",
            "}",
            "static int alloc_descs(unsigned int start, unsigned int cnt, int node,",
            "\t\t       const struct irq_affinity_desc *affinity,",
            "\t\t       struct module *owner)",
            "{",
            "\tstruct irq_desc *desc;",
            "\tint i;",
            "",
            "\t/* Validate affinity mask(s) */",
            "\tif (affinity) {",
            "\t\tfor (i = 0; i < cnt; i++) {",
            "\t\t\tif (cpumask_empty(&affinity[i].mask))",
            "\t\t\t\treturn -EINVAL;",
            "\t\t}",
            "\t}",
            "",
            "\tfor (i = 0; i < cnt; i++) {",
            "\t\tconst struct cpumask *mask = NULL;",
            "\t\tunsigned int flags = 0;",
            "",
            "\t\tif (affinity) {",
            "\t\t\tif (affinity->is_managed) {",
            "\t\t\t\tflags = IRQD_AFFINITY_MANAGED |",
            "\t\t\t\t\tIRQD_MANAGED_SHUTDOWN;",
            "\t\t\t}",
            "\t\t\tflags |= IRQD_AFFINITY_SET;",
            "\t\t\tmask = &affinity->mask;",
            "\t\t\tnode = cpu_to_node(cpumask_first(mask));",
            "\t\t\taffinity++;",
            "\t\t}",
            "",
            "\t\tdesc = alloc_desc(start + i, node, flags, mask, owner);",
            "\t\tif (!desc)",
            "\t\t\tgoto err;",
            "\t\tirq_insert_desc(start + i, desc);",
            "\t\tirq_sysfs_add(start + i, desc);",
            "\t\tirq_add_debugfs_entry(start + i, desc);",
            "\t}",
            "\treturn start;",
            "",
            "err:",
            "\tfor (i--; i >= 0; i--)",
            "\t\tfree_desc(start + i);",
            "\treturn -ENOMEM;",
            "}",
            "static int irq_expand_nr_irqs(unsigned int nr)",
            "{",
            "\tif (nr > MAX_SPARSE_IRQS)",
            "\t\treturn -ENOMEM;",
            "\tnr_irqs = nr;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "irq_sysfs_add, irq_sysfs_del, irq_lock_sparse, irq_unlock_sparse, irq_kobj_release, delayed_free_desc, free_desc, alloc_descs, irq_expand_nr_irqs",
          "description": "包含中断描述符的延迟释放机制与批量分配逻辑，利用RCU机制安全释放资源，实现中断描述符的动态扩展与回收，支持多CPU环境下对中断资源的高效管理。",
          "similarity": 0.46693018078804016
        }
      ]
    }
  ]
}