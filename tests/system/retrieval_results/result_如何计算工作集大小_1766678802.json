{
  "query": "如何计算工作集大小",
  "timestamp": "2025-12-26 00:06:42",
  "retrieved_files": [
    {
      "source_file": "mm/workingset.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:34:58\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workingset.c`\n\n---\n\n# workingset.c 技术文档\n\n## 1. 文件概述\n\n`workingset.c` 实现了 Linux 内核中的 **工作集检测（Workingset Detection）** 机制，用于优化页面回收（page reclaim）策略。该机制通过跟踪页面的访问模式和重故障距离（refault distance），智能判断哪些页面应保留在内存中，从而减少系统颠簸（thrashing）并提升缓存效率。核心思想是：若一个被换出的页面在短时间内再次被访问（即重故障），且其重故障距离小于当前活跃页面数量，则应将其重新激活，以取代可能已不再活跃的现有活跃页面。\n\n## 2. 核心功能\n\n### 主要数据结构\n- **Shadow Entry（影子条目）**：存储在页缓存槽位中的元数据，包含页面被驱逐时的时间戳（eviction counter 快照）、内存控制组 ID、节点 ID 和工作集标志。\n- **node->nonresident_age**：每个 NUMA 节点维护的计数器，记录非驻留页面的“年龄”，用于计算重故障距离。\n\n### 关键宏定义\n- `WORKINGSET_SHIFT`：工作集标识位偏移。\n- `EVICTION_SHIFT` / `EVICTION_MASK`：用于在 xarray 条目中紧凑编码驱逐时间戳的位操作参数。\n- `bucket_order`：当时间戳位数不足时，用于对驱逐事件进行分桶聚合的粒度。\n\n### 核心函数（部分实现）\n- `pack_shadow()`：将内存控制组 ID、节点指针、驱逐计数器值和工作集标志打包成一个 shadow entry。\n- （注：代码片段未完整展示其他关键函数如 `workingset_refault()`、`workingset_activation()` 等，但文档基于完整机制描述）\n\n## 3. 关键实现\n\n### 双 CLOCK 列表模型\n- 每个 NUMA 节点为文件页维护两个 LRU 列表：**inactive list**（不活跃）和 **active list**（活跃）。\n- 新缺页页面加入 inactive list 头部；回收从 inactive list 尾部扫描。\n- 在 inactive list 上被二次访问的页面晋升至 active list；active list 过长时，尾部页面降级到 inactive list。\n\n### 重故障距离（Refault Distance）算法\n1. **驱逐时记录**：页面被驱逐时，将其所在节点的 `nonresident_age` 计数器值（代表累计的驱逐+激活次数）作为时间戳存入 shadow entry。\n2. **重故障时计算**：\n   - 当缺页发生且存在对应 shadow entry 时，读取当前 `nonresident_age` 值（R）与 shadow 中存储的值（E）。\n   - 重故障距离 = `R - E`，表示页面不在内存期间发生的最小页面访问次数。\n3. **激活决策**：\n   - 若 `重故障距离 <= 当前活跃页面总数（file + anon）`，则认为若当时有足够 inactive 空间，该页面本可被激活而避免驱逐。\n   - 因此**乐观地激活**该重故障页面，使其与现有活跃页面竞争内存空间。\n\n### 影子条目压缩存储\n- 利用 xarray 条目的有限位宽（`BITS_PER_XA_VALUE`），通过位域拼接存储：\n  - 节点 ID（`NODES_SHIFT` 位）\n  - 内存控制组 ID（`MEM_CGROUP_ID_SHIFT` 位）\n  - 工作集标志（`WORKINGSET_SHIFT` 位）\n  - 驱逐时间戳（剩余位，必要时通过 `bucket_order` 降低精度）\n\n## 4. 依赖关系\n\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mm_inline.h>` — 提供页框、LRU 列表、页表操作等基础支持。\n- **内存控制组**：`<linux/memcontrol.h>` — 支持按 cgroup 隔离工作集统计。\n- **页缓存与交换**：`<linux/pagemap.h>`, `<linux/swap.h>`, `<linux/shmem_fs.h>` — 处理文件页、匿名页、tmpfs 页的回收逻辑。\n- **xarray 数据结构**：用于高效存储和检索 shadow entries（隐含在 `pack_shadow` 的位操作中）。\n- **DAX 支持**：`<linux/dax.h>` — 确保直接访问持久内存设备的页面也能参与工作集检测。\n\n## 5. 使用场景\n\n- **内存压力下的页面回收**：当系统内存紧张触发 kswapd 或直接回收时，工作集检测机制指导选择最优的牺牲页面。\n- **工作集切换检测**：识别应用程序工作集的动态变化（如新任务启动、旧任务结束），快速淘汰过时缓存。\n- **防止颠簸（Thrashing）**：在活跃工作集大小接近或超过可用内存时，通过重故障距离预测避免频繁换入换出。\n- **混合工作负载优化**：同时处理文件缓存（page cache）和匿名内存（anonymous pages）的工作集，平衡二者内存分配。\n- **容器化环境**：结合 memcg，在多租户系统中为每个容器独立维护工作集状态，避免相互干扰。",
      "similarity": 0.5551118850708008,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/workingset.c",
          "start_line": 418,
          "end_line": 526,
          "content": [
            "bool workingset_test_recent(void *shadow, bool file, bool *workingset,",
            "\t\t\t\tbool flush)",
            "{",
            "\tstruct mem_cgroup *eviction_memcg;",
            "\tstruct lruvec *eviction_lruvec;",
            "\tunsigned long refault_distance;",
            "\tunsigned long workingset_size;",
            "\tunsigned long refault;",
            "\tint memcgid;",
            "\tstruct pglist_data *pgdat;",
            "\tunsigned long eviction;",
            "",
            "\trcu_read_lock();",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tbool recent = lru_gen_test_recent(shadow, file,",
            "\t\t\t\t&eviction_lruvec, &eviction, workingset);",
            "",
            "\t\trcu_read_unlock();",
            "\t\treturn recent;",
            "\t}",
            "",
            "",
            "\tunpack_shadow(shadow, &memcgid, &pgdat, &eviction, workingset);",
            "\teviction <<= bucket_order;",
            "",
            "\t/*",
            "\t * Look up the memcg associated with the stored ID. It might",
            "\t * have been deleted since the folio's eviction.",
            "\t *",
            "\t * Note that in rare events the ID could have been recycled",
            "\t * for a new cgroup that refaults a shared folio. This is",
            "\t * impossible to tell from the available data. However, this",
            "\t * should be a rare and limited disturbance, and activations",
            "\t * are always speculative anyway. Ultimately, it's the aging",
            "\t * algorithm's job to shake out the minimum access frequency",
            "\t * for the active cache.",
            "\t *",
            "\t * XXX: On !CONFIG_MEMCG, this will always return NULL; it",
            "\t * would be better if the root_mem_cgroup existed in all",
            "\t * configurations instead.",
            "\t */",
            "\teviction_memcg = mem_cgroup_from_id(memcgid);",
            "\tif (!mem_cgroup_disabled() &&",
            "\t    (!eviction_memcg || !mem_cgroup_tryget(eviction_memcg))) {",
            "\t\trcu_read_unlock();",
            "\t\treturn false;",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "",
            "\t/*",
            "\t * Flush stats (and potentially sleep) outside the RCU read section.",
            "\t *",
            "\t * Note that workingset_test_recent() itself might be called in RCU read",
            "\t * section (for e.g, in cachestat) - these callers need to skip flushing",
            "\t * stats (via the flush argument).",
            "\t *",
            "\t * XXX: With per-memcg flushing and thresholding, is ratelimiting",
            "\t * still needed here?",
            "\t */",
            "\tif (flush)",
            "\t\tmem_cgroup_flush_stats_ratelimited(eviction_memcg);",
            "",
            "\teviction_lruvec = mem_cgroup_lruvec(eviction_memcg, pgdat);",
            "\trefault = atomic_long_read(&eviction_lruvec->nonresident_age);",
            "",
            "\t/*",
            "\t * Calculate the refault distance",
            "\t *",
            "\t * The unsigned subtraction here gives an accurate distance",
            "\t * across nonresident_age overflows in most cases. There is a",
            "\t * special case: usually, shadow entries have a short lifetime",
            "\t * and are either refaulted or reclaimed along with the inode",
            "\t * before they get too old.  But it is not impossible for the",
            "\t * nonresident_age to lap a shadow entry in the field, which",
            "\t * can then result in a false small refault distance, leading",
            "\t * to a false activation should this old entry actually",
            "\t * refault again.  However, earlier kernels used to deactivate",
            "\t * unconditionally with *every* reclaim invocation for the",
            "\t * longest time, so the occasional inappropriate activation",
            "\t * leading to pressure on the active list is not a problem.",
            "\t */",
            "\trefault_distance = (refault - eviction) & EVICTION_MASK;",
            "",
            "\t/*",
            "\t * Compare the distance to the existing workingset size. We",
            "\t * don't activate pages that couldn't stay resident even if",
            "\t * all the memory was available to the workingset. Whether",
            "\t * workingset competition needs to consider anon or not depends",
            "\t * on having free swap space.",
            "\t */",
            "\tworkingset_size = lruvec_page_state(eviction_lruvec, NR_ACTIVE_FILE);",
            "\tif (!file) {",
            "\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_INACTIVE_FILE);",
            "\t}",
            "\tif (mem_cgroup_get_nr_swap_pages(eviction_memcg) > 0) {",
            "\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_ACTIVE_ANON);",
            "\t\tif (file) {",
            "\t\t\tworkingset_size += lruvec_page_state(eviction_lruvec,",
            "\t\t\t\t\t\t     NR_INACTIVE_ANON);",
            "\t\t}",
            "\t}",
            "",
            "\tmem_cgroup_put(eviction_memcg);",
            "\treturn refault_distance <= workingset_size;",
            "}"
          ],
          "function_name": "workingset_test_recent",
          "description": "实现工作集测试最近访问的判断逻辑，通过对比参考距离与当前工作集大小决定是否激活页面，支持内存组场景下的统计信息处理。",
          "similarity": 0.6257557272911072
        },
        {
          "chunk_id": 4,
          "file_path": "mm/workingset.c",
          "start_line": 712,
          "end_line": 833,
          "content": [
            "static enum lru_status shadow_lru_isolate(struct list_head *item,",
            "\t\t\t\t\t  struct list_lru_one *lru,",
            "\t\t\t\t\t  spinlock_t *lru_lock,",
            "\t\t\t\t\t  void *arg) __must_hold(lru_lock)",
            "{",
            "\tstruct xa_node *node = container_of(item, struct xa_node, private_list);",
            "\tstruct address_space *mapping;",
            "\tint ret;",
            "",
            "\t/*",
            "\t * Page cache insertions and deletions synchronously maintain",
            "\t * the shadow node LRU under the i_pages lock and the",
            "\t * lru_lock.  Because the page cache tree is emptied before",
            "\t * the inode can be destroyed, holding the lru_lock pins any",
            "\t * address_space that has nodes on the LRU.",
            "\t *",
            "\t * We can then safely transition to the i_pages lock to",
            "\t * pin only the address_space of the particular node we want",
            "\t * to reclaim, take the node off-LRU, and drop the lru_lock.",
            "\t */",
            "",
            "\tmapping = container_of(node->array, struct address_space, i_pages);",
            "",
            "\t/* Coming from the list, invert the lock order */",
            "\tif (!xa_trylock(&mapping->i_pages)) {",
            "\t\tspin_unlock_irq(lru_lock);",
            "\t\tret = LRU_RETRY;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* For page cache we need to hold i_lock */",
            "\tif (mapping->host != NULL) {",
            "\t\tif (!spin_trylock(&mapping->host->i_lock)) {",
            "\t\t\txa_unlock(&mapping->i_pages);",
            "\t\t\tspin_unlock_irq(lru_lock);",
            "\t\t\tret = LRU_RETRY;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t}",
            "",
            "\tlist_lru_isolate(lru, item);",
            "\t__dec_node_page_state(virt_to_page(node), WORKINGSET_NODES);",
            "",
            "\tspin_unlock(lru_lock);",
            "",
            "\t/*",
            "\t * The nodes should only contain one or more shadow entries,",
            "\t * no pages, so we expect to be able to remove them all and",
            "\t * delete and free the empty node afterwards.",
            "\t */",
            "\tif (WARN_ON_ONCE(!node->nr_values))",
            "\t\tgoto out_invalid;",
            "\tif (WARN_ON_ONCE(node->count != node->nr_values))",
            "\t\tgoto out_invalid;",
            "\txa_delete_node(node, workingset_update_node);",
            "\t__inc_lruvec_kmem_state(node, WORKINGSET_NODERECLAIM);",
            "",
            "out_invalid:",
            "\txa_unlock_irq(&mapping->i_pages);",
            "\tif (mapping->host != NULL) {",
            "\t\tif (mapping_shrinkable(mapping))",
            "\t\t\tinode_add_lru(mapping->host);",
            "\t\tspin_unlock(&mapping->host->i_lock);",
            "\t}",
            "\tret = LRU_REMOVED_RETRY;",
            "out:",
            "\tcond_resched();",
            "\tspin_lock_irq(lru_lock);",
            "\treturn ret;",
            "}",
            "static unsigned long scan_shadow_nodes(struct shrinker *shrinker,",
            "\t\t\t\t       struct shrink_control *sc)",
            "{",
            "\t/* list_lru lock nests inside the IRQ-safe i_pages lock */",
            "\treturn list_lru_shrink_walk_irq(&shadow_nodes, sc, shadow_lru_isolate,",
            "\t\t\t\t\tNULL);",
            "}",
            "static int __init workingset_init(void)",
            "{",
            "\tstruct shrinker *workingset_shadow_shrinker;",
            "\tunsigned int timestamp_bits;",
            "\tunsigned int max_order;",
            "\tint ret = -ENOMEM;",
            "",
            "\tBUILD_BUG_ON(BITS_PER_LONG < EVICTION_SHIFT);",
            "\t/*",
            "\t * Calculate the eviction bucket size to cover the longest",
            "\t * actionable refault distance, which is currently half of",
            "\t * memory (totalram_pages/2). However, memory hotplug may add",
            "\t * some more pages at runtime, so keep working with up to",
            "\t * double the initial memory by using totalram_pages as-is.",
            "\t */",
            "\ttimestamp_bits = BITS_PER_LONG - EVICTION_SHIFT;",
            "\tmax_order = fls_long(totalram_pages() - 1);",
            "\tif (max_order > timestamp_bits)",
            "\t\tbucket_order = max_order - timestamp_bits;",
            "\tpr_info(\"workingset: timestamp_bits=%d max_order=%d bucket_order=%u\\n\",",
            "\t       timestamp_bits, max_order, bucket_order);",
            "",
            "\tworkingset_shadow_shrinker = shrinker_alloc(SHRINKER_NUMA_AWARE |",
            "\t\t\t\t\t\t    SHRINKER_MEMCG_AWARE,",
            "\t\t\t\t\t\t    \"mm-shadow\");",
            "\tif (!workingset_shadow_shrinker)",
            "\t\tgoto err;",
            "",
            "\tret = __list_lru_init(&shadow_nodes, true, &shadow_nodes_key,",
            "\t\t\t      workingset_shadow_shrinker);",
            "\tif (ret)",
            "\t\tgoto err_list_lru;",
            "",
            "\tworkingset_shadow_shrinker->count_objects = count_shadow_nodes;",
            "\tworkingset_shadow_shrinker->scan_objects = scan_shadow_nodes;",
            "\t/* ->count reports only fully expendable nodes */",
            "\tworkingset_shadow_shrinker->seeks = 0;",
            "",
            "\tshrinker_register(workingset_shadow_shrinker);",
            "\treturn 0;",
            "err_list_lru:",
            "\tshrinker_free(workingset_shadow_shrinker);",
            "err:",
            "\treturn ret;",
            "}"
          ],
          "function_name": "shadow_lru_isolate, scan_shadow_nodes, workingset_init",
          "description": "初始化工作集模块，注册收缩器管理影子节点，定义节点隔离和扫描机制，通过计算时间戳位宽确定桶阶参数以保证参考距离覆盖范围。",
          "similarity": 0.5939692258834839
        },
        {
          "chunk_id": 3,
          "file_path": "mm/workingset.c",
          "start_line": 537,
          "end_line": 689,
          "content": [
            "void workingset_refault(struct folio *folio, void *shadow)",
            "{",
            "\tbool file = folio_is_file_lru(folio);",
            "\tstruct pglist_data *pgdat;",
            "\tstruct mem_cgroup *memcg;",
            "\tstruct lruvec *lruvec;",
            "\tbool workingset;",
            "\tlong nr;",
            "",
            "\tif (lru_gen_enabled()) {",
            "\t\tlru_gen_refault(folio, shadow);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * The activation decision for this folio is made at the level",
            "\t * where the eviction occurred, as that is where the LRU order",
            "\t * during folio reclaim is being determined.",
            "\t *",
            "\t * However, the cgroup that will own the folio is the one that",
            "\t * is actually experiencing the refault event. Make sure the folio is",
            "\t * locked to guarantee folio_memcg() stability throughout.",
            "\t */",
            "\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\tnr = folio_nr_pages(folio);",
            "\tmemcg = folio_memcg(folio);",
            "\tpgdat = folio_pgdat(folio);",
            "\tlruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "",
            "\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + file, nr);",
            "",
            "\tif (!workingset_test_recent(shadow, file, &workingset, true))",
            "\t\treturn;",
            "",
            "\tfolio_set_active(folio);",
            "\tworkingset_age_nonresident(lruvec, nr);",
            "\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + file, nr);",
            "",
            "\t/* Folio was active prior to eviction */",
            "\tif (workingset) {",
            "\t\tfolio_set_workingset(folio);",
            "\t\t/*",
            "\t\t * XXX: Move to folio_add_lru() when it supports new vs",
            "\t\t * putback",
            "\t\t */",
            "\t\tlru_note_cost_refault(folio);",
            "\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + file, nr);",
            "\t}",
            "}",
            "void workingset_activation(struct folio *folio)",
            "{",
            "\tstruct mem_cgroup *memcg;",
            "",
            "\trcu_read_lock();",
            "\t/*",
            "\t * Filter non-memcg pages here, e.g. unmap can call",
            "\t * mark_page_accessed() on VDSO pages.",
            "\t *",
            "\t * XXX: See workingset_refault() - this should return",
            "\t * root_mem_cgroup even for !CONFIG_MEMCG.",
            "\t */",
            "\tmemcg = folio_memcg_rcu(folio);",
            "\tif (!mem_cgroup_disabled() && !memcg)",
            "\t\tgoto out;",
            "\tworkingset_age_nonresident(folio_lruvec(folio), folio_nr_pages(folio));",
            "out:",
            "\trcu_read_unlock();",
            "}",
            "void workingset_update_node(struct xa_node *node)",
            "{",
            "\tstruct address_space *mapping;",
            "\tstruct page *page = virt_to_page(node);",
            "",
            "\t/*",
            "\t * Track non-empty nodes that contain only shadow entries;",
            "\t * unlink those that contain pages or are being freed.",
            "\t *",
            "\t * Avoid acquiring the list_lru lock when the nodes are",
            "\t * already where they should be. The list_empty() test is safe",
            "\t * as node->private_list is protected by the i_pages lock.",
            "\t */",
            "\tmapping = container_of(node->array, struct address_space, i_pages);",
            "\tlockdep_assert_held(&mapping->i_pages.xa_lock);",
            "",
            "\tif (node->count && node->count == node->nr_values) {",
            "\t\tif (list_empty(&node->private_list)) {",
            "\t\t\tlist_lru_add_obj(&shadow_nodes, &node->private_list);",
            "\t\t\t__inc_node_page_state(page, WORKINGSET_NODES);",
            "\t\t}",
            "\t} else {",
            "\t\tif (!list_empty(&node->private_list)) {",
            "\t\t\tlist_lru_del_obj(&shadow_nodes, &node->private_list);",
            "\t\t\t__dec_node_page_state(page, WORKINGSET_NODES);",
            "\t\t}",
            "\t}",
            "}",
            "static unsigned long count_shadow_nodes(struct shrinker *shrinker,",
            "\t\t\t\t\tstruct shrink_control *sc)",
            "{",
            "\tunsigned long max_nodes;",
            "\tunsigned long nodes;",
            "\tunsigned long pages;",
            "",
            "\tnodes = list_lru_shrink_count(&shadow_nodes, sc);",
            "\tif (!nodes)",
            "\t\treturn SHRINK_EMPTY;",
            "",
            "\t/*",
            "\t * Approximate a reasonable limit for the nodes",
            "\t * containing shadow entries. We don't need to keep more",
            "\t * shadow entries than possible pages on the active list,",
            "\t * since refault distances bigger than that are dismissed.",
            "\t *",
            "\t * The size of the active list converges toward 100% of",
            "\t * overall page cache as memory grows, with only a tiny",
            "\t * inactive list. Assume the total cache size for that.",
            "\t *",
            "\t * Nodes might be sparsely populated, with only one shadow",
            "\t * entry in the extreme case. Obviously, we cannot keep one",
            "\t * node for every eligible shadow entry, so compromise on a",
            "\t * worst-case density of 1/8th. Below that, not all eligible",
            "\t * refaults can be detected anymore.",
            "\t *",
            "\t * On 64-bit with 7 xa_nodes per page and 64 slots",
            "\t * each, this will reclaim shadow entries when they consume",
            "\t * ~1.8% of available memory:",
            "\t *",
            "\t * PAGE_SIZE / xa_nodes / node_entries * 8 / PAGE_SIZE",
            "\t */",
            "#ifdef CONFIG_MEMCG",
            "\tif (sc->memcg) {",
            "\t\tstruct lruvec *lruvec;",
            "\t\tint i;",
            "",
            "\t\tmem_cgroup_flush_stats_ratelimited(sc->memcg);",
            "\t\tlruvec = mem_cgroup_lruvec(sc->memcg, NODE_DATA(sc->nid));",
            "\t\tfor (pages = 0, i = 0; i < NR_LRU_LISTS; i++)",
            "\t\t\tpages += lruvec_page_state_local(lruvec,",
            "\t\t\t\t\t\t\t NR_LRU_BASE + i);",
            "\t\tpages += lruvec_page_state_local(",
            "\t\t\tlruvec, NR_SLAB_RECLAIMABLE_B) >> PAGE_SHIFT;",
            "\t\tpages += lruvec_page_state_local(",
            "\t\t\tlruvec, NR_SLAB_UNRECLAIMABLE_B) >> PAGE_SHIFT;",
            "\t} else",
            "#endif",
            "\t\tpages = node_present_pages(sc->nid);",
            "",
            "\tmax_nodes = pages >> (XA_CHUNK_SHIFT - 3);",
            "",
            "\tif (nodes <= max_nodes)",
            "\t\treturn 0;",
            "\treturn nodes - max_nodes;",
            "}"
          ],
          "function_name": "workingset_refault, workingset_activation, workingset_update_node, count_shadow_nodes",
          "description": "提供页面故障后的激活处理、节点更新及影子节点追踪功能，包含基于工作集状态更新节点计数器和触发页面重新激活的逻辑。",
          "similarity": 0.5385826826095581
        },
        {
          "chunk_id": 1,
          "file_path": "mm/workingset.c",
          "start_line": 209,
          "end_line": 314,
          "content": [
            "static void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,",
            "\t\t\t  unsigned long *evictionp, bool *workingsetp)",
            "{",
            "\tunsigned long entry = xa_to_value(shadow);",
            "\tint memcgid, nid;",
            "\tbool workingset;",
            "",
            "\tworkingset = entry & ((1UL << WORKINGSET_SHIFT) - 1);",
            "\tentry >>= WORKINGSET_SHIFT;",
            "\tnid = entry & ((1UL << NODES_SHIFT) - 1);",
            "\tentry >>= NODES_SHIFT;",
            "\tmemcgid = entry & ((1UL << MEM_CGROUP_ID_SHIFT) - 1);",
            "\tentry >>= MEM_CGROUP_ID_SHIFT;",
            "",
            "\t*memcgidp = memcgid;",
            "\t*pgdat = NODE_DATA(nid);",
            "\t*evictionp = entry;",
            "\t*workingsetp = workingset;",
            "}",
            "static bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,",
            "\t\t\t\tunsigned long *token, bool *workingset)",
            "{",
            "\tint memcg_id;",
            "\tunsigned long min_seq;",
            "\tstruct mem_cgroup *memcg;",
            "\tstruct pglist_data *pgdat;",
            "",
            "\tunpack_shadow(shadow, &memcg_id, &pgdat, token, workingset);",
            "",
            "\tmemcg = mem_cgroup_from_id(memcg_id);",
            "\t*lruvec = mem_cgroup_lruvec(memcg, pgdat);",
            "",
            "\tmin_seq = READ_ONCE((*lruvec)->lrugen.min_seq[file]);",
            "\treturn (*token >> LRU_REFS_WIDTH) == (min_seq & (EVICTION_MASK >> LRU_REFS_WIDTH));",
            "}",
            "static void lru_gen_refault(struct folio *folio, void *shadow)",
            "{",
            "\tbool recent;",
            "\tint hist, tier, refs;",
            "\tbool workingset;",
            "\tunsigned long token;",
            "\tstruct lruvec *lruvec;",
            "\tstruct lru_gen_folio *lrugen;",
            "\tint type = folio_is_file_lru(folio);",
            "\tint delta = folio_nr_pages(folio);",
            "",
            "\trcu_read_lock();",
            "",
            "\trecent = lru_gen_test_recent(shadow, type, &lruvec, &token, &workingset);",
            "\tif (lruvec != folio_lruvec(folio))",
            "\t\tgoto unlock;",
            "",
            "\tmod_lruvec_state(lruvec, WORKINGSET_REFAULT_BASE + type, delta);",
            "",
            "\tif (!recent)",
            "\t\tgoto unlock;",
            "",
            "\tlrugen = &lruvec->lrugen;",
            "",
            "\thist = lru_hist_from_seq(READ_ONCE(lrugen->min_seq[type]));",
            "\t/* see the comment in folio_lru_refs() */",
            "\trefs = (token & (BIT(LRU_REFS_WIDTH) - 1)) + workingset;",
            "\ttier = lru_tier_from_refs(refs);",
            "",
            "\tatomic_long_add(delta, &lrugen->refaulted[hist][type][tier]);",
            "\tmod_lruvec_state(lruvec, WORKINGSET_ACTIVATE_BASE + type, delta);",
            "",
            "\t/*",
            "\t * Count the following two cases as stalls:",
            "\t * 1. For pages accessed through page tables, hotter pages pushed out",
            "\t *    hot pages which refaulted immediately.",
            "\t * 2. For pages accessed multiple times through file descriptors,",
            "\t *    they would have been protected by sort_folio().",
            "\t */",
            "\tif (lru_gen_in_fault() || refs >= BIT(LRU_REFS_WIDTH) - 1) {",
            "\t\tset_mask_bits(&folio->flags, 0, LRU_REFS_MASK | BIT(PG_workingset));",
            "\t\tmod_lruvec_state(lruvec, WORKINGSET_RESTORE_BASE + type, delta);",
            "\t}",
            "unlock:",
            "\trcu_read_unlock();",
            "}",
            "static bool lru_gen_test_recent(void *shadow, bool file, struct lruvec **lruvec,",
            "\t\t\t\tunsigned long *token, bool *workingset)",
            "{",
            "\treturn false;",
            "}",
            "static void lru_gen_refault(struct folio *folio, void *shadow)",
            "{",
            "}",
            "void workingset_age_nonresident(struct lruvec *lruvec, unsigned long nr_pages)",
            "{",
            "\t/*",
            "\t * Reclaiming a cgroup means reclaiming all its children in a",
            "\t * round-robin fashion. That means that each cgroup has an LRU",
            "\t * order that is composed of the LRU orders of its child",
            "\t * cgroups; and every page has an LRU position not just in the",
            "\t * cgroup that owns it, but in all of that group's ancestors.",
            "\t *",
            "\t * So when the physical inactive list of a leaf cgroup ages,",
            "\t * the virtual inactive lists of all its parents, including",
            "\t * the root cgroup's, age as well.",
            "\t */",
            "\tdo {",
            "\t\tatomic_long_add(nr_pages, &lruvec->nonresident_age);",
            "\t} while ((lruvec = parent_lruvec(lruvec)));",
            "}"
          ],
          "function_name": "unpack_shadow, lru_gen_test_recent, lru_gen_refault, lru_gen_test_recent, lru_gen_refault, workingset_age_nonresident",
          "description": "包含解码影子条目、测试最近访问、处理页面故障的函数实现，但存在重复函数声明问题，实际功能涉及基于LRU生成的页面激活决策逻辑。",
          "similarity": 0.39364945888519287
        },
        {
          "chunk_id": 0,
          "file_path": "mm/workingset.c",
          "start_line": 1,
          "end_line": 208,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Workingset detection",
            " *",
            " * Copyright (C) 2013 Red Hat, Inc., Johannes Weiner",
            " */",
            "",
            "#include <linux/memcontrol.h>",
            "#include <linux/mm_inline.h>",
            "#include <linux/writeback.h>",
            "#include <linux/shmem_fs.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/atomic.h>",
            "#include <linux/module.h>",
            "#include <linux/swap.h>",
            "#include <linux/dax.h>",
            "#include <linux/fs.h>",
            "#include <linux/mm.h>",
            "",
            "/*",
            " *\t\tDouble CLOCK lists",
            " *",
            " * Per node, two clock lists are maintained for file pages: the",
            " * inactive and the active list.  Freshly faulted pages start out at",
            " * the head of the inactive list and page reclaim scans pages from the",
            " * tail.  Pages that are accessed multiple times on the inactive list",
            " * are promoted to the active list, to protect them from reclaim,",
            " * whereas active pages are demoted to the inactive list when the",
            " * active list grows too big.",
            " *",
            " *   fault ------------------------+",
            " *                                 |",
            " *              +--------------+   |            +-------------+",
            " *   reclaim <- |   inactive   | <-+-- demotion |    active   | <--+",
            " *              +--------------+                +-------------+    |",
            " *                     |                                           |",
            " *                     +-------------- promotion ------------------+",
            " *",
            " *",
            " *\t\tAccess frequency and refault distance",
            " *",
            " * A workload is thrashing when its pages are frequently used but they",
            " * are evicted from the inactive list every time before another access",
            " * would have promoted them to the active list.",
            " *",
            " * In cases where the average access distance between thrashing pages",
            " * is bigger than the size of memory there is nothing that can be",
            " * done - the thrashing set could never fit into memory under any",
            " * circumstance.",
            " *",
            " * However, the average access distance could be bigger than the",
            " * inactive list, yet smaller than the size of memory.  In this case,",
            " * the set could fit into memory if it weren't for the currently",
            " * active pages - which may be used more, hopefully less frequently:",
            " *",
            " *      +-memory available to cache-+",
            " *      |                           |",
            " *      +-inactive------+-active----+",
            " *  a b | c d e f g h i | J K L M N |",
            " *      +---------------+-----------+",
            " *",
            " * It is prohibitively expensive to accurately track access frequency",
            " * of pages.  But a reasonable approximation can be made to measure",
            " * thrashing on the inactive list, after which refaulting pages can be",
            " * activated optimistically to compete with the existing active pages.",
            " *",
            " * Approximating inactive page access frequency - Observations:",
            " *",
            " * 1. When a page is accessed for the first time, it is added to the",
            " *    head of the inactive list, slides every existing inactive page",
            " *    towards the tail by one slot, and pushes the current tail page",
            " *    out of memory.",
            " *",
            " * 2. When a page is accessed for the second time, it is promoted to",
            " *    the active list, shrinking the inactive list by one slot.  This",
            " *    also slides all inactive pages that were faulted into the cache",
            " *    more recently than the activated page towards the tail of the",
            " *    inactive list.",
            " *",
            " * Thus:",
            " *",
            " * 1. The sum of evictions and activations between any two points in",
            " *    time indicate the minimum number of inactive pages accessed in",
            " *    between.",
            " *",
            " * 2. Moving one inactive page N page slots towards the tail of the",
            " *    list requires at least N inactive page accesses.",
            " *",
            " * Combining these:",
            " *",
            " * 1. When a page is finally evicted from memory, the number of",
            " *    inactive pages accessed while the page was in cache is at least",
            " *    the number of page slots on the inactive list.",
            " *",
            " * 2. In addition, measuring the sum of evictions and activations (E)",
            " *    at the time of a page's eviction, and comparing it to another",
            " *    reading (R) at the time the page faults back into memory tells",
            " *    the minimum number of accesses while the page was not cached.",
            " *    This is called the refault distance.",
            " *",
            " * Because the first access of the page was the fault and the second",
            " * access the refault, we combine the in-cache distance with the",
            " * out-of-cache distance to get the complete minimum access distance",
            " * of this page:",
            " *",
            " *      NR_inactive + (R - E)",
            " *",
            " * And knowing the minimum access distance of a page, we can easily",
            " * tell if the page would be able to stay in cache assuming all page",
            " * slots in the cache were available:",
            " *",
            " *   NR_inactive + (R - E) <= NR_inactive + NR_active",
            " *",
            " * If we have swap we should consider about NR_inactive_anon and",
            " * NR_active_anon, so for page cache and anonymous respectively:",
            " *",
            " *   NR_inactive_file + (R - E) <= NR_inactive_file + NR_active_file",
            " *   + NR_inactive_anon + NR_active_anon",
            " *",
            " *   NR_inactive_anon + (R - E) <= NR_inactive_anon + NR_active_anon",
            " *   + NR_inactive_file + NR_active_file",
            " *",
            " * Which can be further simplified to:",
            " *",
            " *   (R - E) <= NR_active_file + NR_inactive_anon + NR_active_anon",
            " *",
            " *   (R - E) <= NR_active_anon + NR_inactive_file + NR_active_file",
            " *",
            " * Put into words, the refault distance (out-of-cache) can be seen as",
            " * a deficit in inactive list space (in-cache).  If the inactive list",
            " * had (R - E) more page slots, the page would not have been evicted",
            " * in between accesses, but activated instead.  And on a full system,",
            " * the only thing eating into inactive list space is active pages.",
            " *",
            " *",
            " *\t\tRefaulting inactive pages",
            " *",
            " * All that is known about the active list is that the pages have been",
            " * accessed more than once in the past.  This means that at any given",
            " * time there is actually a good chance that pages on the active list",
            " * are no longer in active use.",
            " *",
            " * So when a refault distance of (R - E) is observed and there are at",
            " * least (R - E) pages in the userspace workingset, the refaulting page",
            " * is activated optimistically in the hope that (R - E) pages are actually",
            " * used less frequently than the refaulting page - or even not used at",
            " * all anymore.",
            " *",
            " * That means if inactive cache is refaulting with a suitable refault",
            " * distance, we assume the cache workingset is transitioning and put",
            " * pressure on the current workingset.",
            " *",
            " * If this is wrong and demotion kicks in, the pages which are truly",
            " * used more frequently will be reactivated while the less frequently",
            " * used once will be evicted from memory.",
            " *",
            " * But if this is right, the stale pages will be pushed out of memory",
            " * and the used pages get to stay in cache.",
            " *",
            " *\t\tRefaulting active pages",
            " *",
            " * If on the other hand the refaulting pages have recently been",
            " * deactivated, it means that the active list is no longer protecting",
            " * actively used cache from reclaim. The cache is NOT transitioning to",
            " * a different workingset; the existing workingset is thrashing in the",
            " * space allocated to the page cache.",
            " *",
            " *",
            " *\t\tImplementation",
            " *",
            " * For each node's LRU lists, a counter for inactive evictions and",
            " * activations is maintained (node->nonresident_age).",
            " *",
            " * On eviction, a snapshot of this counter (along with some bits to",
            " * identify the node) is stored in the now empty page cache",
            " * slot of the evicted page.  This is called a shadow entry.",
            " *",
            " * On cache misses for which there are shadow entries, an eligible",
            " * refault distance will immediately activate the refaulting page.",
            " */",
            "",
            "#define WORKINGSET_SHIFT 1",
            "#define EVICTION_SHIFT\t((BITS_PER_LONG - BITS_PER_XA_VALUE) +\t\\",
            "\t\t\t WORKINGSET_SHIFT + NODES_SHIFT + \\",
            "\t\t\t MEM_CGROUP_ID_SHIFT)",
            "#define EVICTION_MASK\t(~0UL >> EVICTION_SHIFT)",
            "",
            "/*",
            " * Eviction timestamps need to be able to cover the full range of",
            " * actionable refaults. However, bits are tight in the xarray",
            " * entry, and after storing the identifier for the lruvec there might",
            " * not be enough left to represent every single actionable refault. In",
            " * that case, we have to sacrifice granularity for distance, and group",
            " * evictions into coarser buckets by shaving off lower timestamp bits.",
            " */",
            "static unsigned int bucket_order __read_mostly;",
            "",
            "static void *pack_shadow(int memcgid, pg_data_t *pgdat, unsigned long eviction,",
            "\t\t\t bool workingset)",
            "{",
            "\teviction &= EVICTION_MASK;",
            "\teviction = (eviction << MEM_CGROUP_ID_SHIFT) | memcgid;",
            "\teviction = (eviction << NODES_SHIFT) | pgdat->node_id;",
            "\teviction = (eviction << WORKINGSET_SHIFT) | workingset;",
            "",
            "\treturn xa_mk_value(eviction);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义影子条目打包函数，将内存组ID、节点ID、时间戳等信息编码到xarray值中，用于记录页面被驱逐时的状态信息。",
          "similarity": 0.3804696798324585
        }
      ]
    },
    {
      "source_file": "mm/hugetlb_cgroup.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:06:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `hugetlb_cgroup.c`\n\n---\n\n# hugetlb_cgroup.c 技术文档\n\n## 1. 文件概述\n\n`hugetlb_cgroup.c` 是 Linux 内核中用于实现 **HugeTLB（大页）内存资源控制组（cgroup）** 功能的核心文件。该文件通过 cgroup v1/v2 接口，对不同 HugeTLB 页面大小（如 2MB、1GB 等）的内存使用进行配额限制和统计追踪。它支持两种计费模式：普通分配（fault-based）和预留（reservation-based），并确保在 cgroup 被销毁时将资源正确迁移至父 cgroup，防止资源泄漏。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct hugetlb_cgroup`：每个 cgroup 实例对应的 HugeTLB 控制结构。\n  - 包含两个 `page_counter` 数组：`hugepage[]`（普通使用）和 `rsvd_hugepage[]`（预留使用），分别对应每种 HugeTLB 页面类型。\n  - 包含 per-node 信息 `nodeinfo[]`，用于 NUMA 感知。\n  - 包含事件计数器 `events[][]` 和 `events_local[][]`，用于触发 cgroup 通知。\n- `struct hugetlb_cgroup_per_node`：每个 NUMA 节点上的 HugeTLB cgroup 附加信息（当前未在代码片段中完整定义）。\n\n### 主要函数\n- `hugetlb_cgroup_css_alloc()` / `hugetlb_cgroup_css_free()`：cgroup 子系统实例的创建与销毁。\n- `hugetlb_cgroup_init()`：初始化新 cgroup 的 page_counter，设置最大限制为 `PAGE_COUNTER_MAX` 向下对齐到 HugeTLB 页面大小。\n- `__hugetlb_cgroup_charge_cgroup()` / `hugetlb_cgroup_charge_cgroup()`：对当前任务所属 cgroup 尝试 charge（计费）指定数量的 HugeTLB 页面。\n- `hugetlb_cgroup_move_parent()`：将属于某 cgroup 的 HugeTLB 页面迁移至其父 cgroup。\n- `hugetlb_cgroup_css_offline()`：在 cgroup 离线时，强制将其所有 HugeTLB 资源迁移至父级。\n- `hugetlb_event()`：向上冒泡记录 HugeTLB 事件（如达到限制 `HUGETLB_MAX`）并触发 cgroup 文件通知。\n- 辅助内联函数：\n  - `hugetlb_cgroup_from_css()` / `from_task()`：从 cgroup_subsys_state 或 task 获取 hugetlb_cgroup。\n  - `hugetlb_cgroup_counter_from_cgroup()` / `_rsvd()`：获取对应计费类型的 page_counter。\n  - `hugetlb_cgroup_is_root()` / `parent_hugetlb_cgroup()`：判断是否为根 cgroup 或获取父 cgroup。\n\n## 3. 关键实现\n\n### 资源计费机制\n- 使用 `page_counter` 子系统管理每种 HugeTLB 页面类型的用量和上限。\n- 支持两种独立的计费路径：\n  - **普通分配（fault）**：实际分配物理页面时计费。\n  - **预留（reservation）**：仅预留虚拟地址空间时计费（用于 mmap 等场景）。\n- 计费时通过 RCU 安全地获取当前任务的 cgroup，并使用 `css_tryget()` 确保引用有效性。\n- 若计费失败（超出限制），触发 `HUGETLB_MAX` 事件并通过 `cgroup_file_notify()` 通知用户空间。\n\n### cgroup 生命周期管理\n- **创建**：为每个在线 NUMA 节点分配 `hugetlb_cgroup_per_node` 结构；初始化所有 HugeTLB 类型的 page_counter，父子层级通过 `page_counter` 的 parent 字段建立级联关系。\n- **离线（offline）**：遍历所有 HugeTLB 页面的 active list，调用 `hugetlb_cgroup_move_parent()` 将页面所有权转移给父 cgroup。此过程循环执行直至当前 cgroup 无任何 HugeTLB 使用量，确保资源完全迁移。\n- **销毁**：释放 per-node 数据及主结构体。\n\n### 资源迁移（Reparenting）\n- `hugetlb_cgroup_move_parent()` 在持有 `hugetlb_lock` 时执行，确保页面不会被并发释放或迁移。\n- 仅处理属于目标 cgroup 的页面；若父 cgroup 为空（即目标为根），则 charge 到全局 root cgroup（无硬限制）。\n- 通过 `set_hugetlb_cgroup()` 更新页面所属的 cgroup。\n\n### 限制与优化\n- 对于 order 小于 `HUGETLB_CGROUP_MIN_ORDER`（通常为 3，即 8 个普通页 = 32KB）的 HugeTLB 页面，**不进行 cgroup 计费**，以减少小 HugeTLB 页面的开销。\n- 最大限制设为 `PAGE_COUNTER_MAX` 向下对齐到 HugeTLB 页面大小，避免跨页边界问题。\n- 当前 per-node 分配策略对 offline 节点也分配内存（使用 `NUMA_NO_NODE`），存在内存浪费，注释中指出未来可通过内存热插拔回调优化。\n\n## 4. 依赖关系\n\n- **核心依赖**：\n  - `<linux/cgroup.h>`：cgroup 基础框架。\n  - `<linux/page_counter.h>`：提供层次化内存计数和限制功能。\n  - `<linux/hugetlb.h>`：HugeTLB 核心数据结构（如 `hstate`, `hugepage_activelist`）和锁（`hugetlb_lock`）。\n  - `<linux/hugetlb_cgroup.h>`：HugeTLB cgroup 的公共接口和数据结构定义。\n- **交互模块**：\n  - **HugeTLB 子系统**：在页面分配/释放、预留/取消预留等路径中调用本文件的 charge/uncharge 函数。\n  - **Memory cgroup (memcg)**：共享部分设计思想（如 page_counter），但 HugeTLB cgroup 是独立子系统。\n  - **Scheduler**：通过 `current` 获取当前任务的 cgroup 上下文。\n\n## 5. 使用场景\n\n- **容器资源隔离**：在 Kubernetes/Docker 等容器运行时中，通过 cgroup v1 的 `hugetlb` 子系统或 cgroup v2 的 `hugetlb.` 控制器，限制容器可使用的 HugeTLB 内存总量，防止单个容器耗尽系统大页资源。\n- **高性能计算（HPC）**：为不同 HPC 作业分配专用的 HugeTLB 内存配额，确保关键应用获得确定性内存性能。\n- **数据库优化**：Oracle、MySQL 等数据库使用 HugeTLB 提升 TLB 效率，通过 cgroup 限制其大页使用量，避免影响其他服务。\n- **资源监控与告警**：用户空间可通过读取 cgroup 的 `hugetlb.events` 文件监控 `max` 事件，实现基于 HugeTLB 使用量的自动扩缩容或告警。",
      "similarity": 0.529008686542511,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 242,
          "end_line": 368,
          "content": [
            "static inline void hugetlb_event(struct hugetlb_cgroup *hugetlb, int idx,",
            "\t\t\t\t enum hugetlb_memory_event event)",
            "{",
            "\tatomic_long_inc(&hugetlb->events_local[idx][event]);",
            "\tcgroup_file_notify(&hugetlb->events_local_file[idx]);",
            "",
            "\tdo {",
            "\t\tatomic_long_inc(&hugetlb->events[idx][event]);",
            "\t\tcgroup_file_notify(&hugetlb->events_file[idx]);",
            "\t} while ((hugetlb = parent_hugetlb_cgroup(hugetlb)) &&",
            "\t\t !hugetlb_cgroup_is_root(hugetlb));",
            "}",
            "static int __hugetlb_cgroup_charge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t  struct hugetlb_cgroup **ptr,",
            "\t\t\t\t\t  bool rsvd)",
            "{",
            "\tint ret = 0;",
            "\tstruct page_counter *counter;",
            "\tstruct hugetlb_cgroup *h_cg = NULL;",
            "",
            "\tif (hugetlb_cgroup_disabled())",
            "\t\tgoto done;",
            "\t/*",
            "\t * We don't charge any cgroup if the compound page have less",
            "\t * than 3 pages.",
            "\t */",
            "\tif (huge_page_order(&hstates[idx]) < HUGETLB_CGROUP_MIN_ORDER)",
            "\t\tgoto done;",
            "again:",
            "\trcu_read_lock();",
            "\th_cg = hugetlb_cgroup_from_task(current);",
            "\tif (!css_tryget(&h_cg->css)) {",
            "\t\trcu_read_unlock();",
            "\t\tgoto again;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tif (!page_counter_try_charge(",
            "\t\t    __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, rsvd),",
            "\t\t    nr_pages, &counter)) {",
            "\t\tret = -ENOMEM;",
            "\t\thugetlb_event(h_cg, idx, HUGETLB_MAX);",
            "\t\tcss_put(&h_cg->css);",
            "\t\tgoto done;",
            "\t}",
            "\t/* Reservations take a reference to the css because they do not get",
            "\t * reparented.",
            "\t */",
            "\tif (!rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "done:",
            "\t*ptr = h_cg;",
            "\treturn ret;",
            "}",
            "int hugetlb_cgroup_charge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t struct hugetlb_cgroup **ptr)",
            "{",
            "\treturn __hugetlb_cgroup_charge_cgroup(idx, nr_pages, ptr, false);",
            "}",
            "int hugetlb_cgroup_charge_cgroup_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t      struct hugetlb_cgroup **ptr)",
            "{",
            "\treturn __hugetlb_cgroup_charge_cgroup(idx, nr_pages, ptr, true);",
            "}",
            "static void __hugetlb_cgroup_commit_charge(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t   struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t\t   struct folio *folio, bool rsvd)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !h_cg)",
            "\t\treturn;",
            "",
            "\t__set_hugetlb_cgroup(folio, h_cg, rsvd);",
            "\tif (!rsvd) {",
            "\t\tunsigned long usage =",
            "\t\t\th_cg->nodeinfo[folio_nid(folio)]->usage[idx];",
            "\t\t/*",
            "\t\t * This write is not atomic due to fetching usage and writing",
            "\t\t * to it, but that's fine because we call this with",
            "\t\t * hugetlb_lock held anyway.",
            "\t\t */",
            "\t\tWRITE_ONCE(h_cg->nodeinfo[folio_nid(folio)]->usage[idx],",
            "\t\t\t   usage + nr_pages);",
            "\t}",
            "}",
            "void hugetlb_cgroup_commit_charge(int idx, unsigned long nr_pages,",
            "\t\t\t\t  struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t  struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_commit_charge(idx, nr_pages, h_cg, folio, false);",
            "}",
            "void hugetlb_cgroup_commit_charge_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t       struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t       struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_commit_charge(idx, nr_pages, h_cg, folio, true);",
            "}",
            "static void __hugetlb_cgroup_uncharge_folio(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t   struct folio *folio, bool rsvd)",
            "{",
            "\tstruct hugetlb_cgroup *h_cg;",
            "",
            "\tif (hugetlb_cgroup_disabled())",
            "\t\treturn;",
            "\tlockdep_assert_held(&hugetlb_lock);",
            "\th_cg = __hugetlb_cgroup_from_folio(folio, rsvd);",
            "\tif (unlikely(!h_cg))",
            "\t\treturn;",
            "\t__set_hugetlb_cgroup(folio, NULL, rsvd);",
            "",
            "\tpage_counter_uncharge(__hugetlb_cgroup_counter_from_cgroup(h_cg, idx,",
            "\t\t\t\t\t\t\t\t   rsvd),",
            "\t\t\t      nr_pages);",
            "",
            "\tif (rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "\telse {",
            "\t\tunsigned long usage =",
            "\t\t\th_cg->nodeinfo[folio_nid(folio)]->usage[idx];",
            "\t\t/*",
            "\t\t * This write is not atomic due to fetching usage and writing",
            "\t\t * to it, but that's fine because we call this with",
            "\t\t * hugetlb_lock held anyway.",
            "\t\t */",
            "\t\tWRITE_ONCE(h_cg->nodeinfo[folio_nid(folio)]->usage[idx],",
            "\t\t\t   usage - nr_pages);",
            "\t}",
            "}"
          ],
          "function_name": "hugetlb_event, __hugetlb_cgroup_charge_cgroup, hugetlb_cgroup_charge_cgroup, hugetlb_cgroup_charge_cgroup_rsvd, __hugetlb_cgroup_commit_charge, hugetlb_cgroup_commit_charge, hugetlb_cgroup_commit_charge_rsvd, __hugetlb_cgroup_uncharge_folio",
          "description": "处理HugeTLB页面分配时的计费操作，包含事件记录、尝试充电、提交充电等流程，区分普通页与保留页的计数逻辑，并维护各层级cgroup的使用统计。",
          "similarity": 0.5196468830108643
        },
        {
          "chunk_id": 3,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 381,
          "end_line": 500,
          "content": [
            "void hugetlb_cgroup_uncharge_folio(int idx, unsigned long nr_pages,",
            "\t\t\t\t  struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_uncharge_folio(idx, nr_pages, folio, false);",
            "}",
            "void hugetlb_cgroup_uncharge_folio_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t       struct folio *folio)",
            "{",
            "\t__hugetlb_cgroup_uncharge_folio(idx, nr_pages, folio, true);",
            "}",
            "static void __hugetlb_cgroup_uncharge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t     struct hugetlb_cgroup *h_cg,",
            "\t\t\t\t\t     bool rsvd)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !h_cg)",
            "\t\treturn;",
            "",
            "\tif (huge_page_order(&hstates[idx]) < HUGETLB_CGROUP_MIN_ORDER)",
            "\t\treturn;",
            "",
            "\tpage_counter_uncharge(__hugetlb_cgroup_counter_from_cgroup(h_cg, idx,",
            "\t\t\t\t\t\t\t\t   rsvd),",
            "\t\t\t      nr_pages);",
            "",
            "\tif (rsvd)",
            "\t\tcss_put(&h_cg->css);",
            "}",
            "void hugetlb_cgroup_uncharge_cgroup(int idx, unsigned long nr_pages,",
            "\t\t\t\t    struct hugetlb_cgroup *h_cg)",
            "{",
            "\t__hugetlb_cgroup_uncharge_cgroup(idx, nr_pages, h_cg, false);",
            "}",
            "void hugetlb_cgroup_uncharge_cgroup_rsvd(int idx, unsigned long nr_pages,",
            "\t\t\t\t\t struct hugetlb_cgroup *h_cg)",
            "{",
            "\t__hugetlb_cgroup_uncharge_cgroup(idx, nr_pages, h_cg, true);",
            "}",
            "void hugetlb_cgroup_uncharge_counter(struct resv_map *resv, unsigned long start,",
            "\t\t\t\t     unsigned long end)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !resv || !resv->reservation_counter ||",
            "\t    !resv->css)",
            "\t\treturn;",
            "",
            "\tpage_counter_uncharge(resv->reservation_counter,",
            "\t\t\t      (end - start) * resv->pages_per_hpage);",
            "\tcss_put(resv->css);",
            "}",
            "void hugetlb_cgroup_uncharge_file_region(struct resv_map *resv,",
            "\t\t\t\t\t struct file_region *rg,",
            "\t\t\t\t\t unsigned long nr_pages,",
            "\t\t\t\t\t bool region_del)",
            "{",
            "\tif (hugetlb_cgroup_disabled() || !resv || !rg || !nr_pages)",
            "\t\treturn;",
            "",
            "\tif (rg->reservation_counter && resv->pages_per_hpage &&",
            "\t    !resv->reservation_counter) {",
            "\t\tpage_counter_uncharge(rg->reservation_counter,",
            "\t\t\t\t      nr_pages * resv->pages_per_hpage);",
            "\t\t/*",
            "\t\t * Only do css_put(rg->css) when we delete the entire region",
            "\t\t * because one file_region must hold exactly one css reference.",
            "\t\t */",
            "\t\tif (region_del)",
            "\t\t\tcss_put(rg->css);",
            "\t}",
            "}",
            "static int hugetlb_cgroup_read_numa_stat(struct seq_file *seq, void *dummy)",
            "{",
            "\tint nid;",
            "\tstruct cftype *cft = seq_cft(seq);",
            "\tint idx = MEMFILE_IDX(cft->private);",
            "\tbool legacy = MEMFILE_ATTR(cft->private);",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(seq_css(seq));",
            "\tstruct cgroup_subsys_state *css;",
            "\tunsigned long usage;",
            "",
            "\tif (legacy) {",
            "\t\t/* Add up usage across all nodes for the non-hierarchical total. */",
            "\t\tusage = 0;",
            "\t\tfor_each_node_state(nid, N_MEMORY)",
            "\t\t\tusage += READ_ONCE(h_cg->nodeinfo[nid]->usage[idx]);",
            "\t\tseq_printf(seq, \"total=%lu\", usage * PAGE_SIZE);",
            "",
            "\t\t/* Simply print the per-node usage for the non-hierarchical total. */",
            "\t\tfor_each_node_state(nid, N_MEMORY)",
            "\t\t\tseq_printf(seq, \" N%d=%lu\", nid,",
            "\t\t\t\t   READ_ONCE(h_cg->nodeinfo[nid]->usage[idx]) *",
            "\t\t\t\t\t   PAGE_SIZE);",
            "\t\tseq_putc(seq, '\\n');",
            "\t}",
            "",
            "\t/*",
            "\t * The hierarchical total is pretty much the value recorded by the",
            "\t * counter, so use that.",
            "\t */",
            "\tseq_printf(seq, \"%stotal=%lu\", legacy ? \"hierarchical_\" : \"\",",
            "\t\t   page_counter_read(&h_cg->hugepage[idx]) * PAGE_SIZE);",
            "",
            "\t/*",
            "\t * For each node, transverse the css tree to obtain the hierarchical",
            "\t * node usage.",
            "\t */",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\tusage = 0;",
            "\t\trcu_read_lock();",
            "\t\tcss_for_each_descendant_pre(css, &h_cg->css) {",
            "\t\t\tusage += READ_ONCE(hugetlb_cgroup_from_css(css)",
            "\t\t\t\t\t\t   ->nodeinfo[nid]",
            "\t\t\t\t\t\t   ->usage[idx]);",
            "\t\t}",
            "\t\trcu_read_unlock();",
            "\t\tseq_printf(seq, \" N%d=%lu\", nid, usage * PAGE_SIZE);",
            "\t}",
            "",
            "\tseq_putc(seq, '\\n');",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "hugetlb_cgroup_uncharge_folio, hugetlb_cgroup_uncharge_folio_rsvd, __hugetlb_cgroup_uncharge_cgroup, hugetlb_cgroup_uncharge_cgroup, hugetlb_cgroup_uncharge_cgroup_rsvd, hugetlb_cgroup_uncharge_counter, hugetlb_cgroup_uncharge_file_region, hugetlb_cgroup_read_numa_stat",
          "description": "实现HugeTLB页面释放时的反向计费操作，包含对单个folio和整体cgroup的解除分配逻辑，以及读取NUMA节点统计信息的实现。",
          "similarity": 0.513649582862854
        },
        {
          "chunk_id": 4,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 520,
          "end_line": 626,
          "content": [
            "static u64 hugetlb_cgroup_read_u64(struct cgroup_subsys_state *css,",
            "\t\t\t\t   struct cftype *cft)",
            "{",
            "\tstruct page_counter *counter;",
            "\tstruct page_counter *rsvd_counter;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(css);",
            "",
            "\tcounter = &h_cg->hugepage[MEMFILE_IDX(cft->private)];",
            "\trsvd_counter = &h_cg->rsvd_hugepage[MEMFILE_IDX(cft->private)];",
            "",
            "\tswitch (MEMFILE_ATTR(cft->private)) {",
            "\tcase RES_USAGE:",
            "\t\treturn (u64)page_counter_read(counter) * PAGE_SIZE;",
            "\tcase RES_RSVD_USAGE:",
            "\t\treturn (u64)page_counter_read(rsvd_counter) * PAGE_SIZE;",
            "\tcase RES_LIMIT:",
            "\t\treturn (u64)counter->max * PAGE_SIZE;",
            "\tcase RES_RSVD_LIMIT:",
            "\t\treturn (u64)rsvd_counter->max * PAGE_SIZE;",
            "\tcase RES_MAX_USAGE:",
            "\t\treturn (u64)counter->watermark * PAGE_SIZE;",
            "\tcase RES_RSVD_MAX_USAGE:",
            "\t\treturn (u64)rsvd_counter->watermark * PAGE_SIZE;",
            "\tcase RES_FAILCNT:",
            "\t\treturn counter->failcnt;",
            "\tcase RES_RSVD_FAILCNT:",
            "\t\treturn rsvd_counter->failcnt;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static int hugetlb_cgroup_read_u64_max(struct seq_file *seq, void *v)",
            "{",
            "\tint idx;",
            "\tu64 val;",
            "\tstruct cftype *cft = seq_cft(seq);",
            "\tunsigned long limit;",
            "\tstruct page_counter *counter;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(seq_css(seq));",
            "",
            "\tidx = MEMFILE_IDX(cft->private);",
            "\tcounter = &h_cg->hugepage[idx];",
            "",
            "\tlimit = round_down(PAGE_COUNTER_MAX,",
            "\t\t\t   pages_per_huge_page(&hstates[idx]));",
            "",
            "\tswitch (MEMFILE_ATTR(cft->private)) {",
            "\tcase RES_RSVD_USAGE:",
            "\t\tcounter = &h_cg->rsvd_hugepage[idx];",
            "\t\tfallthrough;",
            "\tcase RES_USAGE:",
            "\t\tval = (u64)page_counter_read(counter);",
            "\t\tseq_printf(seq, \"%llu\\n\", val * PAGE_SIZE);",
            "\t\tbreak;",
            "\tcase RES_RSVD_LIMIT:",
            "\t\tcounter = &h_cg->rsvd_hugepage[idx];",
            "\t\tfallthrough;",
            "\tcase RES_LIMIT:",
            "\t\tval = (u64)counter->max;",
            "\t\tif (val == limit)",
            "\t\t\tseq_puts(seq, \"max\\n\");",
            "\t\telse",
            "\t\t\tseq_printf(seq, \"%llu\\n\", val * PAGE_SIZE);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static ssize_t hugetlb_cgroup_write(struct kernfs_open_file *of,",
            "\t\t\t\t    char *buf, size_t nbytes, loff_t off,",
            "\t\t\t\t    const char *max)",
            "{",
            "\tint ret, idx;",
            "\tunsigned long nr_pages;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(of_css(of));",
            "\tbool rsvd = false;",
            "",
            "\tif (hugetlb_cgroup_is_root(h_cg)) /* Can't set limit on root */",
            "\t\treturn -EINVAL;",
            "",
            "\tbuf = strstrip(buf);",
            "\tret = page_counter_memparse(buf, max, &nr_pages);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tidx = MEMFILE_IDX(of_cft(of)->private);",
            "\tnr_pages = round_down(nr_pages, pages_per_huge_page(&hstates[idx]));",
            "",
            "\tswitch (MEMFILE_ATTR(of_cft(of)->private)) {",
            "\tcase RES_RSVD_LIMIT:",
            "\t\trsvd = true;",
            "\t\tfallthrough;",
            "\tcase RES_LIMIT:",
            "\t\tmutex_lock(&hugetlb_limit_mutex);",
            "\t\tret = page_counter_set_max(",
            "\t\t\t__hugetlb_cgroup_counter_from_cgroup(h_cg, idx, rsvd),",
            "\t\t\tnr_pages);",
            "\t\tmutex_unlock(&hugetlb_limit_mutex);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tret = -EINVAL;",
            "\t\tbreak;",
            "\t}",
            "\treturn ret ?: nbytes;",
            "}"
          ],
          "function_name": "hugetlb_cgroup_read_u64, hugetlb_cgroup_read_u64_max, hugetlb_cgroup_write",
          "description": "提供HugeTLB cgroup的监控接口，实现读取当前使用量、限制等参数的功能，并支持通过接口设置内存限制参数。",
          "similarity": 0.4701756536960602
        },
        {
          "chunk_id": 0,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 1,
          "end_line": 64,
          "content": [
            "/*",
            " *",
            " * Copyright IBM Corporation, 2012",
            " * Author Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>",
            " *",
            " * Cgroup v2",
            " * Copyright (C) 2019 Red Hat, Inc.",
            " * Author: Giuseppe Scrivano <gscrivan@redhat.com>",
            " *",
            " * This program is free software; you can redistribute it and/or modify it",
            " * under the terms of version 2.1 of the GNU Lesser General Public License",
            " * as published by the Free Software Foundation.",
            " *",
            " * This program is distributed in the hope that it would be useful, but",
            " * WITHOUT ANY WARRANTY; without even the implied warranty of",
            " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.",
            " *",
            " */",
            "",
            "#include <linux/cgroup.h>",
            "#include <linux/page_counter.h>",
            "#include <linux/slab.h>",
            "#include <linux/hugetlb.h>",
            "#include <linux/hugetlb_cgroup.h>",
            "",
            "#define MEMFILE_PRIVATE(x, val)\t(((x) << 16) | (val))",
            "#define MEMFILE_IDX(val)\t(((val) >> 16) & 0xffff)",
            "#define MEMFILE_ATTR(val)\t((val) & 0xffff)",
            "",
            "static struct hugetlb_cgroup *root_h_cgroup __read_mostly;",
            "",
            "static inline struct page_counter *",
            "__hugetlb_cgroup_counter_from_cgroup(struct hugetlb_cgroup *h_cg, int idx,",
            "\t\t\t\t     bool rsvd)",
            "{",
            "\tif (rsvd)",
            "\t\treturn &h_cg->rsvd_hugepage[idx];",
            "\treturn &h_cg->hugepage[idx];",
            "}",
            "",
            "static inline struct page_counter *",
            "hugetlb_cgroup_counter_from_cgroup(struct hugetlb_cgroup *h_cg, int idx)",
            "{",
            "\treturn __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, false);",
            "}",
            "",
            "static inline struct page_counter *",
            "hugetlb_cgroup_counter_from_cgroup_rsvd(struct hugetlb_cgroup *h_cg, int idx)",
            "{",
            "\treturn __hugetlb_cgroup_counter_from_cgroup(h_cg, idx, true);",
            "}",
            "",
            "static inline",
            "struct hugetlb_cgroup *hugetlb_cgroup_from_css(struct cgroup_subsys_state *s)",
            "{",
            "\treturn s ? container_of(s, struct hugetlb_cgroup, css) : NULL;",
            "}",
            "",
            "static inline",
            "struct hugetlb_cgroup *hugetlb_cgroup_from_task(struct task_struct *task)",
            "{",
            "\treturn hugetlb_cgroup_from_css(task_css(task, hugetlb_cgrp_id));",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义HugeTLB cgroup子系统的辅助宏和基础结构，声明全局根cgroup指针及用于获取对应cgroup的内联函数，提供page_counter相关操作的封装接口。",
          "similarity": 0.4517258405685425
        },
        {
          "chunk_id": 5,
          "file_path": "mm/hugetlb_cgroup.c",
          "start_line": 632,
          "end_line": 766,
          "content": [
            "static ssize_t hugetlb_cgroup_write_legacy(struct kernfs_open_file *of,",
            "\t\t\t\t\t   char *buf, size_t nbytes, loff_t off)",
            "{",
            "\treturn hugetlb_cgroup_write(of, buf, nbytes, off, \"-1\");",
            "}",
            "static ssize_t hugetlb_cgroup_write_dfl(struct kernfs_open_file *of,",
            "\t\t\t\t\tchar *buf, size_t nbytes, loff_t off)",
            "{",
            "\treturn hugetlb_cgroup_write(of, buf, nbytes, off, \"max\");",
            "}",
            "static ssize_t hugetlb_cgroup_reset(struct kernfs_open_file *of,",
            "\t\t\t\t    char *buf, size_t nbytes, loff_t off)",
            "{",
            "\tint ret = 0;",
            "\tstruct page_counter *counter, *rsvd_counter;",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(of_css(of));",
            "",
            "\tcounter = &h_cg->hugepage[MEMFILE_IDX(of_cft(of)->private)];",
            "\trsvd_counter = &h_cg->rsvd_hugepage[MEMFILE_IDX(of_cft(of)->private)];",
            "",
            "\tswitch (MEMFILE_ATTR(of_cft(of)->private)) {",
            "\tcase RES_MAX_USAGE:",
            "\t\tpage_counter_reset_watermark(counter);",
            "\t\tbreak;",
            "\tcase RES_RSVD_MAX_USAGE:",
            "\t\tpage_counter_reset_watermark(rsvd_counter);",
            "\t\tbreak;",
            "\tcase RES_FAILCNT:",
            "\t\tcounter->failcnt = 0;",
            "\t\tbreak;",
            "\tcase RES_RSVD_FAILCNT:",
            "\t\trsvd_counter->failcnt = 0;",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tret = -EINVAL;",
            "\t\tbreak;",
            "\t}",
            "\treturn ret ?: nbytes;",
            "}",
            "static int __hugetlb_events_show(struct seq_file *seq, bool local)",
            "{",
            "\tint idx;",
            "\tlong max;",
            "\tstruct cftype *cft = seq_cft(seq);",
            "\tstruct hugetlb_cgroup *h_cg = hugetlb_cgroup_from_css(seq_css(seq));",
            "",
            "\tidx = MEMFILE_IDX(cft->private);",
            "",
            "\tif (local)",
            "\t\tmax = atomic_long_read(&h_cg->events_local[idx][HUGETLB_MAX]);",
            "\telse",
            "\t\tmax = atomic_long_read(&h_cg->events[idx][HUGETLB_MAX]);",
            "",
            "\tseq_printf(seq, \"max %lu\\n\", max);",
            "",
            "\treturn 0;",
            "}",
            "static int hugetlb_events_show(struct seq_file *seq, void *v)",
            "{",
            "\treturn __hugetlb_events_show(seq, false);",
            "}",
            "static int hugetlb_events_local_show(struct seq_file *seq, void *v)",
            "{",
            "\treturn __hugetlb_events_show(seq, true);",
            "}",
            "static void __init __hugetlb_cgroup_file_dfl_init(int idx)",
            "{",
            "\tchar buf[32];",
            "\tstruct cftype *cft;",
            "\tstruct hstate *h = &hstates[idx];",
            "",
            "\t/* format the size */",
            "\tmem_fmt(buf, sizeof(buf), huge_page_size(h));",
            "",
            "\t/* Add the limit file */",
            "\tcft = &h->cgroup_files_dfl[0];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.max\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_LIMIT);",
            "\tcft->seq_show = hugetlb_cgroup_read_u64_max;",
            "\tcft->write = hugetlb_cgroup_write_dfl;",
            "\tcft->flags = CFTYPE_NOT_ON_ROOT;",
            "",
            "\t/* Add the reservation limit file */",
            "\tcft = &h->cgroup_files_dfl[1];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.rsvd.max\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_RSVD_LIMIT);",
            "\tcft->seq_show = hugetlb_cgroup_read_u64_max;",
            "\tcft->write = hugetlb_cgroup_write_dfl;",
            "\tcft->flags = CFTYPE_NOT_ON_ROOT;",
            "",
            "\t/* Add the current usage file */",
            "\tcft = &h->cgroup_files_dfl[2];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.current\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_USAGE);",
            "\tcft->seq_show = hugetlb_cgroup_read_u64_max;",
            "\tcft->flags = CFTYPE_NOT_ON_ROOT;",
            "",
            "\t/* Add the current reservation usage file */",
            "\tcft = &h->cgroup_files_dfl[3];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.rsvd.current\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, RES_RSVD_USAGE);",
            "\tcft->seq_show = hugetlb_cgroup_read_u64_max;",
            "\tcft->flags = CFTYPE_NOT_ON_ROOT;",
            "",
            "\t/* Add the events file */",
            "\tcft = &h->cgroup_files_dfl[4];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.events\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, 0);",
            "\tcft->seq_show = hugetlb_events_show;",
            "\tcft->file_offset = offsetof(struct hugetlb_cgroup, events_file[idx]);",
            "\tcft->flags = CFTYPE_NOT_ON_ROOT;",
            "",
            "\t/* Add the events.local file */",
            "\tcft = &h->cgroup_files_dfl[5];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.events.local\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, 0);",
            "\tcft->seq_show = hugetlb_events_local_show;",
            "\tcft->file_offset = offsetof(struct hugetlb_cgroup,",
            "\t\t\t\t    events_local_file[idx]);",
            "\tcft->flags = CFTYPE_NOT_ON_ROOT;",
            "",
            "\t/* Add the numa stat file */",
            "\tcft = &h->cgroup_files_dfl[6];",
            "\tsnprintf(cft->name, MAX_CFTYPE_NAME, \"%s.numa_stat\", buf);",
            "\tcft->private = MEMFILE_PRIVATE(idx, 0);",
            "\tcft->seq_show = hugetlb_cgroup_read_numa_stat;",
            "\tcft->flags = CFTYPE_NOT_ON_ROOT;",
            "",
            "\t/* NULL terminate the last cft */",
            "\tcft = &h->cgroup_files_dfl[7];",
            "\tmemset(cft, 0, sizeof(*cft));",
            "",
            "\tWARN_ON(cgroup_add_dfl_cftypes(&hugetlb_cgrp_subsys,",
            "\t\t\t\t       h->cgroup_files_dfl));",
            "}"
          ],
          "function_name": "hugetlb_cgroup_write_legacy, hugetlb_cgroup_write_dfl, hugetlb_cgroup_reset, __hugetlb_events_show, hugetlb_events_show, hugetlb_events_local_show, __hugetlb_cgroup_file_dfl_init",
          "description": "定义并实现hugetlb cgroup的写操作函数，其中hugetlb_cgroup_write_legacy和hugetlb_cgroup_write_dfl分别处理传统模式和默认模式下的资源限制设置，hugetlb_cgroup_reset重置特定资源计数器的水位线，__hugetlb_cgroup_file_dfl_init初始化默认模式下的cgroup控制文件及事件统计接口",
          "similarity": 0.4499552845954895
        }
      ]
    },
    {
      "source_file": "kernel/bpf/memalloc.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:19:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\memalloc.c`\n\n---\n\n# `bpf/memalloc.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/memalloc.c` 实现了一个专用于 BPF（Berkeley Packet Filter）程序的内存分配器，支持在任意上下文（包括 NMI、中断、不可抢占上下文等）中安全地分配和释放小块内存。该分配器通过每 CPU 的多级缓存桶（per-CPU per-bucket free list）机制，避免在 BPF 程序执行路径中直接调用可能不安全的 `kmalloc()`。缓存桶的填充和回收由 `irq_work` 异步完成，确保主执行路径的低延迟和高可靠性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_mem_cache`**  \n  每个缓存桶的核心结构，包含：\n  - `free_llist` / `free_llist_extra`：无锁链表（llist），用于存储空闲对象。\n  - `active`：本地原子计数器，用于保护对 `free_llist` 的并发访问。\n  - `refill_work`：`irq_work` 结构，用于触发异步填充。\n  - `objcg`：对象 cgroup 指针，用于内存记账。\n  - `unit_size`：该缓存桶中对象的固定大小。\n  - `free_cnt`、`low_watermark`、`high_watermark`、`batch`：缓存管理参数。\n  - `percpu_size`：标识是否为 per-CPU 分配。\n  - RCU 相关字段（`free_by_rcu`、`rcu` 等）：用于延迟释放内存，避免在不可睡眠上下文中调用 `kfree`。\n\n- **`struct bpf_mem_caches`**  \n  包含 `NUM_CACHES`（11 个）不同大小的 `bpf_mem_cache` 实例，对应预定义的内存块尺寸。\n\n- **`sizes[NUM_CACHES]`**  \n  定义了 11 种支持的分配尺寸：`{96, 192, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096}` 字节。\n\n- **`size_index[24]`**  \n  查找表，将请求大小（≤192 字节）映射到对应的缓存桶索引。\n\n### 主要函数\n\n- **`bpf_mem_cache_idx(size_t size)`**  \n  根据请求大小返回对应的缓存桶索引（0~10），超出 `BPF_MEM_ALLOC_SIZE_MAX`（4096）则返回 -1。\n\n- **`__alloc()`**  \n  底层分配函数，根据是否为 per-CPU 类型调用 `kmalloc_node` 或 `__alloc_percpu_gfp`。\n\n- **`add_obj_to_free_list()`**  \n  将对象安全地加入当前 CPU 的空闲链表，使用 `active` 计数器保护。\n\n- **`alloc_bulk()`**  \n  批量分配对象并填充缓存桶，优先从延迟释放队列（如 `free_by_rcu_ttrace`）回收，再尝试从全局分配器分配。\n\n- **`free_one()` / `free_all()`**  \n  释放单个或多个对象，区分普通和 per-CPU 类型。\n\n- **`__free_rcu()` / `__free_rcu_tasks_trace()`**  \n  RCU 回调函数，用于在宽限期结束后真正释放内存。\n\n- **`enque_to_free()` / `do_call_rcu_ttrace()`**  \n  将待释放对象加入 RCU 延迟队列，并触发 RCU 宽限期。\n\n## 3. 关键实现\n\n### 内存布局与对齐\n- 每个分配的对象末尾附加 8 字节的 `struct llist_node`，用于无锁链表管理。\n- 所有分配均对齐至 8 字节边界。\n\n### 并发控制\n- 使用 `local_t active` 计数器保护对 `free_llist` 的访问。在分配/释放时，通过 `inc_active()`/`dec_active()` 禁用中断（尤其在 `CONFIG_PREEMPT_RT` 下），确保 NMI 或中断上下文不会破坏链表结构。\n- `free_llist_extra` 用于在 `active` 忙时暂存释放对象，避免失败。\n\n### 异步填充机制\n- 当缓存桶水位低于 `low_watermark` 时，通过 `irq_work` 触发 `alloc_bulk()`。\n- `alloc_bulk()` 优先从 RCU 延迟释放队列中回收对象，减少全局分配压力。\n- 使用 `set_active_memcg()` 确保内存分配计入正确的 memcg。\n\n### RCU 延迟释放\n- 在不可睡眠上下文（如 NMI）中释放内存时，对象被加入 `free_by_rcu_ttrace` 队列。\n- 通过 `call_rcu_tasks_trace()` 或 `call_rcu()` 触发宽限期，之后在软中断上下文中真正释放。\n- 支持 `rcu_trace_implies_rcu_gp()` 优化，避免双重 RCU 调用。\n\n### 尺寸映射策略\n- 对 ≤192 字节的请求，使用 `size_index` 查找表快速定位桶。\n- 对 >192 字节的请求，使用 `fls(size - 1) - 2` 计算桶索引，覆盖 256~4096 字节范围。\n\n## 4. 依赖关系\n\n- **内存管理**：依赖 `<linux/mm.h>`、`<linux/memcontrol.h>` 进行底层分配和 memcg 记账。\n- **BPF 子系统**：通过 `<linux/bpf.h>` 和 `<linux/bpf_mem_alloc.h>` 与 BPF 运行时集成。\n- **无锁数据结构**：使用 `<linux/llist.h>` 提供的无锁链表。\n- **中断与延迟执行**：依赖 `<linux/irq_work.h>` 实现异步填充。\n- **RCU 机制**：使用 RCU 和 RCU Tasks Trace 宽限期实现安全延迟释放。\n- **架构相关**：使用 `<asm/local.h>` 的 per-CPU 原子操作。\n\n## 5. 使用场景\n\n- **BPF tracing 程序**：当 BPF 程序 attach 到 `kprobe`、`fentry` 等 hook 点时，可能运行在任意内核上下文（包括 NMI、中断、不可抢占区域）。此时标准 `kmalloc` 不安全，必须使用本分配器。\n- **高可靠性内存分配**：在不允许睡眠、不能触发内存回收的上下文中，提供确定性的内存分配能力。\n- **低延迟要求**：通过 per-CPU 缓存避免锁竞争和全局分配器开销，满足 BPF 程序对性能的严苛要求。\n- **内存隔离与记账**：支持通过 `objcg` 将 BPF 内存消耗计入特定 cgroup，便于资源控制。",
      "similarity": 0.5251500010490417,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 68,
          "end_line": 168,
          "content": [
            "static int bpf_mem_cache_idx(size_t size)",
            "{",
            "\tif (!size || size > BPF_MEM_ALLOC_SIZE_MAX)",
            "\t\treturn -1;",
            "",
            "\tif (size <= 192)",
            "\t\treturn size_index[(size - 1) / 8] - 1;",
            "",
            "\treturn fls(size - 1) - 2;",
            "}",
            "static void inc_active(struct bpf_mem_cache *c, unsigned long *flags)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\t/* In RT irq_work runs in per-cpu kthread, so disable",
            "\t\t * interrupts to avoid preemption and interrupts and",
            "\t\t * reduce the chance of bpf prog executing on this cpu",
            "\t\t * when active counter is busy.",
            "\t\t */",
            "\t\tlocal_irq_save(*flags);",
            "\t/* alloc_bulk runs from irq_work which will not preempt a bpf",
            "\t * program that does unit_alloc/unit_free since IRQs are",
            "\t * disabled there. There is no race to increment 'active'",
            "\t * counter. It protects free_llist from corruption in case NMI",
            "\t * bpf prog preempted this loop.",
            "\t */",
            "\tWARN_ON_ONCE(local_inc_return(&c->active) != 1);",
            "}",
            "static void dec_active(struct bpf_mem_cache *c, unsigned long *flags)",
            "{",
            "\tlocal_dec(&c->active);",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tlocal_irq_restore(*flags);",
            "}",
            "static void add_obj_to_free_list(struct bpf_mem_cache *c, void *obj)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tinc_active(c, &flags);",
            "\t__llist_add(obj, &c->free_llist);",
            "\tc->free_cnt++;",
            "\tdec_active(c, &flags);",
            "}",
            "static void alloc_bulk(struct bpf_mem_cache *c, int cnt, int node, bool atomic)",
            "{",
            "\tstruct mem_cgroup *memcg = NULL, *old_memcg;",
            "\tgfp_t gfp;",
            "\tvoid *obj;",
            "\tint i;",
            "",
            "\tgfp = __GFP_NOWARN | __GFP_ACCOUNT;",
            "\tgfp |= atomic ? GFP_NOWAIT : GFP_KERNEL;",
            "",
            "\tfor (i = 0; i < cnt; i++) {",
            "\t\t/*",
            "\t\t * For every 'c' llist_del_first(&c->free_by_rcu_ttrace); is",
            "\t\t * done only by one CPU == current CPU. Other CPUs might",
            "\t\t * llist_add() and llist_del_all() in parallel.",
            "\t\t */",
            "\t\tobj = llist_del_first(&c->free_by_rcu_ttrace);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tif (i >= cnt)",
            "\t\treturn;",
            "",
            "\tfor (; i < cnt; i++) {",
            "\t\tobj = llist_del_first(&c->waiting_for_gp_ttrace);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tif (i >= cnt)",
            "\t\treturn;",
            "",
            "\tmemcg = get_memcg(c);",
            "\told_memcg = set_active_memcg(memcg);",
            "\tfor (; i < cnt; i++) {",
            "\t\t/* Allocate, but don't deplete atomic reserves that typical",
            "\t\t * GFP_ATOMIC would do. irq_work runs on this cpu and kmalloc",
            "\t\t * will allocate from the current numa node which is what we",
            "\t\t * want here.",
            "\t\t */",
            "\t\tobj = __alloc(c, node, gfp);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tset_active_memcg(old_memcg);",
            "\tmem_cgroup_put(memcg);",
            "}",
            "static void free_one(void *obj, bool percpu)",
            "{",
            "\tif (percpu) {",
            "\t\tfree_percpu(((void __percpu **)obj)[1]);",
            "\t\tkfree(obj);",
            "\t\treturn;",
            "\t}",
            "",
            "\tkfree(obj);",
            "}"
          ],
          "function_name": "bpf_mem_cache_idx, inc_active, dec_active, add_obj_to_free_list, alloc_bulk, free_one",
          "description": "实现BPF内存缓存核心控制逻辑，包含大小索引计算、活跃计数器管理、对象回收到自由链表、批量分配与释放流程。通过irq_work异步补充缓存，处理多CPU间的内存对象迁移与回收。",
          "similarity": 0.5001456141471863
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 859,
          "end_line": 938,
          "content": [
            "static void notrace unit_free_rcu(struct bpf_mem_cache *c, void *ptr)",
            "{",
            "\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;",
            "\tunsigned long flags;",
            "",
            "\tc->tgt = *(struct bpf_mem_cache **)llnode;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (local_inc_return(&c->active) == 1) {",
            "\t\tif (__llist_add(llnode, &c->free_by_rcu))",
            "\t\t\tc->free_by_rcu_tail = llnode;",
            "\t} else {",
            "\t\tllist_add(llnode, &c->free_llist_extra_rcu);",
            "\t}",
            "\tlocal_dec(&c->active);",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (!atomic_read(&c->call_rcu_in_progress))",
            "\t\tirq_work_raise(c);",
            "}",
            "void notrace bpf_mem_free(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tstruct bpf_mem_cache *c;",
            "\tint idx;",
            "",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tc = *(void **)(ptr - LLIST_NODE_SZ);",
            "\tidx = bpf_mem_cache_idx(c->unit_size);",
            "\tif (WARN_ON_ONCE(idx < 0))",
            "\t\treturn;",
            "",
            "\tunit_free(this_cpu_ptr(ma->caches)->cache + idx, ptr);",
            "}",
            "void notrace bpf_mem_free_rcu(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tstruct bpf_mem_cache *c;",
            "\tint idx;",
            "",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tc = *(void **)(ptr - LLIST_NODE_SZ);",
            "\tidx = bpf_mem_cache_idx(c->unit_size);",
            "\tif (WARN_ON_ONCE(idx < 0))",
            "\t\treturn;",
            "",
            "\tunit_free_rcu(this_cpu_ptr(ma->caches)->cache + idx, ptr);",
            "}",
            "void notrace bpf_mem_cache_free(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tunit_free(this_cpu_ptr(ma->cache), ptr);",
            "}",
            "void notrace bpf_mem_cache_free_rcu(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tunit_free_rcu(this_cpu_ptr(ma->cache), ptr);",
            "}",
            "void bpf_mem_cache_raw_free(void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tkfree(ptr - LLIST_NODE_SZ);",
            "}",
            "int bpf_mem_alloc_check_size(bool percpu, size_t size)",
            "{",
            "\t/* The size of percpu allocation doesn't have LLIST_NODE_SZ overhead */",
            "\tif ((percpu && size > BPF_MEM_ALLOC_SIZE_MAX) ||",
            "\t    (!percpu && size > BPF_MEM_ALLOC_SIZE_MAX - LLIST_NODE_SZ))",
            "\t\treturn -E2BIG;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "unit_free_rcu, bpf_mem_free, bpf_mem_free_rcu, bpf_mem_cache_free, bpf_mem_cache_free_rcu, bpf_mem_cache_raw_free, bpf_mem_alloc_check_size",
          "description": "提供基于RCU的内存释放接口，unit_free系列函数将对象加入链表实现批量回收，bpf_mem_free系列根据上下文选择普通或RCU释放路径，check_size验证分配尺寸合法性",
          "similarity": 0.49357980489730835
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 381,
          "end_line": 545,
          "content": [
            "static void check_free_by_rcu(struct bpf_mem_cache *c)",
            "{",
            "\tstruct llist_node *llnode, *t;",
            "\tunsigned long flags;",
            "",
            "\t/* drain free_llist_extra_rcu */",
            "\tif (unlikely(!llist_empty(&c->free_llist_extra_rcu))) {",
            "\t\tinc_active(c, &flags);",
            "\t\tllist_for_each_safe(llnode, t, llist_del_all(&c->free_llist_extra_rcu))",
            "\t\t\tif (__llist_add(llnode, &c->free_by_rcu))",
            "\t\t\t\tc->free_by_rcu_tail = llnode;",
            "\t\tdec_active(c, &flags);",
            "\t}",
            "",
            "\tif (llist_empty(&c->free_by_rcu))",
            "\t\treturn;",
            "",
            "\tif (atomic_xchg(&c->call_rcu_in_progress, 1)) {",
            "\t\t/*",
            "\t\t * Instead of kmalloc-ing new rcu_head and triggering 10k",
            "\t\t * call_rcu() to hit rcutree.qhimark and force RCU to notice",
            "\t\t * the overload just ask RCU to hurry up. There could be many",
            "\t\t * objects in free_by_rcu list.",
            "\t\t * This hint reduces memory consumption for an artificial",
            "\t\t * benchmark from 2 Gbyte to 150 Mbyte.",
            "\t\t */",
            "\t\trcu_request_urgent_qs_task(current);",
            "\t\treturn;",
            "\t}",
            "",
            "\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));",
            "",
            "\tinc_active(c, &flags);",
            "\tWRITE_ONCE(c->waiting_for_gp.first, __llist_del_all(&c->free_by_rcu));",
            "\tc->waiting_for_gp_tail = c->free_by_rcu_tail;",
            "\tdec_active(c, &flags);",
            "",
            "\tif (unlikely(READ_ONCE(c->draining))) {",
            "\t\tfree_all(llist_del_all(&c->waiting_for_gp), !!c->percpu_size);",
            "\t\tatomic_set(&c->call_rcu_in_progress, 0);",
            "\t} else {",
            "\t\tcall_rcu_hurry(&c->rcu, __free_by_rcu);",
            "\t}",
            "}",
            "static void bpf_mem_refill(struct irq_work *work)",
            "{",
            "\tstruct bpf_mem_cache *c = container_of(work, struct bpf_mem_cache, refill_work);",
            "\tint cnt;",
            "",
            "\t/* Racy access to free_cnt. It doesn't need to be 100% accurate */",
            "\tcnt = c->free_cnt;",
            "\tif (cnt < c->low_watermark)",
            "\t\t/* irq_work runs on this cpu and kmalloc will allocate",
            "\t\t * from the current numa node which is what we want here.",
            "\t\t */",
            "\t\talloc_bulk(c, c->batch, NUMA_NO_NODE, true);",
            "\telse if (cnt > c->high_watermark)",
            "\t\tfree_bulk(c);",
            "",
            "\tcheck_free_by_rcu(c);",
            "}",
            "static void notrace irq_work_raise(struct bpf_mem_cache *c)",
            "{",
            "\tirq_work_queue(&c->refill_work);",
            "}",
            "static void init_refill_work(struct bpf_mem_cache *c)",
            "{",
            "\tinit_irq_work(&c->refill_work, bpf_mem_refill);",
            "\tif (c->percpu_size) {",
            "\t\tc->low_watermark = 1;",
            "\t\tc->high_watermark = 3;",
            "\t} else if (c->unit_size <= 256) {",
            "\t\tc->low_watermark = 32;",
            "\t\tc->high_watermark = 96;",
            "\t} else {",
            "\t\t/* When page_size == 4k, order-0 cache will have low_mark == 2",
            "\t\t * and high_mark == 6 with batch alloc of 3 individual pages at",
            "\t\t * a time.",
            "\t\t * 8k allocs and above low == 1, high == 3, batch == 1.",
            "\t\t */",
            "\t\tc->low_watermark = max(32 * 256 / c->unit_size, 1);",
            "\t\tc->high_watermark = max(96 * 256 / c->unit_size, 3);",
            "\t}",
            "\tc->batch = max((c->high_watermark - c->low_watermark) / 4 * 3, 1);",
            "}",
            "static void prefill_mem_cache(struct bpf_mem_cache *c, int cpu)",
            "{",
            "\tint cnt = 1;",
            "",
            "\t/* To avoid consuming memory, for non-percpu allocation, assume that",
            "\t * 1st run of bpf prog won't be doing more than 4 map_update_elem from",
            "\t * irq disabled region if unit size is less than or equal to 256.",
            "\t * For all other cases, let us just do one allocation.",
            "\t */",
            "\tif (!c->percpu_size && c->unit_size <= 256)",
            "\t\tcnt = 4;",
            "\talloc_bulk(c, cnt, cpu_to_node(cpu), false);",
            "}",
            "int bpf_mem_alloc_init(struct bpf_mem_alloc *ma, int size, bool percpu)",
            "{",
            "\tstruct bpf_mem_caches *cc; struct bpf_mem_caches __percpu *pcc;",
            "\tstruct bpf_mem_cache *c; struct bpf_mem_cache __percpu *pc;",
            "\tstruct obj_cgroup *objcg = NULL;",
            "\tint cpu, i, unit_size, percpu_size = 0;",
            "",
            "\tif (percpu && size == 0)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* room for llist_node and per-cpu pointer */",
            "\tif (percpu)",
            "\t\tpercpu_size = LLIST_NODE_SZ + sizeof(void *);",
            "\tma->percpu = percpu;",
            "",
            "\tif (size) {",
            "\t\tpc = __alloc_percpu_gfp(sizeof(*pc), 8, GFP_KERNEL);",
            "\t\tif (!pc)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tif (!percpu)",
            "\t\t\tsize += LLIST_NODE_SZ; /* room for llist_node */",
            "\t\tunit_size = size;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\t\tif (memcg_bpf_enabled())",
            "\t\t\tobjcg = get_obj_cgroup_from_current();",
            "#endif",
            "\t\tma->objcg = objcg;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(pc, cpu);",
            "\t\t\tc->unit_size = unit_size;",
            "\t\t\tc->objcg = objcg;",
            "\t\t\tc->percpu_size = percpu_size;",
            "\t\t\tc->tgt = c;",
            "\t\t\tinit_refill_work(c);",
            "\t\t\tprefill_mem_cache(c, cpu);",
            "\t\t}",
            "\t\tma->cache = pc;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tpcc = __alloc_percpu_gfp(sizeof(*cc), 8, GFP_KERNEL);",
            "\tif (!pcc)",
            "\t\treturn -ENOMEM;",
            "#ifdef CONFIG_MEMCG",
            "\tobjcg = get_obj_cgroup_from_current();",
            "#endif",
            "\tma->objcg = objcg;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tcc = per_cpu_ptr(pcc, cpu);",
            "\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\tc = &cc->cache[i];",
            "\t\t\tc->unit_size = sizes[i];",
            "\t\t\tc->objcg = objcg;",
            "\t\t\tc->percpu_size = percpu_size;",
            "\t\t\tc->tgt = c;",
            "",
            "\t\t\tinit_refill_work(c);",
            "\t\t\tprefill_mem_cache(c, cpu);",
            "\t\t}",
            "\t}",
            "",
            "\tma->caches = pcc;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "check_free_by_rcu, bpf_mem_refill, irq_work_raise, init_refill_work, prefill_mem_cache, bpf_mem_alloc_init",
          "description": "初始化内存缓存的刷新工作队列，设置水位标记控制缓存规模，实现动态扩容/缩容。包含预填充缓存的初始化函数，根据对象大小配置不同的低/高水位阈值，通过中断工作线程维护缓存状态。",
          "similarity": 0.4465683698654175
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 576,
          "end_line": 682,
          "content": [
            "int bpf_mem_alloc_percpu_init(struct bpf_mem_alloc *ma, struct obj_cgroup *objcg)",
            "{",
            "\tstruct bpf_mem_caches __percpu *pcc;",
            "",
            "\tpcc = __alloc_percpu_gfp(sizeof(struct bpf_mem_caches), 8, GFP_KERNEL);",
            "\tif (!pcc)",
            "\t\treturn -ENOMEM;",
            "",
            "\tma->caches = pcc;",
            "\tma->objcg = objcg;",
            "\tma->percpu = true;",
            "\treturn 0;",
            "}",
            "int bpf_mem_alloc_percpu_unit_init(struct bpf_mem_alloc *ma, int size)",
            "{",
            "\tstruct bpf_mem_caches *cc; struct bpf_mem_caches __percpu *pcc;",
            "\tint cpu, i, unit_size, percpu_size;",
            "\tstruct obj_cgroup *objcg;",
            "\tstruct bpf_mem_cache *c;",
            "",
            "\ti = bpf_mem_cache_idx(size);",
            "\tif (i < 0)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* room for llist_node and per-cpu pointer */",
            "\tpercpu_size = LLIST_NODE_SZ + sizeof(void *);",
            "",
            "\tunit_size = sizes[i];",
            "\tobjcg = ma->objcg;",
            "\tpcc = ma->caches;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tcc = per_cpu_ptr(pcc, cpu);",
            "\t\tc = &cc->cache[i];",
            "\t\tif (cpu == 0 && c->unit_size)",
            "\t\t\tbreak;",
            "",
            "\t\tc->unit_size = unit_size;",
            "\t\tc->objcg = objcg;",
            "\t\tc->percpu_size = percpu_size;",
            "\t\tc->tgt = c;",
            "",
            "\t\tinit_refill_work(c);",
            "\t\tprefill_mem_cache(c, cpu);",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static void drain_mem_cache(struct bpf_mem_cache *c)",
            "{",
            "\tbool percpu = !!c->percpu_size;",
            "",
            "\t/* No progs are using this bpf_mem_cache, but htab_map_free() called",
            "\t * bpf_mem_cache_free() for all remaining elements and they can be in",
            "\t * free_by_rcu_ttrace or in waiting_for_gp_ttrace lists, so drain those lists now.",
            "\t *",
            "\t * Except for waiting_for_gp_ttrace list, there are no concurrent operations",
            "\t * on these lists, so it is safe to use __llist_del_all().",
            "\t */",
            "\tfree_all(llist_del_all(&c->free_by_rcu_ttrace), percpu);",
            "\tfree_all(llist_del_all(&c->waiting_for_gp_ttrace), percpu);",
            "\tfree_all(__llist_del_all(&c->free_llist), percpu);",
            "\tfree_all(__llist_del_all(&c->free_llist_extra), percpu);",
            "\tfree_all(__llist_del_all(&c->free_by_rcu), percpu);",
            "\tfree_all(__llist_del_all(&c->free_llist_extra_rcu), percpu);",
            "\tfree_all(llist_del_all(&c->waiting_for_gp), percpu);",
            "}",
            "static void check_mem_cache(struct bpf_mem_cache *c)",
            "{",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_by_rcu_ttrace));",
            "\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp_ttrace));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_llist));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_llist_extra));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_by_rcu));",
            "\tWARN_ON_ONCE(!llist_empty(&c->free_llist_extra_rcu));",
            "\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));",
            "}",
            "static void check_leaked_objs(struct bpf_mem_alloc *ma)",
            "{",
            "\tstruct bpf_mem_caches *cc;",
            "\tstruct bpf_mem_cache *c;",
            "\tint cpu, i;",
            "",
            "\tif (ma->cache) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(ma->cache, cpu);",
            "\t\t\tcheck_mem_cache(c);",
            "\t\t}",
            "\t}",
            "\tif (ma->caches) {",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tcc = per_cpu_ptr(ma->caches, cpu);",
            "\t\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\t\tc = &cc->cache[i];",
            "\t\t\t\tcheck_mem_cache(c);",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "}",
            "static void free_mem_alloc_no_barrier(struct bpf_mem_alloc *ma)",
            "{",
            "\tcheck_leaked_objs(ma);",
            "\tfree_percpu(ma->cache);",
            "\tfree_percpu(ma->caches);",
            "\tma->cache = NULL;",
            "\tma->caches = NULL;",
            "}"
          ],
          "function_name": "bpf_mem_alloc_percpu_init, bpf_mem_alloc_percpu_unit_init, drain_mem_cache, check_mem_cache, check_leaked_objs, free_mem_alloc_no_barrier",
          "description": "实现Per-CPU内存分配器的初始化与清理逻辑，包含Per-CPU缓存初始化、对象回收链表遍历、内存泄漏检测及资源释放。提供模块卸载时的强制清理接口，确保所有残留对象被正确释放。",
          "similarity": 0.43978267908096313
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 689,
          "end_line": 807,
          "content": [
            "static void free_mem_alloc(struct bpf_mem_alloc *ma)",
            "{",
            "\t/* waiting_for_gp[_ttrace] lists were drained, but RCU callbacks",
            "\t * might still execute. Wait for them.",
            "\t *",
            "\t * rcu_barrier_tasks_trace() doesn't imply synchronize_rcu_tasks_trace(),",
            "\t * but rcu_barrier_tasks_trace() and rcu_barrier() below are only used",
            "\t * to wait for the pending __free_rcu_tasks_trace() and __free_rcu(),",
            "\t * so if call_rcu(head, __free_rcu) is skipped due to",
            "\t * rcu_trace_implies_rcu_gp(), it will be OK to skip rcu_barrier() by",
            "\t * using rcu_trace_implies_rcu_gp() as well.",
            "\t */",
            "\trcu_barrier(); /* wait for __free_by_rcu */",
            "\trcu_barrier_tasks_trace(); /* wait for __free_rcu */",
            "\tif (!rcu_trace_implies_rcu_gp())",
            "\t\trcu_barrier();",
            "\tfree_mem_alloc_no_barrier(ma);",
            "}",
            "static void free_mem_alloc_deferred(struct work_struct *work)",
            "{",
            "\tstruct bpf_mem_alloc *ma = container_of(work, struct bpf_mem_alloc, work);",
            "",
            "\tfree_mem_alloc(ma);",
            "\tkfree(ma);",
            "}",
            "static void destroy_mem_alloc(struct bpf_mem_alloc *ma, int rcu_in_progress)",
            "{",
            "\tstruct bpf_mem_alloc *copy;",
            "",
            "\tif (!rcu_in_progress) {",
            "\t\t/* Fast path. No callbacks are pending, hence no need to do",
            "\t\t * rcu_barrier-s.",
            "\t\t */",
            "\t\tfree_mem_alloc_no_barrier(ma);",
            "\t\treturn;",
            "\t}",
            "",
            "\tcopy = kmemdup(ma, sizeof(*ma), GFP_KERNEL);",
            "\tif (!copy) {",
            "\t\t/* Slow path with inline barrier-s */",
            "\t\tfree_mem_alloc(ma);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Defer barriers into worker to let the rest of map memory to be freed */",
            "\tmemset(ma, 0, sizeof(*ma));",
            "\tINIT_WORK(&copy->work, free_mem_alloc_deferred);",
            "\tqueue_work(system_unbound_wq, &copy->work);",
            "}",
            "void bpf_mem_alloc_destroy(struct bpf_mem_alloc *ma)",
            "{",
            "\tstruct bpf_mem_caches *cc;",
            "\tstruct bpf_mem_cache *c;",
            "\tint cpu, i, rcu_in_progress;",
            "",
            "\tif (ma->cache) {",
            "\t\trcu_in_progress = 0;",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(ma->cache, cpu);",
            "\t\t\tWRITE_ONCE(c->draining, true);",
            "\t\t\tirq_work_sync(&c->refill_work);",
            "\t\t\tdrain_mem_cache(c);",
            "\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
            "\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
            "\t\t}",
            "\t\tobj_cgroup_put(ma->objcg);",
            "\t\tdestroy_mem_alloc(ma, rcu_in_progress);",
            "\t}",
            "\tif (ma->caches) {",
            "\t\trcu_in_progress = 0;",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tcc = per_cpu_ptr(ma->caches, cpu);",
            "\t\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\t\tc = &cc->cache[i];",
            "\t\t\t\tWRITE_ONCE(c->draining, true);",
            "\t\t\t\tirq_work_sync(&c->refill_work);",
            "\t\t\t\tdrain_mem_cache(c);",
            "\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
            "\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
            "\t\t\t}",
            "\t\t}",
            "\t\tobj_cgroup_put(ma->objcg);",
            "\t\tdestroy_mem_alloc(ma, rcu_in_progress);",
            "\t}",
            "}",
            "static void notrace unit_free(struct bpf_mem_cache *c, void *ptr)",
            "{",
            "\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;",
            "\tunsigned long flags;",
            "\tint cnt = 0;",
            "",
            "\tBUILD_BUG_ON(LLIST_NODE_SZ > 8);",
            "",
            "\t/*",
            "\t * Remember bpf_mem_cache that allocated this object.",
            "\t * The hint is not accurate.",
            "\t */",
            "\tc->tgt = *(struct bpf_mem_cache **)llnode;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (local_inc_return(&c->active) == 1) {",
            "\t\t__llist_add(llnode, &c->free_llist);",
            "\t\tcnt = ++c->free_cnt;",
            "\t} else {",
            "\t\t/* unit_free() cannot fail. Therefore add an object to atomic",
            "\t\t * llist. free_bulk() will drain it. Though free_llist_extra is",
            "\t\t * a per-cpu list we have to use atomic llist_add here, since",
            "\t\t * it also can be interrupted by bpf nmi prog that does another",
            "\t\t * unit_free() into the same free_llist_extra.",
            "\t\t */",
            "\t\tllist_add(llnode, &c->free_llist_extra);",
            "\t}",
            "\tlocal_dec(&c->active);",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (cnt > c->high_watermark)",
            "\t\t/* free few objects from current cpu into global kmalloc pool */",
            "\t\tirq_work_raise(c);",
            "}"
          ],
          "function_name": "free_mem_alloc, free_mem_alloc_deferred, destroy_mem_alloc, bpf_mem_alloc_destroy, unit_free",
          "description": "实现BPF内存分配销毁逻辑，通过RCU屏障等待回调完成并安全释放资源，deferred路径利用workqueue异步释放，destroy_mem_alloc处理缓存清理和RCU状态同步。",
          "similarity": 0.4227328896522522
        }
      ]
    }
  ]
}