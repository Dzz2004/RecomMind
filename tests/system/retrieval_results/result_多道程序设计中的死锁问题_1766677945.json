{
  "query": "多道程序设计中的死锁问题",
  "timestamp": "2025-12-25 23:52:25",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/ww_mutex.h",
      "md_summary": "> 自动生成时间: 2025-10-25 14:56:40\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\ww_mutex.h`\n\n---\n\n# `locking/ww_mutex.h` 技术文档\n\n## 1. 文件概述\n\n`ww_mutex.h` 是 Linux 内核中用于实现 **Wound-Wait (WW) 互斥锁**（`ww_mutex`）的头文件。该机制主要用于解决 **死锁问题**，特别是在图形子系统（如 DRM/KMS）和资源管理场景中，多个事务（transactions）需要以特定顺序获取多个锁时。  \nWW 互斥锁通过为每个锁请求关联一个 **获取上下文**（`ww_acquire_ctx`），并基于事务的优先级或时间戳实现 **Wait-Die** 或 **Wound-Wait** 死锁避免策略。\n\n该文件通过条件编译（`WW_RT` 宏）支持两种底层锁实现：\n- **普通互斥锁**（`mutex`）：用于非实时（non-RT）内核配置。\n- **实时互斥锁**（`rt_mutex`）：用于实时（RT）内核补丁配置，支持优先级继承。\n\n## 2. 核心功能\n\n### 2.1 主要宏定义\n- `MUTEX` / `MUTEX_WAITER`：根据 `WW_RT` 宏分别映射到 `mutex`/`rt_mutex` 及其等待者结构。\n\n### 2.2 等待者链表/红黑树操作函数（抽象接口）\n- `__ww_waiter_first()`：获取等待队列中的第一个等待者。\n- `__ww_waiter_next()` / `__ww_waiter_prev()`：获取下一个/上一个等待者。\n- `__ww_waiter_last()`：获取等待队列中的最后一个等待者。\n- `__ww_waiter_add()`：将等待者插入到指定位置（普通 mutex 使用链表，RT 使用红黑树）。\n\n### 2.3 锁状态查询函数\n- `__ww_mutex_owner()`：获取当前锁的持有者任务。\n- `__ww_mutex_has_waiters()`：检查锁是否有等待者。\n- `lock_wait_lock()` / `unlock_wait_lock()`：获取/释放锁的等待队列自旋锁（`wait_lock`）。\n- `lockdep_assert_wait_lock_held()`：调试时断言 `wait_lock` 已被持有。\n\n### 2.4 WW 互斥锁核心逻辑函数\n- `ww_mutex_lock_acquired()`：在成功获取 `ww_mutex` 后，将其与获取上下文（`ww_ctx`）关联，并执行调试检查。\n- `__ww_ctx_less()`：比较两个获取上下文的优先级（用于决定谁应“等待”或“死亡/被抢占”）。\n- `__ww_mutex_die()`：**Wait-Die 策略**实现：若当前请求者（新事务）发现等待队列中有更老的事务持有其他锁，则唤醒该老事务使其“死亡”（回滚）。\n- `__ww_mutex_wound()`：**Wound-Wait 策略**实现：若当前请求者（老事务）发现锁持有者是更年轻的事务，则“刺伤”（标记 `wounded=1`）该年轻事务，迫使其回滚。\n\n## 3. 关键实现\n\n### 3.1 死锁避免策略\n- **Wait-Die**（`is_wait_die=1`）：\n  - **新事务**请求**老事务**持有的锁 → **新事务等待**。\n  - **新事务**请求**老事务**等待的锁 → **新事务死亡**（回滚）。\n- **Wound-Wait**（`is_wait_die=0`）：\n  - **老事务**请求**新事务**持有的锁 → **新事务被刺伤**（回滚）。\n  - **老事务**请求**新事务**等待的锁 → **老事务等待**。\n\n### 3.2 上下文比较 (`__ww_ctx_less`)\n- **非 RT 模式**：仅基于时间戳（`stamp`），值越大表示事务越新。\n- **RT 模式**：\n  1. 优先比较 **实时优先级**（`prio`），数值越小优先级越高。\n  2. 若均为 **Deadline 调度类**，比较 **截止时间**（`deadline`），越早截止优先级越高。\n  3. 若优先级相同，回退到时间戳比较。\n\n### 3.3 RT 与非 RT 差异\n- **数据结构**：\n  - 非 RT：等待者使用 **双向链表**（`list_head`）。\n  - RT：等待者使用 **红黑树**（`rb_root`），按优先级排序。\n- **插入逻辑**：\n  - 非 RT：`__ww_waiter_add` 显式插入到指定位置。\n  - RT：`__ww_waiter_add` 为空（RT 互斥锁内部自动处理插入）。\n\n### 3.4 调试支持 (`DEBUG_WW_MUTEXES`)\n- 检查 `ww_mutex` 是否被错误地用普通 `mutex_unlock` 释放。\n- 验证上下文一致性（如 `ww_class` 匹配、`contending_lock` 状态等）。\n\n## 4. 依赖关系\n\n- **基础锁机制**：\n  - 非 RT 模式依赖 `<linux/mutex.h>`。\n  - RT 模式依赖 `<linux/rtmutex.h>`。\n- **调度器**：依赖任务结构（`task_struct`）、优先级（`prio`）、调度类（如 `dl_prio`）。\n- **调试框架**：依赖 `lockdep`（`lockdep_assert_held`）和 `DEBUG_LOCKS_WARN_ON`。\n- **原子操作**：使用 `atomic_long_read` 检查锁状态标志（`MUTEX_FLAG_WAITERS`）。\n\n## 5. 使用场景\n\n- **图形子系统**（DRM/KMS）：  \n  多个 GPU 作业（如渲染、合成）需按顺序获取多个缓冲区（buffer）或 CRTC 锁，避免死锁。\n- **资源分配器**：  \n  当多个客户端竞争一组有限资源（如内存区域、I/O 端口）时，通过 WW 互斥锁确保无死锁的分配顺序。\n- **实时系统**（RT 补丁）：  \n  在需要确定性延迟的场景中，结合优先级继承（PI）避免优先级反转，同时通过 WW 策略解决多锁死锁。\n- **文件系统**：  \n  某些文件系统（如 Btrfs）在元数据操作中使用 WW 互斥锁管理多个 extent 锁。",
      "similarity": 0.6000134944915771,
      "chunks": []
    },
    {
      "source_file": "kernel/watchdog.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:51:17\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `watchdog.c`\n\n---\n\n# watchdog.c 技术文档\n\n## 1. 文件概述\n\n`watchdog.c` 是 Linux 内核中实现 **硬锁死（hard lockup）** 和 **软锁死（soft lockup）** 检测机制的核心文件。该机制用于监控系统中 CPU 是否因长时间禁用中断或陷入无限循环而无法响应，从而帮助诊断系统挂死问题。硬锁死指 CPU 完全停止响应中断（包括 NMI），软锁死指内核线程长时间占用 CPU 且未调度其他任务。本文件主要聚焦于硬锁死检测的通用框架和部分实现，软锁死检测逻辑主要在其他文件（如 `softlockup.c`）中实现，但两者共享部分配置和控制逻辑。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `watchdog_enabled`：位掩码，表示当前启用的 watchdog 类型（软/硬锁死检测）。\n- `watchdog_user_enabled`：用户空间是否启用 watchdog（默认 1）。\n- `watchdog_hardlockup_user_enabled`：用户空间是否启用硬锁死检测（默认值取决于架构）。\n- `watchdog_softlockup_user_enabled`：用户空间是否启用软锁死检测（默认 1）。\n- `watchdog_thresh`：锁死检测阈值（秒，默认 10 秒）。\n- `watchdog_cpumask`：参与 watchdog 检测的 CPU 掩码。\n- `hardlockup_panic`：硬锁死发生时是否触发内核 panic（默认由 `CONFIG_BOOTPARAM_HARDLOCKUP_PANIC` 决定）。\n- `sysctl_hardlockup_all_cpu_backtrace`（SMP）：硬锁死时是否打印所有 CPU 的 backtrace。\n- `hardlockup_count`（SYSFS）：记录硬锁死事件发生次数。\n\n### 主要函数\n- `hardlockup_detector_disable(void)`：在启动早期禁用硬锁死检测（例如虚拟机环境）。\n- `hardlockup_panic_setup(char *str)`：解析内核启动参数 `nmi_watchdog=`，配置硬锁死行为。\n- `arch_touch_nmi_watchdog(void)`：架构相关函数，用于在关键路径“触摸”硬 watchdog，防止误报（导出符号）。\n- `watchdog_hardlockup_touch_cpu(unsigned int cpu)`：标记指定 CPU 已被“触摸”。\n- `is_hardlockup(unsigned int cpu)`：检查指定 CPU 是否发生硬锁死（基于高精度定时器中断计数）。\n- `watchdog_hardlockup_kick(void)`：在高精度定时器中断中“踢”硬 watchdog（更新中断计数）。\n- `watchdog_hardlockup_check(unsigned int cpu, struct pt_regs *regs)`：执行硬锁死检测逻辑，打印诊断信息并可能触发 panic。\n- `watchdog_hardlockup_enable/disable(unsigned int cpu)`：弱符号函数，由具体硬 watchdog 实现（如 perf-based）覆盖，用于启停 per-CPU 检测。\n- `watchdog_hardlockup_probe(void)`：弱符号函数，由具体实现提供，用于探测硬 watchdog 硬件/机制是否可用。\n\n### 核心数据结构（Per-CPU）\n- `hrtimer_interrupts`：高精度定时器中断计数器（原子变量）。\n- `hrtimer_interrupts_saved`：上次保存的中断计数值。\n- `watchdog_hardlockup_warned`：是否已为该 CPU 打印过硬锁死警告。\n- `watchdog_hardlockup_touched`：该 CPU 是否被“触摸”过（用于豁免检测）。\n\n## 3. 关键实现\n\n### 硬锁死检测机制（基于高精度定时器）\n当配置 `CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER` 时，硬锁死检测通过监控 **高精度定时器（hrtimer）中断** 的发生频率实现：\n1. **计数更新**：每次 hrtimer 中断发生时，调用 `watchdog_hardlockup_kick()` 原子递增 per-CPU 计数器 `hrtimer_interrupts`。\n2. **检测逻辑**：在 NMI（不可屏蔽中断）上下文（或其他检测点）调用 `watchdog_hardlockup_check()`：\n   - 若 CPU 被“触摸”（`watchdog_hardlockup_touched` 为真），则清除此标记并跳过检测。\n   - 否则调用 `is_hardlockup()`：比较当前 `hrtimer_interrupts` 与上次保存值 `hrtimer_interrupts_saved`。若相等，说明在检测周期内无 hrtimer 中断，判定为硬锁死。\n3. **告警与处理**：\n   - 首次检测到硬锁死时，打印紧急日志（CPU 信息、模块列表、中断跟踪、寄存器状态或栈回溯）。\n   - 若启用 `sysctl_hardlockup_all_cpu_backtrace`，触发其他 CPU 的 backtrace。\n   - 若 `hardlockup_panic` 为真，调用 `nmi_panic()` 触发内核 panic。\n   - 设置 `watchdog_hardlockup_warned` 避免重复告警。\n\n### 启动参数与配置\n- **`nmi_watchdog=` 参数**：通过 `__setup` 宏注册，支持以下值：\n  - `panic`/`nopanic`：设置 `hardlockup_panic`。\n  - `0`/`1`：启用/禁用硬锁死检测。\n  - `r...`：传递参数给 perf-based 检测器（`hardlockup_config_perf_event`）。\n- **早期禁用**：`hardlockup_detector_disable()` 可在解析命令行前禁用硬检测（如 KVM guest）。\n\n### 架构交互与豁免\n- **`arch_touch_nmi_watchdog()`**：允许架构代码或关键内核路径（如 printk）临时豁免硬 watchdog 检测，防止在已知安全的长操作中误报。使用 `raw_cpu_write` 确保在抢占/中断使能环境下安全。\n\n### 弱符号扩展点\n- `watchdog_hardlockup_enable/disable/probe` 声明为 `__weak`，允许不同架构或检测方法（如基于 perf event 的 NMI watchdog）提供具体实现，实现检测机制的可插拔。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - `<linux/nmi.h>`：NMI 处理框架，硬锁死检测通常在 NMI 上下文触发。\n  - `<linux/hrtimer.h>`（隐含）：高精度定时器中断作为检测心跳源。\n  - `<linux/sched/*.h>`：调度器相关（`print_irqtrace_events`, `dump_stack`）。\n  - `<linux/sysctl.h>`：提供 `sysctl_hardlockup_all_cpu_backtrace` 控制接口。\n  - `<linux/sysfs.h>`：暴露 `hardlockup_count` 到 sysfs。\n  - `<asm/irq_regs.h>`：获取中断上下文寄存器状态（`show_regs`）。\n- **配置选项**：\n  - `CONFIG_HARDLOCKUP_DETECTOR`：启用硬锁死检测框架。\n  - `CONFIG_HARDLOCKUP_DETECTOR_COUNTS_HRTIMER`：使用 hrtimer 中断计数实现检测。\n  - `CONFIG_HARDLOCKUP_DETECTOR_SPARC64`：SPARC64 架构默认启用硬检测。\n  - `CONFIG_BOOTPARAM_HARDLOCKUP_PANIC`：设置默认 panic 行为。\n  - `CONFIG_SMP`：多核支持（`all_cpu_backtrace` 功能）。\n  - `CONFIG_SYSFS`：sysfs 接口支持。\n- **其他模块**：依赖具体架构的 NMI 实现（如 x86 的 perf-based NMI watchdog）提供检测触发点。\n\n## 5. 使用场景\n\n- **系统稳定性监控**：在生产服务器或嵌入式设备中持续监控 CPU 响应性，及时发现硬件故障、驱动 bug 或内核死锁导致的系统挂死。\n- **内核调试**：开发人员通过 watchdog 触发的 backtrace 和寄存器转储，定位导致系统无响应的代码路径。\n- **虚拟化环境**：在 hypervisor guest 中可选择性禁用硬 watchdog（因虚拟化开销可能导致误报），通过 `hardlockup_detector_disable()` 或启动参数控制。\n- **实时系统**：结合 CPU 隔离（`isolcpus`）和 watchdog 配置，确保关键 CPU 核心的响应性，同时避免在非关键核上产生干扰。\n- **panic 策略**：通过 `hardlockup_panic` 配置，使系统在硬锁死时自动重启，提高无人值守系统的可用性。",
      "similarity": 0.5871256589889526,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/watchdog.c",
          "start_line": 73,
          "end_line": 217,
          "content": [
            "static ssize_t hardlockup_count_show(struct kobject *kobj, struct kobj_attribute *attr,",
            "\t\t\t\t     char *page)",
            "{",
            "\treturn sysfs_emit(page, \"%u\\n\", hardlockup_count);",
            "}",
            "static __init int kernel_hardlockup_sysfs_init(void)",
            "{",
            "\tsysfs_add_file_to_group(kernel_kobj, &hardlockup_count_attr.attr, NULL);",
            "\treturn 0;",
            "}",
            "void __init hardlockup_detector_disable(void)",
            "{",
            "\twatchdog_hardlockup_user_enabled = 0;",
            "}",
            "static int __init hardlockup_panic_setup(char *str)",
            "{",
            "next:",
            "\tif (!strncmp(str, \"panic\", 5))",
            "\t\thardlockup_panic = 1;",
            "\telse if (!strncmp(str, \"nopanic\", 7))",
            "\t\thardlockup_panic = 0;",
            "\telse if (!strncmp(str, \"0\", 1))",
            "\t\twatchdog_hardlockup_user_enabled = 0;",
            "\telse if (!strncmp(str, \"1\", 1))",
            "\t\twatchdog_hardlockup_user_enabled = 1;",
            "\telse if (!strncmp(str, \"r\", 1))",
            "\t\thardlockup_config_perf_event(str + 1);",
            "\twhile (*(str++)) {",
            "\t\tif (*str == ',') {",
            "\t\t\tstr++;",
            "\t\t\tgoto next;",
            "\t\t}",
            "\t}",
            "\treturn 1;",
            "}",
            "notrace void arch_touch_nmi_watchdog(void)",
            "{",
            "\t/*",
            "\t * Using __raw here because some code paths have",
            "\t * preemption enabled.  If preemption is enabled",
            "\t * then interrupts should be enabled too, in which",
            "\t * case we shouldn't have to worry about the watchdog",
            "\t * going off.",
            "\t */",
            "\traw_cpu_write(watchdog_hardlockup_touched, true);",
            "}",
            "void watchdog_hardlockup_touch_cpu(unsigned int cpu)",
            "{",
            "\tper_cpu(watchdog_hardlockup_touched, cpu) = true;",
            "}",
            "static bool is_hardlockup(unsigned int cpu)",
            "{",
            "\tint hrint = atomic_read(&per_cpu(hrtimer_interrupts, cpu));",
            "",
            "\tif (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * NOTE: we don't need any fancy atomic_t or READ_ONCE/WRITE_ONCE",
            "\t * for hrtimer_interrupts_saved. hrtimer_interrupts_saved is",
            "\t * written/read by a single CPU.",
            "\t */",
            "\tper_cpu(hrtimer_interrupts_saved, cpu) = hrint;",
            "",
            "\treturn false;",
            "}",
            "static void watchdog_hardlockup_kick(void)",
            "{",
            "\tint new_interrupts;",
            "",
            "\tnew_interrupts = atomic_inc_return(this_cpu_ptr(&hrtimer_interrupts));",
            "\twatchdog_buddy_check_hardlockup(new_interrupts);",
            "}",
            "void watchdog_hardlockup_check(unsigned int cpu, struct pt_regs *regs)",
            "{",
            "\tif (per_cpu(watchdog_hardlockup_touched, cpu)) {",
            "\t\tper_cpu(watchdog_hardlockup_touched, cpu) = false;",
            "\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * Check for a hardlockup by making sure the CPU's timer",
            "\t * interrupt is incrementing. The timer interrupt should have",
            "\t * fired multiple times before we overflow'd. If it hasn't",
            "\t * then this is a good indication the cpu is stuck",
            "\t */",
            "\tif (is_hardlockup(cpu)) {",
            "\t\tunsigned int this_cpu = smp_processor_id();",
            "\t\tunsigned long flags;",
            "",
            "#ifdef CONFIG_SYSFS",
            "\t\t++hardlockup_count;",
            "#endif",
            "",
            "\t\t/* Only print hardlockups once. */",
            "\t\tif (per_cpu(watchdog_hardlockup_warned, cpu))",
            "\t\t\treturn;",
            "",
            "\t\t/*",
            "\t\t * Prevent multiple hard-lockup reports if one cpu is already",
            "\t\t * engaged in dumping all cpu back traces.",
            "\t\t */",
            "\t\tif (sysctl_hardlockup_all_cpu_backtrace) {",
            "\t\t\tif (test_and_set_bit_lock(0, &hard_lockup_nmi_warn))",
            "\t\t\t\treturn;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * NOTE: we call printk_cpu_sync_get_irqsave() after printing",
            "\t\t * the lockup message. While it would be nice to serialize",
            "\t\t * that printout, we really want to make sure that if some",
            "\t\t * other CPU somehow locked up while holding the lock associated",
            "\t\t * with printk_cpu_sync_get_irqsave() that we can still at least",
            "\t\t * get the message about the lockup out.",
            "\t\t */",
            "\t\tpr_emerg(\"CPU%u: Watchdog detected hard LOCKUP on cpu %u\\n\", this_cpu, cpu);",
            "\t\tprintk_cpu_sync_get_irqsave(flags);",
            "",
            "\t\tprint_modules();",
            "\t\tprint_irqtrace_events(current);",
            "\t\tif (cpu == this_cpu) {",
            "\t\t\tif (regs)",
            "\t\t\t\tshow_regs(regs);",
            "\t\t\telse",
            "\t\t\t\tdump_stack();",
            "\t\t\tprintk_cpu_sync_put_irqrestore(flags);",
            "\t\t} else {",
            "\t\t\tprintk_cpu_sync_put_irqrestore(flags);",
            "\t\t\ttrigger_single_cpu_backtrace(cpu);",
            "\t\t}",
            "",
            "\t\tif (sysctl_hardlockup_all_cpu_backtrace) {",
            "\t\t\ttrigger_allbutcpu_cpu_backtrace(cpu);",
            "\t\t\tif (!hardlockup_panic)",
            "\t\t\t\tclear_bit_unlock(0, &hard_lockup_nmi_warn);",
            "\t\t}",
            "",
            "\t\tif (hardlockup_panic)",
            "\t\t\tnmi_panic(regs, \"Hard LOCKUP\");",
            "",
            "\t\tper_cpu(watchdog_hardlockup_warned, cpu) = true;",
            "\t} else {",
            "\t\tper_cpu(watchdog_hardlockup_warned, cpu) = false;",
            "\t}",
            "}"
          ],
          "function_name": "hardlockup_count_show, kernel_hardlockup_sysfs_init, hardlockup_detector_disable, hardlockup_panic_setup, arch_touch_nmi_watchdog, watchdog_hardlockup_touch_cpu, is_hardlockup, watchdog_hardlockup_kick, watchdog_hardlockup_check",
          "description": "实现硬锁检测核心逻辑，包含硬锁判断、计数统计、NMI触发电路及异常上报功能，通过中断计数器检测CPU卡顿。",
          "similarity": 0.5655540227890015
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/watchdog.c",
          "start_line": 626,
          "end_line": 802,
          "content": [
            "static void update_touch_ts(void)",
            "{",
            "\t__this_cpu_write(watchdog_touch_ts, get_timestamp());",
            "\tupdate_report_ts();",
            "}",
            "notrace void touch_softlockup_watchdog_sched(void)",
            "{",
            "\t/*",
            "\t * Preemption can be enabled.  It doesn't matter which CPU's watchdog",
            "\t * report period gets restarted here, so use the raw_ operation.",
            "\t */",
            "\traw_cpu_write(watchdog_report_ts, SOFTLOCKUP_DELAY_REPORT);",
            "}",
            "notrace void touch_softlockup_watchdog(void)",
            "{",
            "\ttouch_softlockup_watchdog_sched();",
            "\twq_watchdog_touch(raw_smp_processor_id());",
            "}",
            "void touch_all_softlockup_watchdogs(void)",
            "{",
            "\tint cpu;",
            "",
            "\t/*",
            "\t * watchdog_mutex cannpt be taken here, as this might be called",
            "\t * from (soft)interrupt context, so the access to",
            "\t * watchdog_allowed_cpumask might race with a concurrent update.",
            "\t *",
            "\t * The watchdog time stamp can race against a concurrent real",
            "\t * update as well, the only side effect might be a cycle delay for",
            "\t * the softlockup check.",
            "\t */",
            "\tfor_each_cpu(cpu, &watchdog_allowed_mask) {",
            "\t\tper_cpu(watchdog_report_ts, cpu) = SOFTLOCKUP_DELAY_REPORT;",
            "\t\twq_watchdog_touch(cpu);",
            "\t}",
            "}",
            "void touch_softlockup_watchdog_sync(void)",
            "{",
            "\t__this_cpu_write(softlockup_touch_sync, true);",
            "\t__this_cpu_write(watchdog_report_ts, SOFTLOCKUP_DELAY_REPORT);",
            "}",
            "static int is_softlockup(unsigned long touch_ts,",
            "\t\t\t unsigned long period_ts,",
            "\t\t\t unsigned long now)",
            "{",
            "\tif ((watchdog_enabled & WATCHDOG_SOFTOCKUP_ENABLED) && watchdog_thresh) {",
            "\t\t/*",
            "\t\t * If period_ts has not been updated during a sample_period, then",
            "\t\t * in the subsequent few sample_periods, period_ts might also not",
            "\t\t * be updated, which could indicate a potential softlockup. In",
            "\t\t * this case, if we suspect the cause of the potential softlockup",
            "\t\t * might be interrupt storm, then we need to count the interrupts",
            "\t\t * to find which interrupt is storming.",
            "\t\t */",
            "\t\tif (time_after_eq(now, period_ts + get_softlockup_thresh() / NUM_SAMPLE_PERIODS) &&",
            "\t\t    need_counting_irqs())",
            "\t\t\tstart_counting_irqs();",
            "",
            "\t\t/* Warn about unreasonable delays. */",
            "\t\tif (time_after(now, period_ts + get_softlockup_thresh()))",
            "\t\t\treturn now - touch_ts;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int softlockup_fn(void *data)",
            "{",
            "\tupdate_touch_ts();",
            "\tstop_counting_irqs();",
            "\tcomplete(this_cpu_ptr(&softlockup_completion));",
            "",
            "\treturn 0;",
            "}",
            "static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer)",
            "{",
            "\tunsigned long touch_ts, period_ts, now;",
            "\tstruct pt_regs *regs = get_irq_regs();",
            "\tint duration;",
            "\tint softlockup_all_cpu_backtrace = sysctl_softlockup_all_cpu_backtrace;",
            "\tunsigned long flags;",
            "",
            "\tif (!watchdog_enabled)",
            "\t\treturn HRTIMER_NORESTART;",
            "",
            "\twatchdog_hardlockup_kick();",
            "",
            "\t/* kick the softlockup detector */",
            "\tif (completion_done(this_cpu_ptr(&softlockup_completion))) {",
            "\t\treinit_completion(this_cpu_ptr(&softlockup_completion));",
            "\t\tstop_one_cpu_nowait(smp_processor_id(),",
            "\t\t\t\tsoftlockup_fn, NULL,",
            "\t\t\t\tthis_cpu_ptr(&softlockup_stop_work));",
            "\t}",
            "",
            "\t/* .. and repeat */",
            "\thrtimer_forward_now(hrtimer, ns_to_ktime(sample_period));",
            "",
            "\t/*",
            "\t * Read the current timestamp first. It might become invalid anytime",
            "\t * when a virtual machine is stopped by the host or when the watchog",
            "\t * is touched from NMI.",
            "\t */",
            "\tnow = get_timestamp();",
            "\t/*",
            "\t * If a virtual machine is stopped by the host it can look to",
            "\t * the watchdog like a soft lockup. This function touches the watchdog.",
            "\t */",
            "\tkvm_check_and_clear_guest_paused();",
            "\t/*",
            "\t * The stored timestamp is comparable with @now only when not touched.",
            "\t * It might get touched anytime from NMI. Make sure that is_softlockup()",
            "\t * uses the same (valid) value.",
            "\t */",
            "\tperiod_ts = READ_ONCE(*this_cpu_ptr(&watchdog_report_ts));",
            "",
            "\tupdate_cpustat();",
            "",
            "\t/* Reset the interval when touched by known problematic code. */",
            "\tif (period_ts == SOFTLOCKUP_DELAY_REPORT) {",
            "\t\tif (unlikely(__this_cpu_read(softlockup_touch_sync))) {",
            "\t\t\t/*",
            "\t\t\t * If the time stamp was touched atomically",
            "\t\t\t * make sure the scheduler tick is up to date.",
            "\t\t\t */",
            "\t\t\t__this_cpu_write(softlockup_touch_sync, false);",
            "\t\t\tsched_clock_tick();",
            "\t\t}",
            "",
            "\t\tupdate_report_ts();",
            "\t\treturn HRTIMER_RESTART;",
            "\t}",
            "",
            "\t/* Check for a softlockup. */",
            "\ttouch_ts = __this_cpu_read(watchdog_touch_ts);",
            "\tduration = is_softlockup(touch_ts, period_ts, now);",
            "\tif (unlikely(duration)) {",
            "#ifdef CONFIG_SYSFS",
            "\t\t++softlockup_count;",
            "#endif",
            "",
            "\t\t/*",
            "\t\t * Prevent multiple soft-lockup reports if one cpu is already",
            "\t\t * engaged in dumping all cpu back traces.",
            "\t\t */",
            "\t\tif (softlockup_all_cpu_backtrace) {",
            "\t\t\tif (test_and_set_bit_lock(0, &soft_lockup_nmi_warn))",
            "\t\t\t\treturn HRTIMER_RESTART;",
            "\t\t}",
            "",
            "\t\t/* Start period for the next softlockup warning. */",
            "\t\tupdate_report_ts();",
            "",
            "\t\tprintk_cpu_sync_get_irqsave(flags);",
            "\t\tpr_emerg(\"BUG: soft lockup - CPU#%d stuck for %us! [%s:%d]\\n\",",
            "\t\t\tsmp_processor_id(), duration,",
            "\t\t\tcurrent->comm, task_pid_nr(current));",
            "\t\treport_cpu_status();",
            "\t\tprint_modules();",
            "\t\tprint_irqtrace_events(current);",
            "\t\tif (regs)",
            "\t\t\tshow_regs(regs);",
            "\t\telse",
            "\t\t\tdump_stack();",
            "\t\tprintk_cpu_sync_put_irqrestore(flags);",
            "",
            "\t\tif (softlockup_all_cpu_backtrace) {",
            "\t\t\ttrigger_allbutcpu_cpu_backtrace(smp_processor_id());",
            "\t\t\tif (!softlockup_panic)",
            "\t\t\t\tclear_bit_unlock(0, &soft_lockup_nmi_warn);",
            "\t\t}",
            "",
            "\t\tadd_taint(TAINT_SOFTLOCKUP, LOCKDEP_STILL_OK);",
            "\t\tif (softlockup_panic)",
            "\t\t\tpanic(\"softlockup: hung tasks\");",
            "\t}",
            "",
            "\treturn HRTIMER_RESTART;",
            "}"
          ],
          "function_name": "update_touch_ts, touch_softlockup_watchdog_sched, touch_softlockup_watchdog, touch_all_softlockup_watchdogs, touch_softlockup_watchdog_sync, is_softlockup, softlockup_fn, watchdog_timer_fn",
          "description": "处理软锁检测时序逻辑，包含超时判定算法、任务栈回溯触发机制及异常处理流程，协调硬件定时器与软件检测模块。",
          "similarity": 0.5635852217674255
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/watchdog.c",
          "start_line": 1,
          "end_line": 72,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Detect hard and soft lockups on a system",
            " *",
            " * started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.",
            " *",
            " * Note: Most of this code is borrowed heavily from the original softlockup",
            " * detector, so thanks to Ingo for the initial implementation.",
            " * Some chunks also taken from the old x86-specific nmi watchdog code, thanks",
            " * to those contributors as well.",
            " */",
            "",
            "#define pr_fmt(fmt) \"watchdog: \" fmt",
            "",
            "#include <linux/cpu.h>",
            "#include <linux/init.h>",
            "#include <linux/kprobes.h>",
            "#include <linux/irq.h>",
            "#include <linux/irqdesc.h>",
            "#include <linux/kernel_stat.h>",
            "#include <linux/kvm_para.h>",
            "#include <linux/math64.h>",
            "#include <linux/mm.h>",
            "#include <linux/module.h>",
            "#include <linux/nmi.h>",
            "#include <linux/stop_machine.h>",
            "#include <linux/sysctl.h>",
            "#include <linux/tick.h>",
            "",
            "#include <linux/sched/clock.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/isolation.h>",
            "",
            "#include <asm/irq_regs.h>",
            "",
            "static DEFINE_MUTEX(watchdog_mutex);",
            "",
            "#if defined(CONFIG_HARDLOCKUP_DETECTOR) || defined(CONFIG_HARDLOCKUP_DETECTOR_SPARC64)",
            "# define WATCHDOG_HARDLOCKUP_DEFAULT\t1",
            "#else",
            "# define WATCHDOG_HARDLOCKUP_DEFAULT\t0",
            "#endif",
            "",
            "#define NUM_SAMPLE_PERIODS\t5",
            "",
            "unsigned long __read_mostly watchdog_enabled;",
            "int __read_mostly watchdog_user_enabled = 1;",
            "static int __read_mostly watchdog_hardlockup_user_enabled = WATCHDOG_HARDLOCKUP_DEFAULT;",
            "static int __read_mostly watchdog_softlockup_user_enabled = 1;",
            "int __read_mostly watchdog_thresh = 10;",
            "static int __read_mostly watchdog_thresh_next;",
            "static int __read_mostly watchdog_hardlockup_available;",
            "",
            "struct cpumask watchdog_cpumask __read_mostly;",
            "unsigned long *watchdog_cpumask_bits = cpumask_bits(&watchdog_cpumask);",
            "",
            "#ifdef CONFIG_HARDLOCKUP_DETECTOR",
            "",
            "# ifdef CONFIG_SMP",
            "int __read_mostly sysctl_hardlockup_all_cpu_backtrace;",
            "# endif /* CONFIG_SMP */",
            "",
            "/*",
            " * Should we panic when a soft-lockup or hard-lockup occurs:",
            " */",
            "unsigned int __read_mostly hardlockup_panic =",
            "\t\t\tIS_ENABLED(CONFIG_BOOTPARAM_HARDLOCKUP_PANIC);",
            "",
            "#ifdef CONFIG_SYSFS",
            "",
            "static unsigned int hardlockup_count;",
            ""
          ],
          "function_name": null,
          "description": "定义硬锁和软锁检测模块的全局变量及配置选项，初始化硬锁检测相关数据结构和默认启用状态。",
          "similarity": 0.5287898778915405
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/watchdog.c",
          "start_line": 946,
          "end_line": 1059,
          "content": [
            "void lockup_detector_reconfigure(void)",
            "{",
            "\tmutex_lock(&watchdog_mutex);",
            "\t__lockup_detector_reconfigure(false);",
            "\tmutex_unlock(&watchdog_mutex);",
            "}",
            "static __init void lockup_detector_setup(void)",
            "{",
            "\t/*",
            "\t * If sysctl is off and watchdog got disabled on the command line,",
            "\t * nothing to do here.",
            "\t */",
            "\tlockup_detector_update_enable();",
            "",
            "\tif (!IS_ENABLED(CONFIG_SYSCTL) &&",
            "\t    !(watchdog_enabled && watchdog_thresh))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&watchdog_mutex);",
            "\t__lockup_detector_reconfigure(false);",
            "\tsoftlockup_initialized = true;",
            "\tmutex_unlock(&watchdog_mutex);",
            "}",
            "static void __lockup_detector_reconfigure(bool thresh_changed)",
            "{",
            "\tcpus_read_lock();",
            "\twatchdog_hardlockup_stop();",
            "\tif (thresh_changed)",
            "\t\twatchdog_thresh = READ_ONCE(watchdog_thresh_next);",
            "\tlockup_detector_update_enable();",
            "\twatchdog_hardlockup_start();",
            "\tcpus_read_unlock();",
            "}",
            "void lockup_detector_reconfigure(void)",
            "{",
            "\t__lockup_detector_reconfigure(false);",
            "}",
            "static inline void lockup_detector_setup(void)",
            "{",
            "\t__lockup_detector_reconfigure(false);",
            "}",
            "void lockup_detector_soft_poweroff(void)",
            "{",
            "\twatchdog_enabled = 0;",
            "}",
            "static void proc_watchdog_update(bool thresh_changed)",
            "{",
            "\t/* Remove impossible cpus to keep sysctl output clean. */",
            "\tcpumask_and(&watchdog_cpumask, &watchdog_cpumask, cpu_possible_mask);",
            "\t__lockup_detector_reconfigure(thresh_changed);",
            "}",
            "static int proc_watchdog_common(int which, struct ctl_table *table, int write,",
            "\t\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint err, old, *param = table->data;",
            "",
            "\tmutex_lock(&watchdog_mutex);",
            "",
            "\told = *param;",
            "\tif (!write) {",
            "\t\t/*",
            "\t\t * On read synchronize the userspace interface. This is a",
            "\t\t * racy snapshot.",
            "\t\t */",
            "\t\t*param = (watchdog_enabled & which) != 0;",
            "\t\terr = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\t\t*param = old;",
            "\t} else {",
            "\t\terr = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "\t\tif (!err && old != READ_ONCE(*param))",
            "\t\t\tproc_watchdog_update(false);",
            "\t}",
            "\tmutex_unlock(&watchdog_mutex);",
            "\treturn err;",
            "}",
            "int proc_watchdog(struct ctl_table *table, int write,",
            "\t\t  void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\treturn proc_watchdog_common(WATCHDOG_HARDLOCKUP_ENABLED |",
            "\t\t\t\t    WATCHDOG_SOFTOCKUP_ENABLED,",
            "\t\t\t\t    table, write, buffer, lenp, ppos);",
            "}",
            "int proc_nmi_watchdog(struct ctl_table *table, int write,",
            "\t\t      void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tif (!watchdog_hardlockup_available && write)",
            "\t\treturn -ENOTSUPP;",
            "\treturn proc_watchdog_common(WATCHDOG_HARDLOCKUP_ENABLED,",
            "\t\t\t\t    table, write, buffer, lenp, ppos);",
            "}",
            "int proc_soft_watchdog(struct ctl_table *table, int write,",
            "\t\t\tvoid *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\treturn proc_watchdog_common(WATCHDOG_SOFTOCKUP_ENABLED,",
            "\t\t\t\t    table, write, buffer, lenp, ppos);",
            "}",
            "int proc_watchdog_thresh(struct ctl_table *table, int write,",
            "\t\t\t void *buffer, size_t *lenp, loff_t *ppos)",
            "{",
            "\tint err, old;",
            "",
            "\tmutex_lock(&watchdog_mutex);",
            "",
            "\twatchdog_thresh_next = READ_ONCE(watchdog_thresh);",
            "",
            "\told = watchdog_thresh_next;",
            "\terr = proc_dointvec_minmax(table, write, buffer, lenp, ppos);",
            "",
            "\tif (!err && write && old != READ_ONCE(watchdog_thresh_next))",
            "\t\tproc_watchdog_update(true);",
            "",
            "\tmutex_unlock(&watchdog_mutex);",
            "\treturn err;",
            "}"
          ],
          "function_name": "lockup_detector_reconfigure, lockup_detector_setup, __lockup_detector_reconfigure, lockup_detector_reconfigure, lockup_detector_setup, lockup_detector_soft_poweroff, proc_watchdog_update, proc_watchdog_common, proc_watchdog, proc_nmi_watchdog, proc_soft_watchdog, proc_watchdog_thresh",
          "description": "提供看门狗参数动态配置接口，包含阈值更新、CPU掩码同步、sysctl参数读写控制逻辑，支持硬/软锁步检测模式切换和阈值调节",
          "similarity": 0.5184741020202637
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/watchdog.c",
          "start_line": 833,
          "end_line": 936,
          "content": [
            "static void watchdog_enable(unsigned int cpu)",
            "{",
            "\tstruct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);",
            "\tstruct completion *done = this_cpu_ptr(&softlockup_completion);",
            "",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "",
            "\tinit_completion(done);",
            "\tcomplete(done);",
            "",
            "\t/*",
            "\t * Start the timer first to prevent the hardlockup watchdog triggering",
            "\t * before the timer has a chance to fire.",
            "\t */",
            "\thrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD);",
            "\thrtimer->function = watchdog_timer_fn;",
            "\thrtimer_start(hrtimer, ns_to_ktime(sample_period),",
            "\t\t      HRTIMER_MODE_REL_PINNED_HARD);",
            "",
            "\t/* Initialize timestamp */",
            "\tupdate_touch_ts();",
            "\t/* Enable the hardlockup detector */",
            "\tif (watchdog_enabled & WATCHDOG_HARDLOCKUP_ENABLED)",
            "\t\twatchdog_hardlockup_enable(cpu);",
            "}",
            "static void watchdog_disable(unsigned int cpu)",
            "{",
            "\tstruct hrtimer *hrtimer = this_cpu_ptr(&watchdog_hrtimer);",
            "",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "",
            "\t/*",
            "\t * Disable the hardlockup detector first. That prevents that a large",
            "\t * delay between disabling the timer and disabling the hardlockup",
            "\t * detector causes a false positive.",
            "\t */",
            "\twatchdog_hardlockup_disable(cpu);",
            "\thrtimer_cancel(hrtimer);",
            "\twait_for_completion(this_cpu_ptr(&softlockup_completion));",
            "}",
            "static int softlockup_stop_fn(void *data)",
            "{",
            "\twatchdog_disable(smp_processor_id());",
            "\treturn 0;",
            "}",
            "static void softlockup_stop_all(void)",
            "{",
            "\tint cpu;",
            "",
            "\tif (!softlockup_initialized)",
            "\t\treturn;",
            "",
            "\tfor_each_cpu(cpu, &watchdog_allowed_mask)",
            "\t\tsmp_call_on_cpu(cpu, softlockup_stop_fn, NULL, false);",
            "",
            "\tcpumask_clear(&watchdog_allowed_mask);",
            "}",
            "static int softlockup_start_fn(void *data)",
            "{",
            "\twatchdog_enable(smp_processor_id());",
            "\treturn 0;",
            "}",
            "static void softlockup_start_all(void)",
            "{",
            "\tint cpu;",
            "",
            "\tcpumask_copy(&watchdog_allowed_mask, &watchdog_cpumask);",
            "\tfor_each_cpu(cpu, &watchdog_allowed_mask)",
            "\t\tsmp_call_on_cpu(cpu, softlockup_start_fn, NULL, false);",
            "}",
            "int lockup_detector_online_cpu(unsigned int cpu)",
            "{",
            "\tif (cpumask_test_cpu(cpu, &watchdog_allowed_mask))",
            "\t\twatchdog_enable(cpu);",
            "\treturn 0;",
            "}",
            "int lockup_detector_offline_cpu(unsigned int cpu)",
            "{",
            "\tif (cpumask_test_cpu(cpu, &watchdog_allowed_mask))",
            "\t\twatchdog_disable(cpu);",
            "\treturn 0;",
            "}",
            "static void __lockup_detector_reconfigure(bool thresh_changed)",
            "{",
            "\tcpus_read_lock();",
            "\twatchdog_hardlockup_stop();",
            "",
            "\tsoftlockup_stop_all();",
            "\t/*",
            "\t * To prevent watchdog_timer_fn from using the old interval and",
            "\t * the new watchdog_thresh at the same time, which could lead to",
            "\t * false softlockup reports, it is necessary to update the",
            "\t * watchdog_thresh after the softlockup is completed.",
            "\t */",
            "\tif (thresh_changed)",
            "\t\twatchdog_thresh = READ_ONCE(watchdog_thresh_next);",
            "\tset_sample_period();",
            "\tlockup_detector_update_enable();",
            "\tif (watchdog_enabled && watchdog_thresh)",
            "\t\tsoftlockup_start_all();",
            "",
            "\twatchdog_hardlockup_start();",
            "\tcpus_read_unlock();",
            "}"
          ],
          "function_name": "watchdog_enable, watchdog_disable, softlockup_stop_fn, softlockup_stop_all, softlockup_start_fn, softlockup_start_all, lockup_detector_online_cpu, lockup_detector_offline_cpu, __lockup_detector_reconfigure",
          "description": "定义看门狗启用/禁用逻辑，通过per-CPU定时器实现软锁步检测，包含初始化完成符、启动/取消高精度定时器、关联硬锁步检测模块的操作",
          "similarity": 0.513285756111145
        }
      ]
    },
    {
      "source_file": "kernel/locking/qspinlock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:45:55\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\qspinlock.c`\n\n---\n\n# `locking/qspinlock.c` 技术文档\n\n## 1. 文件概述\n\n`qspinlock.c` 实现了 Linux 内核中的 **排队自旋锁（Queued Spinlock）**，这是一种高性能、可扩展的自旋锁机制，旨在替代传统的 ticket spinlock。该实现基于经典的 **MCS 锁（Mellor-Crummey and Scott lock）** 算法，但针对 Linux 内核的 `spinlock_t` 限制（仅 4 字节）进行了高度优化和压缩，同时保留了原有自旋锁的 API 兼容性。其核心目标是在多核系统中减少缓存行争用（cache line bouncing），提升高并发场景下的锁性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct qnode`**  \n  每 CPU 的队列节点结构，封装了 `mcs_spinlock` 节点，并在启用 `CONFIG_PARAVIRT_SPINLOCKS` 时预留额外空间用于半虚拟化支持。每个 CPU 最多维护 `MAX_NODES=4` 个节点，对应最多 4 层嵌套上下文（task、softirq、hardirq、NMI）。\n\n- **`qnodes[MAX_NODES]`**  \n  每 CPU 对齐分配的 `qnode` 数组，确保在 64 位架构上恰好占用一个 64 字节缓存行（半虚拟化模式下占用两个）。\n\n### 关键辅助函数\n\n- **`encode_tail(cpu, idx)`**  \n  将 CPU 编号（+1 以区分无尾状态）和嵌套索引编码为 32 位尾部值，用于表示队列尾节点。\n\n- **`decode_tail(tail)`**  \n  解码尾部值，返回对应的 `mcs_spinlock` 节点指针。\n\n- **`grab_mcs_node(base, idx)`**  \n  从基础 MCS 节点指针偏移获取指定索引的节点。\n\n### 核心锁操作函数（内联）\n\n- **`clear_pending(lock)`**  \n  清除锁的 pending 位（`*,1,* → *,0,*`）。\n\n- **`clear_pending_set_locked(lock)`**  \n  同时清除 pending 位并设置 locked 位，完成锁获取（`*,1,0 → *,0,1`）。\n\n- **`xchg_tail(lock, tail)`**  \n  原子交换锁的尾部字段，返回旧尾部值，用于将当前节点加入等待队列。\n\n- **`queued_fetch_set_pending_acquire(lock)`**  \n  原子获取锁的当前值并设置 pending 位（`*,*,* → *,1,*`），带有获取语义。\n\n- **`set_locked(lock)`**  \n  直接设置 locked 位以获取锁（`*,*,0 → *,0,1`）。\n\n> 注：上述函数根据 `_Q_PENDING_BITS` 是否等于 8 分为两种实现路径，分别优化字节访问和原子位操作。\n\n## 3. 关键实现\n\n### 锁状态压缩设计\n- 传统 MCS 锁需 8 字节尾指针 + 8 字节 next 指针，但 Linux 要求 `spinlock_t` 仅占 4 字节。\n- 本实现将锁状态压缩为 32 位：\n  - **1 字节 locked 字段**：表示锁是否被持有（优化字节写性能）。\n  - **1 字节 pending 字段**：表示是否有第二个竞争者（避免频繁队列操作）。\n  - **2 字节 tail 字段**：编码 `(cpu+1, idx)`，其中 `idx ∈ [0,3]` 表示嵌套层级。\n- 通过 `cpu+1` 编码区分“无尾”（0）和“CPU 0 的尾节点”。\n\n### 快速路径优化\n- **第一个竞争者**：直接自旋在 `locked` 位，无需分配 MCS 节点。\n- **第二个竞争者**：设置 `pending` 位，避免立即进入慢速队列路径。\n- **第三个及以上竞争者**：才真正进入 MCS 队列，通过 `xchg_tail` 原子更新尾指针。\n\n### 嵌套上下文支持\n- 利用每 CPU 的 `qnodes[4]` 数组支持最多 4 层嵌套（task/softirq/hardirq/NMI）。\n- 通过 `idx` 参数在嵌套时选择不同节点，避免递归死锁。\n\n### 架构适配\n- 针对 `_Q_PENDING_BITS == 8`（如 x86）使用字节级原子操作（`WRITE_ONCE`）。\n- 其他架构使用通用原子位操作（`atomic_fetch_or_acquire` 等）。\n- 依赖架构支持 8/16 位原子操作。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/smp.h>`, `<linux/percpu.h>`：SMP 和每 CPU 变量支持。\n  - `<asm/qspinlock.h>`：架构相关的锁布局定义（如 `_Q_*_MASK`）。\n  - `\"mcs_spinlock.h\"`：MCS 锁基础实现。\n  - `\"qspinlock_stat.h\"`：锁统计信息（若启用）。\n- **配置依赖**：\n  - `CONFIG_PARAVIRT_SPINLOCKS`：半虚拟化自旋锁支持（扩展 `qnode` 大小）。\n- **架构要求**：必须支持 8/16 位原子操作（如 x86、ARM64）。\n\n## 5. 使用场景\n\n- **内核通用自旋锁**：作为 `spin_lock()`/`spin_unlock()` 的底层实现，广泛用于内核临界区保护。\n- **高并发场景**：在多核系统中显著优于传统 ticket spinlock，尤其适用于锁竞争激烈的子系统（如内存管理、调度器、文件系统）。\n- **中断上下文**：支持在 hardirq/NMI 等嵌套上下文中安全使用。\n- **半虚拟化环境**：通过 `CONFIG_PARAVIRT_SPINLOCKS` 与 hypervisor 协作减少自旋开销（如 KVM、Xen）。",
      "similarity": 0.5815324783325195,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 116,
          "end_line": 435,
          "content": [
            "static inline __pure u32 encode_tail(int cpu, int idx)",
            "{",
            "\tu32 tail;",
            "",
            "\ttail  = (cpu + 1) << _Q_TAIL_CPU_OFFSET;",
            "\ttail |= idx << _Q_TAIL_IDX_OFFSET; /* assume < 4 */",
            "",
            "\treturn tail;",
            "}",
            "static __always_inline void clear_pending(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->pending, 0);",
            "}",
            "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->locked_pending, _Q_LOCKED_VAL);",
            "}",
            "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)",
            "{",
            "\t/*",
            "\t * We can use relaxed semantics since the caller ensures that the",
            "\t * MCS node is properly initialized before updating the tail.",
            "\t */",
            "\treturn (u32)xchg_relaxed(&lock->tail,",
            "\t\t\t\t tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;",
            "}",
            "static __always_inline void clear_pending(struct qspinlock *lock)",
            "{",
            "\tatomic_andnot(_Q_PENDING_VAL, &lock->val);",
            "}",
            "static __always_inline void clear_pending_set_locked(struct qspinlock *lock)",
            "{",
            "\tatomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);",
            "}",
            "static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)",
            "{",
            "\tu32 old, new, val = atomic_read(&lock->val);",
            "",
            "\tfor (;;) {",
            "\t\tnew = (val & _Q_LOCKED_PENDING_MASK) | tail;",
            "\t\t/*",
            "\t\t * We can use relaxed semantics since the caller ensures that",
            "\t\t * the MCS node is properly initialized before updating the",
            "\t\t * tail.",
            "\t\t */",
            "\t\told = atomic_cmpxchg_relaxed(&lock->val, val, new);",
            "\t\tif (old == val)",
            "\t\t\tbreak;",
            "",
            "\t\tval = old;",
            "\t}",
            "\treturn old;",
            "}",
            "static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)",
            "{",
            "\treturn atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);",
            "}",
            "static __always_inline void set_locked(struct qspinlock *lock)",
            "{",
            "\tWRITE_ONCE(lock->locked, _Q_LOCKED_VAL);",
            "}",
            "static __always_inline void __pv_init_node(struct mcs_spinlock *node) { }",
            "static __always_inline void __pv_wait_node(struct mcs_spinlock *node,",
            "\t\t\t\t\t   struct mcs_spinlock *prev) { }",
            "static __always_inline void __pv_kick_node(struct qspinlock *lock,",
            "\t\t\t\t\t   struct mcs_spinlock *node) { }",
            "static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,",
            "\t\t\t\t\t\t   struct mcs_spinlock *node)",
            "\t\t\t\t\t\t   { return 0; }",
            "void __lockfunc queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)",
            "{",
            "\tstruct mcs_spinlock *prev, *next, *node;",
            "\tu32 old, tail;",
            "\tint idx;",
            "",
            "\tBUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));",
            "",
            "\tif (pv_enabled())",
            "\t\tgoto pv_queue;",
            "",
            "\tif (virt_spin_lock(lock))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Wait for in-progress pending->locked hand-overs with a bounded",
            "\t * number of spins so that we guarantee forward progress.",
            "\t *",
            "\t * 0,1,0 -> 0,0,1",
            "\t */",
            "\tif (val == _Q_PENDING_VAL) {",
            "\t\tint cnt = _Q_PENDING_LOOPS;",
            "\t\tval = atomic_cond_read_relaxed(&lock->val,",
            "\t\t\t\t\t       (VAL != _Q_PENDING_VAL) || !cnt--);",
            "\t}",
            "",
            "\t/*",
            "\t * If we observe any contention; queue.",
            "\t */",
            "\tif (val & ~_Q_LOCKED_MASK)",
            "\t\tgoto queue;",
            "",
            "\t/*",
            "\t * trylock || pending",
            "\t *",
            "\t * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock",
            "\t */",
            "\tval = queued_fetch_set_pending_acquire(lock);",
            "",
            "\t/*",
            "\t * If we observe contention, there is a concurrent locker.",
            "\t *",
            "\t * Undo and queue; our setting of PENDING might have made the",
            "\t * n,0,0 -> 0,0,0 transition fail and it will now be waiting",
            "\t * on @next to become !NULL.",
            "\t */",
            "\tif (unlikely(val & ~_Q_LOCKED_MASK)) {",
            "",
            "\t\t/* Undo PENDING if we set it. */",
            "\t\tif (!(val & _Q_PENDING_MASK))",
            "\t\t\tclear_pending(lock);",
            "",
            "\t\tgoto queue;",
            "\t}",
            "",
            "\t/*",
            "\t * We're pending, wait for the owner to go away.",
            "\t *",
            "\t * 0,1,1 -> *,1,0",
            "\t *",
            "\t * this wait loop must be a load-acquire such that we match the",
            "\t * store-release that clears the locked bit and create lock",
            "\t * sequentiality; this is because not all",
            "\t * clear_pending_set_locked() implementations imply full",
            "\t * barriers.",
            "\t */",
            "\tif (val & _Q_LOCKED_MASK)",
            "\t\tsmp_cond_load_acquire(&lock->locked, !VAL);",
            "",
            "\t/*",
            "\t * take ownership and clear the pending bit.",
            "\t *",
            "\t * 0,1,0 -> 0,0,1",
            "\t */",
            "\tclear_pending_set_locked(lock);",
            "\tlockevent_inc(lock_pending);",
            "\treturn;",
            "",
            "\t/*",
            "\t * End of pending bit optimistic spinning and beginning of MCS",
            "\t * queuing.",
            "\t */",
            "queue:",
            "\tlockevent_inc(lock_slowpath);",
            "pv_queue:",
            "\tnode = this_cpu_ptr(&qnodes[0].mcs);",
            "\tidx = node->count++;",
            "\ttail = encode_tail(smp_processor_id(), idx);",
            "",
            "\ttrace_contention_begin(lock, LCB_F_SPIN);",
            "",
            "\t/*",
            "\t * 4 nodes are allocated based on the assumption that there will",
            "\t * not be nested NMIs taking spinlocks. That may not be true in",
            "\t * some architectures even though the chance of needing more than",
            "\t * 4 nodes will still be extremely unlikely. When that happens,",
            "\t * we fall back to spinning on the lock directly without using",
            "\t * any MCS node. This is not the most elegant solution, but is",
            "\t * simple enough.",
            "\t */",
            "\tif (unlikely(idx >= MAX_NODES)) {",
            "\t\tlockevent_inc(lock_no_node);",
            "\t\twhile (!queued_spin_trylock(lock))",
            "\t\t\tcpu_relax();",
            "\t\tgoto release;",
            "\t}",
            "",
            "\tnode = grab_mcs_node(node, idx);",
            "",
            "\t/*",
            "\t * Keep counts of non-zero index values:",
            "\t */",
            "\tlockevent_cond_inc(lock_use_node2 + idx - 1, idx);",
            "",
            "\t/*",
            "\t * Ensure that we increment the head node->count before initialising",
            "\t * the actual node. If the compiler is kind enough to reorder these",
            "\t * stores, then an IRQ could overwrite our assignments.",
            "\t */",
            "\tbarrier();",
            "",
            "\tnode->locked = 0;",
            "\tnode->next = NULL;",
            "\tpv_init_node(node);",
            "",
            "\t/*",
            "\t * We touched a (possibly) cold cacheline in the per-cpu queue node;",
            "\t * attempt the trylock once more in the hope someone let go while we",
            "\t * weren't watching.",
            "\t */",
            "\tif (queued_spin_trylock(lock))",
            "\t\tgoto release;",
            "",
            "\t/*",
            "\t * Ensure that the initialisation of @node is complete before we",
            "\t * publish the updated tail via xchg_tail() and potentially link",
            "\t * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\t/*",
            "\t * Publish the updated tail.",
            "\t * We have already touched the queueing cacheline; don't bother with",
            "\t * pending stuff.",
            "\t *",
            "\t * p,*,* -> n,*,*",
            "\t */",
            "\told = xchg_tail(lock, tail);",
            "\tnext = NULL;",
            "",
            "\t/*",
            "\t * if there was a previous node; link it and wait until reaching the",
            "\t * head of the waitqueue.",
            "\t */",
            "\tif (old & _Q_TAIL_MASK) {",
            "\t\tprev = decode_tail(old);",
            "",
            "\t\t/* Link @node into the waitqueue. */",
            "\t\tWRITE_ONCE(prev->next, node);",
            "",
            "\t\tpv_wait_node(node, prev);",
            "\t\tarch_mcs_spin_lock_contended(&node->locked);",
            "",
            "\t\t/*",
            "\t\t * While waiting for the MCS lock, the next pointer may have",
            "\t\t * been set by another lock waiter. We optimistically load",
            "\t\t * the next pointer & prefetch the cacheline for writing",
            "\t\t * to reduce latency in the upcoming MCS unlock operation.",
            "\t\t */",
            "\t\tnext = READ_ONCE(node->next);",
            "\t\tif (next)",
            "\t\t\tprefetchw(next);",
            "\t}",
            "",
            "\t/*",
            "\t * we're at the head of the waitqueue, wait for the owner & pending to",
            "\t * go away.",
            "\t *",
            "\t * *,x,y -> *,0,0",
            "\t *",
            "\t * this wait loop must use a load-acquire such that we match the",
            "\t * store-release that clears the locked bit and create lock",
            "\t * sequentiality; this is because the set_locked() function below",
            "\t * does not imply a full barrier.",
            "\t *",
            "\t * The PV pv_wait_head_or_lock function, if active, will acquire",
            "\t * the lock and return a non-zero value. So we have to skip the",
            "\t * atomic_cond_read_acquire() call. As the next PV queue head hasn't",
            "\t * been designated yet, there is no way for the locked value to become",
            "\t * _Q_SLOW_VAL. So both the set_locked() and the",
            "\t * atomic_cmpxchg_relaxed() calls will be safe.",
            "\t *",
            "\t * If PV isn't active, 0 will be returned instead.",
            "\t *",
            "\t */",
            "\tif ((val = pv_wait_head_or_lock(lock, node)))",
            "\t\tgoto locked;",
            "",
            "\tval = atomic_cond_read_acquire(&lock->val, !(VAL & _Q_LOCKED_PENDING_MASK));",
            "",
            "locked:",
            "\t/*",
            "\t * claim the lock:",
            "\t *",
            "\t * n,0,0 -> 0,0,1 : lock, uncontended",
            "\t * *,*,0 -> *,*,1 : lock, contended",
            "\t *",
            "\t * If the queue head is the only one in the queue (lock value == tail)",
            "\t * and nobody is pending, clear the tail code and grab the lock.",
            "\t * Otherwise, we only need to grab the lock.",
            "\t */",
            "",
            "\t/*",
            "\t * In the PV case we might already have _Q_LOCKED_VAL set, because",
            "\t * of lock stealing; therefore we must also allow:",
            "\t *",
            "\t * n,0,1 -> 0,0,1",
            "\t *",
            "\t * Note: at this point: (val & _Q_PENDING_MASK) == 0, because of the",
            "\t *       above wait condition, therefore any concurrent setting of",
            "\t *       PENDING will make the uncontended transition fail.",
            "\t */",
            "\tif ((val & _Q_TAIL_MASK) == tail) {",
            "\t\tif (atomic_try_cmpxchg_relaxed(&lock->val, &val, _Q_LOCKED_VAL))",
            "\t\t\tgoto release; /* No contention */",
            "\t}",
            "",
            "\t/*",
            "\t * Either somebody is queued behind us or _Q_PENDING_VAL got set",
            "\t * which will then detect the remaining tail and queue behind us",
            "\t * ensuring we'll see a @next.",
            "\t */",
            "\tset_locked(lock);",
            "",
            "\t/*",
            "\t * contended path; wait for next if not observed yet, release.",
            "\t */",
            "\tif (!next)",
            "\t\tnext = smp_cond_load_relaxed(&node->next, (VAL));",
            "",
            "\tarch_mcs_spin_unlock_contended(&next->locked);",
            "\tpv_kick_node(lock, next);",
            "",
            "release:",
            "\ttrace_contention_end(lock, 0);",
            "",
            "\t/*",
            "\t * release the node",
            "\t */",
            "\t__this_cpu_dec(qnodes[0].mcs.count);",
            "}"
          ],
          "function_name": "encode_tail, clear_pending, clear_pending_set_locked, xchg_tail, clear_pending, clear_pending_set_locked, xchg_tail, queued_fetch_set_pending_acquire, set_locked, __pv_init_node, __pv_wait_node, __pv_kick_node, __pv_wait_head_or_lock, queued_spin_lock_slowpath",
          "description": "实现了qspinlock的核心状态转换函数和慢路径获取逻辑，包含尾部编码、挂起状态清除、锁状态设置等原子操作，并通过MCS队列处理锁竞争，支持硬中断、软中断等嵌套场景的递归控制",
          "similarity": 0.5715888738632202
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 1,
          "end_line": 115,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-or-later",
            "/*",
            " * Queued spinlock",
            " *",
            " * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.",
            " * (C) Copyright 2013-2014,2018 Red Hat, Inc.",
            " * (C) Copyright 2015 Intel Corp.",
            " * (C) Copyright 2015 Hewlett-Packard Enterprise Development LP",
            " *",
            " * Authors: Waiman Long <longman@redhat.com>",
            " *          Peter Zijlstra <peterz@infradead.org>",
            " */",
            "",
            "#ifndef _GEN_PV_LOCK_SLOWPATH",
            "",
            "#include <linux/smp.h>",
            "#include <linux/bug.h>",
            "#include <linux/cpumask.h>",
            "#include <linux/percpu.h>",
            "#include <linux/hardirq.h>",
            "#include <linux/mutex.h>",
            "#include <linux/prefetch.h>",
            "#include <asm/byteorder.h>",
            "#include <asm/qspinlock.h>",
            "#include <trace/events/lock.h>",
            "",
            "/*",
            " * Include queued spinlock statistics code",
            " */",
            "#include \"qspinlock_stat.h\"",
            "",
            "/*",
            " * The basic principle of a queue-based spinlock can best be understood",
            " * by studying a classic queue-based spinlock implementation called the",
            " * MCS lock. A copy of the original MCS lock paper (\"Algorithms for Scalable",
            " * Synchronization on Shared-Memory Multiprocessors by Mellor-Crummey and",
            " * Scott\") is available at",
            " *",
            " * https://bugzilla.kernel.org/show_bug.cgi?id=206115",
            " *",
            " * This queued spinlock implementation is based on the MCS lock, however to",
            " * make it fit the 4 bytes we assume spinlock_t to be, and preserve its",
            " * existing API, we must modify it somehow.",
            " *",
            " * In particular; where the traditional MCS lock consists of a tail pointer",
            " * (8 bytes) and needs the next pointer (another 8 bytes) of its own node to",
            " * unlock the next pending (next->locked), we compress both these: {tail,",
            " * next->locked} into a single u32 value.",
            " *",
            " * Since a spinlock disables recursion of its own context and there is a limit",
            " * to the contexts that can nest; namely: task, softirq, hardirq, nmi. As there",
            " * are at most 4 nesting levels, it can be encoded by a 2-bit number. Now",
            " * we can encode the tail by combining the 2-bit nesting level with the cpu",
            " * number. With one byte for the lock value and 3 bytes for the tail, only a",
            " * 32-bit word is now needed. Even though we only need 1 bit for the lock,",
            " * we extend it to a full byte to achieve better performance for architectures",
            " * that support atomic byte write.",
            " *",
            " * We also change the first spinner to spin on the lock bit instead of its",
            " * node; whereby avoiding the need to carry a node from lock to unlock, and",
            " * preserving existing lock API. This also makes the unlock code simpler and",
            " * faster.",
            " *",
            " * N.B. The current implementation only supports architectures that allow",
            " *      atomic operations on smaller 8-bit and 16-bit data types.",
            " *",
            " */",
            "",
            "#include \"mcs_spinlock.h\"",
            "#define MAX_NODES\t4",
            "",
            "/*",
            " * On 64-bit architectures, the mcs_spinlock structure will be 16 bytes in",
            " * size and four of them will fit nicely in one 64-byte cacheline. For",
            " * pvqspinlock, however, we need more space for extra data. To accommodate",
            " * that, we insert two more long words to pad it up to 32 bytes. IOW, only",
            " * two of them can fit in a cacheline in this case. That is OK as it is rare",
            " * to have more than 2 levels of slowpath nesting in actual use. We don't",
            " * want to penalize pvqspinlocks to optimize for a rare case in native",
            " * qspinlocks.",
            " */",
            "struct qnode {",
            "\tstruct mcs_spinlock mcs;",
            "#ifdef CONFIG_PARAVIRT_SPINLOCKS",
            "\tlong reserved[2];",
            "#endif",
            "};",
            "",
            "/*",
            " * The pending bit spinning loop count.",
            " * This heuristic is used to limit the number of lockword accesses",
            " * made by atomic_cond_read_relaxed when waiting for the lock to",
            " * transition out of the \"== _Q_PENDING_VAL\" state. We don't spin",
            " * indefinitely because there's no guarantee that we'll make forward",
            " * progress.",
            " */",
            "#ifndef _Q_PENDING_LOOPS",
            "#define _Q_PENDING_LOOPS\t1",
            "#endif",
            "",
            "/*",
            " * Per-CPU queue node structures; we can never have more than 4 nested",
            " * contexts: task, softirq, hardirq, nmi.",
            " *",
            " * Exactly fits one 64-byte cacheline on a 64-bit architecture.",
            " *",
            " * PV doubles the storage and uses the second cacheline for PV state.",
            " */",
            "static DEFINE_PER_CPU_ALIGNED(struct qnode, qnodes[MAX_NODES]);",
            "",
            "/*",
            " * We must be able to distinguish between no-tail and the tail at 0:0,",
            " * therefore increment the cpu number by one.",
            " */",
            ""
          ],
          "function_name": null,
          "description": "定义了qspinlock的队列节点结构体qnode及其per-CPU数组，用于支持paravirtualization的锁机制，通过压缩尾部指针与锁状态到单个32位值，结合MCS锁算法实现可扩展同步",
          "similarity": 0.5288310647010803
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/qspinlock.c",
          "start_line": 590,
          "end_line": 594,
          "content": [
            "static __init int parse_nopvspin(char *arg)",
            "{",
            "\tnopvspin = true;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "parse_nopvspin",
          "description": "解析内核启动参数以禁用paravirtualization锁机制的初始化函数，通过设置nopvspin标志位控制是否启用特定的虚拟化架构优化特性",
          "similarity": 0.5030996799468994
        }
      ]
    }
  ]
}