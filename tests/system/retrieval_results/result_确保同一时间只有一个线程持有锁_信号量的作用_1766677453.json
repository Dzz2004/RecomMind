{
  "query": "确保同一时间只有一个线程持有锁 信号量的作用",
  "timestamp": "2025-12-25 23:44:13",
  "retrieved_files": [
    {
      "source_file": "kernel/locking/semaphore.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:52:31\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\semaphore.c`\n\n---\n\n# `locking/semaphore.c` 技术文档\n\n## 1. 文件概述\n\n`locking/semaphore.c` 实现了 Linux 内核中的**计数信号量（counting semaphore）**机制。计数信号量允许多个任务（最多为初始计数值）同时持有该锁，当计数值耗尽时，后续请求者将被阻塞，直到有其他任务释放信号量。与互斥锁（mutex）不同，信号量支持更灵活的并发控制，适用于资源池、限流等场景。该文件提供了多种获取和释放信号量的接口，包括可中断、可超时、不可中断等变体，并支持在中断上下文中调用部分函数。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `down(struct semaphore *sem)` | 不可中断地获取信号量，若不可用则睡眠。**已弃用**，建议使用可中断版本。 |\n| `down_interruptible(struct semaphore *sem)` | 可被普通信号中断的获取操作，成功返回 0，被信号中断返回 `-EINTR`。 |\n| `down_killable(struct semaphore *sem)` | 可被致命信号（fatal signal）中断的获取操作，返回值同上。 |\n| `down_trylock(struct semaphore *sem)` | 非阻塞尝试获取信号量，成功返回 0，失败返回 1（**注意返回值与 mutex/spinlock 相反**）。 |\n| `down_timeout(struct semaphore *sem, long timeout)` | 带超时的获取操作，超时返回 `-ETIME`，成功返回 0。 |\n| `up(struct semaphore *sem)` | 释放信号量，可由任意上下文（包括中断）调用，唤醒等待队列中的任务。 |\n\n### 静态辅助函数\n\n- `__down*()` 系列：处理信号量争用时的阻塞逻辑。\n- `__up()`：在有等待者时执行唤醒逻辑。\n- `___down_common()`：通用的阻塞等待实现，支持不同睡眠状态和超时。\n- `__sem_acquire()`：原子减少计数并记录持有者（用于 hung task 检测）。\n\n### 数据结构\n\n- `struct semaphore`（定义在 `<linux/semaphore.h>`）：\n  - `count`：当前可用资源数（>0 表示可立即获取）。\n  - `wait_list`：等待该信号量的任务链表。\n  - `lock`：保护上述成员的原始自旋锁（`raw_spinlock_t`）。\n  - `last_holder`（条件编译）：记录最后持有者，用于 `CONFIG_DETECT_HUNG_TASK_BLOCKER`。\n\n- `struct semaphore_waiter`：\n  - 用于将任务加入等待队列，包含任务指针和唤醒标志（`up`）。\n\n## 3. 关键实现\n\n### 中断安全与自旋锁\n- 所有对外接口（包括 `down*` 和 `up`）均使用 `raw_spin_lock_irqsave()` 获取自旋锁，确保在中断上下文安全。\n- 即使 `down()` 等函数通常在进程上下文调用，也使用 `irqsave` 变体，因为内核某些部分依赖在中断上下文成功调用 `down()`（当确定信号量可用时）。\n\n### 计数语义\n- `sem->count` 表示**还可被获取的次数**。初始值由 `sema_init()` 设置。\n- 获取时：若 `count > 0`，直接减 1；否则加入等待队列。\n- 释放时：若等待队列为空，`count++`；否则唤醒队首任务。\n\n### 等待与唤醒机制\n- 使用 `wake_q`（批量唤醒队列）优化唤醒路径，避免在持有自旋锁时调用 `wake_up_process()`。\n- 等待任务通过 `schedule_timeout()` 睡眠，并在循环中检查：\n  - 是否收到信号（根据睡眠状态判断）。\n  - 是否超时。\n  - 是否被 `__up()` 标记为 `waiter.up = true`（表示已被选中唤醒）。\n\n### Hung Task 支持\n- 当启用 `CONFIG_DETECT_HUNG_TASK_BLOCKER` 时：\n  - 获取信号量时记录当前任务为 `last_holder`。\n  - 释放时若当前任务是持有者，则清除记录。\n  - 提供 `sem_last_holder()` 供 hung task 检测模块查询阻塞源头。\n\n### 返回值约定\n- `down_trylock()` 返回 **0 表示成功**，**1 表示失败**，这与 `mutex_trylock()` 和 `spin_trylock()` **相反**，需特别注意。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/semaphore.h>`：信号量结构体和 API 声明。\n  - `<linux/spinlock.h>`：原始自旋锁实现。\n  - `<linux/sched.h>`、`<linux/sched/wake_q.h>`：任务调度和批量唤醒。\n  - `<trace/events/lock.h>`：锁争用跟踪点。\n  - `<linux/hung_task.h>`：hung task 检测支持。\n\n- **内核配置依赖**：\n  - `CONFIG_DETECT_HUNG_TASK_BLOCKER`：启用信号量持有者跟踪。\n\n- **与其他同步原语关系**：\n  - 与 `mutex.c` 形成对比：mutex 是二值、不可递归、带调试信息的互斥锁；信号量是计数、可被任意任务释放、更轻量。\n  - 底层依赖调度器（`schedule_timeout`）和中断管理（`irqsave`）。\n\n## 5. 使用场景\n\n- **资源池管理**：如限制同时访问某类硬件设备的任务数量。\n- **读写并发控制**：配合其他机制实现多读者/单写者模型。\n- **内核驱动**：设备驱动中控制对共享资源的并发访问。\n- **中断上下文释放**：因 `up()` 可在中断中调用，适用于中断处理程序释放资源的场景。\n- **不可睡眠路径**：使用 `down_trylock()` 在原子上下文尝试获取资源。\n\n> **注意**：由于信号量不强制所有权（任意任务可调用 `up()`），且缺乏死锁检测等调试特性，现代内核开发中更推荐使用 `mutex` 或 `rwsem`，除非明确需要计数语义或多释放者特性。",
      "similarity": 0.6059821248054504,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 46,
          "end_line": 160,
          "content": [
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "\tWRITE_ONCE((sem)->last_holder, (unsigned long)current);",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "\tif (READ_ONCE((sem)->last_holder) == (unsigned long)current)",
            "\t\tWRITE_ONCE((sem)->last_holder, 0UL);",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn READ_ONCE(sem->last_holder);",
            "}",
            "static inline void hung_task_sem_set_holder(struct semaphore *sem)",
            "{",
            "}",
            "static inline void hung_task_sem_clear_if_holder(struct semaphore *sem)",
            "{",
            "}",
            "unsigned long sem_last_holder(struct semaphore *sem)",
            "{",
            "\treturn 0UL;",
            "}",
            "static inline void __sem_acquire(struct semaphore *sem)",
            "{",
            "\tsem->count--;",
            "\thung_task_sem_set_holder(sem);",
            "}",
            "void __sched down(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\t__down(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "}",
            "int __sched down_interruptible(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_interruptible(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_killable(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_killable(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "int __sched down_trylock(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tint count;",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tcount = sem->count - 1;",
            "\tif (likely(count >= 0))",
            "\t\t__sem_acquire(sem);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn (count < 0);",
            "}",
            "int __sched down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\tunsigned long flags;",
            "\tint result = 0;",
            "",
            "\tmight_sleep();",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "\tif (likely(sem->count > 0))",
            "\t\t__sem_acquire(sem);",
            "\telse",
            "\t\tresult = __down_timeout(sem, timeout);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "",
            "\treturn result;",
            "}",
            "void __sched up(struct semaphore *sem)",
            "{",
            "\tunsigned long flags;",
            "\tDEFINE_WAKE_Q(wake_q);",
            "",
            "\traw_spin_lock_irqsave(&sem->lock, flags);",
            "",
            "\thung_task_sem_clear_if_holder(sem);",
            "",
            "\tif (likely(list_empty(&sem->wait_list)))",
            "\t\tsem->count++;",
            "\telse",
            "\t\t__up(sem, &wake_q);",
            "\traw_spin_unlock_irqrestore(&sem->lock, flags);",
            "\tif (!wake_q_empty(&wake_q))",
            "\t\twake_up_q(&wake_q);",
            "}"
          ],
          "function_name": "hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, hung_task_sem_set_holder, hung_task_sem_clear_if_holder, sem_last_holder, __sem_acquire, down, down_interruptible, down_killable, down_trylock, down_timeout, up",
          "description": "实现了信号量的获取与释放核心逻辑，包括down/down_interruptible/down_killable/down_trylock/down_timeout等接口，通过spinlock保护共享资源，维护等待队列并处理任务状态变更，其中包含Hung Task检测相关函数的条件性实现",
          "similarity": 0.6161587238311768
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 252,
          "end_line": 323,
          "content": [
            "static inline int __sched ___down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\t\t\t\tlong timeout)",
            "{",
            "\tstruct semaphore_waiter waiter;",
            "",
            "\tlist_add_tail(&waiter.list, &sem->wait_list);",
            "\twaiter.task = current;",
            "\twaiter.up = false;",
            "",
            "\tfor (;;) {",
            "\t\tif (signal_pending_state(state, current))",
            "\t\t\tgoto interrupted;",
            "\t\tif (unlikely(timeout <= 0))",
            "\t\t\tgoto timed_out;",
            "\t\t__set_current_state(state);",
            "\t\traw_spin_unlock_irq(&sem->lock);",
            "\t\ttimeout = schedule_timeout(timeout);",
            "\t\traw_spin_lock_irq(&sem->lock);",
            "\t\tif (waiter.up) {",
            "\t\t\thung_task_sem_set_holder(sem);",
            "\t\t\treturn 0;",
            "\t\t}",
            "\t}",
            "",
            " timed_out:",
            "\tlist_del(&waiter.list);",
            "\treturn -ETIME;",
            "",
            " interrupted:",
            "\tlist_del(&waiter.list);",
            "\treturn -EINTR;",
            "}",
            "static inline int __sched __down_common(struct semaphore *sem, long state,",
            "\t\t\t\t\tlong timeout)",
            "{",
            "\tint ret;",
            "",
            "\thung_task_set_blocker(sem, BLOCKER_TYPE_SEM);",
            "",
            "\ttrace_contention_begin(sem, 0);",
            "\tret = ___down_common(sem, state, timeout);",
            "\ttrace_contention_end(sem, ret);",
            "",
            "\thung_task_clear_blocker();",
            "",
            "\treturn ret;",
            "}",
            "static noinline void __sched __down(struct semaphore *sem)",
            "{",
            "\t__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_interruptible(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_INTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_killable(struct semaphore *sem)",
            "{",
            "\treturn __down_common(sem, TASK_KILLABLE, MAX_SCHEDULE_TIMEOUT);",
            "}",
            "static noinline int __sched __down_timeout(struct semaphore *sem, long timeout)",
            "{",
            "\treturn __down_common(sem, TASK_UNINTERRUPTIBLE, timeout);",
            "}",
            "static noinline void __sched __up(struct semaphore *sem,",
            "\t\t\t\t  struct wake_q_head *wake_q)",
            "{",
            "\tstruct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,",
            "\t\t\t\t\t\tstruct semaphore_waiter, list);",
            "\tlist_del(&waiter->list);",
            "\twaiter->up = true;",
            "\twake_q_add(wake_q, waiter->task);",
            "}"
          ],
          "function_name": "___down_common, __down_common, __down, __down_interruptible, __down_killable, __down_timeout, __up",
          "description": "实现了信号量的阻塞等待通用逻辑，包含___down_common/__down_common等辅助函数，处理信号量不足时的任务挂起、超时检测、信号处理及唤醒机制，通过循环等待并结合schedule_timeout实现阻塞式资源竞争解决",
          "similarity": 0.5724460482597351
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/semaphore.c",
          "start_line": 1,
          "end_line": 45,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2008 Intel Corporation",
            " * Author: Matthew Wilcox <willy@linux.intel.com>",
            " *",
            " * This file implements counting semaphores.",
            " * A counting semaphore may be acquired 'n' times before sleeping.",
            " * See mutex.c for single-acquisition sleeping locks which enforce",
            " * rules which allow code to be debugged more easily.",
            " */",
            "",
            "/*",
            " * Some notes on the implementation:",
            " *",
            " * The spinlock controls access to the other members of the semaphore.",
            " * down_trylock() and up() can be called from interrupt context, so we",
            " * have to disable interrupts when taking the lock.  It turns out various",
            " * parts of the kernel expect to be able to use down() on a semaphore in",
            " * interrupt context when they know it will succeed, so we have to use",
            " * irqsave variants for down(), down_interruptible() and down_killable()",
            " * too.",
            " *",
            " * The ->count variable represents how many more tasks can acquire this",
            " * semaphore.  If it's zero, there may be tasks waiting on the wait_list.",
            " */",
            "",
            "#include <linux/compiler.h>",
            "#include <linux/kernel.h>",
            "#include <linux/export.h>",
            "#include <linux/sched.h>",
            "#include <linux/sched/debug.h>",
            "#include <linux/sched/wake_q.h>",
            "#include <linux/semaphore.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/ftrace.h>",
            "#include <trace/events/lock.h>",
            "#include <linux/hung_task.h>",
            "",
            "static noinline void __down(struct semaphore *sem);",
            "static noinline int __down_interruptible(struct semaphore *sem);",
            "static noinline int __down_killable(struct semaphore *sem);",
            "static noinline int __down_timeout(struct semaphore *sem, long timeout);",
            "static noinline void __up(struct semaphore *sem, struct wake_q_head *wake_q);",
            "",
            "#ifdef CONFIG_DETECT_HUNG_TASK_BLOCKER"
          ],
          "function_name": null,
          "description": "此代码块定义了计数信号量的基础框架，包含实现计数信号量所需的头文件和注释，声明了多个内联函数及辅助函数，用于处理信号量的获取、释放及Hung Task检测相关逻辑，但由于代码截断，CONFIG_DETECT_HUNG_TASK_BLOCKER部分缺失，上下文不完整",
          "similarity": 0.5705758333206177
        }
      ]
    },
    {
      "source_file": "kernel/locking/osq_lock.c",
      "md_summary": "> 自动生成时间: 2025-10-25 14:43:41\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `locking\\osq_lock.c`\n\n---\n\n# `locking/osq_lock.c` 技术文档\n\n## 1. 文件概述\n\n`osq_lock.c` 实现了一种专为**乐观自旋（Optimistic Spinning）**设计的轻量级排队自旋锁机制，称为 **OSQ（Optimistic Spin Queue）锁**。该机制主要用于支持如互斥锁（mutex）、读写信号量（rwsem）等**可睡眠锁**在争用时进行乐观自旋，以避免不必要的上下文切换和调度开销。OSQ 锁基于 MCS（Mellor-Crummey and Scott）锁的思想，但针对 Linux 内核的调度和抢占模型进行了优化，利用每个 CPU 的静态 per-CPU 节点结构，确保在禁用抢占的自旋上下文中安全使用。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct optimistic_spin_node`：每个 CPU 对应一个静态节点，包含：\n  - `cpu`：编码后的 CPU 编号（实际值 = CPU ID + 1）\n  - `locked`：布尔标志，表示是否已获得锁\n  - `next`：指向队列中下一个节点的指针\n  - `prev`：指向前一个节点的指针\n- `struct optimistic_spin_queue`：OSQ 锁结构体，仅包含一个原子变量 `tail`，用于指向队列尾部（编码后的 CPU 编号），`OSQ_UNLOCKED_VAL`（值为 0）表示无锁。\n\n### 主要函数\n- `bool osq_lock(struct optimistic_spin_queue *lock)`  \n  尝试获取 OSQ 锁。若成功获得锁或决定放弃自旋（如需要调度或前驱被抢占），返回 `true`；若成功排队但未获得锁且需继续等待，则返回 `false`（实际逻辑中，失败路径最终也返回 `false` 表示未获得锁）。\n  \n- `void osq_unlock(struct optimistic_spin_queue *lock)`  \n  释放 OSQ 锁，唤醒队列中的下一个等待者（若存在）。\n\n- `static inline struct optimistic_spin_node *osq_wait_next(...)`  \n  辅助函数，用于在解锁或取消排队时安全地获取下一个节点，并处理队列尾部的原子更新。\n\n- `encode_cpu()` / `decode_cpu()` / `node_cpu()`  \n  用于在 CPU 编号与 per-CPU 节点指针之间进行编码/解码转换，其中 CPU 编号 0 被编码为 1，以 0 表示“无 CPU”（即锁空闲）。\n\n## 3. 关键实现\n\n### Per-CPU 静态节点设计\n- 每个 CPU 拥有一个静态的 `osq_node`（通过 `DEFINE_PER_CPU_SHARED_ALIGNED` 定义），避免动态分配开销。\n- 由于 OSQ 仅在**禁用抢占**的上下文中使用（如 mutex 的乐观自旋阶段），且**不可在中断上下文调用**，因此 per-CPU 节点的生命周期安全。\n\n### 锁获取流程 (`osq_lock`)\n1. **初始化本地节点**：设置 `locked=0`、`next=NULL`，并确保 `cpu` 字段为当前 CPU 编码值。\n2. **原子交换尾指针**：通过 `atomic_xchg(&lock->tail, curr)` 尝试入队。若原值为 `OSQ_UNLOCKED_VAL`，直接获得锁。\n3. **链接到前驱**：若已有前驱（`prev`），通过 `smp_wmb()` 确保内存顺序后，设置 `prev->next = node`。\n4. **自旋等待**：使用 `smp_cond_load_relaxed()` 等待 `node->locked` 变为 1，或满足退出条件（`need_resched()` 或前驱 CPU 被抢占 `vcpu_is_preempted()`）。\n5. **取消排队（Unqueue）**：若需退出自旋：\n   - **Step A**：尝试将 `prev->next` 置为 `NULL`，断开链接。\n   - **Step B**：调用 `osq_wait_next()` 确定下一个节点，并可能将锁尾指针回退。\n   - **Step C**：若存在 `next`，将其与 `prev` 直接链接，完成队列修复。\n\n### 锁释放流程 (`osq_unlock`)\n1. **快速路径**：若当前 CPU 是唯一持有者（`tail == curr`），直接将 `tail` 设为 `OSQ_UNLOCKED_VAL`。\n2. **慢速路径**：\n   - 若本地节点的 `next` 非空，直接设置 `next->locked = 1` 唤醒后继。\n   - 否则调用 `osq_wait_next()` 获取下一个节点（处理并发取消排队的情况），再唤醒。\n\n### 内存屏障与原子操作\n- 使用 `atomic_xchg`、`atomic_cmpxchg_acquire/release` 确保对 `lock->tail` 的操作具有适当的内存序。\n- `smp_wmb()` 保证在设置 `prev->next` 前，本地节点的初始化对其他 CPU 可见。\n- `WRITE_ONCE`/`READ_ONCE` 防止编译器优化破坏并发访问语义。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/percpu.h>`：提供 per-CPU 变量支持（`this_cpu_ptr`, `per_cpu_ptr`）。\n  - `<linux/sched.h>`：提供调度相关函数（`need_resched()`）和虚拟 CPU 抢占检测（`vcpu_is_preempted()`）。\n  - `<linux/osq_lock.h>`：定义 `struct optimistic_spin_queue`、`struct optimistic_spin_node` 及 `OSQ_UNLOCKED_VAL`。\n- **架构依赖**：依赖底层架构的原子操作（`atomic_*`）、内存屏障（`smp_wmb`, `smp_load_acquire`）和 CPU ID 获取（`smp_processor_id()`）。\n- **调度器集成**：与内核调度器紧密协作，通过 `need_resched()` 和 `vcpu_is_preempted()` 决定是否继续自旋。\n\n## 5. 使用场景\n\nOSQ 锁主要用于**可睡眠锁的乐观自旋优化**，典型场景包括：\n- **Mutex（互斥锁）**：在 `mutex_spin_on_owner()` 中，若锁持有者正在运行，当前 CPU 会尝试 OSQ 自旋而非立即睡眠。\n- **Rwsem（读写信号量）**：在写者争用时，若满足条件，会使用 OSQ 进行乐观自旋。\n- **其他睡眠锁**：任何希望在锁争用时避免立即进入睡眠、以降低延迟的同步原语。\n\n其核心价值在于：当锁持有者很可能在**另一个 CPU 上运行且未被抢占**时，通过短暂自旋可避免昂贵的上下文切换，提升性能；同时通过 `vcpu_is_preempted()` 检测虚拟化环境中的抢占，避免在持有者已让出 CPU 时无效自旋。",
      "similarity": 0.5716938376426697,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 213,
          "end_line": 238,
          "content": [
            "void osq_unlock(struct optimistic_spin_queue *lock)",
            "{",
            "\tstruct optimistic_spin_node *node, *next;",
            "\tint curr = encode_cpu(smp_processor_id());",
            "",
            "\t/*",
            "\t * Fast path for the uncontended case.",
            "\t */",
            "\tif (likely(atomic_cmpxchg_release(&lock->tail, curr,",
            "\t\t\t\t\t  OSQ_UNLOCKED_VAL) == curr))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Second most likely case.",
            "\t */",
            "\tnode = this_cpu_ptr(&osq_node);",
            "\tnext = xchg(&node->next, NULL);",
            "\tif (next) {",
            "\t\tWRITE_ONCE(next->locked, 1);",
            "\t\treturn;",
            "\t}",
            "",
            "\tnext = osq_wait_next(lock, node, NULL);",
            "\tif (next)",
            "\t\tWRITE_ONCE(next->locked, 1);",
            "}"
          ],
          "function_name": "osq_unlock",
          "description": "实现osq_unlock函数，处理锁的释放。通过原子比较交换操作快速处理无竞争情况，否则查找并唤醒下一个等待节点，确保锁状态的正确性与线程安全。",
          "similarity": 0.5919311046600342
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 20,
          "end_line": 149,
          "content": [
            "static inline int encode_cpu(int cpu_nr)",
            "{",
            "\treturn cpu_nr + 1;",
            "}",
            "static inline int node_cpu(struct optimistic_spin_node *node)",
            "{",
            "\treturn node->cpu - 1;",
            "}",
            "bool osq_lock(struct optimistic_spin_queue *lock)",
            "{",
            "\tstruct optimistic_spin_node *node = this_cpu_ptr(&osq_node);",
            "\tstruct optimistic_spin_node *prev, *next;",
            "\tint curr = encode_cpu(smp_processor_id());",
            "\tint old;",
            "",
            "\tnode->locked = 0;",
            "\tnode->next = NULL;",
            "\t/*",
            "\t * After this cpu member is initialized for the first time, it",
            "\t * would no longer change in fact. That could avoid cache misses",
            "\t * when spin and access the cpu member by other CPUs.",
            "\t */",
            "\tif (node->cpu != curr)",
            "\t\tnode->cpu = curr;",
            "",
            "\t/*",
            "\t * We need both ACQUIRE (pairs with corresponding RELEASE in",
            "\t * unlock() uncontended, or fastpath) and RELEASE (to publish",
            "\t * the node fields we just initialised) semantics when updating",
            "\t * the lock tail.",
            "\t */",
            "\told = atomic_xchg(&lock->tail, curr);",
            "\tif (old == OSQ_UNLOCKED_VAL)",
            "\t\treturn true;",
            "",
            "\tprev = decode_cpu(old);",
            "\tnode->prev = prev;",
            "",
            "\t/*",
            "\t * osq_lock()\t\t\tunqueue",
            "\t *",
            "\t * node->prev = prev\t\tosq_wait_next()",
            "\t * WMB\t\t\t\tMB",
            "\t * prev->next = node\t\tnext->prev = prev // unqueue-C",
            "\t *",
            "\t * Here 'node->prev' and 'next->prev' are the same variable and we need",
            "\t * to ensure these stores happen in-order to avoid corrupting the list.",
            "\t */",
            "\tsmp_wmb();",
            "",
            "\tWRITE_ONCE(prev->next, node);",
            "",
            "\t/*",
            "\t * Normally @prev is untouchable after the above store; because at that",
            "\t * moment unlock can proceed and wipe the node element from stack.",
            "\t *",
            "\t * However, since our nodes are static per-cpu storage, we're",
            "\t * guaranteed their existence -- this allows us to apply",
            "\t * cmpxchg in an attempt to undo our queueing.",
            "\t */",
            "",
            "\t/*",
            "\t * Wait to acquire the lock or cancellation. Note that need_resched()",
            "\t * will come with an IPI, which will wake smp_cond_load_relaxed() if it",
            "\t * is implemented with a monitor-wait. vcpu_is_preempted() relies on",
            "\t * polling, be careful.",
            "\t */",
            "\tif (smp_cond_load_relaxed(&node->locked, VAL || need_resched() ||",
            "\t\t\t\t  vcpu_is_preempted(node_cpu(node->prev))))",
            "\t\treturn true;",
            "",
            "\t/* unqueue */",
            "\t/*",
            "\t * Step - A  -- stabilize @prev",
            "\t *",
            "\t * Undo our @prev->next assignment; this will make @prev's",
            "\t * unlock()/unqueue() wait for a next pointer since @lock points to us",
            "\t * (or later).",
            "\t */",
            "",
            "\tfor (;;) {",
            "\t\t/*",
            "\t\t * cpu_relax() below implies a compiler barrier which would",
            "\t\t * prevent this comparison being optimized away.",
            "\t\t */",
            "\t\tif (data_race(prev->next) == node &&",
            "\t\t    cmpxchg(&prev->next, node, NULL) == node)",
            "\t\t\tbreak;",
            "",
            "\t\t/*",
            "\t\t * We can only fail the cmpxchg() racing against an unlock(),",
            "\t\t * in which case we should observe @node->locked becoming",
            "\t\t * true.",
            "\t\t */",
            "\t\tif (smp_load_acquire(&node->locked))",
            "\t\t\treturn true;",
            "",
            "\t\tcpu_relax();",
            "",
            "\t\t/*",
            "\t\t * Or we race against a concurrent unqueue()'s step-B, in which",
            "\t\t * case its step-C will write us a new @node->prev pointer.",
            "\t\t */",
            "\t\tprev = READ_ONCE(node->prev);",
            "\t}",
            "",
            "\t/*",
            "\t * Step - B -- stabilize @next",
            "\t *",
            "\t * Similar to unlock(), wait for @node->next or move @lock from @node",
            "\t * back to @prev.",
            "\t */",
            "",
            "\tnext = osq_wait_next(lock, node, prev);",
            "\tif (!next)",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Step - C -- unlink",
            "\t *",
            "\t * @prev is stable because its still waiting for a new @prev->next",
            "\t * pointer, @next is stable because our @node->next pointer is NULL and",
            "\t * it will wait in Step-A.",
            "\t */",
            "",
            "\tWRITE_ONCE(next->prev, prev);",
            "\tWRITE_ONCE(prev->next, next);",
            "",
            "\treturn false;",
            "}"
          ],
          "function_name": "encode_cpu, node_cpu, osq_lock",
          "description": "实现osq_lock函数，负责获取乐观自旋锁。通过原子操作将当前节点插入队列，利用内存屏障保证顺序一致性，并通过循环等待条件满足或被唤醒，最终完成锁的获取过程。",
          "similarity": 0.5536922812461853
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/locking/osq_lock.c",
          "start_line": 1,
          "end_line": 19,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "#include <linux/percpu.h>",
            "#include <linux/sched.h>",
            "#include <linux/osq_lock.h>",
            "",
            "/*",
            " * An MCS like lock especially tailored for optimistic spinning for sleeping",
            " * lock implementations (mutex, rwsem, etc).",
            " *",
            " * Using a single mcs node per CPU is safe because sleeping locks should not be",
            " * called from interrupt context and we have preemption disabled while",
            " * spinning.",
            " */",
            "static DEFINE_PER_CPU_SHARED_ALIGNED(struct optimistic_spin_node, osq_node);",
            "",
            "/*",
            " * We use the value 0 to represent \"no CPU\", thus the encoded value",
            " * will be the CPU number incremented by 1.",
            " */"
          ],
          "function_name": null,
          "description": "定义全局的per-CPU乐观自旋节点osq_node，用于支持多CPU环境下乐观自旋锁的实现。通过encode_cpu和node_cpu函数处理CPU编号转换，为后续锁操作提供基础设施。",
          "similarity": 0.47824665904045105
        }
      ]
    },
    {
      "source_file": "kernel/time/tick-broadcast.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:48:04\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `time\\tick-broadcast.c`\n\n---\n\n# `time/tick-broadcast.c` 技术文档\n\n## 1. 文件概述\n\n`tick-broadcast.c` 实现了 Linux 内核中的 **时钟事件广播机制（tick broadcast）**，用于在某些硬件平台（如部分 x86 系统）上，当本地 APIC 定时器在深度 C 状态（如 C3）下停止工作时，通过一个全局的、始终可用的广播时钟事件设备（broadcast clock event device）来为多个 CPU 提供周期性或单次（oneshot）的时钟中断服务。该机制确保即使本地定时器失效，系统仍能维持正确的调度、时间管理和电源管理功能。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- `tick_broadcast_device`：全局的广播时钟设备封装（`struct tick_device`）\n- `tick_broadcast_mask`：记录当前依赖广播机制接收 tick 的 CPU 掩码\n- `tick_broadcast_on`：记录处于周期性广播模式的 CPU 掩码\n- `tmpmask`：临时 CPU 掩码，用于内部计算\n- `tick_broadcast_forced`：标志位，指示是否强制启用广播模式\n- `tick_oneshot_wakeup_device`（per-CPU）：每个 CPU 可选的专用单次唤醒设备（用于优化）\n\n### 主要函数\n\n- `tick_install_broadcast_device()`：安装或替换广播时钟设备\n- `tick_device_uses_broadcast()`：判断某 CPU 的本地设备是否需依赖广播\n- `tick_is_broadcast_device()`：检查设备是否为当前广播设备\n- `tick_broadcast_update_freq()`：更新广播设备频率\n- `tick_get_broadcast_device()` / `tick_get_broadcast_mask()`：调试接口\n- `tick_set_oneshot_wakeup_device()`：为 CPU 设置专用单次唤醒设备\n- `tick_broadcast_setup_oneshot()` / `tick_broadcast_clear_oneshot()`：管理单次广播模式\n- `tick_oneshot_wakeup_handler()`：专用唤醒设备的中断处理函数\n\n## 3. 关键实现\n\n### 广播设备选择策略\n- 通过 `tick_check_broadcast_device()` 评估候选设备：\n  - 排除 `DUMMY`、`PERCPU` 或 `C3STOP` 特性的设备\n  - 在 oneshot 模式下要求设备支持 `ONESHOT`\n  - 优先选择 `rating` 更高的设备\n\n### 两种广播模式\n- **周期性模式（Periodic）**：通过 `tick_broadcast_start_periodic()` 启动固定频率中断\n- **单次模式（Oneshot）**：通过 `tick_broadcast_setup_oneshot()` 动态编程下次中断时间\n\n### CPU 依赖管理\n- 当 CPU 的本地设备不支持深度睡眠（无 `C3STOP`）或功能不全时，将其加入 `tick_broadcast_mask`\n- 支持为特定 CPU 分配专用的 `tick_oneshot_wakeup_device`，避免全局广播开销\n\n### 安全机制\n- 使用 `tick_broadcast_lock` 自旋锁保护全局状态\n- 通过 `try_module_get()` 确保设备驱动模块不会在使用中被卸载\n- 提供 `err_broadcast()` 作为兜底处理，防止系统完全失去 tick\n\n### 模式切换\n- 若系统已运行在 oneshot 模式，新注册的广播设备会自动切换至 oneshot\n- 通过 `tick_clock_notify()` 通知所有 CPU 重新评估 tick 模式\n\n## 4. 依赖关系\n\n- **内部依赖**：\n  - `tick-internal.h`：tick 子系统内部接口\n  - `clockevents` 框架：设备注册、频率更新、事件处理\n  - `tick-sched.c`：与 per-CPU tick 调度器交互\n- **外部依赖**：\n  - `CONFIG_GENERIC_CLOCKEVENTS`：时钟事件设备基础框架\n  - `CONFIG_TICK_ONESHOT`：单次 tick 模式支持（可选）\n  - `CONFIG_HOTPLUG_CPU`：CPU 热插拔时的广播状态管理（可选）\n- **头文件**：\n  - `<linux/cpumask.h>`、`<linux/smp.h>`：CPU 掩码和 SMP 操作\n  - `<linux/interrupt.h>`、`<linux/hrtimer.h>`：中断和高精度定时器支持\n\n## 5. 使用场景\n\n1. **x86 C3+ 电源状态**：在 Intel/AMD 处理器进入 C3 或更深睡眠状态时，本地 APIC 定时器停止，必须依赖 HPET 或 TSC_DEADLINE 等全局设备广播 tick。\n2. **无本地定时器的架构**：某些嵌入式或旧平台可能缺乏 per-CPU 定时器，完全依赖广播机制。\n3. **调试与监控**：通过 `timer_list` 等工具可查看广播设备状态，辅助诊断 tick 相关问题。\n4. **CPU 热插拔**：在线 CPU 下线时，需将其从广播掩码中移除（`tick_broadcast_oneshot_offline`）。\n5. **动态 tick 模式切换**：系统在周期性 tick 与 NO_HZ（动态 tick）模式间切换时，广播设备需同步调整工作模式。",
      "similarity": 0.563775360584259,
      "chunks": [
        {
          "chunk_id": 3,
          "file_path": "kernel/time/tick-broadcast.c",
          "start_line": 328,
          "end_line": 488,
          "content": [
            "int tick_receive_broadcast(void)",
            "{",
            "\tstruct tick_device *td = this_cpu_ptr(&tick_cpu_device);",
            "\tstruct clock_event_device *evt = td->evtdev;",
            "",
            "\tif (!evt)",
            "\t\treturn -ENODEV;",
            "",
            "\tif (!evt->event_handler)",
            "\t\treturn -EINVAL;",
            "",
            "\tevt->event_handler(evt);",
            "\treturn 0;",
            "}",
            "static bool tick_do_broadcast(struct cpumask *mask)",
            "{",
            "\tint cpu = smp_processor_id();",
            "\tstruct tick_device *td;",
            "\tbool local = false;",
            "",
            "\t/*",
            "\t * Check, if the current cpu is in the mask",
            "\t */",
            "\tif (cpumask_test_cpu(cpu, mask)) {",
            "\t\tstruct clock_event_device *bc = tick_broadcast_device.evtdev;",
            "",
            "\t\tcpumask_clear_cpu(cpu, mask);",
            "\t\t/*",
            "\t\t * We only run the local handler, if the broadcast",
            "\t\t * device is not hrtimer based. Otherwise we run into",
            "\t\t * a hrtimer recursion.",
            "\t\t *",
            "\t\t * local timer_interrupt()",
            "\t\t *   local_handler()",
            "\t\t *     expire_hrtimers()",
            "\t\t *       bc_handler()",
            "\t\t *         local_handler()",
            "\t\t *\t     expire_hrtimers()",
            "\t\t */",
            "\t\tlocal = !(bc->features & CLOCK_EVT_FEAT_HRTIMER);",
            "\t}",
            "",
            "\tif (!cpumask_empty(mask)) {",
            "\t\t/*",
            "\t\t * It might be necessary to actually check whether the devices",
            "\t\t * have different broadcast functions. For now, just use the",
            "\t\t * one of the first device. This works as long as we have this",
            "\t\t * misfeature only on x86 (lapic)",
            "\t\t */",
            "\t\ttd = &per_cpu(tick_cpu_device, cpumask_first(mask));",
            "\t\ttd->evtdev->broadcast(mask);",
            "\t}",
            "\treturn local;",
            "}",
            "static bool tick_do_periodic_broadcast(void)",
            "{",
            "\tcpumask_and(tmpmask, cpu_online_mask, tick_broadcast_mask);",
            "\treturn tick_do_broadcast(tmpmask);",
            "}",
            "static void tick_handle_periodic_broadcast(struct clock_event_device *dev)",
            "{",
            "\tstruct tick_device *td = this_cpu_ptr(&tick_cpu_device);",
            "\tbool bc_local;",
            "",
            "\traw_spin_lock(&tick_broadcast_lock);",
            "",
            "\t/* Handle spurious interrupts gracefully */",
            "\tif (clockevent_state_shutdown(tick_broadcast_device.evtdev)) {",
            "\t\traw_spin_unlock(&tick_broadcast_lock);",
            "\t\treturn;",
            "\t}",
            "",
            "\tbc_local = tick_do_periodic_broadcast();",
            "",
            "\tif (clockevent_state_oneshot(dev)) {",
            "\t\tktime_t next = ktime_add_ns(dev->next_event, TICK_NSEC);",
            "",
            "\t\tclockevents_program_event(dev, next, true);",
            "\t}",
            "\traw_spin_unlock(&tick_broadcast_lock);",
            "",
            "\t/*",
            "\t * We run the handler of the local cpu after dropping",
            "\t * tick_broadcast_lock because the handler might deadlock when",
            "\t * trying to switch to oneshot mode.",
            "\t */",
            "\tif (bc_local)",
            "\t\ttd->evtdev->event_handler(td->evtdev);",
            "}",
            "void tick_broadcast_control(enum tick_broadcast_mode mode)",
            "{",
            "\tstruct clock_event_device *bc, *dev;",
            "\tstruct tick_device *td;",
            "\tint cpu, bc_stopped;",
            "\tunsigned long flags;",
            "",
            "\t/* Protects also the local clockevent device. */",
            "\traw_spin_lock_irqsave(&tick_broadcast_lock, flags);",
            "\ttd = this_cpu_ptr(&tick_cpu_device);",
            "\tdev = td->evtdev;",
            "",
            "\t/*",
            "\t * Is the device not affected by the powerstate ?",
            "\t */",
            "\tif (!dev || !(dev->features & CLOCK_EVT_FEAT_C3STOP))",
            "\t\tgoto out;",
            "",
            "\tif (!tick_device_is_functional(dev))",
            "\t\tgoto out;",
            "",
            "\tcpu = smp_processor_id();",
            "\tbc = tick_broadcast_device.evtdev;",
            "\tbc_stopped = cpumask_empty(tick_broadcast_mask);",
            "",
            "\tswitch (mode) {",
            "\tcase TICK_BROADCAST_FORCE:",
            "\t\ttick_broadcast_forced = 1;",
            "\t\tfallthrough;",
            "\tcase TICK_BROADCAST_ON:",
            "\t\tcpumask_set_cpu(cpu, tick_broadcast_on);",
            "\t\tif (!cpumask_test_and_set_cpu(cpu, tick_broadcast_mask)) {",
            "\t\t\t/*",
            "\t\t\t * Only shutdown the cpu local device, if:",
            "\t\t\t *",
            "\t\t\t * - the broadcast device exists",
            "\t\t\t * - the broadcast device is not a hrtimer based one",
            "\t\t\t * - the broadcast device is in periodic mode to",
            "\t\t\t *   avoid a hiccup during switch to oneshot mode",
            "\t\t\t */",
            "\t\t\tif (bc && !(bc->features & CLOCK_EVT_FEAT_HRTIMER) &&",
            "\t\t\t    tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)",
            "\t\t\t\tclockevents_shutdown(dev);",
            "\t\t}",
            "\t\tbreak;",
            "",
            "\tcase TICK_BROADCAST_OFF:",
            "\t\tif (tick_broadcast_forced)",
            "\t\t\tbreak;",
            "\t\tcpumask_clear_cpu(cpu, tick_broadcast_on);",
            "\t\tif (cpumask_test_and_clear_cpu(cpu, tick_broadcast_mask)) {",
            "\t\t\tif (tick_broadcast_device.mode ==",
            "\t\t\t    TICKDEV_MODE_PERIODIC)",
            "\t\t\t\ttick_setup_periodic(dev, 0);",
            "\t\t}",
            "\t\tbreak;",
            "\t}",
            "",
            "\tif (bc) {",
            "\t\tif (cpumask_empty(tick_broadcast_mask)) {",
            "\t\t\tif (!bc_stopped)",
            "\t\t\t\tclockevents_shutdown(bc);",
            "\t\t} else if (bc_stopped) {",
            "\t\t\tif (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)",
            "\t\t\t\ttick_broadcast_start_periodic(bc);",
            "\t\t\telse",
            "\t\t\t\ttick_broadcast_setup_oneshot(bc, false);",
            "\t\t}",
            "\t}",
            "out:",
            "\traw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);",
            "}"
          ],
          "function_name": "tick_receive_broadcast, tick_do_broadcast, tick_do_periodic_broadcast, tick_handle_periodic_broadcast, tick_broadcast_control",
          "description": "实现广播中断接收、分发及控制逻辑，包含周期性广播处理、中断状态管理及广播模式开关操作，处理多CPU间同步与状态一致性保障。",
          "similarity": 0.561476469039917
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/time/tick-broadcast.c",
          "start_line": 1,
          "end_line": 44,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * This file contains functions which emulate a local clock-event",
            " * device via a broadcast event source.",
            " *",
            " * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>",
            " * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar",
            " * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner",
            " */",
            "#include <linux/cpu.h>",
            "#include <linux/err.h>",
            "#include <linux/hrtimer.h>",
            "#include <linux/interrupt.h>",
            "#include <linux/percpu.h>",
            "#include <linux/profile.h>",
            "#include <linux/sched.h>",
            "#include <linux/smp.h>",
            "#include <linux/module.h>",
            "",
            "#include \"tick-internal.h\"",
            "",
            "/*",
            " * Broadcast support for broken x86 hardware, where the local apic",
            " * timer stops in C3 state.",
            " */",
            "",
            "static struct tick_device tick_broadcast_device;",
            "static cpumask_var_t tick_broadcast_mask __cpumask_var_read_mostly;",
            "static cpumask_var_t tick_broadcast_on __cpumask_var_read_mostly;",
            "static cpumask_var_t tmpmask __cpumask_var_read_mostly;",
            "static int tick_broadcast_forced;",
            "",
            "static __cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(tick_broadcast_lock);",
            "",
            "#ifdef CONFIG_TICK_ONESHOT",
            "static DEFINE_PER_CPU(struct clock_event_device *, tick_oneshot_wakeup_device);",
            "",
            "static void tick_broadcast_setup_oneshot(struct clock_event_device *bc, bool from_periodic);",
            "static void tick_broadcast_clear_oneshot(int cpu);",
            "static void tick_resume_broadcast_oneshot(struct clock_event_device *bc);",
            "# ifdef CONFIG_HOTPLUG_CPU",
            "static void tick_broadcast_oneshot_offline(unsigned int cpu);",
            "# endif",
            "#else"
          ],
          "function_name": null,
          "description": "定义广播时钟设备所需的数据结构和锁，支持x86硬件中因C3状态导致本地APIC定时器停转的问题，包含广播设备、掩码及锁的全局变量，条件编译部分未展开。",
          "similarity": 0.560071587562561
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/time/tick-broadcast.c",
          "start_line": 934,
          "end_line": 1106,
          "content": [
            "static int tick_oneshot_wakeup_control(enum tick_broadcast_state state,",
            "\t\t\t\t       struct tick_device *td,",
            "\t\t\t\t       int cpu)",
            "{",
            "\tstruct clock_event_device *dev, *wd;",
            "",
            "\tdev = td->evtdev;",
            "\tif (td->mode != TICKDEV_MODE_ONESHOT)",
            "\t\treturn -EINVAL;",
            "",
            "\twd = tick_get_oneshot_wakeup_device(cpu);",
            "\tif (!wd)",
            "\t\treturn -ENODEV;",
            "",
            "\tswitch (state) {",
            "\tcase TICK_BROADCAST_ENTER:",
            "\t\tclockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT_STOPPED);",
            "\t\tclockevents_switch_state(wd, CLOCK_EVT_STATE_ONESHOT);",
            "\t\tclockevents_program_event(wd, dev->next_event, 1);",
            "\t\tbreak;",
            "\tcase TICK_BROADCAST_EXIT:",
            "\t\t/* We may have transitioned to oneshot mode while idle */",
            "\t\tif (clockevent_get_state(wd) != CLOCK_EVT_STATE_ONESHOT)",
            "\t\t\treturn -ENODEV;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)",
            "{",
            "\tstruct tick_device *td = this_cpu_ptr(&tick_cpu_device);",
            "\tint cpu = smp_processor_id();",
            "",
            "\tif (!tick_oneshot_wakeup_control(state, td, cpu))",
            "\t\treturn 0;",
            "",
            "\tif (tick_broadcast_device.evtdev)",
            "\t\treturn ___tick_broadcast_oneshot_control(state, td, cpu);",
            "",
            "\t/*",
            "\t * If there is no broadcast or wakeup device, tell the caller not",
            "\t * to go into deep idle.",
            "\t */",
            "\treturn -EBUSY;",
            "}",
            "static void tick_broadcast_clear_oneshot(int cpu)",
            "{",
            "\tcpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);",
            "\tcpumask_clear_cpu(cpu, tick_broadcast_pending_mask);",
            "}",
            "static void tick_broadcast_init_next_event(struct cpumask *mask,",
            "\t\t\t\t\t   ktime_t expires)",
            "{",
            "\tstruct tick_device *td;",
            "\tint cpu;",
            "",
            "\tfor_each_cpu(cpu, mask) {",
            "\t\ttd = &per_cpu(tick_cpu_device, cpu);",
            "\t\tif (td->evtdev)",
            "\t\t\ttd->evtdev->next_event = expires;",
            "\t}",
            "}",
            "static inline ktime_t tick_get_next_period(void)",
            "{",
            "\tktime_t next;",
            "",
            "\t/*",
            "\t * Protect against concurrent updates (store /load tearing on",
            "\t * 32bit). It does not matter if the time is already in the",
            "\t * past. The broadcast device which is about to be programmed will",
            "\t * fire in any case.",
            "\t */",
            "\traw_spin_lock(&jiffies_lock);",
            "\tnext = tick_next_period;",
            "\traw_spin_unlock(&jiffies_lock);",
            "\treturn next;",
            "}",
            "static void tick_broadcast_setup_oneshot(struct clock_event_device *bc,",
            "\t\t\t\t\t bool from_periodic)",
            "{",
            "\tint cpu = smp_processor_id();",
            "\tktime_t nexttick = 0;",
            "",
            "\tif (!bc)",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * When the broadcast device was switched to oneshot by the first",
            "\t * CPU handling the NOHZ change, the other CPUs will reach this",
            "\t * code via hrtimer_run_queues() -> tick_check_oneshot_change()",
            "\t * too. Set up the broadcast device only once!",
            "\t */",
            "\tif (bc->event_handler == tick_handle_oneshot_broadcast) {",
            "\t\t/*",
            "\t\t * The CPU which switched from periodic to oneshot mode",
            "\t\t * set the broadcast oneshot bit for all other CPUs which",
            "\t\t * are in the general (periodic) broadcast mask to ensure",
            "\t\t * that CPUs which wait for the periodic broadcast are",
            "\t\t * woken up.",
            "\t\t *",
            "\t\t * Clear the bit for the local CPU as the set bit would",
            "\t\t * prevent the first tick_broadcast_enter() after this CPU",
            "\t\t * switched to oneshot state to program the broadcast",
            "\t\t * device.",
            "\t\t *",
            "\t\t * This code can also be reached via tick_broadcast_control(),",
            "\t\t * but this cannot avoid the tick_broadcast_clear_oneshot()",
            "\t\t * as that would break the periodic to oneshot transition of",
            "\t\t * secondary CPUs. But that's harmless as the below only",
            "\t\t * clears already cleared bits.",
            "\t\t */",
            "\t\ttick_broadcast_clear_oneshot(cpu);",
            "\t\treturn;",
            "\t}",
            "",
            "",
            "\tbc->event_handler = tick_handle_oneshot_broadcast;",
            "\tbc->next_event = KTIME_MAX;",
            "",
            "\t/*",
            "\t * When the tick mode is switched from periodic to oneshot it must",
            "\t * be ensured that CPUs which are waiting for periodic broadcast",
            "\t * get their wake-up at the next tick.  This is achieved by ORing",
            "\t * tick_broadcast_mask into tick_broadcast_oneshot_mask.",
            "\t *",
            "\t * For other callers, e.g. broadcast device replacement,",
            "\t * tick_broadcast_oneshot_mask must not be touched as this would",
            "\t * set bits for CPUs which are already NOHZ, but not idle. Their",
            "\t * next tick_broadcast_enter() would observe the bit set and fail",
            "\t * to update the expiry time and the broadcast event device.",
            "\t */",
            "\tif (from_periodic) {",
            "\t\tcpumask_copy(tmpmask, tick_broadcast_mask);",
            "\t\t/* Remove the local CPU as it is obviously not idle */",
            "\t\tcpumask_clear_cpu(cpu, tmpmask);",
            "\t\tcpumask_or(tick_broadcast_oneshot_mask, tick_broadcast_oneshot_mask, tmpmask);",
            "",
            "\t\t/*",
            "\t\t * Ensure that the oneshot broadcast handler will wake the",
            "\t\t * CPUs which are still waiting for periodic broadcast.",
            "\t\t */",
            "\t\tnexttick = tick_get_next_period();",
            "\t\ttick_broadcast_init_next_event(tmpmask, nexttick);",
            "",
            "\t\t/*",
            "\t\t * If the underlying broadcast clock event device is",
            "\t\t * already in oneshot state, then there is nothing to do.",
            "\t\t * The device was already armed for the next tick",
            "\t\t * in tick_handle_broadcast_periodic()",
            "\t\t */",
            "\t\tif (clockevent_state_oneshot(bc))",
            "\t\t\treturn;",
            "\t}",
            "",
            "\t/*",
            "\t * When switching from periodic to oneshot mode arm the broadcast",
            "\t * device for the next tick.",
            "\t *",
            "\t * If the broadcast device has been replaced in oneshot mode and",
            "\t * the oneshot broadcast mask is not empty, then arm it to expire",
            "\t * immediately in order to reevaluate the next expiring timer.",
            "\t * @nexttick is 0 and therefore in the past which will cause the",
            "\t * clockevent code to force an event.",
            "\t *",
            "\t * For both cases the programming can be avoided when the oneshot",
            "\t * broadcast mask is empty.",
            "\t *",
            "\t * tick_broadcast_set_event() implicitly switches the broadcast",
            "\t * device to oneshot state.",
            "\t */",
            "\tif (!cpumask_empty(tick_broadcast_oneshot_mask))",
            "\t\ttick_broadcast_set_event(bc, cpu, nexttick);",
            "}"
          ],
          "function_name": "tick_oneshot_wakeup_control, __tick_broadcast_oneshot_control, tick_broadcast_clear_oneshot, tick_broadcast_init_next_event, tick_get_next_period, tick_broadcast_setup_oneshot",
          "description": "tick_oneshot_wakeup_control控制唤醒设备状态切换；__tick_broadcast_oneshot_control协调广播与唤醒设备的单次模式控制；tick_broadcast_setup_oneshot初始化广播设备处理函数并设置下次触发时间；tick_get_next_period获取全局下一次周期时间",
          "similarity": 0.5327617526054382
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/time/tick-broadcast.c",
          "start_line": 779,
          "end_line": 931,
          "content": [
            "static void broadcast_shutdown_local(struct clock_event_device *bc,",
            "\t\t\t\t     struct clock_event_device *dev)",
            "{",
            "\t/*",
            "\t * For hrtimer based broadcasting we cannot shutdown the cpu",
            "\t * local device if our own event is the first one to expire or",
            "\t * if we own the broadcast timer.",
            "\t */",
            "\tif (bc->features & CLOCK_EVT_FEAT_HRTIMER) {",
            "\t\tif (broadcast_needs_cpu(bc, smp_processor_id()))",
            "\t\t\treturn;",
            "\t\tif (dev->next_event < bc->next_event)",
            "\t\t\treturn;",
            "\t}",
            "\tclockevents_switch_state(dev, CLOCK_EVT_STATE_SHUTDOWN);",
            "}",
            "static int ___tick_broadcast_oneshot_control(enum tick_broadcast_state state,",
            "\t\t\t\t\t     struct tick_device *td,",
            "\t\t\t\t\t     int cpu)",
            "{",
            "\tstruct clock_event_device *bc, *dev = td->evtdev;",
            "\tint ret = 0;",
            "\tktime_t now;",
            "",
            "\traw_spin_lock(&tick_broadcast_lock);",
            "\tbc = tick_broadcast_device.evtdev;",
            "",
            "\tif (state == TICK_BROADCAST_ENTER) {",
            "\t\t/*",
            "\t\t * If the current CPU owns the hrtimer broadcast",
            "\t\t * mechanism, it cannot go deep idle and we do not add",
            "\t\t * the CPU to the broadcast mask. We don't have to go",
            "\t\t * through the EXIT path as the local timer is not",
            "\t\t * shutdown.",
            "\t\t */",
            "\t\tret = broadcast_needs_cpu(bc, cpu);",
            "\t\tif (ret)",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * If the broadcast device is in periodic mode, we",
            "\t\t * return.",
            "\t\t */",
            "\t\tif (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC) {",
            "\t\t\t/* If it is a hrtimer based broadcast, return busy */",
            "\t\t\tif (bc->features & CLOCK_EVT_FEAT_HRTIMER)",
            "\t\t\t\tret = -EBUSY;",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {",
            "\t\t\tWARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));",
            "",
            "\t\t\t/* Conditionally shut down the local timer. */",
            "\t\t\tbroadcast_shutdown_local(bc, dev);",
            "",
            "\t\t\t/*",
            "\t\t\t * We only reprogram the broadcast timer if we",
            "\t\t\t * did not mark ourself in the force mask and",
            "\t\t\t * if the cpu local event is earlier than the",
            "\t\t\t * broadcast event. If the current CPU is in",
            "\t\t\t * the force mask, then we are going to be",
            "\t\t\t * woken by the IPI right away; we return",
            "\t\t\t * busy, so the CPU does not try to go deep",
            "\t\t\t * idle.",
            "\t\t\t */",
            "\t\t\tif (cpumask_test_cpu(cpu, tick_broadcast_force_mask)) {",
            "\t\t\t\tret = -EBUSY;",
            "\t\t\t} else if (dev->next_event < bc->next_event) {",
            "\t\t\t\ttick_broadcast_set_event(bc, cpu, dev->next_event);",
            "\t\t\t\t/*",
            "\t\t\t\t * In case of hrtimer broadcasts the",
            "\t\t\t\t * programming might have moved the",
            "\t\t\t\t * timer to this cpu. If yes, remove",
            "\t\t\t\t * us from the broadcast mask and",
            "\t\t\t\t * return busy.",
            "\t\t\t\t */",
            "\t\t\t\tret = broadcast_needs_cpu(bc, cpu);",
            "\t\t\t\tif (ret) {",
            "\t\t\t\t\tcpumask_clear_cpu(cpu,",
            "\t\t\t\t\t\ttick_broadcast_oneshot_mask);",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t}",
            "\t} else {",
            "\t\tif (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {",
            "\t\t\tclockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);",
            "\t\t\t/*",
            "\t\t\t * The cpu which was handling the broadcast",
            "\t\t\t * timer marked this cpu in the broadcast",
            "\t\t\t * pending mask and fired the broadcast",
            "\t\t\t * IPI. So we are going to handle the expired",
            "\t\t\t * event anyway via the broadcast IPI",
            "\t\t\t * handler. No need to reprogram the timer",
            "\t\t\t * with an already expired event.",
            "\t\t\t */",
            "\t\t\tif (cpumask_test_and_clear_cpu(cpu,",
            "\t\t\t\t       tick_broadcast_pending_mask))",
            "\t\t\t\tgoto out;",
            "",
            "\t\t\t/*",
            "\t\t\t * Bail out if there is no next event.",
            "\t\t\t */",
            "\t\t\tif (dev->next_event == KTIME_MAX)",
            "\t\t\t\tgoto out;",
            "\t\t\t/*",
            "\t\t\t * If the pending bit is not set, then we are",
            "\t\t\t * either the CPU handling the broadcast",
            "\t\t\t * interrupt or we got woken by something else.",
            "\t\t\t *",
            "\t\t\t * We are no longer in the broadcast mask, so",
            "\t\t\t * if the cpu local expiry time is already",
            "\t\t\t * reached, we would reprogram the cpu local",
            "\t\t\t * timer with an already expired event.",
            "\t\t\t *",
            "\t\t\t * This can lead to a ping-pong when we return",
            "\t\t\t * to idle and therefore rearm the broadcast",
            "\t\t\t * timer before the cpu local timer was able",
            "\t\t\t * to fire. This happens because the forced",
            "\t\t\t * reprogramming makes sure that the event",
            "\t\t\t * will happen in the future and depending on",
            "\t\t\t * the min_delta setting this might be far",
            "\t\t\t * enough out that the ping-pong starts.",
            "\t\t\t *",
            "\t\t\t * If the cpu local next_event has expired",
            "\t\t\t * then we know that the broadcast timer",
            "\t\t\t * next_event has expired as well and",
            "\t\t\t * broadcast is about to be handled. So we",
            "\t\t\t * avoid reprogramming and enforce that the",
            "\t\t\t * broadcast handler, which did not run yet,",
            "\t\t\t * will invoke the cpu local handler.",
            "\t\t\t *",
            "\t\t\t * We cannot call the handler directly from",
            "\t\t\t * here, because we might be in a NOHZ phase",
            "\t\t\t * and we did not go through the irq_enter()",
            "\t\t\t * nohz fixups.",
            "\t\t\t */",
            "\t\t\tnow = ktime_get();",
            "\t\t\tif (dev->next_event <= now) {",
            "\t\t\t\tcpumask_set_cpu(cpu, tick_broadcast_force_mask);",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "\t\t\t/*",
            "\t\t\t * We got woken by something else. Reprogram",
            "\t\t\t * the cpu local timer device.",
            "\t\t\t */",
            "\t\t\ttick_program_event(dev->next_event, 1);",
            "\t\t}",
            "\t}",
            "out:",
            "\traw_spin_unlock(&tick_broadcast_lock);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "broadcast_shutdown_local, ___tick_broadcast_oneshot_control",
          "description": "broadcast_shutdown_local根据HRTIMER特征决定是否关闭本地定时器；___tick_broadcast_oneshot_control处理单次模式状态转换，包括CPU加入/移出广播掩码、本地定时器关闭及下次事件设置逻辑",
          "similarity": 0.5309146642684937
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/time/tick-broadcast.c",
          "start_line": 670,
          "end_line": 772,
          "content": [
            "void tick_check_oneshot_broadcast_this_cpu(void)",
            "{",
            "\tif (cpumask_test_cpu(smp_processor_id(), tick_broadcast_oneshot_mask)) {",
            "\t\tstruct tick_device *td = this_cpu_ptr(&tick_cpu_device);",
            "",
            "\t\t/*",
            "\t\t * We might be in the middle of switching over from",
            "\t\t * periodic to oneshot. If the CPU has not yet",
            "\t\t * switched over, leave the device alone.",
            "\t\t */",
            "\t\tif (td->mode == TICKDEV_MODE_ONESHOT) {",
            "\t\t\tclockevents_switch_state(td->evtdev,",
            "\t\t\t\t\t      CLOCK_EVT_STATE_ONESHOT);",
            "\t\t}",
            "\t}",
            "}",
            "static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)",
            "{",
            "\tstruct tick_device *td;",
            "\tktime_t now, next_event;",
            "\tint cpu, next_cpu = 0;",
            "\tbool bc_local;",
            "",
            "\traw_spin_lock(&tick_broadcast_lock);",
            "\tdev->next_event = KTIME_MAX;",
            "\tnext_event = KTIME_MAX;",
            "\tcpumask_clear(tmpmask);",
            "\tnow = ktime_get();",
            "\t/* Find all expired events */",
            "\tfor_each_cpu(cpu, tick_broadcast_oneshot_mask) {",
            "\t\t/*",
            "\t\t * Required for !SMP because for_each_cpu() reports",
            "\t\t * unconditionally CPU0 as set on UP kernels.",
            "\t\t */",
            "\t\tif (!IS_ENABLED(CONFIG_SMP) &&",
            "\t\t    cpumask_empty(tick_broadcast_oneshot_mask))",
            "\t\t\tbreak;",
            "",
            "\t\ttd = &per_cpu(tick_cpu_device, cpu);",
            "\t\tif (td->evtdev->next_event <= now) {",
            "\t\t\tcpumask_set_cpu(cpu, tmpmask);",
            "\t\t\t/*",
            "\t\t\t * Mark the remote cpu in the pending mask, so",
            "\t\t\t * it can avoid reprogramming the cpu local",
            "\t\t\t * timer in tick_broadcast_oneshot_control().",
            "\t\t\t */",
            "\t\t\tcpumask_set_cpu(cpu, tick_broadcast_pending_mask);",
            "\t\t} else if (td->evtdev->next_event < next_event) {",
            "\t\t\tnext_event = td->evtdev->next_event;",
            "\t\t\tnext_cpu = cpu;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Remove the current cpu from the pending mask. The event is",
            "\t * delivered immediately in tick_do_broadcast() !",
            "\t */",
            "\tcpumask_clear_cpu(smp_processor_id(), tick_broadcast_pending_mask);",
            "",
            "\t/* Take care of enforced broadcast requests */",
            "\tcpumask_or(tmpmask, tmpmask, tick_broadcast_force_mask);",
            "\tcpumask_clear(tick_broadcast_force_mask);",
            "",
            "\t/*",
            "\t * Sanity check. Catch the case where we try to broadcast to",
            "\t * offline cpus.",
            "\t */",
            "\tif (WARN_ON_ONCE(!cpumask_subset(tmpmask, cpu_online_mask)))",
            "\t\tcpumask_and(tmpmask, tmpmask, cpu_online_mask);",
            "",
            "\t/*",
            "\t * Wakeup the cpus which have an expired event.",
            "\t */",
            "\tbc_local = tick_do_broadcast(tmpmask);",
            "",
            "\t/*",
            "\t * Two reasons for reprogram:",
            "\t *",
            "\t * - The global event did not expire any CPU local",
            "\t * events. This happens in dyntick mode, as the maximum PIT",
            "\t * delta is quite small.",
            "\t *",
            "\t * - There are pending events on sleeping CPUs which were not",
            "\t * in the event mask",
            "\t */",
            "\tif (next_event != KTIME_MAX)",
            "\t\ttick_broadcast_set_event(dev, next_cpu, next_event);",
            "",
            "\traw_spin_unlock(&tick_broadcast_lock);",
            "",
            "\tif (bc_local) {",
            "\t\ttd = this_cpu_ptr(&tick_cpu_device);",
            "\t\ttd->evtdev->event_handler(td->evtdev);",
            "\t}",
            "}",
            "static int broadcast_needs_cpu(struct clock_event_device *bc, int cpu)",
            "{",
            "\tif (!(bc->features & CLOCK_EVT_FEAT_HRTIMER))",
            "\t\treturn 0;",
            "\tif (bc->next_event == KTIME_MAX)",
            "\t\treturn 0;",
            "\treturn bc->bound_on == cpu ? -EBUSY : 0;",
            "}"
          ],
          "function_name": "tick_check_oneshot_broadcast_this_cpu, tick_handle_oneshot_broadcast, broadcast_needs_cpu",
          "description": "tick_check_oneshot_broadcast_this_cpu检查当前CPU是否在单次广播掩码中，若处于单次模式则切换到CLOCK_EVT_STATE_ONESHOT状态；tick_handle_oneshot_broadcast遍历所有广播CPU寻找已到期事件，唤醒相应CPU并设置下次触发时间；broadcast_needs_cpu判断HRTIMER类型设备是否需要当前CPU处理",
          "similarity": 0.5270035862922668
        }
      ]
    }
  ]
}