{
  "query": "银行家算法中资源回收逻辑实现",
  "timestamp": "2025-12-26 01:08:32",
  "retrieved_files": [
    {
      "source_file": "kernel/bpf/bpf_lru_list.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:00:28\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\bpf_lru_list.c`\n\n---\n\n# `bpf/bpf_lru_list.c` 技术文档\n\n## 1. 文件概述\n\n`bpf_lru_list.c` 实现了 BPF（Berkeley Packet Filter）子系统中用于管理 LRU（Least Recently Used，最近最少使用）缓存的通用机制。该机制主要用于 BPF map（如 `lru_hash` 类型）中高效地回收不活跃或未被引用的条目，以控制内存使用并提升缓存命中率。文件提供了基于双链表的活跃/非活跃 LRU 链表管理、本地（per-CPU）缓存支持、引用位（ref bit）跟踪以及自动老化和回收策略。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct bpf_lru_node`：LRU 节点，嵌入在 BPF map 条目中，包含链表指针、类型和引用标志。\n- `struct bpf_lru_list`：全局 LRU 链表结构，维护活跃（ACTIVE）和非活跃（INACTIVE）链表，以及各类节点计数。\n- `struct bpf_lru_locallist`：每个 CPU 的本地 LRU 链表，用于减少锁竞争，包含 FREE 和 PENDING 本地链表。\n- `struct bpf_lru`：LRU 控制结构，包含回调函数（如 `del_from_htab`）、扫描数量（`nr_scans`）等配置。\n\n### 主要函数\n- `__bpf_lru_list_rotate_active()`：轮转活跃链表，将带引用位的节点保留在活跃链表头部，无引用位的移至非活跃链表。\n- `__bpf_lru_list_rotate_inactive()`：轮转非活跃链表，将带引用位的节点提升回活跃链表。\n- `__bpf_lru_list_shrink_inactive()`：从非活跃链表尾部回收无引用位且可删除的节点到指定 free 链表。\n- `__bpf_lru_list_shrink()`：尝试正常回收失败后，强制从非活跃或活跃链表中删除节点（忽略引用位）。\n- `__bpf_lru_node_move()` / `__bpf_lru_node_move_in()` / `__bpf_lru_node_move_to_free()`：节点在不同链表间移动的内部辅助函数。\n- `get_next_cpu()`：用于遍历所有可能 CPU 的辅助函数。\n\n### 关键常量\n- `LOCAL_FREE_TARGET` / `PERCPU_FREE_TARGET`：本地和 per-CPU 回收目标数量（分别为 128 和 4）。\n- `LOCAL_NR_SCANS` / `PERCPU_NR_SCANS`：本地和 per-CPU 扫描上限，等于各自目标值。\n- `BPF_LOCAL_LIST_T_OFFSET`：本地链表类型的偏移量，用于区分全局 LRU 类型和本地类型。\n\n## 3. 关键实现\n\n### LRU 双链表模型\n采用经典的 **Active/Inactive 双链表模型**：\n- **活跃链表（ACTIVE）**：存放近期被访问或引用的节点。\n- **非活跃链表（INACTIVE）**：存放较久未被访问的节点，是回收的主要候选区域。\n- 节点首次插入时通常进入活跃链表；经过一次老化周期后，若无引用则移至非活跃链表。\n\n### 引用位（ref bit）机制\n- 每个 `bpf_lru_node` 包含一个 `ref` 字段（原子读写），表示该节点是否在最近被访问。\n- 在轮转过程中：\n  - 若节点 `ref == 1`，则清零并保留在活跃链表（或从非活跃提升至活跃）。\n  - 若 `ref == 0`，则可能被移至非活跃链表或直接回收。\n- 该机制避免了频繁移动热数据，提高了缓存效率。\n\n### 老化与回收策略\n- **轮转（Rotate）**：\n  - 定期调用 `__bpf_lru_list_rotate()`。\n  - 当非活跃链表长度小于活跃链表时，触发活跃链表轮转。\n  - 非活跃链表总是轮转，从 `next_inactive_rotation` 指针开始，避免每次都从头扫描。\n- **回收（Shrink）**：\n  - 优先从非活跃链表尾部回收无引用且可删除（通过 `del_from_htab` 回调确认）的节点。\n  - 若回收不足，强制从非活跃链表（优先）或活跃链表中删除节点，**忽略引用位**，确保内存压力下能释放资源。\n\n### 本地（Local）与 Per-CPU 优化\n- 支持 **本地链表类型**（`BPF_LRU_LOCAL_LIST_T_FREE` / `PENDING`），用于暂存待处理或刚释放的节点。\n- 通过 `IS_LOCAL_LIST_TYPE()` 宏区分本地与全局类型，防止非法移动。\n- 减少全局锁竞争，提升多核性能。\n\n### 安全移动与指针维护\n- 在移动节点时，若该节点恰好是 `next_inactive_rotation` 指针指向的对象，则自动将其前移，避免悬空指针。\n- 使用 `list_move()` 安全地在链表间转移节点。\n- 所有计数操作（`bpf_lru_list_count_inc/dec`）均带边界检查。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/cpumask.h>`：用于 CPU 遍历（`get_next_cpu`）。\n  - `<linux/spinlock.h>` 和 `<linux/percpu.h>`：支持 per-CPU 数据结构和同步。\n- **内部依赖**：\n  - 依赖 `bpf_lru_list.h` 中定义的数据结构和枚举（如 `enum bpf_lru_list_type`）。\n- **外部回调**：\n  - 通过 `lru->del_from_htab(lru->del_arg, node)` 回调通知上层（如 BPF map 实现）删除哈希表中的条目，实现 LRU 与具体数据结构的解耦。\n\n## 5. 使用场景\n\n- **BPF LRU Hash Map**：该文件是 `BPF_MAP_TYPE_LRU_HASH` 和 `BPF_MAP_TYPE_LRU_PERCPU_HASH` 等 map 类型的核心内存管理组件。\n- **内存压力下的自动回收**：当 map 达到容量上限或系统内存紧张时，触发 shrink 操作释放条目。\n- **高并发环境优化**：通过本地链表和引用位机制，在多核系统上高效管理缓存，减少锁争用。\n- **内核网络与跟踪子系统**：被用于 eBPF 程序中需要高效键值存储且自动淘汰旧数据的场景，如连接跟踪、统计聚合等。",
      "similarity": 0.5524778366088867,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 171,
          "end_line": 272,
          "content": [
            "static void __bpf_lru_list_rotate_inactive(struct bpf_lru *lru,",
            "\t\t\t\t\t   struct bpf_lru_list *l)",
            "{",
            "\tstruct list_head *inactive = &l->lists[BPF_LRU_LIST_T_INACTIVE];",
            "\tstruct list_head *cur, *last, *next = inactive;",
            "\tstruct bpf_lru_node *node;",
            "\tunsigned int i = 0;",
            "",
            "\tif (list_empty(inactive))",
            "\t\treturn;",
            "",
            "\tlast = l->next_inactive_rotation->next;",
            "\tif (last == inactive)",
            "\t\tlast = last->next;",
            "",
            "\tcur = l->next_inactive_rotation;",
            "\twhile (i < lru->nr_scans) {",
            "\t\tif (cur == inactive) {",
            "\t\t\tcur = cur->prev;",
            "\t\t\tcontinue;",
            "\t\t}",
            "",
            "\t\tnode = list_entry(cur, struct bpf_lru_node, list);",
            "\t\tnext = cur->prev;",
            "\t\tif (bpf_lru_node_is_ref(node))",
            "\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);",
            "\t\tif (cur == last)",
            "\t\t\tbreak;",
            "\t\tcur = next;",
            "\t\ti++;",
            "\t}",
            "",
            "\tl->next_inactive_rotation = next;",
            "}",
            "static unsigned int",
            "__bpf_lru_list_shrink_inactive(struct bpf_lru *lru,",
            "\t\t\t       struct bpf_lru_list *l,",
            "\t\t\t       unsigned int tgt_nshrink,",
            "\t\t\t       struct list_head *free_list,",
            "\t\t\t       enum bpf_lru_list_type tgt_free_type)",
            "{",
            "\tstruct list_head *inactive = &l->lists[BPF_LRU_LIST_T_INACTIVE];",
            "\tstruct bpf_lru_node *node, *tmp_node;",
            "\tunsigned int nshrinked = 0;",
            "\tunsigned int i = 0;",
            "",
            "\tlist_for_each_entry_safe_reverse(node, tmp_node, inactive, list) {",
            "\t\tif (bpf_lru_node_is_ref(node)) {",
            "\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);",
            "\t\t} else if (lru->del_from_htab(lru->del_arg, node)) {",
            "\t\t\t__bpf_lru_node_move_to_free(l, node, free_list,",
            "\t\t\t\t\t\t    tgt_free_type);",
            "\t\t\tif (++nshrinked == tgt_nshrink)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "",
            "\t\tif (++i == lru->nr_scans)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\treturn nshrinked;",
            "}",
            "static void __bpf_lru_list_rotate(struct bpf_lru *lru, struct bpf_lru_list *l)",
            "{",
            "\tif (bpf_lru_list_inactive_low(l))",
            "\t\t__bpf_lru_list_rotate_active(lru, l);",
            "",
            "\t__bpf_lru_list_rotate_inactive(lru, l);",
            "}",
            "static unsigned int __bpf_lru_list_shrink(struct bpf_lru *lru,",
            "\t\t\t\t\t  struct bpf_lru_list *l,",
            "\t\t\t\t\t  unsigned int tgt_nshrink,",
            "\t\t\t\t\t  struct list_head *free_list,",
            "\t\t\t\t\t  enum bpf_lru_list_type tgt_free_type)",
            "",
            "{",
            "\tstruct bpf_lru_node *node, *tmp_node;",
            "\tstruct list_head *force_shrink_list;",
            "\tunsigned int nshrinked;",
            "",
            "\tnshrinked = __bpf_lru_list_shrink_inactive(lru, l, tgt_nshrink,",
            "\t\t\t\t\t\t   free_list, tgt_free_type);",
            "\tif (nshrinked)",
            "\t\treturn nshrinked;",
            "",
            "\t/* Do a force shrink by ignoring the reference bit */",
            "\tif (!list_empty(&l->lists[BPF_LRU_LIST_T_INACTIVE]))",
            "\t\tforce_shrink_list = &l->lists[BPF_LRU_LIST_T_INACTIVE];",
            "\telse",
            "\t\tforce_shrink_list = &l->lists[BPF_LRU_LIST_T_ACTIVE];",
            "",
            "\tlist_for_each_entry_safe_reverse(node, tmp_node, force_shrink_list,",
            "\t\t\t\t\t list) {",
            "\t\tif (lru->del_from_htab(lru->del_arg, node)) {",
            "\t\t\t__bpf_lru_node_move_to_free(l, node, free_list,",
            "\t\t\t\t\t\t    tgt_free_type);",
            "\t\t\treturn 1;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "__bpf_lru_list_rotate_inactive, __bpf_lru_list_shrink_inactive, __bpf_lru_list_rotate, __bpf_lru_list_shrink",
          "description": "执行非活跃列表回收操作，通过遍历节点依据引用状态或哈希表删除条件进行节点迁移和资源缩减。",
          "similarity": 0.6052977442741394
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 22,
          "end_line": 126,
          "content": [
            "static int get_next_cpu(int cpu)",
            "{",
            "\tcpu = cpumask_next(cpu, cpu_possible_mask);",
            "\tif (cpu >= nr_cpu_ids)",
            "\t\tcpu = cpumask_first(cpu_possible_mask);",
            "\treturn cpu;",
            "}",
            "static bool bpf_lru_node_is_ref(const struct bpf_lru_node *node)",
            "{",
            "\treturn READ_ONCE(node->ref);",
            "}",
            "static void bpf_lru_node_clear_ref(struct bpf_lru_node *node)",
            "{",
            "\tWRITE_ONCE(node->ref, 0);",
            "}",
            "static void bpf_lru_list_count_inc(struct bpf_lru_list *l,",
            "\t\t\t\t   enum bpf_lru_list_type type)",
            "{",
            "\tif (type < NR_BPF_LRU_LIST_COUNT)",
            "\t\tl->counts[type]++;",
            "}",
            "static void bpf_lru_list_count_dec(struct bpf_lru_list *l,",
            "\t\t\t\t   enum bpf_lru_list_type type)",
            "{",
            "\tif (type < NR_BPF_LRU_LIST_COUNT)",
            "\t\tl->counts[type]--;",
            "}",
            "static void __bpf_lru_node_move_to_free(struct bpf_lru_list *l,",
            "\t\t\t\t\tstruct bpf_lru_node *node,",
            "\t\t\t\t\tstruct list_head *free_list,",
            "\t\t\t\t\tenum bpf_lru_list_type tgt_free_type)",
            "{",
            "\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)))",
            "\t\treturn;",
            "",
            "\t/* If the removing node is the next_inactive_rotation candidate,",
            "\t * move the next_inactive_rotation pointer also.",
            "\t */",
            "\tif (&node->list == l->next_inactive_rotation)",
            "\t\tl->next_inactive_rotation = l->next_inactive_rotation->prev;",
            "",
            "\tbpf_lru_list_count_dec(l, node->type);",
            "",
            "\tnode->type = tgt_free_type;",
            "\tlist_move(&node->list, free_list);",
            "}",
            "static void __bpf_lru_node_move_in(struct bpf_lru_list *l,",
            "\t\t\t\t   struct bpf_lru_node *node,",
            "\t\t\t\t   enum bpf_lru_list_type tgt_type)",
            "{",
            "\tif (WARN_ON_ONCE(!IS_LOCAL_LIST_TYPE(node->type)) ||",
            "\t    WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(tgt_type)))",
            "\t\treturn;",
            "",
            "\tbpf_lru_list_count_inc(l, tgt_type);",
            "\tnode->type = tgt_type;",
            "\tbpf_lru_node_clear_ref(node);",
            "\tlist_move(&node->list, &l->lists[tgt_type]);",
            "}",
            "static void __bpf_lru_node_move(struct bpf_lru_list *l,",
            "\t\t\t\tstruct bpf_lru_node *node,",
            "\t\t\t\tenum bpf_lru_list_type tgt_type)",
            "{",
            "\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)) ||",
            "\t    WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(tgt_type)))",
            "\t\treturn;",
            "",
            "\tif (node->type != tgt_type) {",
            "\t\tbpf_lru_list_count_dec(l, node->type);",
            "\t\tbpf_lru_list_count_inc(l, tgt_type);",
            "\t\tnode->type = tgt_type;",
            "\t}",
            "\tbpf_lru_node_clear_ref(node);",
            "",
            "\t/* If the moving node is the next_inactive_rotation candidate,",
            "\t * move the next_inactive_rotation pointer also.",
            "\t */",
            "\tif (&node->list == l->next_inactive_rotation)",
            "\t\tl->next_inactive_rotation = l->next_inactive_rotation->prev;",
            "",
            "\tlist_move(&node->list, &l->lists[tgt_type]);",
            "}",
            "static bool bpf_lru_list_inactive_low(const struct bpf_lru_list *l)",
            "{",
            "\treturn l->counts[BPF_LRU_LIST_T_INACTIVE] <",
            "\t\tl->counts[BPF_LRU_LIST_T_ACTIVE];",
            "}",
            "static void __bpf_lru_list_rotate_active(struct bpf_lru *lru,",
            "\t\t\t\t\t struct bpf_lru_list *l)",
            "{",
            "\tstruct list_head *active = &l->lists[BPF_LRU_LIST_T_ACTIVE];",
            "\tstruct bpf_lru_node *node, *tmp_node, *first_node;",
            "\tunsigned int i = 0;",
            "",
            "\tfirst_node = list_first_entry(active, struct bpf_lru_node, list);",
            "\tlist_for_each_entry_safe_reverse(node, tmp_node, active, list) {",
            "\t\tif (bpf_lru_node_is_ref(node))",
            "\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);",
            "\t\telse",
            "\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_INACTIVE);",
            "",
            "\t\tif (++i == lru->nr_scans || node == first_node)",
            "\t\t\tbreak;",
            "\t}",
            "}"
          ],
          "function_name": "get_next_cpu, bpf_lru_node_is_ref, bpf_lru_node_clear_ref, bpf_lru_list_count_inc, bpf_lru_list_count_dec, __bpf_lru_node_move_to_free, __bpf_lru_node_move_in, __bpf_lru_node_move, bpf_lru_list_inactive_low, __bpf_lru_list_rotate_active",
          "description": "实现LRU节点状态迁移、计数维护及列表旋转逻辑，包括节点引用判断、类型转换、活跃/非活跃列表平衡操作。",
          "similarity": 0.517046332359314
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 295,
          "end_line": 407,
          "content": [
            "static void __local_list_flush(struct bpf_lru_list *l,",
            "\t\t\t       struct bpf_lru_locallist *loc_l)",
            "{",
            "\tstruct bpf_lru_node *node, *tmp_node;",
            "",
            "\tlist_for_each_entry_safe_reverse(node, tmp_node,",
            "\t\t\t\t\t local_pending_list(loc_l), list) {",
            "\t\tif (bpf_lru_node_is_ref(node))",
            "\t\t\t__bpf_lru_node_move_in(l, node, BPF_LRU_LIST_T_ACTIVE);",
            "\t\telse",
            "\t\t\t__bpf_lru_node_move_in(l, node,",
            "\t\t\t\t\t       BPF_LRU_LIST_T_INACTIVE);",
            "\t}",
            "}",
            "static void bpf_lru_list_push_free(struct bpf_lru_list *l,",
            "\t\t\t\t   struct bpf_lru_node *node)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)))",
            "\t\treturn;",
            "",
            "\traw_spin_lock_irqsave(&l->lock, flags);",
            "\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_FREE);",
            "\traw_spin_unlock_irqrestore(&l->lock, flags);",
            "}",
            "static void bpf_lru_list_pop_free_to_local(struct bpf_lru *lru,",
            "\t\t\t\t\t   struct bpf_lru_locallist *loc_l)",
            "{",
            "\tstruct bpf_lru_list *l = &lru->common_lru.lru_list;",
            "\tstruct bpf_lru_node *node, *tmp_node;",
            "\tunsigned int nfree = 0;",
            "",
            "\traw_spin_lock(&l->lock);",
            "",
            "\t__local_list_flush(l, loc_l);",
            "",
            "\t__bpf_lru_list_rotate(lru, l);",
            "",
            "\tlist_for_each_entry_safe(node, tmp_node, &l->lists[BPF_LRU_LIST_T_FREE],",
            "\t\t\t\t list) {",
            "\t\t__bpf_lru_node_move_to_free(l, node, local_free_list(loc_l),",
            "\t\t\t\t\t    BPF_LRU_LOCAL_LIST_T_FREE);",
            "\t\tif (++nfree == lru->target_free)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tif (nfree < lru->target_free)",
            "\t\t__bpf_lru_list_shrink(lru, l, lru->target_free - nfree,",
            "\t\t\t\t      local_free_list(loc_l),",
            "\t\t\t\t      BPF_LRU_LOCAL_LIST_T_FREE);",
            "",
            "\traw_spin_unlock(&l->lock);",
            "}",
            "static void __local_list_add_pending(struct bpf_lru *lru,",
            "\t\t\t\t     struct bpf_lru_locallist *loc_l,",
            "\t\t\t\t     int cpu,",
            "\t\t\t\t     struct bpf_lru_node *node,",
            "\t\t\t\t     u32 hash)",
            "{",
            "\t*(u32 *)((void *)node + lru->hash_offset) = hash;",
            "\tnode->cpu = cpu;",
            "\tnode->type = BPF_LRU_LOCAL_LIST_T_PENDING;",
            "\tbpf_lru_node_clear_ref(node);",
            "\tlist_add(&node->list, local_pending_list(loc_l));",
            "}",
            "static void bpf_common_lru_push_free(struct bpf_lru *lru,",
            "\t\t\t\t     struct bpf_lru_node *node)",
            "{",
            "\tu8 node_type = READ_ONCE(node->type);",
            "\tunsigned long flags;",
            "",
            "\tif (WARN_ON_ONCE(node_type == BPF_LRU_LIST_T_FREE) ||",
            "\t    WARN_ON_ONCE(node_type == BPF_LRU_LOCAL_LIST_T_FREE))",
            "\t\treturn;",
            "",
            "\tif (node_type == BPF_LRU_LOCAL_LIST_T_PENDING) {",
            "\t\tstruct bpf_lru_locallist *loc_l;",
            "",
            "\t\tloc_l = per_cpu_ptr(lru->common_lru.local_list, node->cpu);",
            "",
            "\t\traw_spin_lock_irqsave(&loc_l->lock, flags);",
            "",
            "\t\tif (unlikely(node->type != BPF_LRU_LOCAL_LIST_T_PENDING)) {",
            "\t\t\traw_spin_unlock_irqrestore(&loc_l->lock, flags);",
            "\t\t\tgoto check_lru_list;",
            "\t\t}",
            "",
            "\t\tnode->type = BPF_LRU_LOCAL_LIST_T_FREE;",
            "\t\tbpf_lru_node_clear_ref(node);",
            "\t\tlist_move(&node->list, local_free_list(loc_l));",
            "",
            "\t\traw_spin_unlock_irqrestore(&loc_l->lock, flags);",
            "\t\treturn;",
            "\t}",
            "",
            "check_lru_list:",
            "\tbpf_lru_list_push_free(&lru->common_lru.lru_list, node);",
            "}",
            "static void bpf_percpu_lru_push_free(struct bpf_lru *lru,",
            "\t\t\t\t     struct bpf_lru_node *node)",
            "{",
            "\tstruct bpf_lru_list *l;",
            "\tunsigned long flags;",
            "",
            "\tl = per_cpu_ptr(lru->percpu_lru, node->cpu);",
            "",
            "\traw_spin_lock_irqsave(&l->lock, flags);",
            "",
            "\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_FREE);",
            "",
            "\traw_spin_unlock_irqrestore(&l->lock, flags);",
            "}"
          ],
          "function_name": "__local_list_flush, bpf_lru_list_push_free, bpf_lru_list_pop_free_to_local, __local_list_add_pending, bpf_common_lru_push_free, bpf_percpu_lru_push_free",
          "description": "管理Per-CPU本地列表的刷新与节点分发，实现空闲节点向本地列表迁移及跨CPU的节点调度控制。",
          "similarity": 0.47719433903694153
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 697,
          "end_line": 703,
          "content": [
            "void bpf_lru_destroy(struct bpf_lru *lru)",
            "{",
            "\tif (lru->percpu)",
            "\t\tfree_percpu(lru->percpu_lru);",
            "\telse",
            "\t\tfree_percpu(lru->common_lru.local_list);",
            "}"
          ],
          "function_name": "bpf_lru_destroy",
          "description": "该代码段实现了一个用于销毁BPF LRU结构体的函数，核心功能是释放与per-CPU关联的LRU列表内存。函数根据`percpu`标志选择性地调用`free_percpu`释放`percpu_lru`或`common_lru.local_list`。由于代码仅展示内存释放逻辑且未包含完整结构体定义，存在上下文不完整的风险。",
          "similarity": 0.4613056480884552
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/bpf_lru_list.c",
          "start_line": 1,
          "end_line": 21,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2016 Facebook",
            " */",
            "#include <linux/cpumask.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/percpu.h>",
            "",
            "#include \"bpf_lru_list.h\"",
            "",
            "#define LOCAL_FREE_TARGET\t\t(128)",
            "#define LOCAL_NR_SCANS\t\t\tLOCAL_FREE_TARGET",
            "",
            "#define PERCPU_FREE_TARGET\t\t(4)",
            "#define PERCPU_NR_SCANS\t\t\tPERCPU_FREE_TARGET",
            "",
            "/* Helpers to get the local list index */",
            "#define LOCAL_LIST_IDX(t)\t((t) - BPF_LOCAL_LIST_T_OFFSET)",
            "#define LOCAL_FREE_LIST_IDX\tLOCAL_LIST_IDX(BPF_LRU_LOCAL_LIST_T_FREE)",
            "#define LOCAL_PENDING_LIST_IDX\tLOCAL_LIST_IDX(BPF_LRU_LOCAL_LIST_T_PENDING)",
            "#define IS_LOCAL_LIST_TYPE(t)\t((t) >= BPF_LOCAL_LIST_T_OFFSET)",
            ""
          ],
          "function_name": null,
          "description": "定义BPF LRU本地列表类型及其相关宏，用于标识不同类型的本地列表节点（FREE/PENDING）及获取其索引。",
          "similarity": 0.3894323408603668
        }
      ]
    },
    {
      "source_file": "mm/list_lru.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:35:23\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `list_lru.c`\n\n---\n\n# list_lru.c 技术文档\n\n## 1. 文件概述\n\n`list_lru.c` 实现了 Linux 内核中通用的 **List-based LRU（Least Recently Used）基础设施**，用于管理可回收对象的双向链表。该机制支持按 NUMA 节点（node）和内存控制组（memcg）进行细粒度组织，便于内存压力下的高效回收。主要服务于 slab 分配器等子系统，作为 shrinker 框架的一部分，在内存紧张时协助释放非活跃对象。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct list_lru`：顶层 LRU 管理结构，包含 per-node 的 `list_lru_node`\n- `struct list_lru_node`：每个 NUMA 节点对应的 LRU 节点，含自旋锁和总项数\n- `struct list_lru_one`：实际存储对象链表和计数的单元（per-memcg per-node）\n- `struct list_lru_memcg`：当启用 `CONFIG_MEMCG` 时，为每个 memcg 存储 per-node 的 `list_lru_one`\n\n### 主要导出函数\n- `list_lru_add()` / `list_lru_add_obj()`：向 LRU 添加对象\n- `list_lru_del()` / `list_lru_del_obj()`：从 LRU 删除对象\n- `list_lru_isolate()` / `list_lru_isolate_move()`：在回收过程中隔离对象\n- `list_lru_count_one()` / `list_lru_count_node()`：查询 LRU 中对象数量\n- `list_lru_walk_one()` / `list_lru_walk_node()`：遍历并处理 LRU 中的对象（用于 shrinker 回调）\n\n### 内部辅助函数\n- `list_lru_from_memcg_idx()`：根据 memcg ID 获取对应的 `list_lru_one`\n- `__list_lru_walk_one()`：带锁的 LRU 遍历核心逻辑\n- `list_lru_register()` / `list_lru_unregister()`：注册/注销 memcg-aware 的 LRU（用于全局追踪）\n\n## 3. 关键实现\n\n### 内存控制组（memcg）支持\n- 通过 `CONFIG_MEMCG` 条件编译控制 memcg 相关逻辑\n- 使用 XArray (`lru->xa`) 动态存储每个 memcg 对应的 `list_lru_memcg` 结构\n- 每个 memcg 在每个 NUMA 节点上拥有独立的 `list_lru_one`，实现资源隔离\n- 全局 `memcg_list_lrus` 链表和 `list_lrus_mutex` 用于跟踪所有 memcg-aware 的 LRU 实例\n\n### 并发控制\n- 每个 NUMA 节点 (`list_lru_node`) 拥有独立的自旋锁 (`nlru->lock`)\n- 所有对 LRU 链表的操作（增、删、遍历）均在对应节点锁保护下进行\n- 提供 `_irq` 版本的遍历函数（`list_lru_walk_one_irq`）用于中断上下文\n\n### 回收遍历机制\n- `list_lru_walk_*` 函数接受回调函数 `isolate`，由调用者定义回收策略\n- 回调返回值控制遍历行为：\n  - `LRU_REMOVED`：成功移除\n  - `LRU_REMOVED_RETRY`：移除后需重新开始遍历（锁曾被释放）\n  - `LRU_RETRY`：未移除但需重新开始遍历\n  - `LRU_ROTATE`：将对象移到链表尾部（标记为最近使用）\n  - `LRU_SKIP`：跳过当前对象\n  - `LRU_STOP`：立即停止遍历\n- 通过 `nr_to_walk` 限制单次遍历的最大对象数，防止长时间持锁\n\n### Shrinker 集成\n- 当向空的 `list_lru_one` 添加首个对象时，调用 `set_shrinker_bit()` 标记该 memcg/node 需要被 shrinker 处理\n- `lru_shrinker_id()` 返回关联的 shrinker ID，用于通知内存回收子系统\n\n### 对象归属识别\n- `list_lru_add_obj()` / `list_lru_del_obj()` 通过 `mem_cgroup_from_slab_obj()` 自动获取对象所属的 memcg\n- 使用 `page_to_nid(virt_to_page(item))` 确定对象所在的 NUMA 节点\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/list_lru.h>`：定义核心数据结构和 API\n  - `<linux/memcontrol.h>`：memcg 相关接口（如 `memcg_kmem_id`）\n  - `\"slab.h\"` 和 `\"internal.h\"`：slab 分配器内部接口（如 `mem_cgroup_from_slab_obj`）\n- **配置依赖**：\n  - `CONFIG_MEMCG`：决定是否编译 memcg 相关代码\n  - `CONFIG_NUMA`：影响 per-node 数据结构的大小（通过 `nr_node_ids`）\n- **子系统依赖**：\n  - Slab 分配器：作为主要使用者，管理可回收 slab 对象\n  - Memory Control Group (memcg)：提供内存隔离和记账\n  - Shrinker 框架：通过 shrinker 回调触发 LRU 遍历回收\n\n## 5. 使用场景\n\n- **Slab 对象回收**：当系统内存压力大时，shrinker 通过 `list_lru_walk_*` 遍历 inactive slab 对象链表，释放可回收对象\n- **Per-memcg 内存限制**：在 cgroup 内存超限时，仅遍历该 memcg 对应的 LRU 部分，实现精确回收\n- **NUMA 感知管理**：按 NUMA 节点分离 LRU 链表，减少远程内存访问，提升性能\n- **通用 LRU 容器**：任何需要按 LRU 策略管理可回收对象的内核子系统均可使用此基础设施（如 dentry、inode 缓存等）",
      "similarity": 0.542671263217926,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "mm/list_lru.c",
          "start_line": 425,
          "end_line": 551,
          "content": [
            "static void memcg_reparent_list_lru(struct list_lru *lru,",
            "\t\t\t\t    int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tint i;",
            "",
            "\tfor_each_node(i)",
            "\t\tmemcg_reparent_list_lru_node(lru, i, src_idx, dst_memcg);",
            "",
            "\tmemcg_list_lru_free(lru, src_idx);",
            "}",
            "void memcg_reparent_list_lrus(struct mem_cgroup *memcg, struct mem_cgroup *parent)",
            "{",
            "\tstruct cgroup_subsys_state *css;",
            "\tstruct list_lru *lru;",
            "\tint src_idx = memcg->kmemcg_id;",
            "",
            "\t/*",
            "\t * Change kmemcg_id of this cgroup and all its descendants to the",
            "\t * parent's id, and then move all entries from this cgroup's list_lrus",
            "\t * to ones of the parent.",
            "\t *",
            "\t * After we have finished, all list_lrus corresponding to this cgroup",
            "\t * are guaranteed to remain empty. So we can safely free this cgroup's",
            "\t * list lrus in memcg_list_lru_free().",
            "\t *",
            "\t * Changing ->kmemcg_id to the parent can prevent memcg_list_lru_alloc()",
            "\t * from allocating list lrus for this cgroup after memcg_list_lru_free()",
            "\t * call.",
            "\t */",
            "\trcu_read_lock();",
            "\tcss_for_each_descendant_pre(css, &memcg->css) {",
            "\t\tstruct mem_cgroup *child;",
            "",
            "\t\tchild = mem_cgroup_from_css(css);",
            "\t\tWRITE_ONCE(child->kmemcg_id, parent->kmemcg_id);",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_for_each_entry(lru, &memcg_list_lrus, list)",
            "\t\tmemcg_reparent_list_lru(lru, src_idx, parent);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static inline bool memcg_list_lru_allocated(struct mem_cgroup *memcg,",
            "\t\t\t\t\t    struct list_lru *lru)",
            "{",
            "\tint idx = memcg->kmemcg_id;",
            "",
            "\treturn idx < 0 || xa_load(&lru->xa, idx);",
            "}",
            "int memcg_list_lru_alloc(struct mem_cgroup *memcg, struct list_lru *lru,",
            "\t\t\t gfp_t gfp)",
            "{",
            "\tint i;",
            "\tunsigned long flags;",
            "\tstruct list_lru_memcg_table {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tstruct mem_cgroup *memcg;",
            "\t} *table;",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "",
            "\tif (!list_lru_memcg_aware(lru) || memcg_list_lru_allocated(memcg, lru))",
            "\t\treturn 0;",
            "",
            "\tgfp &= GFP_RECLAIM_MASK;",
            "\ttable = kmalloc_array(memcg->css.cgroup->level, sizeof(*table), gfp);",
            "\tif (!table)",
            "\t\treturn -ENOMEM;",
            "",
            "\t/*",
            "\t * Because the list_lru can be reparented to the parent cgroup's",
            "\t * list_lru, we should make sure that this cgroup and all its",
            "\t * ancestors have allocated list_lru_memcg.",
            "\t */",
            "\tfor (i = 0; memcg; memcg = parent_mem_cgroup(memcg), i++) {",
            "\t\tif (memcg_list_lru_allocated(memcg, lru))",
            "\t\t\tbreak;",
            "",
            "\t\ttable[i].memcg = memcg;",
            "\t\ttable[i].mlru = memcg_init_list_lru_one(gfp);",
            "\t\tif (!table[i].mlru) {",
            "\t\t\twhile (i--)",
            "\t\t\t\tkfree(table[i].mlru);",
            "\t\t\tkfree(table);",
            "\t\t\treturn -ENOMEM;",
            "\t\t}",
            "\t}",
            "",
            "\txas_lock_irqsave(&xas, flags);",
            "\twhile (i--) {",
            "\t\tint index = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\tstruct list_lru_memcg *mlru = table[i].mlru;",
            "",
            "\t\txas_set(&xas, index);",
            "retry:",
            "\t\tif (unlikely(index < 0 || xas_error(&xas) || xas_load(&xas))) {",
            "\t\t\tkfree(mlru);",
            "\t\t} else {",
            "\t\t\txas_store(&xas, mlru);",
            "\t\t\tif (xas_error(&xas) == -ENOMEM) {",
            "\t\t\t\txas_unlock_irqrestore(&xas, flags);",
            "\t\t\t\tif (xas_nomem(&xas, gfp))",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\txas_lock_irqsave(&xas, flags);",
            "\t\t\t\t/*",
            "\t\t\t\t * The xas lock has been released, this memcg",
            "\t\t\t\t * can be reparented before us. So reload",
            "\t\t\t\t * memcg id. More details see the comments",
            "\t\t\t\t * in memcg_reparent_list_lrus().",
            "\t\t\t\t */",
            "\t\t\t\tindex = READ_ONCE(table[i].memcg->kmemcg_id);",
            "\t\t\t\tif (index < 0)",
            "\t\t\t\t\txas_set_err(&xas, 0);",
            "\t\t\t\telse if (!xas_error(&xas) && index != xas.xa_index)",
            "\t\t\t\t\txas_set(&xas, index);",
            "\t\t\t\tgoto retry;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "\t/* xas_nomem() is used to free memory instead of memory allocation. */",
            "\tif (xas.xa_alloc)",
            "\t\txas_nomem(&xas, gfp);",
            "\txas_unlock_irqrestore(&xas, flags);",
            "\tkfree(table);",
            "",
            "\treturn xas_error(&xas);",
            "}"
          ],
          "function_name": "memcg_reparent_list_lru, memcg_reparent_list_lrus, memcg_list_lru_allocated, memcg_list_lru_alloc",
          "description": "实现内存组层级间的LRU列表迁移与分配机制，包含递归子组处理、动态分配/释放LRU结构体及冲突解决逻辑。",
          "similarity": 0.5331941246986389
        },
        {
          "chunk_id": 5,
          "file_path": "mm/list_lru.c",
          "start_line": 556,
          "end_line": 605,
          "content": [
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "}",
            "int __list_lru_init(struct list_lru *lru, bool memcg_aware,",
            "\t\t    struct lock_class_key *key, struct shrinker *shrinker)",
            "{",
            "\tint i;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (shrinker)",
            "\t\tlru->shrinker_id = shrinker->id;",
            "\telse",
            "\t\tlru->shrinker_id = -1;",
            "#endif",
            "",
            "\tlru->node = kcalloc(nr_node_ids, sizeof(*lru->node), GFP_KERNEL);",
            "\tif (!lru->node)",
            "\t\treturn -ENOMEM;",
            "",
            "\tfor_each_node(i) {",
            "\t\tspin_lock_init(&lru->node[i].lock);",
            "\t\tif (key)",
            "\t\t\tlockdep_set_class(&lru->node[i].lock, key);",
            "\t\tinit_one_lru(&lru->node[i].lru);",
            "\t}",
            "",
            "\tmemcg_init_list_lru(lru, memcg_aware);",
            "\tlist_lru_register(lru);",
            "",
            "\treturn 0;",
            "}",
            "void list_lru_destroy(struct list_lru *lru)",
            "{",
            "\t/* Already destroyed or not yet initialized? */",
            "\tif (!lru->node)",
            "\t\treturn;",
            "",
            "\tlist_lru_unregister(lru);",
            "",
            "\tmemcg_destroy_list_lru(lru);",
            "\tkfree(lru->node);",
            "\tlru->node = NULL;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tlru->shrinker_id = -1;",
            "#endif",
            "}"
          ],
          "function_name": "memcg_init_list_lru, memcg_destroy_list_lru, __list_lru_init, list_lru_destroy",
          "description": "该代码段实现了基于内存控制组（MEMCG）的LRU列表管理功能。  \n`__list_lru_init` 初始化 `list_lru` 结构体并注册到系统，其中包含 MEMCG 相关的 shrinker ID 设置及节点锁初始化；`list_lru_destroy` 反向清理资源，但 `memcg_init_list_lru` 和 `memcg_destroy_list_lru` 的具体实现缺失，上下文不完整。",
          "similarity": 0.5051091313362122
        },
        {
          "chunk_id": 1,
          "file_path": "mm/list_lru.c",
          "start_line": 22,
          "end_line": 129,
          "content": [
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn lru->memcg_aware;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_add(&lru->list, &memcg_list_lrus);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\tmutex_lock(&list_lrus_mutex);",
            "\tlist_del(&lru->list);",
            "\tmutex_unlock(&list_lrus_mutex);",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn lru->shrinker_id;",
            "}",
            "static void list_lru_register(struct list_lru *lru)",
            "{",
            "}",
            "static void list_lru_unregister(struct list_lru *lru)",
            "{",
            "}",
            "static int lru_shrinker_id(struct list_lru *lru)",
            "{",
            "\treturn -1;",
            "}",
            "static inline bool list_lru_memcg_aware(struct list_lru *lru)",
            "{",
            "\treturn false;",
            "}",
            "bool list_lru_add(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_add_tail(item, &l->list);",
            "\t\t/* Set shrinker bit if the first element was added */",
            "\t\tif (!l->nr_items++)",
            "\t\t\tset_shrinker_bit(memcg, nid, lru_shrinker_id(lru));",
            "\t\tnlru->nr_items++;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_add_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_add(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_add(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}",
            "bool list_lru_del(struct list_lru *lru, struct list_head *item, int nid,",
            "\t\t    struct mem_cgroup *memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tstruct list_lru_one *l;",
            "",
            "\tspin_lock(&nlru->lock);",
            "\tif (!list_empty(item)) {",
            "\t\tl = list_lru_from_memcg_idx(lru, nid, memcg_kmem_id(memcg));",
            "\t\tlist_del_init(item);",
            "\t\tl->nr_items--;",
            "\t\tnlru->nr_items--;",
            "\t\tspin_unlock(&nlru->lock);",
            "\t\treturn true;",
            "\t}",
            "\tspin_unlock(&nlru->lock);",
            "\treturn false;",
            "}",
            "bool list_lru_del_obj(struct list_lru *lru, struct list_head *item)",
            "{",
            "\tbool ret;",
            "\tint nid = page_to_nid(virt_to_page(item));",
            "",
            "\tif (list_lru_memcg_aware(lru)) {",
            "\t\trcu_read_lock();",
            "\t\tret = list_lru_del(lru, item, nid, mem_cgroup_from_slab_obj(item));",
            "\t\trcu_read_unlock();",
            "\t} else {",
            "\t\tret = list_lru_del(lru, item, nid, NULL);",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "list_lru_memcg_aware, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_register, list_lru_unregister, lru_shrinker_id, list_lru_memcg_aware, list_lru_add, list_lru_add_obj, list_lru_del, list_lru_del_obj",
          "description": "实现了LRU列表的添加/删除操作，支持MemCG感知的节点和内存组粒度管理，包含处理多核、内存组切换及RCU安全访问的逻辑。",
          "similarity": 0.48006337881088257
        },
        {
          "chunk_id": 3,
          "file_path": "mm/list_lru.c",
          "start_line": 289,
          "end_line": 400,
          "content": [
            "unsigned long",
            "list_lru_walk_one_irq(struct list_lru *lru, int nid, struct mem_cgroup *memcg,",
            "\t\t      list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t      unsigned long *nr_to_walk)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tunsigned long ret;",
            "",
            "\tspin_lock_irq(&nlru->lock);",
            "\tret = __list_lru_walk_one(lru, nid, memcg_kmem_id(memcg), isolate,",
            "\t\t\t\t  cb_arg, nr_to_walk);",
            "\tspin_unlock_irq(&nlru->lock);",
            "\treturn ret;",
            "}",
            "unsigned long list_lru_walk_node(struct list_lru *lru, int nid,",
            "\t\t\t\t list_lru_walk_cb isolate, void *cb_arg,",
            "\t\t\t\t unsigned long *nr_to_walk)",
            "{",
            "\tlong isolated = 0;",
            "",
            "\tisolated += list_lru_walk_one(lru, nid, NULL, isolate, cb_arg,",
            "\t\t\t\t      nr_to_walk);",
            "",
            "#ifdef CONFIG_MEMCG",
            "\tif (*nr_to_walk > 0 && list_lru_memcg_aware(lru)) {",
            "\t\tstruct list_lru_memcg *mlru;",
            "\t\tunsigned long index;",
            "",
            "\t\txa_for_each(&lru->xa, index, mlru) {",
            "\t\t\tstruct list_lru_node *nlru = &lru->node[nid];",
            "",
            "\t\t\tspin_lock(&nlru->lock);",
            "\t\t\tisolated += __list_lru_walk_one(lru, nid, index,",
            "\t\t\t\t\t\t\tisolate, cb_arg,",
            "\t\t\t\t\t\t\tnr_to_walk);",
            "\t\t\tspin_unlock(&nlru->lock);",
            "",
            "\t\t\tif (*nr_to_walk <= 0)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "#endif",
            "",
            "\treturn isolated;",
            "}",
            "static void init_one_lru(struct list_lru_one *l)",
            "{",
            "\tINIT_LIST_HEAD(&l->list);",
            "\tl->nr_items = 0;",
            "}",
            "static void memcg_list_lru_free(struct list_lru *lru, int src_idx)",
            "{",
            "\tstruct list_lru_memcg *mlru = xa_erase_irq(&lru->xa, src_idx);",
            "",
            "\t/*",
            "\t * The __list_lru_walk_one() can walk the list of this node.",
            "\t * We need kvfree_rcu() here. And the walking of the list",
            "\t * is under lru->node[nid]->lock, which can serve as a RCU",
            "\t * read-side critical section.",
            "\t */",
            "\tif (mlru)",
            "\t\tkvfree_rcu(mlru, rcu);",
            "}",
            "static inline void memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)",
            "{",
            "\tif (memcg_aware)",
            "\t\txa_init_flags(&lru->xa, XA_FLAGS_LOCK_IRQ);",
            "\tlru->memcg_aware = memcg_aware;",
            "}",
            "static void memcg_destroy_list_lru(struct list_lru *lru)",
            "{",
            "\tXA_STATE(xas, &lru->xa, 0);",
            "\tstruct list_lru_memcg *mlru;",
            "",
            "\tif (!list_lru_memcg_aware(lru))",
            "\t\treturn;",
            "",
            "\txas_lock_irq(&xas);",
            "\txas_for_each(&xas, mlru, ULONG_MAX) {",
            "\t\tkfree(mlru);",
            "\t\txas_store(&xas, NULL);",
            "\t}",
            "\txas_unlock_irq(&xas);",
            "}",
            "static void memcg_reparent_list_lru_node(struct list_lru *lru, int nid,",
            "\t\t\t\t\t int src_idx, struct mem_cgroup *dst_memcg)",
            "{",
            "\tstruct list_lru_node *nlru = &lru->node[nid];",
            "\tint dst_idx = dst_memcg->kmemcg_id;",
            "\tstruct list_lru_one *src, *dst;",
            "",
            "\t/*",
            "\t * Since list_lru_{add,del} may be called under an IRQ-safe lock,",
            "\t * we have to use IRQ-safe primitives here to avoid deadlock.",
            "\t */",
            "\tspin_lock_irq(&nlru->lock);",
            "",
            "\tsrc = list_lru_from_memcg_idx(lru, nid, src_idx);",
            "\tif (!src)",
            "\t\tgoto out;",
            "\tdst = list_lru_from_memcg_idx(lru, nid, dst_idx);",
            "",
            "\tlist_splice_init(&src->list, &dst->list);",
            "",
            "\tif (src->nr_items) {",
            "\t\tdst->nr_items += src->nr_items;",
            "\t\tset_shrinker_bit(dst_memcg, nid, lru_shrinker_id(lru));",
            "\t\tsrc->nr_items = 0;",
            "\t}",
            "out:",
            "\tspin_unlock_irq(&nlru->lock);",
            "}"
          ],
          "function_name": "list_lru_walk_one_irq, list_lru_walk_node, init_one_lru, memcg_list_lru_free, memcg_init_list_lru, memcg_destroy_list_lru, memcg_reparent_list_lru_node",
          "description": "包含LRU节点初始化、内存组间列表迁移、资源释放等高级操作，涉及XA表管理、中断安全锁操作及内存组重新归属处理。",
          "similarity": 0.4731229543685913
        },
        {
          "chunk_id": 0,
          "file_path": "mm/list_lru.c",
          "start_line": 1,
          "end_line": 21,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/*",
            " * Copyright (c) 2013 Red Hat, Inc. and Parallels Inc. All rights reserved.",
            " * Authors: David Chinner and Glauber Costa",
            " *",
            " * Generic LRU infrastructure",
            " */",
            "#include <linux/kernel.h>",
            "#include <linux/module.h>",
            "#include <linux/mm.h>",
            "#include <linux/list_lru.h>",
            "#include <linux/slab.h>",
            "#include <linux/mutex.h>",
            "#include <linux/memcontrol.h>",
            "#include \"slab.h\"",
            "#include \"internal.h\"",
            "",
            "#ifdef CONFIG_MEMCG",
            "static LIST_HEAD(memcg_list_lrus);",
            "static DEFINE_MUTEX(list_lrus_mutex);",
            ""
          ],
          "function_name": null,
          "description": "定义了支持内存控制组（MemCG）的LRU基础设施，声明了全局链表头memcg_list_lrus和互斥锁list_lrus_mutex，用于管理MemCG环境下的LRU列表注册与注销操作。",
          "similarity": 0.4497831165790558
        }
      ]
    },
    {
      "source_file": "mm/swap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:25:49\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `swap.c`\n\n---\n\n# swap.c 技术文档\n\n## 1. 文件概述\n\n`swap.c` 是 Linux 内核内存管理子系统（MM）中的核心文件之一，主要负责页面回收（page reclaim）、LRU（Least Recently Used）链表管理、页面释放以及与交换（swap）机制相关的底层支持逻辑。尽管文件名为 `swap.c`，但其功能不仅限于交换，而是涵盖了通用的页面生命周期管理、LRU 链表操作、页面引用计数释放、可回收性判断等关键内存管理任务。该文件为页面缓存（page cache）、匿名页（anonymous pages）和大页（huge pages）提供统一的释放与 LRU 管理接口。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `page_cluster`：控制一次 I/O 操作中尝试换入/换出的页面数量（以 2 的幂表示），默认值由系统配置决定。\n- `page_cluster_max`：`page_cluster` 的最大允许值（31，即最多 2^31 页，实际受架构限制）。\n\n### 主要数据结构\n- `struct lru_rotate`：每个 CPU 私有的结构，用于在中断禁用上下文中批量处理需移至 LRU 链表尾部的页面（如 `folio_rotate_reclaimable` 场景）。\n- `struct cpu_fbatches`：每个 CPU 私有的 folio 批处理结构，包含多个 folio_batch，用于高效地向 LRU 链表添加、停用或激活页面，避免频繁获取 LRU 锁。\n\n### 主要函数\n- `__folio_put()`：释放一个 folio 的核心函数，根据 folio 类型（设备内存、大页、普通页）调用相应的释放路径。\n- `put_pages_list()`：批量释放通过 `lru` 字段链接的页面列表，常用于网络子系统或 compound page 释放。\n- `lru_add_fn()`：将 folio 添加到对应 LRU 链表的回调函数，处理可回收性（evictable/unevictable）状态转换和统计计数。\n- `folio_batch_move_lru()`：批量执行 LRU 操作（如添加、移动），在持有 LRU 锁期间完成所有 folio 的处理。\n- `folio_rotate_reclaimable()`：在写回完成后，若页面仍可回收，则将其移至 inactive LRU 链表尾部，以延迟其被回收的时间。\n- `lru_note_cost()`：记录 LRU 扫描过程中的 I/O 和旋转（rotation）成本，用于后续调整 anon/file LRU 的扫描比例。\n\n## 3. 关键实现\n\n### LRU 批处理机制\n为减少 LRU 锁竞争，内核采用 per-CPU 批处理（`folio_batch`）方式暂存待处理的 folio。当批处理满或遇到大页（`folio_test_large`）时，才批量获取 LRU 锁并执行操作（如 `lru_add_fn`）。这显著提升了高并发场景下的性能。\n\n### 可回收性管理\n页面是否可回收由 `folio_evictable()` 判断，主要依据是否被 mlock 锁定。在添加到 LRU 时：\n- 若页面变为可回收（原为 unevictable），则增加 `UNEVICTABLE_PGRESCUED` 统计；\n- 若页面不可回收，则清除 active 标志，设置 unevictable 标志，并重置 `mlock_count`，同时增加 `UNEVICTABLE_PGCULLED` 统计。\n\n### 页面释放路径\n`__folio_put()` 是 folio 引用计数归零后的释放入口：\n1. 设备内存 folio 调用 `free_zone_device_folio()`\n2. 大页 folio 调用 `free_huge_folio()`\n3. 普通 folio 先从 LRU 移除（若在 LRU 上），然后解绑内存控制组（memcg），最后调用 `free_unref_page()` 释放到伙伴系统。\n\n### LRU 旋转优化\n`folio_rotate_reclaimable()` 在写回结束时，若页面干净且未锁定，则将其移至 inactive LRU 尾部。此操作通过 per-CPU 的 `lru_rotate` 批处理完成，仅在必要时获取 LRU 锁，避免影响写回关键路径性能。\n\n### 成本跟踪\n`lru_note_cost()` 通过累加 `nr_io * SWAP_CLUSTER_MAX + nr_rotated` 来量化扫描成本，用于动态调整匿名页与文件页 LRU 的扫描比例，优化内存回收效率。\n\n## 4. 依赖关系\n\n- **内存管理核心**：依赖 `<linux/mm.h>`、`<linux/pagevec.h>`、`\"internal.h\"` 等，使用伙伴系统、LRU 框架、内存控制组（memcg）等基础组件。\n- **交换子系统**：虽不直接实现 swap read/write，但为 `vmscan.c` 中的页面回收提供 LRU 操作接口，是 swap 机制的支撑模块。\n- **大页支持**：通过 `hugetlb.h` 与大页子系统交互，特殊处理大页释放。\n- **设备内存**：通过 `memremap.h` 支持持久内存（pmem）等 zone device 页面的释放。\n- **跟踪与统计**：使用 tracepoint（`trace/events/pagemap.h`）和 VM 统计（`kernel_stat.h`）进行性能分析。\n- **SMP 支持**：大量使用 per-CPU 变量（`DEFINE_PER_CPU`）和本地锁（`local_lock_t`）优化多核性能。\n\n## 5. 使用场景\n\n- **页面回收（Reclaim）**：当内存压力触发 kswapd 或 direct reclaim 时，`vmscan.c` 调用本文件的 LRU 操作函数来隔离、释放页面。\n- **页面缓存释放**：文件系统或网络子系统在释放 page cache 页面时，通过 `__folio_put()` 或 `put_pages_list()` 触发 LRU 移除和内存释放。\n- **写回完成处理**：块设备或文件系统在完成脏页写回后，调用 `folio_rotate_reclaimable()` 更新页面在 LRU 中的位置。\n- **内存控制组（cgroup）**：memcg 回收内存时，复用本文件的 LRU 批处理和 folio 释放逻辑。\n- **大页与设备内存管理**：透明大页（THP）或持久内存应用释放页面时，通过统一的 `__folio_put()` 接口分发到专用释放函数。\n- **系统调优**：管理员通过 `/proc/sys/vm/page-cluster` 调整 `page_cluster` 值，影响 swap 和 page cache 的 I/O 批量大小。",
      "similarity": 0.5382083058357239,
      "chunks": [
        {
          "chunk_id": 6,
          "file_path": "mm/swap.c",
          "start_line": 781,
          "end_line": 896,
          "content": [
            "void lru_add_drain_cpu_zone(struct zone *zone)",
            "{",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tlru_add_drain_cpu(smp_processor_id());",
            "\tdrain_local_pages(zone);",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "\tmlock_drain_local();",
            "}",
            "static void lru_add_drain_per_cpu(struct work_struct *dummy)",
            "{",
            "\tlru_add_and_bh_lrus_drain();",
            "}",
            "static bool cpu_needs_drain(unsigned int cpu)",
            "{",
            "\tstruct cpu_fbatches *fbatches = &per_cpu(cpu_fbatches, cpu);",
            "",
            "\t/* Check these in order of likelihood that they're not zero */",
            "\treturn folio_batch_count(&fbatches->lru_add) ||",
            "\t\tdata_race(folio_batch_count(&per_cpu(lru_rotate.fbatch, cpu))) ||",
            "\t\tfolio_batch_count(&fbatches->lru_deactivate_file) ||",
            "\t\tfolio_batch_count(&fbatches->lru_deactivate) ||",
            "\t\tfolio_batch_count(&fbatches->lru_lazyfree) ||",
            "\t\tfolio_batch_count(&fbatches->activate) ||",
            "\t\tneed_mlock_drain(cpu) ||",
            "\t\thas_bh_in_lru(cpu, NULL);",
            "}",
            "static inline void __lru_add_drain_all(bool force_all_cpus)",
            "{",
            "\t/*",
            "\t * lru_drain_gen - Global pages generation number",
            "\t *",
            "\t * (A) Definition: global lru_drain_gen = x implies that all generations",
            "\t *     0 < n <= x are already *scheduled* for draining.",
            "\t *",
            "\t * This is an optimization for the highly-contended use case where a",
            "\t * user space workload keeps constantly generating a flow of pages for",
            "\t * each CPU.",
            "\t */",
            "\tstatic unsigned int lru_drain_gen;",
            "\tstatic struct cpumask has_work;",
            "\tstatic DEFINE_MUTEX(lock);",
            "\tunsigned cpu, this_gen;",
            "",
            "\t/*",
            "\t * Make sure nobody triggers this path before mm_percpu_wq is fully",
            "\t * initialized.",
            "\t */",
            "\tif (WARN_ON(!mm_percpu_wq))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Guarantee folio_batch counter stores visible by this CPU",
            "\t * are visible to other CPUs before loading the current drain",
            "\t * generation.",
            "\t */",
            "\tsmp_mb();",
            "",
            "\t/*",
            "\t * (B) Locally cache global LRU draining generation number",
            "\t *",
            "\t * The read barrier ensures that the counter is loaded before the mutex",
            "\t * is taken. It pairs with smp_mb() inside the mutex critical section",
            "\t * at (D).",
            "\t */",
            "\tthis_gen = smp_load_acquire(&lru_drain_gen);",
            "",
            "\tmutex_lock(&lock);",
            "",
            "\t/*",
            "\t * (C) Exit the draining operation if a newer generation, from another",
            "\t * lru_add_drain_all(), was already scheduled for draining. Check (A).",
            "\t */",
            "\tif (unlikely(this_gen != lru_drain_gen && !force_all_cpus))",
            "\t\tgoto done;",
            "",
            "\t/*",
            "\t * (D) Increment global generation number",
            "\t *",
            "\t * Pairs with smp_load_acquire() at (B), outside of the critical",
            "\t * section. Use a full memory barrier to guarantee that the",
            "\t * new global drain generation number is stored before loading",
            "\t * folio_batch counters.",
            "\t *",
            "\t * This pairing must be done here, before the for_each_online_cpu loop",
            "\t * below which drains the page vectors.",
            "\t *",
            "\t * Let x, y, and z represent some system CPU numbers, where x < y < z.",
            "\t * Assume CPU #z is in the middle of the for_each_online_cpu loop",
            "\t * below and has already reached CPU #y's per-cpu data. CPU #x comes",
            "\t * along, adds some pages to its per-cpu vectors, then calls",
            "\t * lru_add_drain_all().",
            "\t *",
            "\t * If the paired barrier is done at any later step, e.g. after the",
            "\t * loop, CPU #x will just exit at (C) and miss flushing out all of its",
            "\t * added pages.",
            "\t */",
            "\tWRITE_ONCE(lru_drain_gen, lru_drain_gen + 1);",
            "\tsmp_mb();",
            "",
            "\tcpumask_clear(&has_work);",
            "\tfor_each_online_cpu(cpu) {",
            "\t\tstruct work_struct *work = &per_cpu(lru_add_drain_work, cpu);",
            "",
            "\t\tif (cpu_needs_drain(cpu)) {",
            "\t\t\tINIT_WORK(work, lru_add_drain_per_cpu);",
            "\t\t\tqueue_work_on(cpu, mm_percpu_wq, work);",
            "\t\t\t__cpumask_set_cpu(cpu, &has_work);",
            "\t\t}",
            "\t}",
            "",
            "\tfor_each_cpu(cpu, &has_work)",
            "\t\tflush_work(&per_cpu(lru_add_drain_work, cpu));",
            "",
            "done:",
            "\tmutex_unlock(&lock);",
            "}"
          ],
          "function_name": "lru_add_drain_cpu_zone, lru_add_drain_per_cpu, cpu_needs_drain, __lru_add_drain_all",
          "description": "__lru_add_drain_all 管理全局LRU排水过程，通过generation-based机制避免重复处理，利用互斥锁和smp_mb屏障确保跨CPU数据可见性，并调度工作队列处理各CPU的待处理页面批次。",
          "similarity": 0.5192455053329468
        },
        {
          "chunk_id": 1,
          "file_path": "mm/swap.c",
          "start_line": 77,
          "end_line": 190,
          "content": [
            "static void __page_cache_release(struct folio *folio, struct lruvec **lruvecp,",
            "\t\tunsigned long *flagsp)",
            "{",
            "\tif (folio_test_lru(folio)) {",
            "\t\tfolio_lruvec_relock_irqsave(folio, lruvecp, flagsp);",
            "\t\tlruvec_del_folio(*lruvecp, folio);",
            "\t\t__folio_clear_lru_flags(folio);",
            "\t}",
            "}",
            "static void page_cache_release(struct folio *folio)",
            "{",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags;",
            "",
            "\t__page_cache_release(folio, &lruvec, &flags);",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "}",
            "void __folio_put(struct folio *folio)",
            "{",
            "\tif (unlikely(folio_is_zone_device(folio))) {",
            "\t\tfree_zone_device_folio(folio);",
            "\t\treturn;",
            "\t} else if (folio_test_hugetlb(folio)) {",
            "\t\tfree_huge_folio(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tpage_cache_release(folio);",
            "\tfolio_unqueue_deferred_split(folio);",
            "\tmem_cgroup_uncharge(folio);",
            "\tfree_unref_page(&folio->page, folio_order(folio));",
            "}",
            "void put_pages_list(struct list_head *pages)",
            "{",
            "\tstruct folio_batch fbatch;",
            "\tstruct folio *folio, *next;",
            "",
            "\tfolio_batch_init(&fbatch);",
            "\tlist_for_each_entry_safe(folio, next, pages, lru) {",
            "\t\tif (!folio_put_testzero(folio))",
            "\t\t\tcontinue;",
            "\t\tif (folio_test_hugetlb(folio)) {",
            "\t\t\tfree_huge_folio(folio);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\t/* LRU flag must be clear because it's passed using the lru */",
            "\t\tif (folio_batch_add(&fbatch, folio) > 0)",
            "\t\t\tcontinue;",
            "\t\tfree_unref_folios(&fbatch);",
            "\t}",
            "",
            "\tif (fbatch.nr)",
            "\t\tfree_unref_folios(&fbatch);",
            "\tINIT_LIST_HEAD(pages);",
            "}",
            "static void lru_add_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tint was_unevictable = folio_test_clear_unevictable(folio);",
            "\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\tVM_BUG_ON_FOLIO(folio_test_lru(folio), folio);",
            "",
            "\t/*",
            "\t * Is an smp_mb__after_atomic() still required here, before",
            "\t * folio_evictable() tests the mlocked flag, to rule out the possibility",
            "\t * of stranding an evictable folio on an unevictable LRU?  I think",
            "\t * not, because __munlock_folio() only clears the mlocked flag",
            "\t * while the LRU lock is held.",
            "\t *",
            "\t * (That is not true of __page_cache_release(), and not necessarily",
            "\t * true of folios_put(): but those only clear the mlocked flag after",
            "\t * folio_put_testzero() has excluded any other users of the folio.)",
            "\t */",
            "\tif (folio_evictable(folio)) {",
            "\t\tif (was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGRESCUED, nr_pages);",
            "\t} else {",
            "\t\tfolio_clear_active(folio);",
            "\t\tfolio_set_unevictable(folio);",
            "\t\t/*",
            "\t\t * folio->mlock_count = !!folio_test_mlocked(folio)?",
            "\t\t * But that leaves __mlock_folio() in doubt whether another",
            "\t\t * actor has already counted the mlock or not.  Err on the",
            "\t\t * safe side, underestimate, let page reclaim fix it, rather",
            "\t\t * than leaving a page on the unevictable LRU indefinitely.",
            "\t\t */",
            "\t\tfolio->mlock_count = 0;",
            "\t\tif (!was_unevictable)",
            "\t\t\t__count_vm_events(UNEVICTABLE_PGCULLED, nr_pages);",
            "\t}",
            "",
            "\tlruvec_add_folio(lruvec, folio);",
            "\ttrace_mm_lru_insertion(folio);",
            "}",
            "static void folio_batch_move_lru(struct folio_batch *fbatch, move_fn_t move_fn)",
            "{",
            "\tint i;",
            "\tstruct lruvec *lruvec = NULL;",
            "\tunsigned long flags = 0;",
            "",
            "\tfor (i = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "",
            "\t\tfolio_lruvec_relock_irqsave(folio, &lruvec, &flags);",
            "\t\tmove_fn(lruvec, folio);",
            "",
            "\t\tfolio_set_lru(folio);",
            "\t}",
            "",
            "\tif (lruvec)",
            "\t\tunlock_page_lruvec_irqrestore(lruvec, flags);",
            "\tfolios_put(fbatch);",
            "}"
          ],
          "function_name": "__page_cache_release, page_cache_release, __folio_put, put_pages_list, lru_add_fn, folio_batch_move_lru",
          "description": "实现了页面缓存释放和LRU列表维护逻辑，包含__page_cache_release用于从LRU列表移除页面，page_cache_release处理普通页面释放流程，__folio_put负责释放非设备映射和大页，put_pages_list批量处理页面释放，lru_add_fn将页面添加到LRU列表并根据是否可交换设置相应标志。",
          "similarity": 0.5132810473442078
        },
        {
          "chunk_id": 2,
          "file_path": "mm/swap.c",
          "start_line": 211,
          "end_line": 317,
          "content": [
            "static void folio_batch_add_and_move(struct folio_batch *fbatch,",
            "\t\tstruct folio *folio, move_fn_t move_fn)",
            "{",
            "\tif (folio_batch_add(fbatch, folio) && !folio_test_large(folio) &&",
            "\t    !lru_cache_disabled())",
            "\t\treturn;",
            "\tfolio_batch_move_lru(fbatch, move_fn);",
            "}",
            "static void lru_move_tail_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (!folio_test_unevictable(folio)) {",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_clear_active(folio);",
            "\t\tlruvec_add_folio_tail(lruvec, folio);",
            "\t\t__count_vm_events(PGROTATED, folio_nr_pages(folio));",
            "\t}",
            "}",
            "void folio_rotate_reclaimable(struct folio *folio)",
            "{",
            "\tif (!folio_test_locked(folio) && !folio_test_dirty(folio) &&",
            "\t    !folio_test_unevictable(folio)) {",
            "\t\tstruct folio_batch *fbatch;",
            "\t\tunsigned long flags;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock_irqsave(&lru_rotate.lock, flags);",
            "\t\tfbatch = this_cpu_ptr(&lru_rotate.fbatch);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, lru_move_tail_fn);",
            "\t\tlocal_unlock_irqrestore(&lru_rotate.lock, flags);",
            "\t}",
            "}",
            "void lru_note_cost(struct lruvec *lruvec, bool file,",
            "\t\t   unsigned int nr_io, unsigned int nr_rotated)",
            "{",
            "\tunsigned long cost;",
            "",
            "\t/*",
            "\t * Reflect the relative cost of incurring IO and spending CPU",
            "\t * time on rotations. This doesn't attempt to make a precise",
            "\t * comparison, it just says: if reloads are about comparable",
            "\t * between the LRU lists, or rotations are overwhelmingly",
            "\t * different between them, adjust scan balance for CPU work.",
            "\t */",
            "\tcost = nr_io * SWAP_CLUSTER_MAX + nr_rotated;",
            "",
            "\tdo {",
            "\t\tunsigned long lrusize;",
            "",
            "\t\t/*",
            "\t\t * Hold lruvec->lru_lock is safe here, since",
            "\t\t * 1) The pinned lruvec in reclaim, or",
            "\t\t * 2) From a pre-LRU page during refault (which also holds the",
            "\t\t *    rcu lock, so would be safe even if the page was on the LRU",
            "\t\t *    and could move simultaneously to a new lruvec).",
            "\t\t */",
            "\t\tspin_lock_irq(&lruvec->lru_lock);",
            "\t\t/* Record cost event */",
            "\t\tif (file)",
            "\t\t\tlruvec->file_cost += cost;",
            "\t\telse",
            "\t\t\tlruvec->anon_cost += cost;",
            "",
            "\t\t/*",
            "\t\t * Decay previous events",
            "\t\t *",
            "\t\t * Because workloads change over time (and to avoid",
            "\t\t * overflow) we keep these statistics as a floating",
            "\t\t * average, which ends up weighing recent refaults",
            "\t\t * more than old ones.",
            "\t\t */",
            "\t\tlrusize = lruvec_page_state(lruvec, NR_INACTIVE_ANON) +",
            "\t\t\t  lruvec_page_state(lruvec, NR_ACTIVE_ANON) +",
            "\t\t\t  lruvec_page_state(lruvec, NR_INACTIVE_FILE) +",
            "\t\t\t  lruvec_page_state(lruvec, NR_ACTIVE_FILE);",
            "",
            "\t\tif (lruvec->file_cost + lruvec->anon_cost > lrusize / 4) {",
            "\t\t\tlruvec->file_cost /= 2;",
            "\t\t\tlruvec->anon_cost /= 2;",
            "\t\t}",
            "\t\tspin_unlock_irq(&lruvec->lru_lock);",
            "\t} while ((lruvec = parent_lruvec(lruvec)));",
            "}",
            "void lru_note_cost_refault(struct folio *folio)",
            "{",
            "\tlru_note_cost(folio_lruvec(folio), folio_is_file_lru(folio),",
            "\t\t      folio_nr_pages(folio), 0);",
            "}",
            "static void folio_activate_fn(struct lruvec *lruvec, struct folio *folio)",
            "{",
            "\tif (!folio_test_active(folio) && !folio_test_unevictable(folio)) {",
            "\t\tlong nr_pages = folio_nr_pages(folio);",
            "",
            "\t\tlruvec_del_folio(lruvec, folio);",
            "\t\tfolio_set_active(folio);",
            "\t\tlruvec_add_folio(lruvec, folio);",
            "\t\ttrace_mm_lru_activate(folio);",
            "",
            "\t\t__count_vm_events(PGACTIVATE, nr_pages);",
            "\t\t__count_memcg_events(lruvec_memcg(lruvec), PGACTIVATE,",
            "\t\t\t\t     nr_pages);",
            "\t}",
            "}"
          ],
          "function_name": "folio_batch_add_and_move, lru_move_tail_fn, folio_rotate_reclaimable, lru_note_cost, lru_note_cost_refault, folio_activate_fn",
          "description": "提供LRU列表页面移动和成本统计功能，folio_batch_add_and_move处理页面批量移动，lru_move_tail_fn将页面移动到LRU尾部，folio_rotate_reclaimable将可回收页面转移到冷列表，lru_note_cost记录页面访问成本用于调整扫描策略，folio_activate_fn激活页面至活动列表。",
          "similarity": 0.500395655632019
        },
        {
          "chunk_id": 8,
          "file_path": "mm/swap.c",
          "start_line": 1079,
          "end_line": 1111,
          "content": [
            "void __folio_batch_release(struct folio_batch *fbatch)",
            "{",
            "\tif (!fbatch->percpu_pvec_drained) {",
            "\t\tlru_add_drain();",
            "\t\tfbatch->percpu_pvec_drained = true;",
            "\t}",
            "\tfolios_put(fbatch);",
            "}",
            "void folio_batch_remove_exceptionals(struct folio_batch *fbatch)",
            "{",
            "\tunsigned int i, j;",
            "",
            "\tfor (i = 0, j = 0; i < folio_batch_count(fbatch); i++) {",
            "\t\tstruct folio *folio = fbatch->folios[i];",
            "\t\tif (!xa_is_value(folio))",
            "\t\t\tfbatch->folios[j++] = folio;",
            "\t}",
            "\tfbatch->nr = j;",
            "}",
            "void __init swap_setup(void)",
            "{",
            "\tunsigned long megs = totalram_pages() >> (20 - PAGE_SHIFT);",
            "",
            "\t/* Use a smaller cluster for small-memory machines */",
            "\tif (megs < 16)",
            "\t\tpage_cluster = 2;",
            "\telse",
            "\t\tpage_cluster = 3;",
            "\t/*",
            "\t * Right now other parts of the system means that we",
            "\t * _really_ don't want to cluster much more",
            "\t */",
            "}"
          ],
          "function_name": "__folio_batch_release, folio_batch_remove_exceptionals, swap_setup",
          "description": "__folio_batch_release 标记并释放页面批次引用，folio_batch_remove_exceptionals 清理异常条目；swap_setup 初始化页面聚类参数，根据内存大小调整page_cluster值。",
          "similarity": 0.4874364137649536
        },
        {
          "chunk_id": 3,
          "file_path": "mm/swap.c",
          "start_line": 332,
          "end_line": 460,
          "content": [
            "static void folio_activate_drain(int cpu)",
            "{",
            "\tstruct folio_batch *fbatch = &per_cpu(cpu_fbatches.activate, cpu);",
            "",
            "\tif (folio_batch_count(fbatch))",
            "\t\tfolio_batch_move_lru(fbatch, folio_activate_fn);",
            "}",
            "void folio_activate(struct folio *folio)",
            "{",
            "\tif (!folio_test_active(folio) && !folio_test_unevictable(folio)) {",
            "\t\tstruct folio_batch *fbatch;",
            "",
            "\t\tfolio_get(folio);",
            "\t\tif (!folio_test_clear_lru(folio)) {",
            "\t\t\tfolio_put(folio);",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\tlocal_lock(&cpu_fbatches.lock);",
            "\t\tfbatch = this_cpu_ptr(&cpu_fbatches.activate);",
            "\t\tfolio_batch_add_and_move(fbatch, folio, folio_activate_fn);",
            "\t\tlocal_unlock(&cpu_fbatches.lock);",
            "\t}",
            "}",
            "static inline void folio_activate_drain(int cpu)",
            "{",
            "}",
            "void folio_activate(struct folio *folio)",
            "{",
            "\tstruct lruvec *lruvec;",
            "",
            "\tif (folio_test_clear_lru(folio)) {",
            "\t\tlruvec = folio_lruvec_lock_irq(folio);",
            "\t\tfolio_activate_fn(lruvec, folio);",
            "\t\tunlock_page_lruvec_irq(lruvec);",
            "\t\tfolio_set_lru(folio);",
            "\t}",
            "}",
            "static void __lru_cache_activate_folio(struct folio *folio)",
            "{",
            "\tstruct folio_batch *fbatch;",
            "\tint i;",
            "",
            "\tlocal_lock(&cpu_fbatches.lock);",
            "\tfbatch = this_cpu_ptr(&cpu_fbatches.lru_add);",
            "",
            "\t/*",
            "\t * Search backwards on the optimistic assumption that the folio being",
            "\t * activated has just been added to this batch. Note that only",
            "\t * the local batch is examined as a !LRU folio could be in the",
            "\t * process of being released, reclaimed, migrated or on a remote",
            "\t * batch that is currently being drained. Furthermore, marking",
            "\t * a remote batch's folio active potentially hits a race where",
            "\t * a folio is marked active just after it is added to the inactive",
            "\t * list causing accounting errors and BUG_ON checks to trigger.",
            "\t */",
            "\tfor (i = folio_batch_count(fbatch) - 1; i >= 0; i--) {",
            "\t\tstruct folio *batch_folio = fbatch->folios[i];",
            "",
            "\t\tif (batch_folio == folio) {",
            "\t\t\tfolio_set_active(folio);",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tlocal_unlock(&cpu_fbatches.lock);",
            "}",
            "static void folio_inc_refs(struct folio *folio)",
            "{",
            "\tunsigned long new_flags, old_flags = READ_ONCE(folio->flags);",
            "",
            "\tif (folio_test_unevictable(folio))",
            "\t\treturn;",
            "",
            "\tif (!folio_test_referenced(folio)) {",
            "\t\tfolio_set_referenced(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!folio_test_workingset(folio)) {",
            "\t\tfolio_set_workingset(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* see the comment on MAX_NR_TIERS */",
            "\tdo {",
            "\t\tnew_flags = old_flags & LRU_REFS_MASK;",
            "\t\tif (new_flags == LRU_REFS_MASK)",
            "\t\t\tbreak;",
            "",
            "\t\tnew_flags += BIT(LRU_REFS_PGOFF);",
            "\t\tnew_flags |= old_flags & ~LRU_REFS_MASK;",
            "\t} while (!try_cmpxchg(&folio->flags, &old_flags, new_flags));",
            "}",
            "static void folio_inc_refs(struct folio *folio)",
            "{",
            "}",
            "void folio_mark_accessed(struct folio *folio)",
            "{",
            "\tif (lru_gen_enabled()) {",
            "\t\tfolio_inc_refs(folio);",
            "\t\treturn;",
            "\t}",
            "",
            "\tif (!folio_test_referenced(folio)) {",
            "\t\tfolio_set_referenced(folio);",
            "\t} else if (folio_test_unevictable(folio)) {",
            "\t\t/*",
            "\t\t * Unevictable pages are on the \"LRU_UNEVICTABLE\" list. But,",
            "\t\t * this list is never rotated or maintained, so marking an",
            "\t\t * unevictable page accessed has no effect.",
            "\t\t */",
            "\t} else if (!folio_test_active(folio)) {",
            "\t\t/*",
            "\t\t * If the folio is on the LRU, queue it for activation via",
            "\t\t * cpu_fbatches.activate. Otherwise, assume the folio is in a",
            "\t\t * folio_batch, mark it active and it'll be moved to the active",
            "\t\t * LRU on the next drain.",
            "\t\t */",
            "\t\tif (folio_test_lru(folio))",
            "\t\t\tfolio_activate(folio);",
            "\t\telse",
            "\t\t\t__lru_cache_activate_folio(folio);",
            "\t\tfolio_clear_referenced(folio);",
            "\t\tworkingset_activation(folio);",
            "\t}",
            "\tif (folio_test_idle(folio))",
            "\t\tfolio_clear_idle(folio);",
            "}"
          ],
          "function_name": "folio_activate_drain, folio_activate, folio_activate_drain, folio_activate, __lru_cache_activate_folio, folio_inc_refs, folio_inc_refs, folio_mark_accessed",
          "description": "实现页面激活和引用计数管理，folio_activate_drain和folio_activate将页面从非活动列表转至活动列表，__lru_cache_activate_folio处理本地批次中的页面激活，folio_inc_refs更新页面引用标志，folio_mark_accessed标记页面访问状态并触发激活流程。",
          "similarity": 0.47848138213157654
        }
      ]
    }
  ]
}