{
  "query": "微内核设计模式",
  "timestamp": "2025-12-26 01:52:48",
  "retrieved_files": [
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.5767377614974976,
      "chunks": [
        {
          "chunk_id": 19,
          "file_path": "mm/mempolicy.c",
          "start_line": 3268,
          "end_line": 3394,
          "content": [
            "void numa_default_policy(void)",
            "{",
            "\tdo_set_mempolicy(MPOL_DEFAULT, 0, NULL);",
            "}",
            "int mpol_parse_str(char *str, struct mempolicy **mpol)",
            "{",
            "\tstruct mempolicy *new = NULL;",
            "\tunsigned short mode_flags;",
            "\tnodemask_t nodes;",
            "\tchar *nodelist = strchr(str, ':');",
            "\tchar *flags = strchr(str, '=');",
            "\tint err = 1, mode;",
            "",
            "\tif (flags)",
            "\t\t*flags++ = '\\0';\t/* terminate mode string */",
            "",
            "\tif (nodelist) {",
            "\t\t/* NUL-terminate mode or flags string */",
            "\t\t*nodelist++ = '\\0';",
            "\t\tif (nodelist_parse(nodelist, nodes))",
            "\t\t\tgoto out;",
            "\t\tif (!nodes_subset(nodes, node_states[N_MEMORY]))",
            "\t\t\tgoto out;",
            "\t} else",
            "\t\tnodes_clear(nodes);",
            "",
            "\tmode = match_string(policy_modes, MPOL_MAX, str);",
            "\tif (mode < 0)",
            "\t\tgoto out;",
            "",
            "\tswitch (mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\t/*",
            "\t\t * Insist on a nodelist of one node only, although later",
            "\t\t * we use first_node(nodes) to grab a single node, so here",
            "\t\t * nodelist (or nodes) cannot be empty.",
            "\t\t */",
            "\t\tif (nodelist) {",
            "\t\t\tchar *rest = nodelist;",
            "\t\t\twhile (isdigit(*rest))",
            "\t\t\t\trest++;",
            "\t\t\tif (*rest)",
            "\t\t\t\tgoto out;",
            "\t\t\tif (nodes_empty(nodes))",
            "\t\t\t\tgoto out;",
            "\t\t}",
            "\t\tbreak;",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t/*",
            "\t\t * Default to online nodes with memory if no nodelist",
            "\t\t */",
            "\t\tif (!nodelist)",
            "\t\t\tnodes = node_states[N_MEMORY];",
            "\t\tbreak;",
            "\tcase MPOL_LOCAL:",
            "\t\t/*",
            "\t\t * Don't allow a nodelist;  mpol_new() checks flags",
            "\t\t */",
            "\t\tif (nodelist)",
            "\t\t\tgoto out;",
            "\t\tbreak;",
            "\tcase MPOL_DEFAULT:",
            "\t\t/*",
            "\t\t * Insist on a empty nodelist",
            "\t\t */",
            "\t\tif (!nodelist)",
            "\t\t\terr = 0;",
            "\t\tgoto out;",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\t\t/*",
            "\t\t * Insist on a nodelist",
            "\t\t */",
            "\t\tif (!nodelist)",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tmode_flags = 0;",
            "\tif (flags) {",
            "\t\t/*",
            "\t\t * Currently, we only support two mutually exclusive",
            "\t\t * mode flags.",
            "\t\t */",
            "\t\tif (!strcmp(flags, \"static\"))",
            "\t\t\tmode_flags |= MPOL_F_STATIC_NODES;",
            "\t\telse if (!strcmp(flags, \"relative\"))",
            "\t\t\tmode_flags |= MPOL_F_RELATIVE_NODES;",
            "\t\telse",
            "\t\t\tgoto out;",
            "\t}",
            "",
            "\tnew = mpol_new(mode, mode_flags, &nodes);",
            "\tif (IS_ERR(new))",
            "\t\tgoto out;",
            "",
            "\t/*",
            "\t * Save nodes for mpol_to_str() to show the tmpfs mount options",
            "\t * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.",
            "\t */",
            "\tif (mode != MPOL_PREFERRED) {",
            "\t\tnew->nodes = nodes;",
            "\t} else if (nodelist) {",
            "\t\tnodes_clear(new->nodes);",
            "\t\tnode_set(first_node(nodes), new->nodes);",
            "\t} else {",
            "\t\tnew->mode = MPOL_LOCAL;",
            "\t}",
            "",
            "\t/*",
            "\t * Save nodes for contextualization: this will be used to \"clone\"",
            "\t * the mempolicy in a specific context [cpuset] at a later time.",
            "\t */",
            "\tnew->w.user_nodemask = nodes;",
            "",
            "\terr = 0;",
            "",
            "out:",
            "\t/* Restore string for error message */",
            "\tif (nodelist)",
            "\t\t*--nodelist = ':';",
            "\tif (flags)",
            "\t\t*--flags = '=';",
            "\tif (!err)",
            "\t\t*mpol = new;",
            "\treturn err;",
            "}"
          ],
          "function_name": "numa_default_policy, mpol_parse_str",
          "description": "numa_default_policy设置系统默认内存策略为默认模式，mpol_parse_str解析内存策略字符串，根据模式类型处理节点列表和标志位，构建对应的内存策略结构体。",
          "similarity": 0.5794645547866821
        },
        {
          "chunk_id": 11,
          "file_path": "mm/mempolicy.c",
          "start_line": 1855,
          "end_line": 1971,
          "content": [
            "static int kernel_get_mempolicy(int __user *policy,",
            "\t\t\t\tunsigned long __user *nmask,",
            "\t\t\t\tunsigned long maxnode,",
            "\t\t\t\tunsigned long addr,",
            "\t\t\t\tunsigned long flags)",
            "{",
            "\tint err;",
            "\tint pval;",
            "\tnodemask_t nodes;",
            "",
            "\tif (nmask != NULL && maxnode < nr_node_ids)",
            "\t\treturn -EINVAL;",
            "",
            "\taddr = untagged_addr(addr);",
            "",
            "\terr = do_get_mempolicy(&pval, &nodes, addr, flags);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (policy && put_user(pval, policy))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (nmask)",
            "\t\terr = copy_nodes_to_user(nmask, maxnode, &nodes);",
            "",
            "\treturn err;",
            "}",
            "bool vma_migratable(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * DAX device mappings require predictable access latency, so avoid",
            "\t * incurring periodic faults.",
            "\t */",
            "\tif (vma_is_dax(vma))",
            "\t\treturn false;",
            "",
            "\tif (is_vm_hugetlb_page(vma) &&",
            "\t\t!hugepage_migration_supported(hstate_vma(vma)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Migration allocates pages in the highest zone. If we cannot",
            "\t * do so then migration (at least from node to node) is not",
            "\t * possible.",
            "\t */",
            "\tif (vma->vm_file &&",
            "\t\tgfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))",
            "\t\t\t< policy_zone)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "bool vma_policy_mof(struct vm_area_struct *vma)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->get_policy) {",
            "\t\tbool ret = false;",
            "\t\tpgoff_t ilx;\t\t/* ignored here */",
            "",
            "\t\tpol = vma->vm_ops->get_policy(vma, vma->vm_start, &ilx);",
            "\t\tif (pol && (pol->flags & MPOL_F_MOF))",
            "\t\t\tret = true;",
            "\t\tmpol_cond_put(pol);",
            "",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tpol = vma->vm_policy;",
            "\tif (!pol)",
            "\t\tpol = get_task_policy(current);",
            "",
            "\treturn pol->flags & MPOL_F_MOF;",
            "}",
            "bool apply_policy_zone(struct mempolicy *policy, enum zone_type zone)",
            "{",
            "\tenum zone_type dynamic_policy_zone = policy_zone;",
            "",
            "\tBUG_ON(dynamic_policy_zone == ZONE_MOVABLE);",
            "",
            "\t/*",
            "\t * if policy->nodes has movable memory only,",
            "\t * we apply policy when gfp_zone(gfp) = ZONE_MOVABLE only.",
            "\t *",
            "\t * policy->nodes is intersect with node_states[N_MEMORY].",
            "\t * so if the following test fails, it implies",
            "\t * policy->nodes has movable memory only.",
            "\t */",
            "\tif (!nodes_intersects(policy->nodes, node_states[N_HIGH_MEMORY]))",
            "\t\tdynamic_policy_zone = ZONE_MOVABLE;",
            "",
            "\treturn zone >= dynamic_policy_zone;",
            "}",
            "static unsigned int weighted_interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int node;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "retry:",
            "\t/* to prevent miscount use tsk->mems_allowed_seq to detect rebind */",
            "\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\tnode = current->il_prev;",
            "\tif (!current->il_weight || !node_isset(node, policy->nodes)) {",
            "\t\tnode = next_node_in(node, policy->nodes);",
            "\t\tif (read_mems_allowed_retry(cpuset_mems_cookie))",
            "\t\t\tgoto retry;",
            "\t\tif (node == MAX_NUMNODES)",
            "\t\t\treturn node;",
            "\t\tcurrent->il_prev = node;",
            "\t\tcurrent->il_weight = get_il_weight(node);",
            "\t}",
            "\tcurrent->il_weight--;",
            "\treturn node;",
            "}"
          ],
          "function_name": "kernel_get_mempolicy, vma_migratable, vma_policy_mof, apply_policy_zone, weighted_interleave_nodes",
          "description": "kernel_get_mempolicy 获取当前内存策略参数并复制到用户空间；vma_migratable 判断虚拟内存区域是否支持迁移；vma_policy_mof 检查VMA是否启用了MOF（Migration On Fault）策略；apply_policy_zone 确定当前zone是否满足策略要求；weighted_interleave_nodes 计算加权交错分配的目标节点。",
          "similarity": 0.5382041931152344
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.5356654524803162
        },
        {
          "chunk_id": 15,
          "file_path": "mm/mempolicy.c",
          "start_line": 2631,
          "end_line": 2732,
          "content": [
            "static unsigned long alloc_pages_bulk_array_preferred_many(gfp_t gfp, int nid,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tgfp_t preferred_gfp;",
            "\tunsigned long nr_allocated = 0;",
            "",
            "\tpreferred_gfp = gfp | __GFP_NOWARN;",
            "\tpreferred_gfp &= ~(__GFP_DIRECT_RECLAIM | __GFP_NOFAIL);",
            "",
            "\tnr_allocated  = alloc_pages_bulk_noprof(preferred_gfp, nid, &pol->nodes,",
            "\t\t\t\t\t   nr_pages, NULL, page_array);",
            "",
            "\tif (nr_allocated < nr_pages)",
            "\t\tnr_allocated += alloc_pages_bulk_noprof(gfp, numa_node_id(), NULL,",
            "\t\t\t\tnr_pages - nr_allocated, NULL,",
            "\t\t\t\tpage_array + nr_allocated);",
            "\treturn nr_allocated;",
            "}",
            "unsigned long alloc_pages_bulk_array_mempolicy_noprof(gfp_t gfp,",
            "\t\tunsigned long nr_pages, struct page **page_array)",
            "{",
            "\tstruct mempolicy *pol = &default_policy;",
            "\tnodemask_t *nodemask;",
            "\tint nid;",
            "",
            "\tif (!in_interrupt() && !(gfp & __GFP_THISNODE))",
            "\t\tpol = get_task_policy(current);",
            "",
            "\tif (pol->mode == MPOL_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_interleave(gfp, pol,",
            "\t\t\t\t\t\t\t nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_WEIGHTED_INTERLEAVE)",
            "\t\treturn alloc_pages_bulk_array_weighted_interleave(",
            "\t\t\t\t  gfp, pol, nr_pages, page_array);",
            "",
            "\tif (pol->mode == MPOL_PREFERRED_MANY)",
            "\t\treturn alloc_pages_bulk_array_preferred_many(gfp,",
            "\t\t\t\tnuma_node_id(), pol, nr_pages, page_array);",
            "",
            "\tnid = numa_node_id();",
            "\tnodemask = policy_nodemask(gfp, pol, NO_INTERLEAVE_INDEX, &nid);",
            "\treturn alloc_pages_bulk_noprof(gfp, nid, nodemask,",
            "\t\t\t\t       nr_pages, NULL, page_array);",
            "}",
            "int vma_dup_policy(struct vm_area_struct *src, struct vm_area_struct *dst)",
            "{",
            "\tstruct mempolicy *pol = mpol_dup(src->vm_policy);",
            "",
            "\tif (IS_ERR(pol))",
            "\t\treturn PTR_ERR(pol);",
            "\tdst->vm_policy = pol;",
            "\treturn 0;",
            "}",
            "bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)",
            "{",
            "\tif (!a || !b)",
            "\t\treturn false;",
            "\tif (a->mode != b->mode)",
            "\t\treturn false;",
            "\tif (a->flags != b->flags)",
            "\t\treturn false;",
            "\tif (a->home_node != b->home_node)",
            "\t\treturn false;",
            "\tif (mpol_store_user_nodemask(a))",
            "\t\tif (!nodes_equal(a->w.user_nodemask, b->w.user_nodemask))",
            "\t\t\treturn false;",
            "",
            "\tswitch (a->mode) {",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn !!nodes_equal(a->nodes, b->nodes);",
            "\tcase MPOL_LOCAL:",
            "\t\treturn true;",
            "\tdefault:",
            "\t\tBUG();",
            "\t\treturn false;",
            "\t}",
            "}",
            "static void sp_insert(struct shared_policy *sp, struct sp_node *new)",
            "{",
            "\tstruct rb_node **p = &sp->root.rb_node;",
            "\tstruct rb_node *parent = NULL;",
            "\tstruct sp_node *nd;",
            "",
            "\twhile (*p) {",
            "\t\tparent = *p;",
            "\t\tnd = rb_entry(parent, struct sp_node, nd);",
            "\t\tif (new->start < nd->start)",
            "\t\t\tp = &(*p)->rb_left;",
            "\t\telse if (new->end > nd->end)",
            "\t\t\tp = &(*p)->rb_right;",
            "\t\telse",
            "\t\t\tBUG();",
            "\t}",
            "\trb_link_node(&new->nd, parent, p);",
            "\trb_insert_color(&new->nd, &sp->root);",
            "}"
          ],
          "function_name": "alloc_pages_bulk_array_preferred_many, alloc_pages_bulk_array_mempolicy_noprof, vma_dup_policy, __mpol_equal, sp_insert",
          "description": "实现带优先节点策略的批量页面分配，根据当前节点和策略节点掩码尝试分配内存，优先满足首选节点需求。vma_dup_policy复制VMA内存策略，__mpol_equal比较两个内存策略是否相同，sp_insert将共享策略节点插入RB树。",
          "similarity": 0.5308370590209961
        },
        {
          "chunk_id": 12,
          "file_path": "mm/mempolicy.c",
          "start_line": 2024,
          "end_line": 2135,
          "content": [
            "static unsigned int interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int nid;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "\t/* to prevent miscount, use tsk->mems_allowed_seq to detect rebind */",
            "\tdo {",
            "\t\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\t\tnid = next_node_in(current->il_prev, policy->nodes);",
            "\t} while (read_mems_allowed_retry(cpuset_mems_cookie));",
            "",
            "\tif (nid < MAX_NUMNODES)",
            "\t\tcurrent->il_prev = nid;",
            "\treturn nid;",
            "}",
            "unsigned int mempolicy_slab_node(void)",
            "{",
            "\tstruct mempolicy *policy;",
            "\tint node = numa_mem_id();",
            "",
            "\tif (!in_task())",
            "\t\treturn node;",
            "",
            "\tpolicy = current->mempolicy;",
            "\tif (!policy)",
            "\t\treturn node;",
            "",
            "\tswitch (policy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\t\treturn first_node(policy->nodes);",
            "",
            "\tcase MPOL_INTERLEAVE:",
            "\t\treturn interleave_nodes(policy);",
            "",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\treturn weighted_interleave_nodes(policy);",
            "",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\t{",
            "\t\tstruct zoneref *z;",
            "",
            "\t\t/*",
            "\t\t * Follow bind policy behavior and start allocation at the",
            "\t\t * first node.",
            "\t\t */",
            "\t\tstruct zonelist *zonelist;",
            "\t\tenum zone_type highest_zoneidx = gfp_zone(GFP_KERNEL);",
            "\t\tzonelist = &NODE_DATA(node)->node_zonelists[ZONELIST_FALLBACK];",
            "\t\tz = first_zones_zonelist(zonelist, highest_zoneidx,",
            "\t\t\t\t\t\t\t&policy->nodes);",
            "\t\treturn z->zone ? zone_to_nid(z->zone) : node;",
            "\t}",
            "\tcase MPOL_LOCAL:",
            "\t\treturn node;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "}",
            "static unsigned int read_once_policy_nodemask(struct mempolicy *pol,",
            "\t\t\t\t\t      nodemask_t *mask)",
            "{",
            "\t/*",
            "\t * barrier stabilizes the nodemask locally so that it can be iterated",
            "\t * over safely without concern for changes. Allocators validate node",
            "\t * selection does not violate mems_allowed, so this is safe.",
            "\t */",
            "\tbarrier();",
            "\tmemcpy(mask, &pol->nodes, sizeof(nodemask_t));",
            "\tbarrier();",
            "\treturn nodes_weight(*mask);",
            "}",
            "static unsigned int weighted_interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tstruct weighted_interleave_state *state;",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nr_nodes;",
            "\tu8 *table = NULL;",
            "\tunsigned int weight_total = 0;",
            "\tu8 weight;",
            "\tint nid = 0;",
            "",
            "\tnr_nodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nr_nodes)",
            "\t\treturn numa_node_id();",
            "",
            "\trcu_read_lock();",
            "",
            "\tstate = rcu_dereference(wi_state);",
            "\t/* Uninitialized wi_state means we should assume all weights are 1 */",
            "\tif (state)",
            "\t\ttable = state->iw_table;",
            "",
            "\t/* calculate the total weight */",
            "\tfor_each_node_mask(nid, nodemask)",
            "\t\tweight_total += table ? table[nid] : 1;",
            "",
            "\t/* Calculate the node offset based on totals */",
            "\ttarget = ilx % weight_total;",
            "\tnid = first_node(nodemask);",
            "\twhile (target) {",
            "\t\t/* detect system default usage */",
            "\t\tweight = table ? table[nid] : 1;",
            "\t\tif (target < weight)",
            "\t\t\tbreak;",
            "\t\ttarget -= weight;",
            "\t\tnid = next_node_in(nid, nodemask);",
            "\t}",
            "\trcu_read_unlock();",
            "\treturn nid;",
            "}"
          ],
          "function_name": "interleave_nodes, mempolicy_slab_node, read_once_policy_nodemask, weighted_interleave_nid",
          "description": "interleave_nodes 计算交错分配的下一个节点；mempolicy_slab_node 根据内存策略返回Slab分配的节点；read_once_policy_nodemask 安全读取策略节点掩码；weighted_interleave_nid 基于权重计算加权交错分配的目标节点。",
          "similarity": 0.5215084552764893
        }
      ]
    },
    {
      "source_file": "mm/memblock.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:38:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memblock.c`\n\n---\n\n# memblock.c 技术文档\n\n## 1. 文件概述\n\n`memblock.c` 实现了 Linux 内核早期启动阶段的内存管理机制——**memblock**。该机制用于在常规内存分配器（如 buddy allocator）尚未初始化之前，对物理内存进行粗粒度的区域管理。它将系统内存抽象为若干连续的内存区域（regions），支持“可用内存”（memory）、“保留内存”（reserved）和“物理内存”（physmem，部分架构支持）三种类型，为内核早期初始化提供内存添加、查询和分配能力。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct memblock_region`：表示一个连续的物理内存区域，包含基地址（base）、大小（size）、NUMA 节点 ID 和属性标志。\n- `struct memblock_type`：管理一类内存区域的集合，包含区域数组、当前数量（cnt）、最大容量（max）和名称。\n- `struct memblock`：全局 memblock 管理结构，包含 `memory` 和 `reserved` 两种类型的 `memblock_type`，以及分配方向（bottom_up）和当前分配上限（current_limit）。\n- `physmem`（条件编译）：描述不受 `mem=` 参数限制的实际物理内存布局。\n\n### 主要函数与变量\n- `memblock_add()` / `memblock_add_node()`：向 memblock 添加可用内存区域。\n- `memblock_reserve()`：标记内存区域为保留（不可用于动态分配）。\n- `memblock_phys_alloc*()` / `memblock_alloc*()`：分配物理或虚拟地址的内存。\n- `memblock_overlaps_region()`：判断指定区域是否与某类 memblock 区域重叠。\n- `__memblock_find_range_bottom_up()`：从低地址向高地址查找满足条件的空闲内存范围。\n- 全局变量 `memblock`：静态初始化的主 memblock 结构体。\n- `max_low_pfn`, `min_low_pfn`, `max_pfn`, `max_possible_pfn`：记录 PFN（页帧号）边界信息。\n\n### 配置宏\n- `INIT_MEMBLOCK_REGIONS`：初始内存/保留区域数组大小（默认 128）。\n- `CONFIG_HAVE_MEMBLOCK_PHYS_MAP`：启用 `physmem` 类型支持。\n- `CONFIG_MEMBLOCK_KHO_SCRATCH`：支持仅从特定标记（KHO_SCRATCH）区域分配内存。\n- `CONFIG_ARCH_KEEP_MEMBLOCK`：决定是否在初始化完成后保留 memblock 数据结构。\n\n## 3. 关键实现\n\n### 初始化与存储\n- `memblock` 结构体在编译时静态初始化，其 `memory` 和 `reserved` 的区域数组分别使用 `memblock_memory_init_regions` 和 `memblock_reserved_init_regions`，初始容量由 `INIT_MEMBLOCK_*_REGIONS` 定义。\n- 每个 `memblock_type` 的 `cnt` 初始设为 1，但实际第一个条目为空的占位符，有效区域从索引 1 开始（后续代码处理）。\n- 支持通过 `memblock_allow_resize()` 动态扩容区域数组，但需谨慎避免与 initrd 等关键区域冲突。\n\n### 内存区域管理\n- 使用 `for_each_memblock_type` 宏遍历指定类型的区域。\n- `memblock_addrs_overlap()` 通过比较区间端点判断两个物理内存区域是否重叠。\n- `memblock_overlaps_region()` 封装了对某类所有区域的重叠检测。\n\n### 分配策略\n- 默认采用 **top-down**（从高地址向低地址）分配策略，可通过 `memblock_set_bottom_up(true)` 切换为 **bottom-up**。\n- 分配时受 `current_limit` 限制（默认 `MEMBLOCK_ALLOC_ANYWHERE` 表示无限制）。\n- 支持基于 NUMA 节点、对齐要求、内存属性（如 `MEMBLOCK_MIRROR`、`MEMBLOCK_KHO_SCRATCH`）的精细控制。\n- `choose_memblock_flags()` 根据 `kho_scratch_only` 和镜像内存存在性动态选择分配标志。\n\n### 安全与调试\n- `memblock_cap_size()` 防止地址计算溢出（确保 `base + size <= PHYS_ADDR_MAX`）。\n- 条件编译的 `memblock_dbg()` 宏用于调试输出（需开启 `memblock_debug`）。\n- 使用 `__initdata_memblock` 属性标记仅在初始化阶段使用的数据，便于后续释放。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/memblock.h>`：定义 memblock API 和数据结构。\n  - `<linux/kernel.h>`, `<linux/init.h>`：提供基础内核功能和初始化宏。\n  - `<linux/pfn.h>`：PFN 相关操作。\n  - `<asm/sections.h>`：访问内核链接段信息。\n  - 架构相关头文件（如 `internal.h`）。\n- **配置依赖**：\n  - `CONFIG_NUMA`：影响 `contig_page_data` 的定义。\n  - `CONFIG_KEXEC_HANDOVER`：引入 kexec 相关头文件。\n  - `CONFIG_HAVE_MEMBLOCK_PHYS_MAP`：启用 `physmem` 支持。\n- **后续移交**：在 `mem_init()` 中，memblock 管理的内存会被释放给 buddy allocator，完成内存管理权移交。\n\n## 5. 使用场景\n\n- **内核早期初始化**：在 `start_kernel()` 初期，架构代码（如 `setup_arch()`）调用 `memblock_add()` 注册可用物理内存，调用 `memblock_reserve()` 保留内核镜像、设备树、initrd 等关键区域。\n- **早期内存分配**：在 slab/buddy 分配器就绪前，使用 `memblock_alloc()` 分配大块连续内存（如页表、中断向量表、ACPI 表解析缓冲区）。\n- **内存布局查询**：通过 `for_each_memblock()` 等宏遍历内存区域，用于构建 e820 表、EFI 内存映射或 NUMA 拓扑。\n- **特殊分配需求**：支持从镜像内存（`MEMBLOCK_MIRROR`）或 KHO scratch 区域分配，满足安全启动或崩溃转储等场景。\n- **调试与分析**：通过 debugfs 接口（未在片段中体现）导出 memblock 布局，辅助内存问题诊断。",
      "similarity": 0.5740589499473572,
      "chunks": [
        {
          "chunk_id": 12,
          "file_path": "mm/memblock.c",
          "start_line": 2233,
          "end_line": 2340,
          "content": [
            "static void __init __free_pages_memory(unsigned long start, unsigned long end)",
            "{",
            "\tint order;",
            "",
            "\twhile (start < end) {",
            "\t\t/*",
            "\t\t * Free the pages in the largest chunks alignment allows.",
            "\t\t *",
            "\t\t * __ffs() behaviour is undefined for 0. start == 0 is",
            "\t\t * MAX_PAGE_ORDER-aligned, set order to MAX_PAGE_ORDER for",
            "\t\t * the case.",
            "\t\t */",
            "\t\tif (start)",
            "\t\t\torder = min_t(int, MAX_PAGE_ORDER, __ffs(start));",
            "\t\telse",
            "\t\t\torder = MAX_PAGE_ORDER;",
            "",
            "\t\twhile (start + (1UL << order) > end)",
            "\t\t\torder--;",
            "",
            "\t\tmemblock_free_pages(pfn_to_page(start), start, order);",
            "",
            "\t\tstart += (1UL << order);",
            "\t}",
            "}",
            "static unsigned long __init __free_memory_core(phys_addr_t start,",
            "\t\t\t\t phys_addr_t end)",
            "{",
            "\tunsigned long start_pfn = PFN_UP(start);",
            "\tunsigned long end_pfn = min_t(unsigned long,",
            "\t\t\t\t      PFN_DOWN(end), max_low_pfn);",
            "",
            "\tif (start_pfn >= end_pfn)",
            "\t\treturn 0;",
            "",
            "\t__free_pages_memory(start_pfn, end_pfn);",
            "",
            "\treturn end_pfn - start_pfn;",
            "}",
            "static void __init memmap_init_reserved_pages(void)",
            "{",
            "\tstruct memblock_region *region;",
            "\tphys_addr_t start, end;",
            "\tint nid;",
            "\tunsigned long max_reserved;",
            "",
            "\t/*",
            "\t * set nid on all reserved pages and also treat struct",
            "\t * pages for the NOMAP regions as PageReserved",
            "\t */",
            "repeat:",
            "\tmax_reserved = memblock.reserved.max;",
            "\tfor_each_mem_region(region) {",
            "\t\tnid = memblock_get_region_node(region);",
            "\t\tstart = region->base;",
            "\t\tend = start + region->size;",
            "",
            "\t\tif (memblock_is_nomap(region))",
            "\t\t\treserve_bootmem_region(start, end, nid);",
            "",
            "\t\tmemblock_set_node(start, region->size, &memblock.reserved, nid);",
            "\t}",
            "\t/*",
            "\t * 'max' is changed means memblock.reserved has been doubled its",
            "\t * array, which may result a new reserved region before current",
            "\t * 'start'. Now we should repeat the procedure to set its node id.",
            "\t */",
            "\tif (max_reserved != memblock.reserved.max)",
            "\t\tgoto repeat;",
            "",
            "\t/*",
            "\t * initialize struct pages for reserved regions that don't have",
            "\t * the MEMBLOCK_RSRV_NOINIT flag set",
            "\t */",
            "\tfor_each_reserved_mem_region(region) {",
            "\t\tif (!memblock_is_reserved_noinit(region)) {",
            "\t\t\tnid = memblock_get_region_node(region);",
            "\t\t\tstart = region->base;",
            "\t\t\tend = start + region->size;",
            "",
            "\t\t\tif (!numa_valid_node(nid))",
            "\t\t\t\tnid = early_pfn_to_nid(PFN_DOWN(start));",
            "",
            "\t\t\treserve_bootmem_region(start, end, nid);",
            "\t\t}",
            "\t}",
            "}",
            "static unsigned long __init free_low_memory_core_early(void)",
            "{",
            "\tunsigned long count = 0;",
            "\tphys_addr_t start, end;",
            "\tu64 i;",
            "",
            "\tmemblock_clear_hotplug(0, -1);",
            "",
            "\tmemmap_init_reserved_pages();",
            "",
            "\t/*",
            "\t * We need to use NUMA_NO_NODE instead of NODE_DATA(0)->node_id",
            "\t *  because in some case like Node0 doesn't have RAM installed",
            "\t *  low ram will be on Node1",
            "\t */",
            "\tfor_each_free_mem_range(i, NUMA_NO_NODE, MEMBLOCK_NONE, &start, &end,",
            "\t\t\t\tNULL)",
            "\t\tcount += __free_memory_core(start, end);",
            "",
            "\treturn count;",
            "}"
          ],
          "function_name": "__free_pages_memory, __free_memory_core, memmap_init_reserved_pages, free_low_memory_core_early",
          "description": "核心实现内存页面释放逻辑，初始化保留区域页结构并处理低内存核心区域的提前释放操作",
          "similarity": 0.5930061340332031
        },
        {
          "chunk_id": 11,
          "file_path": "mm/memblock.c",
          "start_line": 2094,
          "end_line": 2203,
          "content": [
            "static void __init_memblock memblock_dump(struct memblock_type *type)",
            "{",
            "\tphys_addr_t base, end, size;",
            "\tenum memblock_flags flags;",
            "\tint idx;",
            "\tstruct memblock_region *rgn;",
            "",
            "\tpr_info(\" %s.cnt  = 0x%lx\\n\", type->name, type->cnt);",
            "",
            "\tfor_each_memblock_type(idx, type, rgn) {",
            "\t\tchar nid_buf[32] = \"\";",
            "",
            "\t\tbase = rgn->base;",
            "\t\tsize = rgn->size;",
            "\t\tend = base + size - 1;",
            "\t\tflags = rgn->flags;",
            "#ifdef CONFIG_NUMA",
            "\t\tif (numa_valid_node(memblock_get_region_node(rgn)))",
            "\t\t\tsnprintf(nid_buf, sizeof(nid_buf), \" on node %d\",",
            "\t\t\t\t memblock_get_region_node(rgn));",
            "#endif",
            "\t\tpr_info(\" %s[%#x]\\t[%pa-%pa], %pa bytes%s flags: %#x\\n\",",
            "\t\t\ttype->name, idx, &base, &end, &size, nid_buf, flags);",
            "\t}",
            "}",
            "void __init memblock_allow_resize(void)",
            "{",
            "\tmemblock_can_resize = 1;",
            "}",
            "static int __init early_memblock(char *p)",
            "{",
            "\tif (p && strstr(p, \"debug\"))",
            "\t\tmemblock_debug = 1;",
            "\treturn 0;",
            "}",
            "static void __init free_memmap(unsigned long start_pfn, unsigned long end_pfn)",
            "{",
            "\tstruct page *start_pg, *end_pg;",
            "\tphys_addr_t pg, pgend;",
            "",
            "\t/*",
            "\t * Convert start_pfn/end_pfn to a struct page pointer.",
            "\t */",
            "\tstart_pg = pfn_to_page(start_pfn - 1) + 1;",
            "\tend_pg = pfn_to_page(end_pfn - 1) + 1;",
            "",
            "\t/*",
            "\t * Convert to physical addresses, and round start upwards and end",
            "\t * downwards.",
            "\t */",
            "\tpg = PAGE_ALIGN(__pa(start_pg));",
            "\tpgend = __pa(end_pg) & PAGE_MASK;",
            "",
            "\t/*",
            "\t * If there are free pages between these, free the section of the",
            "\t * memmap array.",
            "\t */",
            "\tif (pg < pgend)",
            "\t\tmemblock_phys_free(pg, pgend - pg);",
            "}",
            "static void __init free_unused_memmap(void)",
            "{",
            "\tunsigned long start, end, prev_end = 0;",
            "\tint i;",
            "",
            "\tif (!IS_ENABLED(CONFIG_HAVE_ARCH_PFN_VALID) ||",
            "\t    IS_ENABLED(CONFIG_SPARSEMEM_VMEMMAP))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * This relies on each bank being in address order.",
            "\t * The banks are sorted previously in bootmem_init().",
            "\t */",
            "\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, NULL) {",
            "#ifdef CONFIG_SPARSEMEM",
            "\t\t/*",
            "\t\t * Take care not to free memmap entries that don't exist",
            "\t\t * due to SPARSEMEM sections which aren't present.",
            "\t\t */",
            "\t\tstart = min(start, ALIGN(prev_end, PAGES_PER_SECTION));",
            "#endif",
            "\t\t/*",
            "\t\t * Align down here since many operations in VM subsystem",
            "\t\t * presume that there are no holes in the memory map inside",
            "\t\t * a pageblock",
            "\t\t */",
            "\t\tstart = pageblock_start_pfn(start);",
            "",
            "\t\t/*",
            "\t\t * If we had a previous bank, and there is a space",
            "\t\t * between the current bank and the previous, free it.",
            "\t\t */",
            "\t\tif (prev_end && prev_end < start)",
            "\t\t\tfree_memmap(prev_end, start);",
            "",
            "\t\t/*",
            "\t\t * Align up here since many operations in VM subsystem",
            "\t\t * presume that there are no holes in the memory map inside",
            "\t\t * a pageblock",
            "\t\t */",
            "\t\tprev_end = pageblock_align(end);",
            "\t}",
            "",
            "#ifdef CONFIG_SPARSEMEM",
            "\tif (!IS_ALIGNED(prev_end, PAGES_PER_SECTION)) {",
            "\t\tprev_end = pageblock_align(end);",
            "\t\tfree_memmap(prev_end, ALIGN(prev_end, PAGES_PER_SECTION));",
            "\t}",
            "#endif",
            "}"
          ],
          "function_name": "memblock_dump, memblock_allow_resize, early_memblock, free_memmap, free_unused_memmap",
          "description": "提供内存块状态调试、调整支持、早期内存处理及未使用memmap释放功能，用于优化内存映射管理",
          "similarity": 0.5678741931915283
        },
        {
          "chunk_id": 1,
          "file_path": "mm/memblock.c",
          "start_line": 192,
          "end_line": 297,
          "content": [
            "static inline phys_addr_t memblock_cap_size(phys_addr_t base, phys_addr_t *size)",
            "{",
            "\treturn *size = min(*size, PHYS_ADDR_MAX - base);",
            "}",
            "unsigned long __init_memblock",
            "memblock_addrs_overlap(phys_addr_t base1, phys_addr_t size1, phys_addr_t base2,",
            "\t\t       phys_addr_t size2)",
            "{",
            "\treturn ((base1 < (base2 + size2)) && (base2 < (base1 + size1)));",
            "}",
            "bool __init_memblock memblock_overlaps_region(struct memblock_type *type,",
            "\t\t\t\t\tphys_addr_t base, phys_addr_t size)",
            "{",
            "\tunsigned long i;",
            "",
            "\tmemblock_cap_size(base, &size);",
            "",
            "\tfor (i = 0; i < type->cnt; i++)",
            "\t\tif (memblock_addrs_overlap(base, size, type->regions[i].base,",
            "\t\t\t\t\t   type->regions[i].size))",
            "\t\t\tbreak;",
            "\treturn i < type->cnt;",
            "}",
            "static phys_addr_t __init_memblock",
            "__memblock_find_range_bottom_up(phys_addr_t start, phys_addr_t end,",
            "\t\t\t\tphys_addr_t size, phys_addr_t align, int nid,",
            "\t\t\t\tenum memblock_flags flags)",
            "{",
            "\tphys_addr_t this_start, this_end, cand;",
            "\tu64 i;",
            "",
            "\tfor_each_free_mem_range(i, nid, flags, &this_start, &this_end, NULL) {",
            "\t\tthis_start = clamp(this_start, start, end);",
            "\t\tthis_end = clamp(this_end, start, end);",
            "",
            "\t\tcand = round_up(this_start, align);",
            "\t\tif (cand < this_end && this_end - cand >= size)",
            "\t\t\treturn cand;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static phys_addr_t __init_memblock",
            "__memblock_find_range_top_down(phys_addr_t start, phys_addr_t end,",
            "\t\t\t       phys_addr_t size, phys_addr_t align, int nid,",
            "\t\t\t       enum memblock_flags flags)",
            "{",
            "\tphys_addr_t this_start, this_end, cand;",
            "\tu64 i;",
            "",
            "\tfor_each_free_mem_range_reverse(i, nid, flags, &this_start, &this_end,",
            "\t\t\t\t\tNULL) {",
            "\t\tthis_start = clamp(this_start, start, end);",
            "\t\tthis_end = clamp(this_end, start, end);",
            "",
            "\t\tif (this_end < size)",
            "\t\t\tcontinue;",
            "",
            "\t\tcand = round_down(this_end - size, align);",
            "\t\tif (cand >= this_start)",
            "\t\t\treturn cand;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static phys_addr_t __init_memblock memblock_find_in_range_node(phys_addr_t size,",
            "\t\t\t\t\tphys_addr_t align, phys_addr_t start,",
            "\t\t\t\t\tphys_addr_t end, int nid,",
            "\t\t\t\t\tenum memblock_flags flags)",
            "{",
            "\t/* pump up @end */",
            "\tif (end == MEMBLOCK_ALLOC_ACCESSIBLE ||",
            "\t    end == MEMBLOCK_ALLOC_NOLEAKTRACE)",
            "\t\tend = memblock.current_limit;",
            "",
            "\t/* avoid allocating the first page */",
            "\tstart = max_t(phys_addr_t, start, PAGE_SIZE);",
            "\tend = max(start, end);",
            "",
            "\tif (memblock_bottom_up())",
            "\t\treturn __memblock_find_range_bottom_up(start, end, size, align,",
            "\t\t\t\t\t\t       nid, flags);",
            "\telse",
            "\t\treturn __memblock_find_range_top_down(start, end, size, align,",
            "\t\t\t\t\t\t      nid, flags);",
            "}",
            "static phys_addr_t __init_memblock memblock_find_in_range(phys_addr_t start,",
            "\t\t\t\t\tphys_addr_t end, phys_addr_t size,",
            "\t\t\t\t\tphys_addr_t align)",
            "{",
            "\tphys_addr_t ret;",
            "\tenum memblock_flags flags = choose_memblock_flags();",
            "",
            "again:",
            "\tret = memblock_find_in_range_node(size, align, start, end,",
            "\t\t\t\t\t    NUMA_NO_NODE, flags);",
            "",
            "\tif (!ret && (flags & MEMBLOCK_MIRROR)) {",
            "\t\tpr_warn_ratelimited(\"Could not allocate %pap bytes of mirrored memory\\n\",",
            "\t\t\t&size);",
            "\t\tflags &= ~MEMBLOCK_MIRROR;",
            "\t\tgoto again;",
            "\t}",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "memblock_cap_size, memblock_addrs_overlap, memblock_overlaps_region, __memblock_find_range_bottom_up, __memblock_find_range_top_down, memblock_find_in_range_node, memblock_find_in_range",
          "description": "实现内存区域地址重叠检测与分配策略选择逻辑，包含范围查找算法（底向顶/顶向底）及镜像内存分配失败回退机制。",
          "similarity": 0.5629721879959106
        },
        {
          "chunk_id": 6,
          "file_path": "mm/memblock.c",
          "start_line": 1052,
          "end_line": 1196,
          "content": [
            "int __init_memblock memblock_mark_mirror(phys_addr_t base, phys_addr_t size)",
            "{",
            "\tif (!mirrored_kernelcore)",
            "\t\treturn 0;",
            "",
            "\tsystem_has_some_mirror = true;",
            "",
            "\treturn memblock_setclr_flag(&memblock.memory, base, size, 1, MEMBLOCK_MIRROR);",
            "}",
            "int __init_memblock memblock_mark_nomap(phys_addr_t base, phys_addr_t size)",
            "{",
            "\treturn memblock_setclr_flag(&memblock.memory, base, size, 1, MEMBLOCK_NOMAP);",
            "}",
            "int __init_memblock memblock_clear_nomap(phys_addr_t base, phys_addr_t size)",
            "{",
            "\treturn memblock_setclr_flag(&memblock.memory, base, size, 0, MEMBLOCK_NOMAP);",
            "}",
            "int __init_memblock memblock_reserved_mark_noinit(phys_addr_t base, phys_addr_t size)",
            "{",
            "\treturn memblock_setclr_flag(&memblock.reserved, base, size, 1,",
            "\t\t\t\t    MEMBLOCK_RSRV_NOINIT);",
            "}",
            "__init int memblock_mark_kho_scratch(phys_addr_t base, phys_addr_t size)",
            "{",
            "\treturn memblock_setclr_flag(&memblock.memory, base, size, 1,",
            "\t\t\t\t    MEMBLOCK_KHO_SCRATCH);",
            "}",
            "__init int memblock_clear_kho_scratch(phys_addr_t base, phys_addr_t size)",
            "{",
            "\treturn memblock_setclr_flag(&memblock.memory, base, size, 0,",
            "\t\t\t\t    MEMBLOCK_KHO_SCRATCH);",
            "}",
            "static bool should_skip_region(struct memblock_type *type,",
            "\t\t\t       struct memblock_region *m,",
            "\t\t\t       int nid, int flags)",
            "{",
            "\tint m_nid = memblock_get_region_node(m);",
            "",
            "\t/* we never skip regions when iterating memblock.reserved or physmem */",
            "\tif (type != memblock_memory)",
            "\t\treturn false;",
            "",
            "\t/* only memory regions are associated with nodes, check it */",
            "\tif (numa_valid_node(nid) && nid != m_nid)",
            "\t\treturn true;",
            "",
            "\t/* skip hotpluggable memory regions if needed */",
            "\tif (movable_node_is_enabled() && memblock_is_hotpluggable(m) &&",
            "\t    !(flags & MEMBLOCK_HOTPLUG))",
            "\t\treturn true;",
            "",
            "\t/* if we want mirror memory skip non-mirror memory regions */",
            "\tif ((flags & MEMBLOCK_MIRROR) && !memblock_is_mirror(m))",
            "\t\treturn true;",
            "",
            "\t/* skip nomap memory unless we were asked for it explicitly */",
            "\tif (!(flags & MEMBLOCK_NOMAP) && memblock_is_nomap(m))",
            "\t\treturn true;",
            "",
            "\t/* skip driver-managed memory unless we were asked for it explicitly */",
            "\tif (!(flags & MEMBLOCK_DRIVER_MANAGED) && memblock_is_driver_managed(m))",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * In early alloc during kexec handover, we can only consider",
            "\t * MEMBLOCK_KHO_SCRATCH regions for the allocations",
            "\t */",
            "\tif ((flags & MEMBLOCK_KHO_SCRATCH) && !memblock_is_kho_scratch(m))",
            "\t\treturn true;",
            "",
            "\treturn false;",
            "}",
            "void __next_mem_range(u64 *idx, int nid, enum memblock_flags flags,",
            "\t\t      struct memblock_type *type_a,",
            "\t\t      struct memblock_type *type_b, phys_addr_t *out_start,",
            "\t\t      phys_addr_t *out_end, int *out_nid)",
            "{",
            "\tint idx_a = *idx & 0xffffffff;",
            "\tint idx_b = *idx >> 32;",
            "",
            "\tfor (; idx_a < type_a->cnt; idx_a++) {",
            "\t\tstruct memblock_region *m = &type_a->regions[idx_a];",
            "",
            "\t\tphys_addr_t m_start = m->base;",
            "\t\tphys_addr_t m_end = m->base + m->size;",
            "\t\tint\t    m_nid = memblock_get_region_node(m);",
            "",
            "\t\tif (should_skip_region(type_a, m, nid, flags))",
            "\t\t\tcontinue;",
            "",
            "\t\tif (!type_b) {",
            "\t\t\tif (out_start)",
            "\t\t\t\t*out_start = m_start;",
            "\t\t\tif (out_end)",
            "\t\t\t\t*out_end = m_end;",
            "\t\t\tif (out_nid)",
            "\t\t\t\t*out_nid = m_nid;",
            "\t\t\tidx_a++;",
            "\t\t\t*idx = (u32)idx_a | (u64)idx_b << 32;",
            "\t\t\treturn;",
            "\t\t}",
            "",
            "\t\t/* scan areas before each reservation */",
            "\t\tfor (; idx_b < type_b->cnt + 1; idx_b++) {",
            "\t\t\tstruct memblock_region *r;",
            "\t\t\tphys_addr_t r_start;",
            "\t\t\tphys_addr_t r_end;",
            "",
            "\t\t\tr = &type_b->regions[idx_b];",
            "\t\t\tr_start = idx_b ? r[-1].base + r[-1].size : 0;",
            "\t\t\tr_end = idx_b < type_b->cnt ?",
            "\t\t\t\tr->base : PHYS_ADDR_MAX;",
            "",
            "\t\t\t/*",
            "\t\t\t * if idx_b advanced past idx_a,",
            "\t\t\t * break out to advance idx_a",
            "\t\t\t */",
            "\t\t\tif (r_start >= m_end)",
            "\t\t\t\tbreak;",
            "\t\t\t/* if the two regions intersect, we're done */",
            "\t\t\tif (m_start < r_end) {",
            "\t\t\t\tif (out_start)",
            "\t\t\t\t\t*out_start =",
            "\t\t\t\t\t\tmax(m_start, r_start);",
            "\t\t\t\tif (out_end)",
            "\t\t\t\t\t*out_end = min(m_end, r_end);",
            "\t\t\t\tif (out_nid)",
            "\t\t\t\t\t*out_nid = m_nid;",
            "\t\t\t\t/*",
            "\t\t\t\t * The region which ends first is",
            "\t\t\t\t * advanced for the next iteration.",
            "\t\t\t\t */",
            "\t\t\t\tif (m_end <= r_end)",
            "\t\t\t\t\tidx_a++;",
            "\t\t\t\telse",
            "\t\t\t\t\tidx_b++;",
            "\t\t\t\t*idx = (u32)idx_a | (u64)idx_b << 32;",
            "\t\t\t\treturn;",
            "\t\t\t}",
            "\t\t}",
            "\t}",
            "",
            "\t/* signal end of iteration */",
            "\t*idx = ULLONG_MAX;",
            "}"
          ],
          "function_name": "memblock_mark_mirror, memblock_mark_nomap, memblock_clear_nomap, memblock_reserved_mark_noinit, memblock_mark_kho_scratch, memblock_clear_kho_scratch, should_skip_region, __next_mem_range",
          "description": "提供内存区域标记/清除接口，实现基于条件过滤的内存区域遍历逻辑，支持跳过特定属性（如非映射、非驱动管理等）的内存区域",
          "similarity": 0.5431995391845703
        },
        {
          "chunk_id": 9,
          "file_path": "mm/memblock.c",
          "start_line": 1607,
          "end_line": 1734,
          "content": [
            "phys_addr_t __init memblock_phys_alloc_range(phys_addr_t size,",
            "\t\t\t\t\t     phys_addr_t align,",
            "\t\t\t\t\t     phys_addr_t start,",
            "\t\t\t\t\t     phys_addr_t end)",
            "{",
            "\tmemblock_dbg(\"%s: %llu bytes align=0x%llx from=%pa max_addr=%pa %pS\\n\",",
            "\t\t     __func__, (u64)size, (u64)align, &start, &end,",
            "\t\t     (void *)_RET_IP_);",
            "\treturn memblock_alloc_range_nid(size, align, start, end, NUMA_NO_NODE,",
            "\t\t\t\t\tfalse);",
            "}",
            "phys_addr_t __init memblock_phys_alloc_try_nid(phys_addr_t size, phys_addr_t align, int nid)",
            "{",
            "\treturn memblock_alloc_range_nid(size, align, 0,",
            "\t\t\t\t\tMEMBLOCK_ALLOC_ACCESSIBLE, nid, false);",
            "}",
            "void __init memblock_free_late(phys_addr_t base, phys_addr_t size)",
            "{",
            "\tphys_addr_t cursor, end;",
            "",
            "\tend = base + size - 1;",
            "\tmemblock_dbg(\"%s: [%pa-%pa] %pS\\n\",",
            "\t\t     __func__, &base, &end, (void *)_RET_IP_);",
            "\tkmemleak_free_part_phys(base, size);",
            "\tcursor = PFN_UP(base);",
            "\tend = PFN_DOWN(base + size);",
            "",
            "\tfor (; cursor < end; cursor++) {",
            "\t\tmemblock_free_pages(pfn_to_page(cursor), cursor, 0);",
            "\t\ttotalram_pages_inc();",
            "\t}",
            "}",
            "phys_addr_t __init_memblock memblock_reserved_kern_size(phys_addr_t limit, int nid)",
            "{",
            "\tstruct memblock_region *r;",
            "\tphys_addr_t total = 0;",
            "",
            "\tfor_each_reserved_mem_region(r) {",
            "\t\tphys_addr_t size = r->size;",
            "",
            "\t\tif (r->base > limit)",
            "\t\t\tbreak;",
            "",
            "\t\tif (r->base + r->size > limit)",
            "\t\t\tsize = limit - r->base;",
            "",
            "\t\tif (nid == memblock_get_region_node(r) || !numa_valid_node(nid))",
            "\t\t\tif (r->flags & MEMBLOCK_RSRV_KERN)",
            "\t\t\t\ttotal += size;",
            "\t}",
            "",
            "\treturn total;",
            "}",
            "unsigned long __init memblock_estimated_nr_free_pages(void)",
            "{",
            "\treturn PHYS_PFN(memblock_phys_mem_size() - memblock_reserved_size());",
            "}",
            "static phys_addr_t __init_memblock __find_max_addr(phys_addr_t limit)",
            "{",
            "\tphys_addr_t max_addr = PHYS_ADDR_MAX;",
            "\tstruct memblock_region *r;",
            "",
            "\t/*",
            "\t * translate the memory @limit size into the max address within one of",
            "\t * the memory memblock regions, if the @limit exceeds the total size",
            "\t * of those regions, max_addr will keep original value PHYS_ADDR_MAX",
            "\t */",
            "\tfor_each_mem_region(r) {",
            "\t\tif (limit <= r->size) {",
            "\t\t\tmax_addr = r->base + limit;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\tlimit -= r->size;",
            "\t}",
            "",
            "\treturn max_addr;",
            "}",
            "void __init memblock_enforce_memory_limit(phys_addr_t limit)",
            "{",
            "\tphys_addr_t max_addr;",
            "",
            "\tif (!limit)",
            "\t\treturn;",
            "",
            "\tmax_addr = __find_max_addr(limit);",
            "",
            "\t/* @limit exceeds the total size of the memory, do nothing */",
            "\tif (max_addr == PHYS_ADDR_MAX)",
            "\t\treturn;",
            "",
            "\t/* truncate both memory and reserved regions */",
            "\tmemblock_remove_range(&memblock.memory, max_addr,",
            "\t\t\t      PHYS_ADDR_MAX);",
            "\tmemblock_remove_range(&memblock.reserved, max_addr,",
            "\t\t\t      PHYS_ADDR_MAX);",
            "}",
            "void __init memblock_cap_memory_range(phys_addr_t base, phys_addr_t size)",
            "{",
            "\tint start_rgn, end_rgn;",
            "\tint i, ret;",
            "",
            "\tif (!size)",
            "\t\treturn;",
            "",
            "\tif (!memblock_memory->total_size) {",
            "\t\tpr_warn(\"%s: No memory registered yet\\n\", __func__);",
            "\t\treturn;",
            "\t}",
            "",
            "\tret = memblock_isolate_range(&memblock.memory, base, size,",
            "\t\t\t\t\t\t&start_rgn, &end_rgn);",
            "\tif (ret)",
            "\t\treturn;",
            "",
            "\t/* remove all the MAP regions */",
            "\tfor (i = memblock.memory.cnt - 1; i >= end_rgn; i--)",
            "\t\tif (!memblock_is_nomap(&memblock.memory.regions[i]))",
            "\t\t\tmemblock_remove_region(&memblock.memory, i);",
            "",
            "\tfor (i = start_rgn - 1; i >= 0; i--)",
            "\t\tif (!memblock_is_nomap(&memblock.memory.regions[i]))",
            "\t\t\tmemblock_remove_region(&memblock.memory, i);",
            "",
            "\t/* truncate the reserved regions */",
            "\tmemblock_remove_range(&memblock.reserved, 0, base);",
            "\tmemblock_remove_range(&memblock.reserved,",
            "\t\t\tbase + size, PHYS_ADDR_MAX);",
            "}"
          ],
          "function_name": "memblock_phys_alloc_range, memblock_phys_alloc_try_nid, memblock_free_late, memblock_reserved_kern_size, memblock_estimated_nr_free_pages, __find_max_addr, memblock_enforce_memory_limit, memblock_cap_memory_range",
          "description": "实现物理内存分配/释放控制，包含内存上限强制限制、空闲页面估算、内存区域截断等管理功能，支持对保留内存的容量统计",
          "similarity": 0.540441632270813
        }
      ]
    },
    {
      "source_file": "mm/mm_init.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:50:02\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mm_init.c`\n\n---\n\n# mm_init.c 技术文档\n\n## 1. 文件概述\n\n`mm_init.c` 是 Linux 内核内存管理子系统（Memory Management, MM）中的一个初始化和调试辅助文件。其主要作用包括：\n\n- 提供内存初始化过程的验证与调试功能（在 `CONFIG_DEBUG_MEMORY_INIT` 启用时）\n- 初始化内存相关的全局参数和 sysfs 接口\n- 解析内核启动命令行参数（如 `kernelcore` 和 `movablecore`），用于控制不可移动与可移动内存区域的分配策略\n- 在 SMP 系统中动态计算 `vm_committed_as` 的批处理阈值，以优化内存提交统计的性能\n\n该文件不直接参与页分配或虚拟内存管理的核心逻辑，而是为内存子系统的正确性验证、配置和可观测性提供支持。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能描述 |\n|--------|--------|\n| `mminit_verify_zonelist()` | 验证并打印每个 NUMA 节点的 zonelist 结构，用于调试内存区域组织 |\n| `mminit_verify_pageflags_layout()` | 验证 `struct page` 中用于存储节点、区域、节区等元数据的位域布局是否无重叠且对齐正确 |\n| `set_mminit_loglevel()` | 解析 `mminit_loglevel` 内核参数，设置内存初始化调试日志级别 |\n| `mm_compute_batch()` | 根据系统内存总量和 CPU 数量，计算 `vm_committed_as` per-CPU 计数器的批处理阈值 |\n| `mm_compute_batch_notifier()` | 内存热插拔事件回调，重新计算 `vm_committed_as` 批处理值 |\n| `mm_sysfs_init()` | 创建 `/sys/kernel/mm` sysfs 目录，用于暴露内存子系统信息 |\n| `cmdline_parse_core()` | 辅助函数，解析带百分比或字节单位的内存大小参数 |\n| `cmdline_parse_kernelcore()` / `cmdline_parse_movablecore()` | 解析 `kernelcore=` 和 `movablecore=` 内核启动参数 |\n\n### 主要全局变量\n\n| 变量名 | 类型/说明 |\n|--------|---------|\n| `mminit_loglevel` | 调试日志级别（仅当 `CONFIG_DEBUG_MEMORY_INIT` 启用） |\n| `mm_kobj` | 指向 `/sys/kernel/mm` 的 kobject 指针 |\n| `vm_committed_as_batch` | `vm_committed_as` per-CPU 计数器的批处理阈值（SMP） |\n| `required_kernelcore` / `required_kernelcore_percent` | 用户指定的不可移动内存需求（页数或百分比） |\n| `required_movablecore` / `required_movablecore_percent` | 用户指定的可移动内存需求（页数或百分比） |\n| `mirrored_kernelcore` | 是否启用镜像式 kernelcore 布局 |\n| `arch_zone_lowest_possible_pfn[]` / `arch_zone_highest_possible_pfn[]` | 架构定义的各内存区域（ZONE）的 PFN 范围 |\n| `zone_movable_pfn[]` | 各 NUMA 节点上 ZONE_MOVABLE 的起始 PFN |\n| `deferred_struct_pages` | 标记是否延迟初始化 struct page 实例 |\n\n## 3. 关键实现\n\n### 3.1 内存初始化调试（`CONFIG_DEBUG_MEMORY_INIT`）\n\n- **Zonelist 验证**：`mminit_verify_zonelist()` 遍历所有在线 NUMA 节点，打印其“通用”（general）和“本节点优先”（thisnode）两种 zonelist 的组成，帮助开发者确认内存区域的 fallback 顺序是否符合预期。\n- **Page Flags 布局验证**：`mminit_verify_pageflags_layout()` 检查 `struct page` 中用于编码物理位置（section/node/zone）的位域是否：\n  - 总宽度不超过 `BITS_PER_LONG`\n  - 各字段偏移（`_PGSHIFT`）与宽度一致\n  - 位掩码无重叠（通过 `or_mask == add_mask` 验证）\n\n### 3.2 内存区域划分策略\n\n- 通过 `kernelcore=` 和 `movablecore=` 参数，用户可显式指定系统中用于**不可移动分配**（如内核数据结构）和**可移动分配**（如用户页、可迁移 slab）的内存大小。\n- 支持 `kernelcore=mirror` 模式，在支持内存镜像的平台上启用特殊布局。\n- 参数值可为绝对字节数（如 `512M`）或总内存百分比（如 `40%`）。\n\n### 3.3 `vm_committed_as` 批处理优化（SMP）\n\n- `vm_committed_as` 是一个 per-CPU 计数器，跟踪已提交虚拟内存总量。\n- 为减少原子操作开销，当本地计数器变化超过 `vm_committed_as_batch` 时才同步到全局值。\n- `mm_compute_batch()` 根据 overcommit 策略动态调整 batch 大小：\n  - `OVERCOMMIT_NEVER`：batch = 总内存 / CPU数 / 256（约 0.4%）\n  - 其他策略：batch = 总内存 / CPU数 / 4（25%）\n- 注册内存热插拔通知器，确保内存容量变化后重新计算 batch 值。\n\n### 3.4 Sysfs 接口初始化\n\n- `mm_sysfs_init()` 在内核早期创建 `/sys/kernel/mm` 目录，作为内存子系统其他模块（如 compaction、numa、transparent_hugepage 等）注册 sysfs 属性的基础。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/memory.h>`、`<linux/memblock.h>`：内存块和热插拔管理\n  - `<linux/page-isolation.h>`、`<linux/cma.h>`：连续内存分配和页面隔离\n  - `\"internal.h\"`、`\"slab.h\"`：MM 子系统内部接口\n  - `<asm/setup.h>`：架构相关内存布局信息\n- **配置依赖**：\n  - `CONFIG_DEBUG_MEMORY_INIT`：启用调试验证功能\n  - `CONFIG_SMP`：启用 `vm_committed_as_batch` 优化\n  - `CONFIG_SYSFS`：支持 mm sysfs 目录创建\n- **被依赖模块**：\n  - 内存初始化流程（`mm_init()` in `init/main.c`）\n  - 页面分配器（`page_alloc.c`）使用 `zone_movable_pfn` 等变量\n  - 内存热插拔子系统调用 batch 重计算回调\n\n## 5. 使用场景\n\n- **内核开发与调试**：开发者启用 `CONFIG_DEBUG_MEMORY_INIT` 并设置 `mminit_loglevel`，可在启动时验证内存拓扑结构和 page 结构体布局的正确性。\n- **系统部署调优**：管理员通过 `kernelcore=` 或 `movablecore=` 参数，强制划分不可移动/可移动内存区域，以优化透明大页（THP）或避免内存碎片。\n- **高可靠性系统**：使用 `kernelcore=mirror` 在支持的硬件上启用内存镜像，提升容错能力。\n- **大规模 SMP 系统**：自动调整 `vm_committed_as_batch` 减少锁竞争，提升多进程内存密集型应用的性能。\n- **运行时监控**：`/sys/kernel/mm` 为用户空间工具（如 `numastat`、`cma` 调试接口）提供统一入口点。",
      "similarity": 0.5650604963302612,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "mm/mm_init.c",
          "start_line": 151,
          "end_line": 259,
          "content": [
            "static __init int set_mminit_loglevel(char *str)",
            "{",
            "\tget_option(&str, &mminit_loglevel);",
            "\treturn 0;",
            "}",
            "void mm_compute_batch(int overcommit_policy)",
            "{",
            "\tu64 memsized_batch;",
            "\ts32 nr = num_present_cpus();",
            "\ts32 batch = max_t(s32, nr*2, 32);",
            "\tunsigned long ram_pages = totalram_pages();",
            "",
            "\t/*",
            "\t * For policy OVERCOMMIT_NEVER, set batch size to 0.4% of",
            "\t * (total memory/#cpus), and lift it to 25% for other policies",
            "\t * to easy the possible lock contention for percpu_counter",
            "\t * vm_committed_as, while the max limit is INT_MAX",
            "\t */",
            "\tif (overcommit_policy == OVERCOMMIT_NEVER)",
            "\t\tmemsized_batch = min_t(u64, ram_pages/nr/256, INT_MAX);",
            "\telse",
            "\t\tmemsized_batch = min_t(u64, ram_pages/nr/4, INT_MAX);",
            "",
            "\tvm_committed_as_batch = max_t(s32, memsized_batch, batch);",
            "}",
            "static int __meminit mm_compute_batch_notifier(struct notifier_block *self,",
            "\t\t\t\t\tunsigned long action, void *arg)",
            "{",
            "\tswitch (action) {",
            "\tcase MEM_ONLINE:",
            "\tcase MEM_OFFLINE:",
            "\t\tmm_compute_batch(sysctl_overcommit_memory);",
            "\t\tbreak;",
            "\tdefault:",
            "\t\tbreak;",
            "\t}",
            "\treturn NOTIFY_OK;",
            "}",
            "static int __init mm_compute_batch_init(void)",
            "{",
            "\tmm_compute_batch(sysctl_overcommit_memory);",
            "\thotplug_memory_notifier(mm_compute_batch_notifier, MM_COMPUTE_BATCH_PRI);",
            "\treturn 0;",
            "}",
            "static int __init mm_sysfs_init(void)",
            "{",
            "\tmm_kobj = kobject_create_and_add(\"mm\", kernel_kobj);",
            "\tif (!mm_kobj)",
            "\t\treturn -ENOMEM;",
            "",
            "\treturn 0;",
            "}",
            "static int __init cmdline_parse_core(char *p, unsigned long *core,",
            "\t\t\t\t     unsigned long *percent)",
            "{",
            "\tunsigned long long coremem;",
            "\tchar *endptr;",
            "",
            "\tif (!p)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Value may be a percentage of total memory, otherwise bytes */",
            "\tcoremem = simple_strtoull(p, &endptr, 0);",
            "\tif (*endptr == '%') {",
            "\t\t/* Paranoid check for percent values greater than 100 */",
            "\t\tWARN_ON(coremem > 100);",
            "",
            "\t\t*percent = coremem;",
            "\t} else {",
            "\t\tcoremem = memparse(p, &p);",
            "\t\t/* Paranoid check that UL is enough for the coremem value */",
            "\t\tWARN_ON((coremem >> PAGE_SHIFT) > ULONG_MAX);",
            "",
            "\t\t*core = coremem >> PAGE_SHIFT;",
            "\t\t*percent = 0UL;",
            "\t}",
            "\treturn 0;",
            "}",
            "static int __init cmdline_parse_kernelcore(char *p)",
            "{",
            "\t/* parse kernelcore=mirror */",
            "\tif (parse_option_str(p, \"mirror\")) {",
            "\t\tmirrored_kernelcore = true;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\treturn cmdline_parse_core(p, &required_kernelcore,",
            "\t\t\t\t  &required_kernelcore_percent);",
            "}",
            "static int __init cmdline_parse_movablecore(char *p)",
            "{",
            "\treturn cmdline_parse_core(p, &required_movablecore,",
            "\t\t\t\t  &required_movablecore_percent);",
            "}",
            "static unsigned long __init early_calculate_totalpages(void)",
            "{",
            "\tunsigned long totalpages = 0;",
            "\tunsigned long start_pfn, end_pfn;",
            "\tint i, nid;",
            "",
            "\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {",
            "\t\tunsigned long pages = end_pfn - start_pfn;",
            "",
            "\t\ttotalpages += pages;",
            "\t\tif (pages)",
            "\t\t\tnode_set_state(nid, N_MEMORY);",
            "\t}",
            "\treturn totalpages;",
            "}"
          ],
          "function_name": "set_mminit_loglevel, mm_compute_batch, mm_compute_batch_notifier, mm_compute_batch_init, mm_sysfs_init, cmdline_parse_core, cmdline_parse_kernelcore, cmdline_parse_movablecore, early_calculate_totalpages",
          "description": "初始化内存批次计算逻辑，注册内存变化通知回调，解析命令行参数以确定内核核心和可移动内存需求",
          "similarity": 0.55765300989151
        },
        {
          "chunk_id": 3,
          "file_path": "mm/mm_init.c",
          "start_line": 320,
          "end_line": 557,
          "content": [
            "static void __init find_usable_zone_for_movable(void)",
            "{",
            "\tint zone_index;",
            "\tfor (zone_index = MAX_NR_ZONES - 1; zone_index >= 0; zone_index--) {",
            "\t\tif (zone_index == ZONE_MOVABLE)",
            "\t\t\tcontinue;",
            "",
            "\t\tif (arch_zone_highest_possible_pfn[zone_index] >",
            "\t\t\t\tarch_zone_lowest_possible_pfn[zone_index])",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tVM_BUG_ON(zone_index == -1);",
            "\tmovable_zone = zone_index;",
            "}",
            "static void __init find_zone_movable_pfns_for_nodes(void)",
            "{",
            "\tint i, nid;",
            "\tunsigned long usable_startpfn;",
            "\tunsigned long kernelcore_node, kernelcore_remaining;",
            "\t/* save the state before borrow the nodemask */",
            "\tnodemask_t saved_node_state = node_states[N_MEMORY];",
            "\tunsigned long totalpages = early_calculate_totalpages();",
            "\tint usable_nodes = nodes_weight(node_states[N_MEMORY]);",
            "\tstruct memblock_region *r;",
            "",
            "\t/* Need to find movable_zone earlier when movable_node is specified. */",
            "\tfind_usable_zone_for_movable();",
            "",
            "\t/*",
            "\t * If movable_node is specified, ignore kernelcore and movablecore",
            "\t * options.",
            "\t */",
            "\tif (movable_node_is_enabled()) {",
            "\t\tfor_each_mem_region(r) {",
            "\t\t\tif (!memblock_is_hotpluggable(r))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tnid = memblock_get_region_node(r);",
            "",
            "\t\t\tusable_startpfn = PFN_DOWN(r->base);",
            "\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?",
            "\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :",
            "\t\t\t\tusable_startpfn;",
            "\t\t}",
            "",
            "\t\tgoto out2;",
            "\t}",
            "",
            "\t/*",
            "\t * If kernelcore=mirror is specified, ignore movablecore option",
            "\t */",
            "\tif (mirrored_kernelcore) {",
            "\t\tbool mem_below_4gb_not_mirrored = false;",
            "",
            "\t\tif (!memblock_has_mirror()) {",
            "\t\t\tpr_warn(\"The system has no mirror memory, ignore kernelcore=mirror.\\n\");",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tif (is_kdump_kernel()) {",
            "\t\t\tpr_warn(\"The system is under kdump, ignore kernelcore=mirror.\\n\");",
            "\t\t\tgoto out;",
            "\t\t}",
            "",
            "\t\tfor_each_mem_region(r) {",
            "\t\t\tif (memblock_is_mirror(r))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tnid = memblock_get_region_node(r);",
            "",
            "\t\t\tusable_startpfn = memblock_region_memory_base_pfn(r);",
            "",
            "\t\t\tif (usable_startpfn < PHYS_PFN(SZ_4G)) {",
            "\t\t\t\tmem_below_4gb_not_mirrored = true;",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "",
            "\t\t\tzone_movable_pfn[nid] = zone_movable_pfn[nid] ?",
            "\t\t\t\tmin(usable_startpfn, zone_movable_pfn[nid]) :",
            "\t\t\t\tusable_startpfn;",
            "\t\t}",
            "",
            "\t\tif (mem_below_4gb_not_mirrored)",
            "\t\t\tpr_warn(\"This configuration results in unmirrored kernel memory.\\n\");",
            "",
            "\t\tgoto out2;",
            "\t}",
            "",
            "\t/*",
            "\t * If kernelcore=nn% or movablecore=nn% was specified, calculate the",
            "\t * amount of necessary memory.",
            "\t */",
            "\tif (required_kernelcore_percent)",
            "\t\trequired_kernelcore = (totalpages * 100 * required_kernelcore_percent) /",
            "\t\t\t\t       10000UL;",
            "\tif (required_movablecore_percent)",
            "\t\trequired_movablecore = (totalpages * 100 * required_movablecore_percent) /",
            "\t\t\t\t\t10000UL;",
            "",
            "\t/*",
            "\t * If movablecore= was specified, calculate what size of",
            "\t * kernelcore that corresponds so that memory usable for",
            "\t * any allocation type is evenly spread. If both kernelcore",
            "\t * and movablecore are specified, then the value of kernelcore",
            "\t * will be used for required_kernelcore if it's greater than",
            "\t * what movablecore would have allowed.",
            "\t */",
            "\tif (required_movablecore) {",
            "\t\tunsigned long corepages;",
            "",
            "\t\t/*",
            "\t\t * Round-up so that ZONE_MOVABLE is at least as large as what",
            "\t\t * was requested by the user",
            "\t\t */",
            "\t\trequired_movablecore =",
            "\t\t\troundup(required_movablecore, MAX_ORDER_NR_PAGES);",
            "\t\trequired_movablecore = min(totalpages, required_movablecore);",
            "\t\tcorepages = totalpages - required_movablecore;",
            "",
            "\t\trequired_kernelcore = max(required_kernelcore, corepages);",
            "\t}",
            "",
            "\t/*",
            "\t * If kernelcore was not specified or kernelcore size is larger",
            "\t * than totalpages, there is no ZONE_MOVABLE.",
            "\t */",
            "\tif (!required_kernelcore || required_kernelcore >= totalpages)",
            "\t\tgoto out;",
            "",
            "\t/* usable_startpfn is the lowest possible pfn ZONE_MOVABLE can be at */",
            "\tusable_startpfn = arch_zone_lowest_possible_pfn[movable_zone];",
            "",
            "restart:",
            "\t/* Spread kernelcore memory as evenly as possible throughout nodes */",
            "\tkernelcore_node = required_kernelcore / usable_nodes;",
            "\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\tunsigned long start_pfn, end_pfn;",
            "",
            "\t\t/*",
            "\t\t * Recalculate kernelcore_node if the division per node",
            "\t\t * now exceeds what is necessary to satisfy the requested",
            "\t\t * amount of memory for the kernel",
            "\t\t */",
            "\t\tif (required_kernelcore < kernelcore_node)",
            "\t\t\tkernelcore_node = required_kernelcore / usable_nodes;",
            "",
            "\t\t/*",
            "\t\t * As the map is walked, we track how much memory is usable",
            "\t\t * by the kernel using kernelcore_remaining. When it is",
            "\t\t * 0, the rest of the node is usable by ZONE_MOVABLE",
            "\t\t */",
            "\t\tkernelcore_remaining = kernelcore_node;",
            "",
            "\t\t/* Go through each range of PFNs within this node */",
            "\t\tfor_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {",
            "\t\t\tunsigned long size_pages;",
            "",
            "\t\t\tstart_pfn = max(start_pfn, zone_movable_pfn[nid]);",
            "\t\t\tif (start_pfn >= end_pfn)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* Account for what is only usable for kernelcore */",
            "\t\t\tif (start_pfn < usable_startpfn) {",
            "\t\t\t\tunsigned long kernel_pages;",
            "\t\t\t\tkernel_pages = min(end_pfn, usable_startpfn)",
            "\t\t\t\t\t\t\t\t- start_pfn;",
            "",
            "\t\t\t\tkernelcore_remaining -= min(kernel_pages,",
            "\t\t\t\t\t\t\tkernelcore_remaining);",
            "\t\t\t\trequired_kernelcore -= min(kernel_pages,",
            "\t\t\t\t\t\t\trequired_kernelcore);",
            "",
            "\t\t\t\t/* Continue if range is now fully accounted */",
            "\t\t\t\tif (end_pfn <= usable_startpfn) {",
            "",
            "\t\t\t\t\t/*",
            "\t\t\t\t\t * Push zone_movable_pfn to the end so",
            "\t\t\t\t\t * that if we have to rebalance",
            "\t\t\t\t\t * kernelcore across nodes, we will",
            "\t\t\t\t\t * not double account here",
            "\t\t\t\t\t */",
            "\t\t\t\t\tzone_movable_pfn[nid] = end_pfn;",
            "\t\t\t\t\tcontinue;",
            "\t\t\t\t}",
            "\t\t\t\tstart_pfn = usable_startpfn;",
            "\t\t\t}",
            "",
            "\t\t\t/*",
            "\t\t\t * The usable PFN range for ZONE_MOVABLE is from",
            "\t\t\t * start_pfn->end_pfn. Calculate size_pages as the",
            "\t\t\t * number of pages used as kernelcore",
            "\t\t\t */",
            "\t\t\tsize_pages = end_pfn - start_pfn;",
            "\t\t\tif (size_pages > kernelcore_remaining)",
            "\t\t\t\tsize_pages = kernelcore_remaining;",
            "\t\t\tzone_movable_pfn[nid] = start_pfn + size_pages;",
            "",
            "\t\t\t/*",
            "\t\t\t * Some kernelcore has been met, update counts and",
            "\t\t\t * break if the kernelcore for this node has been",
            "\t\t\t * satisfied",
            "\t\t\t */",
            "\t\t\trequired_kernelcore -= min(required_kernelcore,",
            "\t\t\t\t\t\t\t\tsize_pages);",
            "\t\t\tkernelcore_remaining -= size_pages;",
            "\t\t\tif (!kernelcore_remaining)",
            "\t\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * If there is still required_kernelcore, we do another pass with one",
            "\t * less node in the count. This will push zone_movable_pfn[nid] further",
            "\t * along on the nodes that still have memory until kernelcore is",
            "\t * satisfied",
            "\t */",
            "\tusable_nodes--;",
            "\tif (usable_nodes && required_kernelcore > usable_nodes)",
            "\t\tgoto restart;",
            "",
            "out2:",
            "\t/* Align start of ZONE_MOVABLE on all nids to MAX_ORDER_NR_PAGES */",
            "\tfor (nid = 0; nid < MAX_NUMNODES; nid++) {",
            "\t\tunsigned long start_pfn, end_pfn;",
            "",
            "\t\tzone_movable_pfn[nid] =",
            "\t\t\troundup(zone_movable_pfn[nid], MAX_ORDER_NR_PAGES);",
            "",
            "\t\tget_pfn_range_for_nid(nid, &start_pfn, &end_pfn);",
            "\t\tif (zone_movable_pfn[nid] >= end_pfn)",
            "\t\t\tzone_movable_pfn[nid] = 0;",
            "\t}",
            "",
            "out:",
            "\t/* restore the node_state */",
            "\tnode_states[N_MEMORY] = saved_node_state;",
            "}"
          ],
          "function_name": "find_usable_zone_for_movable, find_zone_movable_pfns_for_nodes",
          "description": "确定可移动内存区域位置，根据内核核心/可移动核心比例分配PFN范围，平衡内存分布并设置ZONE_MOVABLE起始地址",
          "similarity": 0.5573514699935913
        },
        {
          "chunk_id": 17,
          "file_path": "mm/mm_init.c",
          "start_line": 2711,
          "end_line": 2806,
          "content": [
            "static void __init mem_init_print_info(void)",
            "{",
            "\tunsigned long physpages, codesize, datasize, rosize, bss_size;",
            "\tunsigned long init_code_size, init_data_size;",
            "",
            "\tphyspages = get_num_physpages();",
            "\tcodesize = _etext - _stext;",
            "\tdatasize = _edata - _sdata;",
            "\trosize = __end_rodata - __start_rodata;",
            "\tbss_size = __bss_stop - __bss_start;",
            "\tinit_data_size = __init_end - __init_begin;",
            "\tinit_code_size = _einittext - _sinittext;",
            "",
            "\t/*",
            "\t * Detect special cases and adjust section sizes accordingly:",
            "\t * 1) .init.* may be embedded into .data sections",
            "\t * 2) .init.text.* may be out of [__init_begin, __init_end],",
            "\t *    please refer to arch/tile/kernel/vmlinux.lds.S.",
            "\t * 3) .rodata.* may be embedded into .text or .data sections.",
            "\t */",
            "#define adj_init_size(start, end, size, pos, adj) \\",
            "\tdo { \\",
            "\t\tif (&start[0] <= &pos[0] && &pos[0] < &end[0] && size > adj) \\",
            "\t\t\tsize -= adj; \\",
            "\t} while (0)",
            "",
            "\tadj_init_size(__init_begin, __init_end, init_data_size,",
            "\t\t     _sinittext, init_code_size);",
            "\tadj_init_size(_stext, _etext, codesize, _sinittext, init_code_size);",
            "\tadj_init_size(_sdata, _edata, datasize, __init_begin, init_data_size);",
            "\tadj_init_size(_stext, _etext, codesize, __start_rodata, rosize);",
            "\tadj_init_size(_sdata, _edata, datasize, __start_rodata, rosize);",
            "",
            "#undef\tadj_init_size",
            "",
            "\tpr_info(\"Memory: %luK/%luK available (%luK kernel code, %luK rwdata, %luK rodata, %luK init, %luK bss, %luK reserved, %luK cma-reserved\"",
            "#ifdef\tCONFIG_HIGHMEM",
            "\t\t\", %luK highmem\"",
            "#endif",
            "\t\t\")\\n\",",
            "\t\tK(nr_free_pages()), K(physpages),",
            "\t\tcodesize / SZ_1K, datasize / SZ_1K, rosize / SZ_1K,",
            "\t\t(init_data_size + init_code_size) / SZ_1K, bss_size / SZ_1K,",
            "\t\tK(physpages - totalram_pages() - totalcma_pages),",
            "\t\tK(totalcma_pages)",
            "#ifdef\tCONFIG_HIGHMEM",
            "\t\t, K(totalhigh_pages())",
            "#endif",
            "\t\t);",
            "}",
            "void __init mm_core_init(void)",
            "{",
            "\t/* Initializations relying on SMP setup */",
            "\tbuild_all_zonelists(NULL);",
            "\tpage_alloc_init_cpuhp();",
            "",
            "\t/*",
            "\t * page_ext requires contiguous pages,",
            "\t * bigger than MAX_PAGE_ORDER unless SPARSEMEM.",
            "\t */",
            "\tpage_ext_init_flatmem();",
            "\tmem_debugging_and_hardening_init();",
            "\tkfence_alloc_pool_and_metadata();",
            "\treport_meminit();",
            "\tkmsan_init_shadow();",
            "\tstack_depot_early_init();",
            "",
            "\t/*",
            "\t * KHO memory setup must happen while memblock is still active, but",
            "\t * as close as possible to buddy initialization",
            "\t */",
            "\tkho_memory_init();",
            "",
            "\tmem_init();",
            "\tmem_init_print_info();",
            "\tkmem_cache_init();",
            "\t/*",
            "\t * page_owner must be initialized after buddy is ready, and also after",
            "\t * slab is ready so that stack_depot_init() works properly",
            "\t */",
            "\tpage_ext_init_flatmem_late();",
            "\tkmemleak_init();",
            "\tptlock_cache_init();",
            "\tpgtable_cache_init();",
            "\tdebug_objects_mem_init();",
            "\tvmalloc_init();",
            "\t/* If no deferred init page_ext now, as vmap is fully initialized */",
            "\tif (!deferred_struct_pages)",
            "\t\tpage_ext_init();",
            "\t/* Should be run before the first non-init thread is created */",
            "\tinit_espfix_bsp();",
            "\t/* Should be run after espfix64 is set up. */",
            "\tpti_init();",
            "\tkmsan_init_runtime();",
            "\tmm_cache_init();",
            "}"
          ],
          "function_name": "mem_init_print_info, mm_core_init",
          "description": "mem_init_print_info计算并打印内存统计信息，包括可用页面数、各段代码数据大小及保留区域。mm_core_init初始化内存核心组件，构建zonelists，初始化slab/kmem_cache，启用调试对象跟踪，设置虚拟内存管理，最后根据是否延迟结构页初始化page_ext模块。",
          "similarity": 0.5471330881118774
        },
        {
          "chunk_id": 6,
          "file_path": "mm/mm_init.c",
          "start_line": 912,
          "end_line": 1017,
          "content": [
            "static void __init memmap_init_zone_range(struct zone *zone,",
            "\t\t\t\t\t  unsigned long start_pfn,",
            "\t\t\t\t\t  unsigned long end_pfn,",
            "\t\t\t\t\t  unsigned long *hole_pfn)",
            "{",
            "\tunsigned long zone_start_pfn = zone->zone_start_pfn;",
            "\tunsigned long zone_end_pfn = zone_start_pfn + zone->spanned_pages;",
            "\tint nid = zone_to_nid(zone), zone_id = zone_idx(zone);",
            "",
            "\tstart_pfn = clamp(start_pfn, zone_start_pfn, zone_end_pfn);",
            "\tend_pfn = clamp(end_pfn, zone_start_pfn, zone_end_pfn);",
            "",
            "\tif (start_pfn >= end_pfn)",
            "\t\treturn;",
            "",
            "\tmemmap_init_range(end_pfn - start_pfn, nid, zone_id, start_pfn,",
            "\t\t\t  zone_end_pfn, MEMINIT_EARLY, NULL, MIGRATE_MOVABLE);",
            "",
            "\tif (*hole_pfn < start_pfn)",
            "\t\tinit_unavailable_range(*hole_pfn, start_pfn, zone_id, nid);",
            "",
            "\t*hole_pfn = end_pfn;",
            "}",
            "static void __init memmap_init(void)",
            "{",
            "\tunsigned long start_pfn, end_pfn;",
            "\tunsigned long hole_pfn = 0;",
            "\tint i, j, zone_id = 0, nid;",
            "",
            "\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, &nid) {",
            "\t\tstruct pglist_data *node = NODE_DATA(nid);",
            "",
            "\t\tfor (j = 0; j < MAX_NR_ZONES; j++) {",
            "\t\t\tstruct zone *zone = node->node_zones + j;",
            "",
            "\t\t\tif (!populated_zone(zone))",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tmemmap_init_zone_range(zone, start_pfn, end_pfn,",
            "\t\t\t\t\t       &hole_pfn);",
            "\t\t\tzone_id = j;",
            "\t\t}",
            "\t}",
            "",
            "#ifdef CONFIG_SPARSEMEM",
            "\t/*",
            "\t * Initialize the memory map for hole in the range [memory_end,",
            "\t * section_end].",
            "\t * Append the pages in this hole to the highest zone in the last",
            "\t * node.",
            "\t * The call to init_unavailable_range() is outside the ifdef to",
            "\t * silence the compiler warining about zone_id set but not used;",
            "\t * for FLATMEM it is a nop anyway",
            "\t */",
            "\tend_pfn = round_up(end_pfn, PAGES_PER_SECTION);",
            "\tif (hole_pfn < end_pfn)",
            "#endif",
            "\t\tinit_unavailable_range(hole_pfn, end_pfn, zone_id, nid);",
            "}",
            "static void __ref __init_zone_device_page(struct page *page, unsigned long pfn,",
            "\t\t\t\t\t  unsigned long zone_idx, int nid,",
            "\t\t\t\t\t  struct dev_pagemap *pgmap)",
            "{",
            "",
            "\t__init_single_page(page, pfn, zone_idx, nid);",
            "",
            "\t/*",
            "\t * Mark page reserved as it will need to wait for onlining",
            "\t * phase for it to be fully associated with a zone.",
            "\t *",
            "\t * We can use the non-atomic __set_bit operation for setting",
            "\t * the flag as we are still initializing the pages.",
            "\t */",
            "\t__SetPageReserved(page);",
            "",
            "\t/*",
            "\t * ZONE_DEVICE pages union ->lru with a ->pgmap back pointer",
            "\t * and zone_device_data.  It is a bug if a ZONE_DEVICE page is",
            "\t * ever freed or placed on a driver-private list.",
            "\t */",
            "\tpage->pgmap = pgmap;",
            "\tpage->zone_device_data = NULL;",
            "",
            "\t/*",
            "\t * Mark the block movable so that blocks are reserved for",
            "\t * movable at startup. This will force kernel allocations",
            "\t * to reserve their blocks rather than leaking throughout",
            "\t * the address space during boot when many long-lived",
            "\t * kernel allocations are made.",
            "\t *",
            "\t * Please note that MEMINIT_HOTPLUG path doesn't clear memmap",
            "\t * because this is done early in section_activate()",
            "\t */",
            "\tif (pageblock_aligned(pfn)) {",
            "\t\tset_pageblock_migratetype(page, MIGRATE_MOVABLE);",
            "\t\tcond_resched();",
            "\t}",
            "",
            "\t/*",
            "\t * ZONE_DEVICE pages are released directly to the driver page allocator",
            "\t * which will set the page count to 1 when allocating the page.",
            "\t */",
            "\tif (pgmap->type == MEMORY_DEVICE_PRIVATE ||",
            "\t    pgmap->type == MEMORY_DEVICE_COHERENT)",
            "\t\tset_page_count(page, 0);",
            "}"
          ],
          "function_name": "memmap_init_zone_range, memmap_init, __init_zone_device_page",
          "description": "遍历各节点和区，调用memmap_init_range初始化内存映射，处理稀疏内存中洞的不可用范围，并调整ZONE_MOVABLE范围以适应架构需求。",
          "similarity": 0.5454516410827637
        },
        {
          "chunk_id": 9,
          "file_path": "mm/mm_init.c",
          "start_line": 1313,
          "end_line": 1420,
          "content": [
            "static unsigned long __init calc_memmap_size(unsigned long spanned_pages,",
            "\t\t\t\t\t\tunsigned long present_pages)",
            "{",
            "\tunsigned long pages = spanned_pages;",
            "",
            "\t/*",
            "\t * Provide a more accurate estimation if there are holes within",
            "\t * the zone and SPARSEMEM is in use. If there are holes within the",
            "\t * zone, each populated memory region may cost us one or two extra",
            "\t * memmap pages due to alignment because memmap pages for each",
            "\t * populated regions may not be naturally aligned on page boundary.",
            "\t * So the (present_pages >> 4) heuristic is a tradeoff for that.",
            "\t */",
            "\tif (spanned_pages > present_pages + (present_pages >> 4) &&",
            "\t    IS_ENABLED(CONFIG_SPARSEMEM))",
            "\t\tpages = present_pages;",
            "",
            "\treturn PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;",
            "}",
            "static void pgdat_init_split_queue(struct pglist_data *pgdat)",
            "{",
            "\tstruct deferred_split *ds_queue = &pgdat->deferred_split_queue;",
            "",
            "\tspin_lock_init(&ds_queue->split_queue_lock);",
            "\tINIT_LIST_HEAD(&ds_queue->split_queue);",
            "\tds_queue->split_queue_len = 0;",
            "}",
            "static void pgdat_init_split_queue(struct pglist_data *pgdat) {}",
            "static void pgdat_init_kcompactd(struct pglist_data *pgdat)",
            "{",
            "\tinit_waitqueue_head(&pgdat->kcompactd_wait);",
            "}",
            "static void pgdat_init_kcompactd(struct pglist_data *pgdat) {}",
            "static void __meminit pgdat_init_internals(struct pglist_data *pgdat)",
            "{",
            "\tint i;",
            "",
            "\tpgdat_resize_init(pgdat);",
            "\tpgdat_kswapd_lock_init(pgdat);",
            "",
            "\tpgdat_init_split_queue(pgdat);",
            "\tpgdat_init_kcompactd(pgdat);",
            "",
            "\tinit_waitqueue_head(&pgdat->kswapd_wait);",
            "\tinit_waitqueue_head(&pgdat->pfmemalloc_wait);",
            "",
            "\tfor (i = 0; i < NR_VMSCAN_THROTTLE; i++)",
            "\t\tinit_waitqueue_head(&pgdat->reclaim_wait[i]);",
            "",
            "\tpgdat_page_ext_init(pgdat);",
            "\tlruvec_init(&pgdat->__lruvec);",
            "}",
            "static void __meminit zone_init_internals(struct zone *zone, enum zone_type idx, int nid,",
            "\t\t\t\t\t\t\tunsigned long remaining_pages)",
            "{",
            "\tatomic_long_set(&zone->managed_pages, remaining_pages);",
            "\tzone_set_nid(zone, nid);",
            "\tzone->name = zone_names[idx];",
            "\tzone->zone_pgdat = NODE_DATA(nid);",
            "\tspin_lock_init(&zone->lock);",
            "\tzone_seqlock_init(zone);",
            "\tzone_pcp_init(zone);",
            "}",
            "static void __meminit zone_init_free_lists(struct zone *zone)",
            "{",
            "\tunsigned int order, t;",
            "\tfor_each_migratetype_order(order, t) {",
            "\t\tINIT_LIST_HEAD(&zone->free_area[order].free_list[t]);",
            "\t\tzone->free_area[order].nr_free = 0;",
            "\t}",
            "",
            "#ifdef CONFIG_UNACCEPTED_MEMORY",
            "\tINIT_LIST_HEAD(&zone->unaccepted_pages);",
            "#endif",
            "}",
            "void __meminit init_currently_empty_zone(struct zone *zone,",
            "\t\t\t\t\tunsigned long zone_start_pfn,",
            "\t\t\t\t\tunsigned long size)",
            "{",
            "\tstruct pglist_data *pgdat = zone->zone_pgdat;",
            "\tint zone_idx = zone_idx(zone) + 1;",
            "",
            "\tif (zone_idx > pgdat->nr_zones)",
            "\t\tpgdat->nr_zones = zone_idx;",
            "",
            "\tzone->zone_start_pfn = zone_start_pfn;",
            "",
            "\tmminit_dprintk(MMINIT_TRACE, \"memmap_init\",",
            "\t\t\t\"Initialising map node %d zone %lu pfns %lu -> %lu\\n\",",
            "\t\t\tpgdat->node_id,",
            "\t\t\t(unsigned long)zone_idx(zone),",
            "\t\t\tzone_start_pfn, (zone_start_pfn + size));",
            "",
            "\tzone_init_free_lists(zone);",
            "\tzone->initialized = 1;",
            "}",
            "static unsigned long __init usemap_size(unsigned long zone_start_pfn, unsigned long zonesize)",
            "{",
            "\tunsigned long usemapsize;",
            "",
            "\tzonesize += zone_start_pfn & (pageblock_nr_pages-1);",
            "\tusemapsize = roundup(zonesize, pageblock_nr_pages);",
            "\tusemapsize = usemapsize >> pageblock_order;",
            "\tusemapsize *= NR_PAGEBLOCK_BITS;",
            "\tusemapsize = roundup(usemapsize, BITS_PER_LONG);",
            "",
            "\treturn usemapsize / BITS_PER_BYTE;",
            "}"
          ],
          "function_name": "calc_memmap_size, pgdat_init_split_queue, pgdat_init_split_queue, pgdat_init_kcompactd, pgdat_init_kcompactd, pgdat_init_internals, zone_init_internals, zone_init_free_lists, init_currently_empty_zone, usemap_size",
          "description": "计算内存映射所需大小，初始化split队列、kcompactd等待队列等内部结构，并配置区的自由列表及空闲页面管理机制。",
          "similarity": 0.5364534854888916
        }
      ]
    }
  ]
}