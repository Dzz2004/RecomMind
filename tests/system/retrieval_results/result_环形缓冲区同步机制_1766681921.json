{
  "query": "环形缓冲区同步机制",
  "timestamp": "2025-12-26 00:58:41",
  "retrieved_files": [
    {
      "source_file": "kernel/trace/ring_buffer.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:07:21\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `trace\\ring_buffer.c`\n\n---\n\n# `trace/ring_buffer.c` 技术文档\n\n## 1. 文件概述\n\n`trace/ring_buffer.c` 实现了 Linux 内核中通用的高性能环形缓冲区（ring buffer）机制，主要用于跟踪（tracing）子系统。该缓冲区支持多 CPU 并发写入、单读者或多读者无锁读取，并通过时间戳压缩、事件类型编码和页面交换等技术优化内存使用和性能。该实现是 ftrace、perf 和其他内核跟踪工具的核心基础设施。\n\n## 2. 核心功能\n\n### 主要函数\n- `ring_buffer_print_entry_header()`：输出环形缓冲区条目头部格式说明，用于调试或用户空间解析。\n- `ring_buffer_event_length()`：返回事件有效载荷（payload）的长度，对 TIME_EXTEND 类型自动跳过扩展头。\n- `rb_event_data()`（内联）：返回指向事件实际数据的指针，处理 TIME_EXTEND 和不同长度编码。\n- `rb_event_length()`：返回完整事件结构（含头部）的字节长度。\n- `rb_event_ts_length()`：返回 TIME_EXTEND 事件及其后续数据事件的总长度。\n- `rb_event_data_length()`：计算数据类型事件的总长度（含头部）。\n- `rb_null_event()` / `rb_event_set_padding()`：判断或设置空/填充事件。\n\n### 关键数据结构（隐含或引用）\n- `struct ring_buffer_event`：环形缓冲区中每个事件的通用头部结构。\n- `struct buffer_data_page`：每个 CPU 缓冲区页面的封装，包含数据和元数据。\n- 每 CPU 页面链表：每个 CPU 拥有独立的环形页面链，写者仅写本地 CPU 缓冲区。\n\n### 核心常量与宏\n- `RINGBUF_TYPE_PADDING`、`RINGBUF_TYPE_TIME_EXTEND`、`RINGBUF_TYPE_TIME_STAMP`、`RINGBUF_TYPE_DATA`：事件类型标识。\n- `RB_ALIGNMENT` / `RB_ARCH_ALIGNMENT`：数据对齐策略，根据架构是否支持 64 位对齐访问调整。\n- `RB_MAX_SMALL_DATA`：小数据事件的最大长度（基于 4 字节对齐和类型长度上限）。\n- `TS_MSB` / `ABS_TS_MASK`：用于处理 59 位时间戳的高位截断与恢复。\n\n## 3. 关键实现\n\n### 无锁读写架构\n- **写者**：每个 CPU 只能写入其对应的 per-CPU 缓冲区，通过原子操作和内存屏障保证写入一致性，无需全局锁。\n- **读者**：每个 per-CPU 缓冲区维护一个独立的“reader page”。当 reader page 被读完后，通过原子交换（未来使用 `cmpxchg`）将其与环形缓冲区中的一个页面互换。交换后，原 reader page 不再被写者访问，读者可安全地将其用于 splice、复制或释放。\n\n### 事件编码与压缩\n- 事件头部使用紧凑位域编码：\n  - `type_len`（5 位）：事件类型或小数据长度（≤31）。\n  - `time_delta`（27 位）：相对于前一事件的时间增量。\n  - `array`（32 位）：用于存储大长度值或事件数据。\n- **TIME_EXTEND 事件**：当时间增量超出 27 位或需要绝对时间戳时，插入一个 8 字节的 TIME_EXTEND 事件，后跟实际数据事件。\n- **数据长度编码**：\n  - 若 `type_len > 0` 且 ≤ `RINGBUF_TYPE_DATA_TYPE_LEN_MAX`，则数据长度 = `type_len * RB_ALIGNMENT`，数据从 `array[0]` 开始。\n  - 否则，数据长度存储在 `array[0]`，实际数据从 `array[1]` 开始。\n\n### 时间戳处理\n- 绝对时间戳仅保留低 59 位（`ABS_TS_MASK`），高 5 位（`TS_MSB`）若非零需单独保存并在读取时恢复，以支持长时间运行的跟踪。\n\n### 内存对齐优化\n- 在支持 64 位对齐访问的架构上（`CONFIG_HAVE_64BIT_ALIGNED_ACCESS`），强制 8 字节对齐（`RB_FORCE_8BYTE_ALIGNMENT`），提升访问性能；否则使用 4 字节对齐。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/ring_buffer.h>`：定义公共 API 和数据结构。\n  - `<linux/trace_clock.h>`、`<linux/sched/clock.h>`：提供高精度时间戳源。\n  - `<linux/percpu.h>`：支持 per-CPU 缓冲区分配。\n  - `<linux/spinlock.h>`、`<asm/local.h>`：提供底层原子操作和锁原语。\n  - `<linux/trace_recursion.h>`：防止跟踪递归。\n- **子系统依赖**：\n  - **ftrace**：主要消费者，用于函数跟踪、事件跟踪等。\n  - **perf**：通过 ring buffer 获取性能事件数据。\n  - **Security Module**：通过 `<linux/security.h>` 集成 LSM 钩子（如 trace 访问控制）。\n- **架构依赖**：依赖 `CONFIG_HAVE_64BIT_ALIGNED_ACCESS` 配置项优化对齐策略。\n\n## 5. 使用场景\n\n- **内核跟踪（ftrace）**：记录函数调用、上下文切换、中断等事件，数据写入 per-CPU ring buffer，用户通过 `tracefs` 读取。\n- **性能分析（perf）**：perf 工具通过 ring buffer 接收内核采样事件（如 PMU 中断、软件事件）。\n- **实时监控与调试**：开发者或运维人员通过读取 ring buffer 内容分析系统行为、延迟或错误。\n- **自测试（selftest）**：文件包含自测试逻辑（依赖 `<linux/kthread.h>`），用于验证 ring buffer 功能正确性。\n- **低开销事件记录**：由于其无锁设计和压缩编码，适用于高频事件记录场景（如每秒百万级事件）。",
      "similarity": 0.5807390213012695,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 45,
          "end_line": 148,
          "content": [
            "int ring_buffer_print_entry_header(struct trace_seq *s)",
            "{",
            "\ttrace_seq_puts(s, \"# compressed entry header\\n\");",
            "\ttrace_seq_puts(s, \"\\ttype_len    :    5 bits\\n\");",
            "\ttrace_seq_puts(s, \"\\ttime_delta  :   27 bits\\n\");",
            "\ttrace_seq_puts(s, \"\\tarray       :   32 bits\\n\");",
            "\ttrace_seq_putc(s, '\\n');",
            "\ttrace_seq_printf(s, \"\\tpadding     : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_PADDING);",
            "\ttrace_seq_printf(s, \"\\ttime_extend : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_TIME_EXTEND);",
            "\ttrace_seq_printf(s, \"\\ttime_stamp : type == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_TIME_STAMP);",
            "\ttrace_seq_printf(s, \"\\tdata max type_len  == %d\\n\",",
            "\t\t\t RINGBUF_TYPE_DATA_TYPE_LEN_MAX);",
            "",
            "\treturn !trace_seq_has_overflowed(s);",
            "}",
            "static inline bool rb_null_event(struct ring_buffer_event *event)",
            "{",
            "\treturn event->type_len == RINGBUF_TYPE_PADDING && !event->time_delta;",
            "}",
            "static void rb_event_set_padding(struct ring_buffer_event *event)",
            "{",
            "\t/* padding has a NULL time_delta */",
            "\tevent->type_len = RINGBUF_TYPE_PADDING;",
            "\tevent->time_delta = 0;",
            "}",
            "static unsigned",
            "rb_event_data_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned length;",
            "",
            "\tif (event->type_len)",
            "\t\tlength = event->type_len * RB_ALIGNMENT;",
            "\telse",
            "\t\tlength = event->array[0];",
            "\treturn length + RB_EVNT_HDR_SIZE;",
            "}",
            "static inline unsigned",
            "rb_event_length(struct ring_buffer_event *event)",
            "{",
            "\tswitch (event->type_len) {",
            "\tcase RINGBUF_TYPE_PADDING:",
            "\t\tif (rb_null_event(event))",
            "\t\t\t/* undefined */",
            "\t\t\treturn -1;",
            "\t\treturn  event->array[0] + RB_EVNT_HDR_SIZE;",
            "",
            "\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\treturn RB_LEN_TIME_EXTEND;",
            "",
            "\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\treturn RB_LEN_TIME_STAMP;",
            "",
            "\tcase RINGBUF_TYPE_DATA:",
            "\t\treturn rb_event_data_length(event);",
            "\tdefault:",
            "\t\tWARN_ON_ONCE(1);",
            "\t}",
            "\t/* not hit */",
            "\treturn 0;",
            "}",
            "static inline unsigned",
            "rb_event_ts_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned len = 0;",
            "",
            "\tif (extended_time(event)) {",
            "\t\t/* time extends include the data event after it */",
            "\t\tlen = RB_LEN_TIME_EXTEND;",
            "\t\tevent = skip_time_extend(event);",
            "\t}",
            "\treturn len + rb_event_length(event);",
            "}",
            "unsigned ring_buffer_event_length(struct ring_buffer_event *event)",
            "{",
            "\tunsigned length;",
            "",
            "\tif (extended_time(event))",
            "\t\tevent = skip_time_extend(event);",
            "",
            "\tlength = rb_event_length(event);",
            "\tif (event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX)",
            "\t\treturn length;",
            "\tlength -= RB_EVNT_HDR_SIZE;",
            "\tif (length > RB_MAX_SMALL_DATA + sizeof(event->array[0]))",
            "                length -= sizeof(event->array[0]);",
            "\treturn length;",
            "}",
            "static u64 rb_event_time_stamp(struct ring_buffer_event *event)",
            "{",
            "\tu64 ts;",
            "",
            "\tts = event->array[0];",
            "\tts <<= TS_SHIFT;",
            "\tts += event->time_delta;",
            "",
            "\treturn ts;",
            "}",
            "static void rb_init_page(struct buffer_data_page *bpage)",
            "{",
            "\tlocal_set(&bpage->commit, 0);",
            "}"
          ],
          "function_name": "ring_buffer_print_entry_header, rb_null_event, rb_event_set_padding, rb_event_data_length, rb_event_length, rb_event_ts_length, ring_buffer_event_length, rb_event_time_stamp, rb_init_page",
          "description": "实现环形缓冲区事件解析功能，包括打印事件头信息、识别空事件、设置填充事件、计算不同事件类型的数据长度及时间戳，提供事件长度和时间戳读取接口",
          "similarity": 0.6298920512199402
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 993,
          "end_line": 1149,
          "content": [
            "static inline bool",
            "rb_wait_cond(struct rb_irq_work *rbwork, struct trace_buffer *buffer,",
            "\t     int cpu, int full, ring_buffer_cond_fn cond, void *data)",
            "{",
            "\tif (rb_watermark_hit(buffer, cpu, full))",
            "\t\treturn true;",
            "",
            "\tif (cond(data))",
            "\t\treturn true;",
            "",
            "\t/*",
            "\t * The events can happen in critical sections where",
            "\t * checking a work queue can cause deadlocks.",
            "\t * After adding a task to the queue, this flag is set",
            "\t * only to notify events to try to wake up the queue",
            "\t * using irq_work.",
            "\t *",
            "\t * We don't clear it even if the buffer is no longer",
            "\t * empty. The flag only causes the next event to run",
            "\t * irq_work to do the work queue wake up. The worse",
            "\t * that can happen if we race with !trace_empty() is that",
            "\t * an event will cause an irq_work to try to wake up",
            "\t * an empty queue.",
            "\t *",
            "\t * There's no reason to protect this flag either, as",
            "\t * the work queue and irq_work logic will do the necessary",
            "\t * synchronization for the wake ups. The only thing",
            "\t * that is necessary is that the wake up happens after",
            "\t * a task has been queued. It's OK for spurious wake ups.",
            "\t */",
            "\tif (full)",
            "\t\trbwork->full_waiters_pending = true;",
            "\telse",
            "\t\trbwork->waiters_pending = true;",
            "",
            "\treturn false;",
            "}",
            "static bool rb_wait_once(void *data)",
            "{",
            "\tlong *once = data;",
            "",
            "\t/* wait_event() actually calls this twice before scheduling*/",
            "\tif (*once > 1)",
            "\t\treturn true;",
            "",
            "\t(*once)++;",
            "\treturn false;",
            "}",
            "int ring_buffer_wait(struct trace_buffer *buffer, int cpu, int full,",
            "\t\t     ring_buffer_cond_fn cond, void *data)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct wait_queue_head *waitq;",
            "\tstruct rb_irq_work *rbwork;",
            "\tlong once = 0;",
            "\tint ret = 0;",
            "",
            "\tif (!cond) {",
            "\t\tcond = rb_wait_once;",
            "\t\tdata = &once;",
            "\t}",
            "",
            "\t/*",
            "\t * Depending on what the caller is waiting for, either any",
            "\t * data in any cpu buffer, or a specific buffer, put the",
            "\t * caller on the appropriate wait queue.",
            "\t */",
            "\tif (cpu == RING_BUFFER_ALL_CPUS) {",
            "\t\trbwork = &buffer->irq_work;",
            "\t\t/* Full only makes sense on per cpu reads */",
            "\t\tfull = 0;",
            "\t} else {",
            "\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\t\treturn -ENODEV;",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\trbwork = &cpu_buffer->irq_work;",
            "\t}",
            "",
            "\tif (full)",
            "\t\twaitq = &rbwork->full_waiters;",
            "\telse",
            "\t\twaitq = &rbwork->waiters;",
            "",
            "\tret = wait_event_interruptible((*waitq),",
            "\t\t\t\trb_wait_cond(rbwork, buffer, cpu, full, cond, data));",
            "",
            "\treturn ret;",
            "}",
            "__poll_t ring_buffer_poll_wait(struct trace_buffer *buffer, int cpu,",
            "\t\t\t  struct file *filp, poll_table *poll_table, int full)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct rb_irq_work *rbwork;",
            "",
            "\tif (cpu == RING_BUFFER_ALL_CPUS) {",
            "\t\trbwork = &buffer->irq_work;",
            "\t\tfull = 0;",
            "\t} else {",
            "\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))",
            "\t\t\treturn EPOLLERR;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\trbwork = &cpu_buffer->irq_work;",
            "\t}",
            "",
            "\tif (full) {",
            "\t\tunsigned long flags;",
            "",
            "\t\tpoll_wait(filp, &rbwork->full_waiters, poll_table);",
            "",
            "\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\tif (!cpu_buffer->shortest_full ||",
            "\t\t    cpu_buffer->shortest_full > full)",
            "\t\t\tcpu_buffer->shortest_full = full;",
            "\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t\tif (full_hit(buffer, cpu, full))",
            "\t\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\t\t/*",
            "\t\t * Only allow full_waiters_pending update to be seen after",
            "\t\t * the shortest_full is set. If the writer sees the",
            "\t\t * full_waiters_pending flag set, it will compare the",
            "\t\t * amount in the ring buffer to shortest_full. If the amount",
            "\t\t * in the ring buffer is greater than the shortest_full",
            "\t\t * percent, it will call the irq_work handler to wake up",
            "\t\t * this list. The irq_handler will reset shortest_full",
            "\t\t * back to zero. That's done under the reader_lock, but",
            "\t\t * the below smp_mb() makes sure that the update to",
            "\t\t * full_waiters_pending doesn't leak up into the above.",
            "\t\t */",
            "\t\tsmp_mb();",
            "\t\trbwork->full_waiters_pending = true;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tpoll_wait(filp, &rbwork->waiters, poll_table);",
            "\trbwork->waiters_pending = true;",
            "",
            "\t/*",
            "\t * There's a tight race between setting the waiters_pending and",
            "\t * checking if the ring buffer is empty.  Once the waiters_pending bit",
            "\t * is set, the next event will wake the task up, but we can get stuck",
            "\t * if there's only a single event in.",
            "\t *",
            "\t * FIXME: Ideally, we need a memory barrier on the writer side as well,",
            "\t * but adding a memory barrier to all events will cause too much of a",
            "\t * performance hit in the fast path.  We only need a memory barrier when",
            "\t * the buffer goes from empty to having content.  But as this race is",
            "\t * extremely small, and it's not a problem if another event comes in, we",
            "\t * will fix it later.",
            "\t */",
            "\tsmp_mb();",
            "",
            "\tif ((cpu == RING_BUFFER_ALL_CPUS && !ring_buffer_empty(buffer)) ||",
            "\t    (cpu != RING_BUFFER_ALL_CPUS && !ring_buffer_empty_cpu(buffer, cpu)))",
            "\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "rb_wait_cond, rb_wait_once, ring_buffer_wait, ring_buffer_poll_wait",
          "description": "实现环形缓冲区的等待逻辑，通过rb_wait_cond判断条件是否满足，ring_buffer_wait将调用者加入等待队列并阻塞直至条件成立或被中断，ring_buffer_poll_wait支持poll风格的等待并返回EPOLLIN标志。",
          "similarity": 0.6261725425720215
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 2052,
          "end_line": 2363,
          "content": [
            "static bool",
            "rb_insert_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct list_head *pages = &cpu_buffer->new_pages;",
            "\tunsigned long flags;",
            "\tbool success;",
            "\tint retries;",
            "",
            "\t/* Can be called at early boot up, where interrupts must not been enabled */",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t/*",
            "\t * We are holding the reader lock, so the reader page won't be swapped",
            "\t * in the ring buffer. Now we are racing with the writer trying to",
            "\t * move head page and the tail page.",
            "\t * We are going to adapt the reader page update process where:",
            "\t * 1. We first splice the start and end of list of new pages between",
            "\t *    the head page and its previous page.",
            "\t * 2. We cmpxchg the prev_page->next to point from head page to the",
            "\t *    start of new pages list.",
            "\t * 3. Finally, we update the head->prev to the end of new list.",
            "\t *",
            "\t * We will try this process 10 times, to make sure that we don't keep",
            "\t * spinning.",
            "\t */",
            "\tretries = 10;",
            "\tsuccess = false;",
            "\twhile (retries--) {",
            "\t\tstruct list_head *head_page, *prev_page, *r;",
            "\t\tstruct list_head *last_page, *first_page;",
            "\t\tstruct list_head *head_page_with_bit;",
            "\t\tstruct buffer_page *hpage = rb_set_head_page(cpu_buffer);",
            "",
            "\t\tif (!hpage)",
            "\t\t\tbreak;",
            "\t\thead_page = &hpage->list;",
            "\t\tprev_page = head_page->prev;",
            "",
            "\t\tfirst_page = pages->next;",
            "\t\tlast_page  = pages->prev;",
            "",
            "\t\thead_page_with_bit = (struct list_head *)",
            "\t\t\t\t     ((unsigned long)head_page | RB_PAGE_HEAD);",
            "",
            "\t\tlast_page->next = head_page_with_bit;",
            "\t\tfirst_page->prev = prev_page;",
            "",
            "\t\tr = cmpxchg(&prev_page->next, head_page_with_bit, first_page);",
            "",
            "\t\tif (r == head_page_with_bit) {",
            "\t\t\t/*",
            "\t\t\t * yay, we replaced the page pointer to our new list,",
            "\t\t\t * now, we just have to update to head page's prev",
            "\t\t\t * pointer to point to end of list",
            "\t\t\t */",
            "\t\t\thead_page->prev = last_page;",
            "\t\t\tsuccess = true;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t}",
            "",
            "\tif (success)",
            "\t\tINIT_LIST_HEAD(pages);",
            "\t/*",
            "\t * If we weren't successful in adding in new pages, warn and stop",
            "\t * tracing",
            "\t */",
            "\tRB_WARN_ON(cpu_buffer, !success);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "",
            "\t/* free pages if they weren't inserted */",
            "\tif (!success) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\t list) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            "\treturn success;",
            "}",
            "static void rb_update_pages(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tbool success;",
            "",
            "\tif (cpu_buffer->nr_pages_to_update > 0)",
            "\t\tsuccess = rb_insert_pages(cpu_buffer);",
            "\telse",
            "\t\tsuccess = rb_remove_pages(cpu_buffer,",
            "\t\t\t\t\t-cpu_buffer->nr_pages_to_update);",
            "",
            "\tif (success)",
            "\t\tcpu_buffer->nr_pages += cpu_buffer->nr_pages_to_update;",
            "}",
            "static void update_pages_handler(struct work_struct *work)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = container_of(work,",
            "\t\t\tstruct ring_buffer_per_cpu, update_pages_work);",
            "\trb_update_pages(cpu_buffer);",
            "\tcomplete(&cpu_buffer->update_done);",
            "}",
            "int ring_buffer_resize(struct trace_buffer *buffer, unsigned long size,",
            "\t\t\tint cpu_id)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tunsigned long nr_pages;",
            "\tint cpu, err;",
            "",
            "\t/*",
            "\t * Always succeed at resizing a non-existent buffer:",
            "\t */",
            "\tif (!buffer)",
            "\t\treturn 0;",
            "",
            "\t/* Make sure the requested buffer exists */",
            "\tif (cpu_id != RING_BUFFER_ALL_CPUS &&",
            "\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))",
            "\t\treturn 0;",
            "",
            "\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);",
            "",
            "\t/* we need a minimum of two pages */",
            "\tif (nr_pages < 2)",
            "\t\tnr_pages = 2;",
            "",
            "\t/* prevent another thread from changing buffer sizes */",
            "\tmutex_lock(&buffer->mutex);",
            "\tatomic_inc(&buffer->resizing);",
            "",
            "\tif (cpu_id == RING_BUFFER_ALL_CPUS) {",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\t\terr = -EBUSY;",
            "\t\t\t\tgoto out_err_unlock;",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* calculate the pages to update */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "",
            "\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "\t\t\t/*",
            "\t\t\t * nothing more to do for removing pages or no update",
            "\t\t\t */",
            "\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)",
            "\t\t\t\tcontinue;",
            "\t\t\t/*",
            "\t\t\t * to add pages, make sure all new pages can be",
            "\t\t\t * allocated without receiving ENOMEM",
            "\t\t\t */",
            "\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\t\tif (__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t\t&cpu_buffer->new_pages)) {",
            "\t\t\t\t/* not enough memory for new pages */",
            "\t\t\t\terr = -ENOMEM;",
            "\t\t\t\tgoto out_err;",
            "\t\t\t}",
            "",
            "\t\t\tcond_resched();",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "\t\t/*",
            "\t\t * Fire off all the required work handlers",
            "\t\t * We can't schedule on offline CPUs, but it's not necessary",
            "\t\t * since we can change their buffer sizes without any race.",
            "\t\t */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\t/* Can't run something on an offline CPU. */",
            "\t\t\tif (!cpu_online(cpu)) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t\t} else {",
            "\t\t\t\t/* Run directly if possible. */",
            "\t\t\t\tmigrate_disable();",
            "\t\t\t\tif (cpu != smp_processor_id()) {",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t\tschedule_work_on(cpu,",
            "\t\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\t} else {",
            "\t\t\t\t\tupdate_pages_handler(&cpu_buffer->update_pages_work);",
            "\t\t\t\t\tmigrate_enable();",
            "\t\t\t\t}",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\t/* wait for all the updates to complete */",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\tif (!cpu_buffer->nr_pages_to_update)",
            "\t\t\t\tcontinue;",
            "",
            "\t\t\tif (cpu_online(cpu))",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\t}",
            "",
            "\t\tcpus_read_unlock();",
            "\t} else {",
            "\t\tcpu_buffer = buffer->buffers[cpu_id];",
            "",
            "\t\tif (nr_pages == cpu_buffer->nr_pages)",
            "\t\t\tgoto out;",
            "",
            "\t\t/*",
            "\t\t * Don't succeed if resizing is disabled, as a reader might be",
            "\t\t * manipulating the ring buffer and is expecting a sane state while",
            "\t\t * this is true.",
            "\t\t */",
            "\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {",
            "\t\t\terr = -EBUSY;",
            "\t\t\tgoto out_err_unlock;",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = nr_pages -",
            "\t\t\t\t\t\tcpu_buffer->nr_pages;",
            "",
            "\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);",
            "\t\tif (cpu_buffer->nr_pages_to_update > 0 &&",
            "\t\t\t__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,",
            "\t\t\t\t\t    &cpu_buffer->new_pages)) {",
            "\t\t\terr = -ENOMEM;",
            "\t\t\tgoto out_err;",
            "\t\t}",
            "",
            "\t\tcpus_read_lock();",
            "",
            "\t\t/* Can't run something on an offline CPU. */",
            "\t\tif (!cpu_online(cpu_id))",
            "\t\t\trb_update_pages(cpu_buffer);",
            "\t\telse {",
            "\t\t\t/* Run directly if possible. */",
            "\t\t\tmigrate_disable();",
            "\t\t\tif (cpu_id == smp_processor_id()) {",
            "\t\t\t\trb_update_pages(cpu_buffer);",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t} else {",
            "\t\t\t\tmigrate_enable();",
            "\t\t\t\tschedule_work_on(cpu_id,",
            "\t\t\t\t\t\t &cpu_buffer->update_pages_work);",
            "\t\t\t\twait_for_completion(&cpu_buffer->update_done);",
            "\t\t\t}",
            "\t\t}",
            "",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "\t\tcpus_read_unlock();",
            "\t}",
            "",
            " out:",
            "\t/*",
            "\t * The ring buffer resize can happen with the ring buffer",
            "\t * enabled, so that the update disturbs the tracing as little",
            "\t * as possible. But if the buffer is disabled, we do not need",
            "\t * to worry about that, and we can take the time to verify",
            "\t * that the buffer is not corrupt.",
            "\t */",
            "\tif (atomic_read(&buffer->record_disabled)) {",
            "\t\tatomic_inc(&buffer->record_disabled);",
            "\t\t/*",
            "\t\t * Even though the buffer was disabled, we must make sure",
            "\t\t * that it is truly disabled before calling rb_check_pages.",
            "\t\t * There could have been a race between checking",
            "\t\t * record_disable and incrementing it.",
            "\t\t */",
            "\t\tsynchronize_rcu();",
            "\t\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\t\tunsigned long flags;",
            "",
            "\t\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\t\t\trb_check_pages(cpu_buffer);",
            "\t\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "\t\t}",
            "\t\tatomic_dec(&buffer->record_disabled);",
            "\t}",
            "",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn 0;",
            "",
            " out_err:",
            "\tfor_each_buffer_cpu(buffer, cpu) {",
            "\t\tstruct buffer_page *bpage, *tmp;",
            "",
            "\t\tcpu_buffer = buffer->buffers[cpu];",
            "\t\tcpu_buffer->nr_pages_to_update = 0;",
            "",
            "\t\tif (list_empty(&cpu_buffer->new_pages))",
            "\t\t\tcontinue;",
            "",
            "\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,",
            "\t\t\t\t\tlist) {",
            "\t\t\tlist_del_init(&bpage->list);",
            "\t\t\tfree_buffer_page(bpage);",
            "\t\t}",
            "\t}",
            " out_err_unlock:",
            "\tatomic_dec(&buffer->resizing);",
            "\tmutex_unlock(&buffer->mutex);",
            "\treturn err;",
            "}"
          ],
          "function_name": "rb_insert_pages, rb_update_pages, update_pages_handler, ring_buffer_resize",
          "description": "实现将新分配的缓冲页插入到环形缓冲区的头部，通过CAS操作确保线程安全地更新链表结构，若失败则释放内存资源。包含调整缓冲区大小的核心逻辑，协调多CPU上的页面分配与更新操作。",
          "similarity": 0.6240429878234863
        },
        {
          "chunk_id": 20,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 4388,
          "end_line": 4493,
          "content": [
            "static void rb_iter_reset(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;",
            "",
            "\t/* Iterator usage is expected to have record disabled */",
            "\titer->head_page = cpu_buffer->reader_page;",
            "\titer->head = cpu_buffer->reader_page->read;",
            "\titer->next_event = iter->head;",
            "",
            "\titer->cache_reader_page = iter->head_page;",
            "\titer->cache_read = cpu_buffer->read;",
            "\titer->cache_pages_removed = cpu_buffer->pages_removed;",
            "",
            "\tif (iter->head) {",
            "\t\titer->read_stamp = cpu_buffer->read_stamp;",
            "\t\titer->page_stamp = cpu_buffer->reader_page->page->time_stamp;",
            "\t} else {",
            "\t\titer->read_stamp = iter->head_page->page->time_stamp;",
            "\t\titer->page_stamp = iter->read_stamp;",
            "\t}",
            "}",
            "void ring_buffer_iter_reset(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tunsigned long flags;",
            "",
            "\tif (!iter)",
            "\t\treturn;",
            "",
            "\tcpu_buffer = iter->cpu_buffer;",
            "",
            "\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);",
            "\trb_iter_reset(iter);",
            "\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);",
            "}",
            "int ring_buffer_iter_empty(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "\tstruct buffer_page *reader;",
            "\tstruct buffer_page *head_page;",
            "\tstruct buffer_page *commit_page;",
            "\tstruct buffer_page *curr_commit_page;",
            "\tunsigned commit;",
            "\tu64 curr_commit_ts;",
            "\tu64 commit_ts;",
            "",
            "\tcpu_buffer = iter->cpu_buffer;",
            "\treader = cpu_buffer->reader_page;",
            "\thead_page = cpu_buffer->head_page;",
            "\tcommit_page = READ_ONCE(cpu_buffer->commit_page);",
            "\tcommit_ts = commit_page->page->time_stamp;",
            "",
            "\t/*",
            "\t * When the writer goes across pages, it issues a cmpxchg which",
            "\t * is a mb(), which will synchronize with the rmb here.",
            "\t * (see rb_tail_page_update())",
            "\t */",
            "\tsmp_rmb();",
            "\tcommit = rb_page_commit(commit_page);",
            "\t/* We want to make sure that the commit page doesn't change */",
            "\tsmp_rmb();",
            "",
            "\t/* Make sure commit page didn't change */",
            "\tcurr_commit_page = READ_ONCE(cpu_buffer->commit_page);",
            "\tcurr_commit_ts = READ_ONCE(curr_commit_page->page->time_stamp);",
            "",
            "\t/* If the commit page changed, then there's more data */",
            "\tif (curr_commit_page != commit_page ||",
            "\t    curr_commit_ts != commit_ts)",
            "\t\treturn 0;",
            "",
            "\t/* Still racy, as it may return a false positive, but that's OK */",
            "\treturn ((iter->head_page == commit_page && iter->head >= commit) ||",
            "\t\t(iter->head_page == reader && commit_page == head_page &&",
            "\t\t head_page->read == commit &&",
            "\t\t iter->head == rb_page_commit(cpu_buffer->reader_page)));",
            "}",
            "static void",
            "rb_update_read_stamp(struct ring_buffer_per_cpu *cpu_buffer,",
            "\t\t     struct ring_buffer_event *event)",
            "{",
            "\tu64 delta;",
            "",
            "\tswitch (event->type_len) {",
            "\tcase RINGBUF_TYPE_PADDING:",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\tdelta = rb_event_time_stamp(event);",
            "\t\tcpu_buffer->read_stamp += delta;",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\tdelta = rb_event_time_stamp(event);",
            "\t\tdelta = rb_fix_abs_ts(delta, cpu_buffer->read_stamp);",
            "\t\tcpu_buffer->read_stamp = delta;",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_DATA:",
            "\t\tcpu_buffer->read_stamp += event->time_delta;",
            "\t\treturn;",
            "",
            "\tdefault:",
            "\t\tRB_WARN_ON(cpu_buffer, 1);",
            "\t}",
            "}"
          ],
          "function_name": "rb_iter_reset, ring_buffer_iter_reset, ring_buffer_iter_empty, rb_update_read_stamp",
          "description": "该代码段主要管理环形缓冲区迭代器状态及时间戳更新机制。  \n`rb_iter_reset` 和 `ring_buffer_iter_reset` 用于安全重置迭代器状态，后者通过锁保护确保多线程环境下的一致性；`ring_buffer_iter_empty` 检测迭代器当前位置是否已到达缓冲区末尾；`rb_update_read_stamp` 根据事件类型动态调整读取时间戳以支持时间戳校准。  \n所有函数协同保障了在高并发场景下对环形缓冲区事件的精确追踪与状态同步。",
          "similarity": 0.616288423538208
        },
        {
          "chunk_id": 21,
          "file_path": "kernel/trace/ring_buffer.c",
          "start_line": 4511,
          "end_line": 4617,
          "content": [
            "static void",
            "rb_update_iter_read_stamp(struct ring_buffer_iter *iter,",
            "\t\t\t  struct ring_buffer_event *event)",
            "{",
            "\tu64 delta;",
            "",
            "\tswitch (event->type_len) {",
            "\tcase RINGBUF_TYPE_PADDING:",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_TIME_EXTEND:",
            "\t\tdelta = rb_event_time_stamp(event);",
            "\t\titer->read_stamp += delta;",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_TIME_STAMP:",
            "\t\tdelta = rb_event_time_stamp(event);",
            "\t\tdelta = rb_fix_abs_ts(delta, iter->read_stamp);",
            "\t\titer->read_stamp = delta;",
            "\t\treturn;",
            "",
            "\tcase RINGBUF_TYPE_DATA:",
            "\t\titer->read_stamp += event->time_delta;",
            "\t\treturn;",
            "",
            "\tdefault:",
            "\t\tRB_WARN_ON(iter->cpu_buffer, 1);",
            "\t}",
            "}",
            "static void rb_advance_reader(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tstruct ring_buffer_event *event;",
            "\tstruct buffer_page *reader;",
            "\tunsigned length;",
            "",
            "\treader = rb_get_reader_page(cpu_buffer);",
            "",
            "\t/* This function should not be called when buffer is empty */",
            "\tif (RB_WARN_ON(cpu_buffer, !reader))",
            "\t\treturn;",
            "",
            "\tevent = rb_reader_event(cpu_buffer);",
            "",
            "\tif (event->type_len <= RINGBUF_TYPE_DATA_TYPE_LEN_MAX)",
            "\t\tcpu_buffer->read++;",
            "",
            "\trb_update_read_stamp(cpu_buffer, event);",
            "",
            "\tlength = rb_event_length(event);",
            "\tcpu_buffer->reader_page->read += length;",
            "\tcpu_buffer->read_bytes += length;",
            "}",
            "static void rb_advance_iter(struct ring_buffer_iter *iter)",
            "{",
            "\tstruct ring_buffer_per_cpu *cpu_buffer;",
            "",
            "\tcpu_buffer = iter->cpu_buffer;",
            "",
            "\t/* If head == next_event then we need to jump to the next event */",
            "\tif (iter->head == iter->next_event) {",
            "\t\t/* If the event gets overwritten again, there's nothing to do */",
            "\t\tif (rb_iter_head_event(iter) == NULL)",
            "\t\t\treturn;",
            "\t}",
            "",
            "\titer->head = iter->next_event;",
            "",
            "\t/*",
            "\t * Check if we are at the end of the buffer.",
            "\t */",
            "\tif (iter->next_event >= rb_page_size(iter->head_page)) {",
            "\t\t/* discarded commits can make the page empty */",
            "\t\tif (iter->head_page == cpu_buffer->commit_page)",
            "\t\t\treturn;",
            "\t\trb_inc_iter(iter);",
            "\t\treturn;",
            "\t}",
            "",
            "\trb_update_iter_read_stamp(iter, iter->event);",
            "}",
            "static int rb_lost_events(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\treturn cpu_buffer->lost_events;",
            "}",
            "static inline bool rb_reader_lock(struct ring_buffer_per_cpu *cpu_buffer)",
            "{",
            "\tif (likely(!in_nmi())) {",
            "\t\traw_spin_lock(&cpu_buffer->reader_lock);",
            "\t\treturn true;",
            "\t}",
            "",
            "\t/*",
            "\t * If an NMI die dumps out the content of the ring buffer",
            "\t * trylock must be used to prevent a deadlock if the NMI",
            "\t * preempted a task that holds the ring buffer locks. If",
            "\t * we get the lock then all is fine, if not, then continue",
            "\t * to do the read, but this can corrupt the ring buffer,",
            "\t * so it must be permanently disabled from future writes.",
            "\t * Reading from NMI is a oneshot deal.",
            "\t */",
            "\tif (raw_spin_trylock(&cpu_buffer->reader_lock))",
            "\t\treturn true;",
            "",
            "\t/* Continue without locking, but disable the ring buffer */",
            "\tatomic_inc(&cpu_buffer->record_disabled);",
            "\treturn false;",
            "}"
          ],
          "function_name": "rb_update_iter_read_stamp, rb_advance_reader, rb_advance_iter, rb_lost_events, rb_reader_lock",
          "description": "该代码段实现了环形缓冲区（ring buffer）的读取逻辑，包含事件时间戳更新、读指针推进、迭代器状态管理和锁控制等功能。  \n`rb_update_iter_read_stamp` 根据事件类型动态调整迭代器的读取时间戳，确保时间序列一致性；`rb_advance_reader` 和 `rb_advance_iter` 共同推进读取位置并维护事件追踪状态。  \n`rb_lost_events` 提供丢失事件计数接口，`rb_reader_lock` 在 NMI 场景下通过尝试加锁避免死锁，保障多线程环境下的数据安全。",
          "similarity": 0.6096900105476379
        }
      ]
    },
    {
      "source_file": "kernel/bpf/ringbuf.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:29:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\ringbuf.c`\n\n---\n\n# `bpf/ringbuf.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/ringbuf.c` 实现了 BPF（Berkeley Packet Filter）子系统中的**环形缓冲区（Ring Buffer）**机制，用于在内核与用户空间之间高效、安全地传递数据。该机制支持两种生产者模式：**内核生产者**（如 BPF 程序）和**用户空间生产者**，并提供内存映射（`mmap`）、等待队列通知、并发控制等核心功能，是 BPF 数据输出（如 perf event 替代方案）的关键基础设施。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_ringbuf`**  \n  环形缓冲区的核心结构体，包含：\n  - `waitq`：等待队列，用于通知用户空间有新数据\n  - `work`：IRQ 工作项，用于异步唤醒等待队列\n  - `mask`：环形缓冲区大小掩码（`data_sz - 1`），用于快速取模\n  - `pages` / `nr_pages`：物理页数组，支持双映射\n  - `spinlock`：用于内核生产者的自旋锁（SMP 对齐）\n  - `busy`：原子变量，用于用户空间生产者的互斥访问（避免持有自旋锁过久）\n  - `consumer_pos` / `producer_pos` / `pending_pos`：消费者、生产者和待提交位置（各自独占一页，支持不同 mmap 权限）\n  - `data[]`：实际数据存储区域（页对齐）\n\n- **`struct bpf_ringbuf_map`**  \n  封装标准 `bpf_map`，关联一个 `bpf_ringbuf` 实例。\n\n- **`struct bpf_ringbuf_hdr`**  \n  8 字节记录头，包含：\n  - `len`：记录有效载荷长度\n  - `pg_off`：记录在页内的偏移（用于跨页处理）\n\n### 主要函数\n\n- **`bpf_ringbuf_area_alloc()`**  \n  分配并初始化环形缓冲区的虚拟内存区域，采用**双映射数据页**技术简化环绕处理。\n\n- **`bpf_ringbuf_alloc()`**  \n  初始化 `bpf_ringbuf` 结构体，设置锁、等待队列、IRQ 工作项及初始位置。\n\n- **`bpf_ringbuf_free()`**  \n  释放环形缓冲区占用的虚拟内存和物理页。\n\n- **`ringbuf_map_alloc()`**  \n  BPF map 分配器回调，验证参数并创建 `bpf_ringbuf_map`。\n\n- **`ringbuf_map_free()`**  \n  BPF map 释放器回调，清理资源。\n\n- **`ringbuf_map_*_elem()` / `ringbuf_map_get_next_key()`**  \n  禁用标准 map 操作（返回 `-ENOTSUPP`），因为 ringbuf 不支持键值操作。\n\n- **`bpf_ringbuf_notify()`**  \n  IRQ 工作回调，唤醒所有等待数据的用户进程。\n\n## 3. 关键实现\n\n### 双映射数据页（Double-Mapped Data Pages）\n\n为简化环形缓冲区**环绕（wrap-around）**时的数据读取逻辑，数据页被**连续映射两次**：\n```\n[meta pages][data pages][data pages (same as first copy)]\n```\n当读取跨越缓冲区末尾时，可直接线性读取第二份映射，无需特殊处理。此设计同时适用于内核和用户空间 `mmap`。\n\n### 权限隔离与安全\n\n- **`consumer_pos` 和 `producer_pos` 各占独立页**，允许通过 `mmap` 设置不同权限：\n  - **内核生产者模式**：`producer_pos` 和数据页对用户空间为**只读**，防止篡改。\n  - **用户空间生产者模式**：仅 `consumer_pos` 对用户空间为**只读**，内核需严格验证用户提交的记录。\n\n### 并发控制策略\n\n- **内核生产者**：使用 `raw_spinlock_t` 保证多生产者安全。\n- **用户空间生产者**：使用 `atomic_t busy` 原子变量，避免在 BPF 程序回调期间长期持有 IRQ 自旋锁（可能导致死锁或延迟）。若 `busy` 被占用，`__bpf_user_ringbuf_peek()` 返回 `-EBUSY`。\n\n### 内存布局与对齐\n\n- 非 `mmap` 部分（`waitq` 到 `pending_pos`）大小由 `RINGBUF_PGOFF` 定义。\n- `consumer_pos`、`producer_pos` 和 `data` 均按 `PAGE_SIZE` 对齐，确保可独立映射。\n- 总元数据页数：`RINGBUF_NR_META_PAGES = RINGBUF_PGOFF + 2`（含 consumer/producer 页）。\n\n### 大小限制\n\n- 最大记录大小：`RINGBUF_MAX_RECORD_SZ = UINT_MAX / 4`（约 1GB）。\n- 最大缓冲区大小受 `bpf_ringbuf_hdr.pg_off`（32 位页偏移）限制，理论最大约 **64GB**。\n\n## 4. 依赖关系\n\n- **BPF 子系统**：依赖 `bpf_map` 基础设施（`bpf_map_area_alloc/free`、`bpf_map_init_from_attr`）。\n- **内存管理**：使用 `alloc_pages_node`、`vmap`/`vunmap`、`__free_page` 管理物理页和虚拟映射。\n- **同步机制**：依赖 `wait_queue`、`irq_work`、`raw_spinlock` 和 `atomic_t`。\n- **BTF（BPF Type Format）**：包含 BTF 相关头文件，可能用于未来类型验证（当前未直接使用）。\n- **用户 API**：与 `uapi/linux/bpf.h` 中的 `BPF_F_NUMA_NODE` 等标志交互。\n\n## 5. 使用场景\n\n- **BPF 程序输出数据**：替代 `bpf_perf_event_output()`，提供更低开销、更高吞吐的内核到用户空间数据通道。\n- **用户空间主动提交数据**：允许用户程序通过 ringbuf 向内核提交样本（需内核验证）。\n- **实时监控与追踪**：用于 eBPF 监控工具（如 `bpftrace`、`libbpf` 应用）高效收集内核事件。\n- **NUMA 感知分配**：支持通过 `BPF_F_NUMA_NODE` 标志在指定 NUMA 节点分配内存，优化性能。",
      "similarity": 0.5696254968643188,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 335,
          "end_line": 447,
          "content": [
            "static u64 ringbuf_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tstruct bpf_ringbuf *rb;",
            "\tint nr_data_pages;",
            "\tint nr_meta_pages;",
            "\tu64 usage = sizeof(struct bpf_ringbuf_map);",
            "",
            "\trb = container_of(map, struct bpf_ringbuf_map, map)->rb;",
            "\tusage += (u64)rb->nr_pages << PAGE_SHIFT;",
            "\tnr_meta_pages = RINGBUF_NR_META_PAGES;",
            "\tnr_data_pages = map->max_entries >> PAGE_SHIFT;",
            "\tusage += (nr_meta_pages + 2 * nr_data_pages) * sizeof(struct page *);",
            "\treturn usage;",
            "}",
            "static size_t bpf_ringbuf_rec_pg_off(struct bpf_ringbuf *rb,",
            "\t\t\t\t     struct bpf_ringbuf_hdr *hdr)",
            "{",
            "\treturn ((void *)hdr - (void *)rb) >> PAGE_SHIFT;",
            "}",
            "static void bpf_ringbuf_commit(void *sample, u64 flags, bool discard)",
            "{",
            "\tunsigned long rec_pos, cons_pos;",
            "\tstruct bpf_ringbuf_hdr *hdr;",
            "\tstruct bpf_ringbuf *rb;",
            "\tu32 new_len;",
            "",
            "\thdr = sample - BPF_RINGBUF_HDR_SZ;",
            "\trb = bpf_ringbuf_restore_from_rec(hdr);",
            "\tnew_len = hdr->len ^ BPF_RINGBUF_BUSY_BIT;",
            "\tif (discard)",
            "\t\tnew_len |= BPF_RINGBUF_DISCARD_BIT;",
            "",
            "\t/* update record header with correct final size prefix */",
            "\txchg(&hdr->len, new_len);",
            "",
            "\t/* if consumer caught up and is waiting for our record, notify about",
            "\t * new data availability",
            "\t */",
            "\trec_pos = (void *)hdr - (void *)rb->data;",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos) & rb->mask;",
            "",
            "\tif (flags & BPF_RB_FORCE_WAKEUP)",
            "\t\tirq_work_queue(&rb->work);",
            "\telse if (cons_pos == rec_pos && !(flags & BPF_RB_NO_WAKEUP))",
            "\t\tirq_work_queue(&rb->work);",
            "}",
            "static int __bpf_user_ringbuf_peek(struct bpf_ringbuf *rb, void **sample, u32 *size)",
            "{",
            "\tint err;",
            "\tu32 hdr_len, sample_len, total_len, flags, *hdr;",
            "\tu64 cons_pos, prod_pos;",
            "",
            "\t/* Synchronizes with smp_store_release() in user-space producer. */",
            "\tprod_pos = smp_load_acquire(&rb->producer_pos);",
            "\tif (prod_pos % 8)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Synchronizes with smp_store_release() in __bpf_user_ringbuf_sample_release() */",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos);",
            "\tif (cons_pos >= prod_pos)",
            "\t\treturn -ENODATA;",
            "",
            "\thdr = (u32 *)((uintptr_t)rb->data + (uintptr_t)(cons_pos & rb->mask));",
            "\t/* Synchronizes with smp_store_release() in user-space producer. */",
            "\thdr_len = smp_load_acquire(hdr);",
            "\tflags = hdr_len & (BPF_RINGBUF_BUSY_BIT | BPF_RINGBUF_DISCARD_BIT);",
            "\tsample_len = hdr_len & ~flags;",
            "\ttotal_len = round_up(sample_len + BPF_RINGBUF_HDR_SZ, 8);",
            "",
            "\t/* The sample must fit within the region advertised by the producer position. */",
            "\tif (total_len > prod_pos - cons_pos)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* The sample must fit within the data region of the ring buffer. */",
            "\tif (total_len > ringbuf_total_data_sz(rb))",
            "\t\treturn -E2BIG;",
            "",
            "\t/* The sample must fit into a struct bpf_dynptr. */",
            "\terr = bpf_dynptr_check_size(sample_len);",
            "\tif (err)",
            "\t\treturn -E2BIG;",
            "",
            "\tif (flags & BPF_RINGBUF_DISCARD_BIT) {",
            "\t\t/* If the discard bit is set, the sample should be skipped.",
            "\t\t *",
            "\t\t * Update the consumer pos, and return -EAGAIN so the caller",
            "\t\t * knows to skip this sample and try to read the next one.",
            "\t\t */",
            "\t\tsmp_store_release(&rb->consumer_pos, cons_pos + total_len);",
            "\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\tif (flags & BPF_RINGBUF_BUSY_BIT)",
            "\t\treturn -ENODATA;",
            "",
            "\t*sample = (void *)((uintptr_t)rb->data +",
            "\t\t\t   (uintptr_t)((cons_pos + BPF_RINGBUF_HDR_SZ) & rb->mask));",
            "\t*size = sample_len;",
            "\treturn 0;",
            "}",
            "static void __bpf_user_ringbuf_sample_release(struct bpf_ringbuf *rb, size_t size, u64 flags)",
            "{",
            "\tu64 consumer_pos;",
            "\tu32 rounded_size = round_up(size + BPF_RINGBUF_HDR_SZ, 8);",
            "",
            "\t/* Using smp_load_acquire() is unnecessary here, as the busy-bit",
            "\t * prevents another task from writing to consumer_pos after it was read",
            "\t * by this task with smp_load_acquire() in __bpf_user_ringbuf_peek().",
            "\t */",
            "\tconsumer_pos = rb->consumer_pos;",
            "\t /* Synchronizes with smp_load_acquire() in user-space producer. */",
            "\tsmp_store_release(&rb->consumer_pos, consumer_pos + rounded_size);",
            "}"
          ],
          "function_name": "ringbuf_map_mem_usage, bpf_ringbuf_rec_pg_off, bpf_ringbuf_commit, __bpf_user_ringbuf_peek, __bpf_user_ringbuf_sample_release",
          "description": "提供了环形缓冲区的内存占用统计、记录位置转换、样本提交及消费操作。包含用户态生产者与消费者的同步机制，通过忙位防止竞态条件，确保样本数据完整性校验和消费进度更新的有序性。",
          "similarity": 0.6593323349952698
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 150,
          "end_line": 258,
          "content": [
            "static void bpf_ringbuf_notify(struct irq_work *work)",
            "{",
            "\tstruct bpf_ringbuf *rb = container_of(work, struct bpf_ringbuf, work);",
            "",
            "\twake_up_all(&rb->waitq);",
            "}",
            "static void bpf_ringbuf_free(struct bpf_ringbuf *rb)",
            "{",
            "\t/* copy pages pointer and nr_pages to local variable, as we are going",
            "\t * to unmap rb itself with vunmap() below",
            "\t */",
            "\tstruct page **pages = rb->pages;",
            "\tint i, nr_pages = rb->nr_pages;",
            "",
            "\tvunmap(rb);",
            "\tfor (i = 0; i < nr_pages; i++)",
            "\t\t__free_page(pages[i]);",
            "\tbpf_map_area_free(pages);",
            "}",
            "static void ringbuf_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tbpf_ringbuf_free(rb_map->rb);",
            "\tbpf_map_area_free(rb_map);",
            "}",
            "static long ringbuf_map_update_elem(struct bpf_map *map, void *key, void *value,",
            "\t\t\t\t    u64 flags)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static long ringbuf_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int ringbuf_map_get_next_key(struct bpf_map *map, void *key,",
            "\t\t\t\t    void *next_key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int ringbuf_map_mmap_kern(struct bpf_map *map, struct vm_area_struct *vma)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "",
            "\tif (vma->vm_flags & VM_WRITE) {",
            "\t\t/* allow writable mapping for the consumer_pos only */",
            "\t\tif (vma->vm_pgoff != 0 || vma->vm_end - vma->vm_start != PAGE_SIZE)",
            "\t\t\treturn -EPERM;",
            "\t}",
            "\t/* remap_vmalloc_range() checks size and offset constraints */",
            "\treturn remap_vmalloc_range(vma, rb_map->rb,",
            "\t\t\t\t   vma->vm_pgoff + RINGBUF_PGOFF);",
            "}",
            "static int ringbuf_map_mmap_user(struct bpf_map *map, struct vm_area_struct *vma)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "",
            "\tif (vma->vm_flags & VM_WRITE) {",
            "\t\tif (vma->vm_pgoff == 0)",
            "\t\t\t/* Disallow writable mappings to the consumer pointer,",
            "\t\t\t * and allow writable mappings to both the producer",
            "\t\t\t * position, and the ring buffer data itself.",
            "\t\t\t */",
            "\t\t\treturn -EPERM;",
            "\t}",
            "\t/* remap_vmalloc_range() checks size and offset constraints */",
            "\treturn remap_vmalloc_range(vma, rb_map->rb, vma->vm_pgoff + RINGBUF_PGOFF);",
            "}",
            "static unsigned long ringbuf_avail_data_sz(struct bpf_ringbuf *rb)",
            "{",
            "\tunsigned long cons_pos, prod_pos;",
            "",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos);",
            "\tprod_pos = smp_load_acquire(&rb->producer_pos);",
            "\treturn prod_pos - cons_pos;",
            "}",
            "static u32 ringbuf_total_data_sz(const struct bpf_ringbuf *rb)",
            "{",
            "\treturn rb->mask + 1;",
            "}",
            "static __poll_t ringbuf_map_poll_kern(struct bpf_map *map, struct file *filp,",
            "\t\t\t\t      struct poll_table_struct *pts)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tpoll_wait(filp, &rb_map->rb->waitq, pts);",
            "",
            "\tif (ringbuf_avail_data_sz(rb_map->rb))",
            "\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\treturn 0;",
            "}",
            "static __poll_t ringbuf_map_poll_user(struct bpf_map *map, struct file *filp,",
            "\t\t\t\t      struct poll_table_struct *pts)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tpoll_wait(filp, &rb_map->rb->waitq, pts);",
            "",
            "\tif (ringbuf_avail_data_sz(rb_map->rb) < ringbuf_total_data_sz(rb_map->rb))",
            "\t\treturn EPOLLOUT | EPOLLWRNORM;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "bpf_ringbuf_notify, bpf_ringbuf_free, ringbuf_map_free, ringbuf_map_update_elem, ringbuf_map_delete_elem, ringbuf_map_get_next_key, ringbuf_map_mmap_kern, ringbuf_map_mmap_user, ringbuf_avail_data_sz, ringbuf_total_data_sz, ringbuf_map_poll_kern, ringbuf_map_poll_user",
          "description": "实现了环形缓冲区的事件通知、资源释放、内存映射控制及I/O监控功能。包含针对用户态和内核态的差异化mmap处理逻辑，通过spinlock和atomic_t实现并发控制，提供poll接口检测缓冲区可用数据状态。",
          "similarity": 0.6508512496948242
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 1,
          "end_line": 149,
          "content": [
            "#include <linux/bpf.h>",
            "#include <linux/btf.h>",
            "#include <linux/err.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/slab.h>",
            "#include <linux/filter.h>",
            "#include <linux/mm.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/wait.h>",
            "#include <linux/poll.h>",
            "#include <linux/kmemleak.h>",
            "#include <uapi/linux/btf.h>",
            "#include <linux/btf_ids.h>",
            "",
            "#define RINGBUF_CREATE_FLAG_MASK (BPF_F_NUMA_NODE)",
            "",
            "/* non-mmap()'able part of bpf_ringbuf (everything up to consumer page) */",
            "#define RINGBUF_PGOFF \\",
            "\t(offsetof(struct bpf_ringbuf, consumer_pos) >> PAGE_SHIFT)",
            "/* consumer page and producer page */",
            "#define RINGBUF_POS_PAGES 2",
            "#define RINGBUF_NR_META_PAGES (RINGBUF_PGOFF + RINGBUF_POS_PAGES)",
            "",
            "#define RINGBUF_MAX_RECORD_SZ (UINT_MAX/4)",
            "",
            "struct bpf_ringbuf {",
            "\twait_queue_head_t waitq;",
            "\tstruct irq_work work;",
            "\tu64 mask;",
            "\tstruct page **pages;",
            "\tint nr_pages;",
            "\traw_spinlock_t spinlock ____cacheline_aligned_in_smp;",
            "\t/* For user-space producer ring buffers, an atomic_t busy bit is used",
            "\t * to synchronize access to the ring buffers in the kernel, rather than",
            "\t * the spinlock that is used for kernel-producer ring buffers. This is",
            "\t * done because the ring buffer must hold a lock across a BPF program's",
            "\t * callback:",
            "\t *",
            "\t *    __bpf_user_ringbuf_peek() // lock acquired",
            "\t * -> program callback_fn()",
            "\t * -> __bpf_user_ringbuf_sample_release() // lock released",
            "\t *",
            "\t * It is unsafe and incorrect to hold an IRQ spinlock across what could",
            "\t * be a long execution window, so we instead simply disallow concurrent",
            "\t * access to the ring buffer by kernel consumers, and return -EBUSY from",
            "\t * __bpf_user_ringbuf_peek() if the busy bit is held by another task.",
            "\t */",
            "\tatomic_t busy ____cacheline_aligned_in_smp;",
            "\t/* Consumer and producer counters are put into separate pages to",
            "\t * allow each position to be mapped with different permissions.",
            "\t * This prevents a user-space application from modifying the",
            "\t * position and ruining in-kernel tracking. The permissions of the",
            "\t * pages depend on who is producing samples: user-space or the",
            "\t * kernel. Note that the pending counter is placed in the same",
            "\t * page as the producer, so that it shares the same cache line.",
            "\t *",
            "\t * Kernel-producer",
            "\t * ---------------",
            "\t * The producer position and data pages are mapped as r/o in",
            "\t * userspace. For this approach, bits in the header of samples are",
            "\t * used to signal to user-space, and to other producers, whether a",
            "\t * sample is currently being written.",
            "\t *",
            "\t * User-space producer",
            "\t * -------------------",
            "\t * Only the page containing the consumer position is mapped r/o in",
            "\t * user-space. User-space producers also use bits of the header to",
            "\t * communicate to the kernel, but the kernel must carefully check and",
            "\t * validate each sample to ensure that they're correctly formatted, and",
            "\t * fully contained within the ring buffer.",
            "\t */",
            "\tunsigned long consumer_pos __aligned(PAGE_SIZE);",
            "\tunsigned long producer_pos __aligned(PAGE_SIZE);",
            "\tunsigned long pending_pos;",
            "\tchar data[] __aligned(PAGE_SIZE);",
            "};",
            "",
            "struct bpf_ringbuf_map {",
            "\tstruct bpf_map map;",
            "\tstruct bpf_ringbuf *rb;",
            "};",
            "",
            "/* 8-byte ring buffer record header structure */",
            "struct bpf_ringbuf_hdr {",
            "\tu32 len;",
            "\tu32 pg_off;",
            "};",
            "",
            "static struct bpf_ringbuf *bpf_ringbuf_area_alloc(size_t data_sz, int numa_node)",
            "{",
            "\tconst gfp_t flags = GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL |",
            "\t\t\t    __GFP_NOWARN | __GFP_ZERO;",
            "\tint nr_meta_pages = RINGBUF_NR_META_PAGES;",
            "\tint nr_data_pages = data_sz >> PAGE_SHIFT;",
            "\tint nr_pages = nr_meta_pages + nr_data_pages;",
            "\tstruct page **pages, *page;",
            "\tstruct bpf_ringbuf *rb;",
            "\tsize_t array_size;",
            "\tint i;",
            "",
            "\t/* Each data page is mapped twice to allow \"virtual\"",
            "\t * continuous read of samples wrapping around the end of ring",
            "\t * buffer area:",
            "\t * ------------------------------------------------------",
            "\t * | meta pages |  real data pages  |  same data pages  |",
            "\t * ------------------------------------------------------",
            "\t * |            | 1 2 3 4 5 6 7 8 9 | 1 2 3 4 5 6 7 8 9 |",
            "\t * ------------------------------------------------------",
            "\t * |            | TA             DA | TA             DA |",
            "\t * ------------------------------------------------------",
            "\t *                               ^^^^^^^",
            "\t *                                  |",
            "\t * Here, no need to worry about special handling of wrapped-around",
            "\t * data due to double-mapped data pages. This works both in kernel and",
            "\t * when mmap()'ed in user-space, simplifying both kernel and",
            "\t * user-space implementations significantly.",
            "\t */",
            "\tarray_size = (nr_meta_pages + 2 * nr_data_pages) * sizeof(*pages);",
            "\tpages = bpf_map_area_alloc(array_size, numa_node);",
            "\tif (!pages)",
            "\t\treturn NULL;",
            "",
            "\tfor (i = 0; i < nr_pages; i++) {",
            "\t\tpage = alloc_pages_node(numa_node, flags, 0);",
            "\t\tif (!page) {",
            "\t\t\tnr_pages = i;",
            "\t\t\tgoto err_free_pages;",
            "\t\t}",
            "\t\tpages[i] = page;",
            "\t\tif (i >= nr_meta_pages)",
            "\t\t\tpages[nr_data_pages + i] = page;",
            "\t}",
            "",
            "\trb = vmap(pages, nr_meta_pages + 2 * nr_data_pages,",
            "\t\t  VM_MAP | VM_USERMAP, PAGE_KERNEL);",
            "\tif (rb) {",
            "\t\tkmemleak_not_leak(pages);",
            "\t\trb->pages = pages;",
            "\t\trb->nr_pages = nr_pages;",
            "\t\treturn rb;",
            "\t}",
            "",
            "err_free_pages:",
            "\tfor (i = 0; i < nr_pages; i++)",
            "\t\t__free_page(pages[i]);",
            "\tbpf_map_area_free(pages);",
            "\treturn NULL;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了bpf_ringbuf结构体及其相关宏，用于管理BPF环形缓冲区的元数据和数据区域。通过页面数组实现环形缓冲区的虚拟连续读取，支持用户态和内核态生产者的差异化权限控制，其中包含消费者/生产者位置指针、忙位原子变量及锁保护的元数据。",
          "similarity": 0.5281429886817932
        }
      ]
    },
    {
      "source_file": "kernel/bpf/queue_stack_maps.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:28:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\queue_stack_maps.c`\n\n---\n\n# bpf/queue_stack_maps.c 技术文档\n\n## 1. 文件概述\n\n`bpf/queue_stack_maps.c` 实现了 BPF（Berkeley Packet Filter）子系统中的两种特殊映射类型：**队列（Queue）** 和 **栈（Stack）**。这两种映射提供先进先出（FIFO）和后进先出（LIFO）的数据结构语义，用于在 eBPF 程序与用户空间之间高效传递数据。该文件定义了共享的底层数据结构 `struct bpf_queue_stack`，并通过两套不同的操作函数（`queue_map_ops` 和 `stack_map_ops`）分别暴露队列和栈的行为。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_queue_stack`**  \n  队列/栈映射的底层实现结构体，包含：\n  - `struct bpf_map map`：继承自通用 BPF 映射结构\n  - `raw_spinlock_t lock`：保护并发访问的原始自旋锁\n  - `u32 head, tail`：环形缓冲区的头尾指针\n  - `u32 size`：缓冲区实际大小（`max_entries + 1`）\n  - `char elements[]`：变长数组，存储实际元素数据（8 字节对齐）\n\n### 主要函数\n\n- **内存管理**\n  - `queue_stack_map_alloc_check()`：验证创建参数合法性\n  - `queue_stack_map_alloc()`：分配并初始化映射内存\n  - `queue_stack_map_free()`：释放映射内存\n  - `queue_stack_map_mem_usage()`：计算内存占用\n\n- **通用操作（返回错误）**\n  - `queue_stack_map_lookup_elem()`：不支持按键查找（返回 `NULL`）\n  - `queue_stack_map_update_elem()`：不支持按键更新（返回 `-EINVAL`）\n  - `queue_stack_map_delete_elem()`：不支持按键删除（返回 `-EINVAL`）\n  - `queue_stack_map_get_next_key()`：不支持迭代（返回 `-EINVAL`）\n\n- **队列专用操作**\n  - `queue_map_peek_elem()`：查看队首元素（不删除）\n  - `queue_map_pop_elem()`：弹出队首元素\n\n- **栈专用操作**\n  - `stack_map_peek_elem()`：查看栈顶元素（不删除）\n  - `stack_map_pop_elem()`：弹出栈顶元素\n\n- **共享写入操作**\n  - `queue_stack_map_push_elem()`：向队列尾部/栈顶部插入元素（支持 `BPF_EXIST` 覆盖模式）\n\n- **辅助函数**\n  - `bpf_queue_stack()`：从 `bpf_map` 指针转换为具体结构体指针\n  - `queue_stack_map_is_empty()`：判断是否为空\n  - `queue_stack_map_is_full()`：判断是否已满\n\n### 操作函数表\n\n- **`queue_map_ops`**：队列映射的操作函数集合\n- **`stack_map_ops`**：栈映射的操作函数集合\n\n## 3. 关键实现\n\n### 环形缓冲区设计\n- 使用 `head` 和 `tail` 指针实现环形缓冲区\n- 缓冲区实际大小为 `max_entries + 1`，通过牺牲一个槽位区分空/满状态：\n  - **空条件**：`head == tail`\n  - **满条件**：`(head + 1) % size == tail`\n\n### 并发安全机制\n- 使用 `raw_spinlock_t` 保护所有操作\n- 特殊处理 NMI（不可屏蔽中断）上下文：\n  - 在 NMI 中使用 `raw_spin_trylock_irqsave()` 避免死锁\n  - 在其他上下文使用 `raw_spin_lock_irqsave()`\n\n### 队列 vs 栈行为差异\n- **队列（FIFO）**：\n  - `pop/peek` 操作从 `tail` 读取\n  - `push` 操作写入 `head`\n- **栈（LIFO）**：\n  - `pop/peek` 操作从 `head - 1` 读取\n  - `push` 操作写入 `head`\n\n### 覆盖写入策略\n- 当缓冲区满时，若指定 `BPF_EXIST` 标志：\n  - **队列**：自动推进 `tail` 指针覆盖最旧元素\n  - **栈**：自动推进 `tail` 指针（逻辑上丢弃最旧元素）\n\n### 内存布局\n- 元数据（`struct bpf_queue_stack`）与元素存储区（`elements[]`）连续分配\n- 元素存储区按 8 字节对齐，确保跨架构兼容性\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/bpf.h>`：BPF 核心定义\n  - `<linux/list.h>`：内核链表操作\n  - `<linux/slab.h>`：内存分配接口\n  - `<linux/btf_ids.h>`：BTF（BPF Type Format）类型信息\n  - `\"percpu_freelist.h\"`：每 CPU 空闲列表（虽包含但未直接使用）\n\n- **内核子系统**：\n  - **BPF 子系统**：作为 BPF 映射类型注册到核心框架\n  - **内存管理子系统**：通过 `bpf_map_area_alloc/free` 分配内存\n  - **锁机制**：依赖内核自旋锁实现并发控制\n\n- **BTF 支持**：\n  - 通过 `BTF_ID_LIST_SINGLE` 声明类型信息，支持运行时类型检查\n\n## 5. 使用场景\n\n- **eBPF 程序与用户空间通信**：\n  - 用户空间通过 `bpf()` 系统调用操作队列/栈\n  - eBPF 程序通过 `bpf_map_push/pop/peek_elem()` 辅助函数访问\n\n- **高性能数据传递**：\n  - 适用于需要顺序处理数据的场景（如事件日志、采样数据）\n  - 队列用于生产者-消费者模型\n  - 栈用于需要后进先出语义的场景（如调用栈跟踪）\n\n- **资源受限环境**：\n  - 固定大小缓冲区避免动态内存分配开销\n  - 无锁设计（仅自旋锁）保证低延迟\n\n- **典型应用**：\n  - 网络数据包采样（队列）\n  - 函数调用跟踪（栈）\n  - 性能监控事件缓冲（队列）",
      "similarity": 0.5542126893997192,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/queue_stack_maps.c",
          "start_line": 1,
          "end_line": 29,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * queue_stack_maps.c: BPF queue and stack maps",
            " *",
            " * Copyright (c) 2018 Politecnico di Torino",
            " */",
            "#include <linux/bpf.h>",
            "#include <linux/list.h>",
            "#include <linux/slab.h>",
            "#include <linux/btf_ids.h>",
            "#include \"percpu_freelist.h\"",
            "",
            "#define QUEUE_STACK_CREATE_FLAG_MASK \\",
            "\t(BPF_F_NUMA_NODE | BPF_F_ACCESS_MASK)",
            "",
            "struct bpf_queue_stack {",
            "\tstruct bpf_map map;",
            "\traw_spinlock_t lock;",
            "\tu32 head, tail;",
            "\tu32 size; /* max_entries + 1 */",
            "",
            "\tchar elements[] __aligned(8);",
            "};",
            "",
            "static struct bpf_queue_stack *bpf_queue_stack(struct bpf_map *map)",
            "{",
            "\treturn container_of(map, struct bpf_queue_stack, map);",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了BPF队列/栈映射的数据结构，包含环形缓冲区所需的head/tail指针、最大容量及元素存储区域，通过container_of宏关联到bpf_map结构",
          "similarity": 0.5706714391708374
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/queue_stack_maps.c",
          "start_line": 30,
          "end_line": 132,
          "content": [
            "static bool queue_stack_map_is_empty(struct bpf_queue_stack *qs)",
            "{",
            "\treturn qs->head == qs->tail;",
            "}",
            "static bool queue_stack_map_is_full(struct bpf_queue_stack *qs)",
            "{",
            "\tu32 head = qs->head + 1;",
            "",
            "\tif (unlikely(head >= qs->size))",
            "\t\thead = 0;",
            "",
            "\treturn head == qs->tail;",
            "}",
            "static int queue_stack_map_alloc_check(union bpf_attr *attr)",
            "{",
            "\t/* check sanity of attributes */",
            "\tif (attr->max_entries == 0 || attr->key_size != 0 ||",
            "\t    attr->value_size == 0 ||",
            "\t    attr->map_flags & ~QUEUE_STACK_CREATE_FLAG_MASK ||",
            "\t    !bpf_map_flags_access_ok(attr->map_flags))",
            "\t\treturn -EINVAL;",
            "",
            "\tif (attr->value_size > KMALLOC_MAX_SIZE)",
            "\t\t/* if value_size is bigger, the user space won't be able to",
            "\t\t * access the elements.",
            "\t\t */",
            "\t\treturn -E2BIG;",
            "",
            "\treturn 0;",
            "}",
            "static void queue_stack_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_queue_stack *qs = bpf_queue_stack(map);",
            "",
            "\tbpf_map_area_free(qs);",
            "}",
            "static long __queue_map_get(struct bpf_map *map, void *value, bool delete)",
            "{",
            "\tstruct bpf_queue_stack *qs = bpf_queue_stack(map);",
            "\tunsigned long flags;",
            "\tint err = 0;",
            "\tvoid *ptr;",
            "",
            "\tif (in_nmi()) {",
            "\t\tif (!raw_spin_trylock_irqsave(&qs->lock, flags))",
            "\t\t\treturn -EBUSY;",
            "\t} else {",
            "\t\traw_spin_lock_irqsave(&qs->lock, flags);",
            "\t}",
            "",
            "\tif (queue_stack_map_is_empty(qs)) {",
            "\t\tmemset(value, 0, qs->map.value_size);",
            "\t\terr = -ENOENT;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tptr = &qs->elements[qs->tail * qs->map.value_size];",
            "\tmemcpy(value, ptr, qs->map.value_size);",
            "",
            "\tif (delete) {",
            "\t\tif (unlikely(++qs->tail >= qs->size))",
            "\t\t\tqs->tail = 0;",
            "\t}",
            "",
            "out:",
            "\traw_spin_unlock_irqrestore(&qs->lock, flags);",
            "\treturn err;",
            "}",
            "static long __stack_map_get(struct bpf_map *map, void *value, bool delete)",
            "{",
            "\tstruct bpf_queue_stack *qs = bpf_queue_stack(map);",
            "\tunsigned long flags;",
            "\tint err = 0;",
            "\tvoid *ptr;",
            "\tu32 index;",
            "",
            "\tif (in_nmi()) {",
            "\t\tif (!raw_spin_trylock_irqsave(&qs->lock, flags))",
            "\t\t\treturn -EBUSY;",
            "\t} else {",
            "\t\traw_spin_lock_irqsave(&qs->lock, flags);",
            "\t}",
            "",
            "\tif (queue_stack_map_is_empty(qs)) {",
            "\t\tmemset(value, 0, qs->map.value_size);",
            "\t\terr = -ENOENT;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\tindex = qs->head - 1;",
            "\tif (unlikely(index >= qs->size))",
            "\t\tindex = qs->size - 1;",
            "",
            "\tptr = &qs->elements[index * qs->map.value_size];",
            "\tmemcpy(value, ptr, qs->map.value_size);",
            "",
            "\tif (delete)",
            "\t\tqs->head = index;",
            "",
            "out:",
            "\traw_spin_unlock_irqrestore(&qs->lock, flags);",
            "\treturn err;",
            "}"
          ],
          "function_name": "queue_stack_map_is_empty, queue_stack_map_is_full, queue_stack_map_alloc_check, queue_stack_map_free, __queue_map_get, __stack_map_get",
          "description": "实现队列/栈的基本操作逻辑，包括空/满判断、内存分配校验、元素获取与删除操作，采用自旋锁保护临界区并处理NMI场景下的中断安全获取",
          "similarity": 0.5087862014770508
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/queue_stack_maps.c",
          "start_line": 165,
          "end_line": 245,
          "content": [
            "static long queue_map_peek_elem(struct bpf_map *map, void *value)",
            "{",
            "\treturn __queue_map_get(map, value, false);",
            "}",
            "static long stack_map_peek_elem(struct bpf_map *map, void *value)",
            "{",
            "\treturn __stack_map_get(map, value, false);",
            "}",
            "static long queue_map_pop_elem(struct bpf_map *map, void *value)",
            "{",
            "\treturn __queue_map_get(map, value, true);",
            "}",
            "static long stack_map_pop_elem(struct bpf_map *map, void *value)",
            "{",
            "\treturn __stack_map_get(map, value, true);",
            "}",
            "static long queue_stack_map_push_elem(struct bpf_map *map, void *value,",
            "\t\t\t\t      u64 flags)",
            "{",
            "\tstruct bpf_queue_stack *qs = bpf_queue_stack(map);",
            "\tunsigned long irq_flags;",
            "\tint err = 0;",
            "\tvoid *dst;",
            "",
            "\t/* BPF_EXIST is used to force making room for a new element in case the",
            "\t * map is full",
            "\t */",
            "\tbool replace = (flags & BPF_EXIST);",
            "",
            "\t/* Check supported flags for queue and stack maps */",
            "\tif (flags & BPF_NOEXIST || flags > BPF_EXIST)",
            "\t\treturn -EINVAL;",
            "",
            "\tif (in_nmi()) {",
            "\t\tif (!raw_spin_trylock_irqsave(&qs->lock, irq_flags))",
            "\t\t\treturn -EBUSY;",
            "\t} else {",
            "\t\traw_spin_lock_irqsave(&qs->lock, irq_flags);",
            "\t}",
            "",
            "\tif (queue_stack_map_is_full(qs)) {",
            "\t\tif (!replace) {",
            "\t\t\terr = -E2BIG;",
            "\t\t\tgoto out;",
            "\t\t}",
            "\t\t/* advance tail pointer to overwrite oldest element */",
            "\t\tif (unlikely(++qs->tail >= qs->size))",
            "\t\t\tqs->tail = 0;",
            "\t}",
            "",
            "\tdst = &qs->elements[qs->head * qs->map.value_size];",
            "\tmemcpy(dst, value, qs->map.value_size);",
            "",
            "\tif (unlikely(++qs->head >= qs->size))",
            "\t\tqs->head = 0;",
            "",
            "out:",
            "\traw_spin_unlock_irqrestore(&qs->lock, irq_flags);",
            "\treturn err;",
            "}",
            "static long queue_stack_map_update_elem(struct bpf_map *map, void *key,",
            "\t\t\t\t\tvoid *value, u64 flags)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "static long queue_stack_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "static int queue_stack_map_get_next_key(struct bpf_map *map, void *key,",
            "\t\t\t\t\tvoid *next_key)",
            "{",
            "\treturn -EINVAL;",
            "}",
            "static u64 queue_stack_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tu64 usage = sizeof(struct bpf_queue_stack);",
            "",
            "\tusage += ((u64)map->max_entries + 1) * map->value_size;",
            "\treturn usage;",
            "}"
          ],
          "function_name": "queue_map_peek_elem, stack_map_peek_elem, queue_map_pop_elem, stack_map_pop_elem, queue_stack_map_push_elem, queue_stack_map_update_elem, queue_stack_map_delete_elem, queue_stack_map_get_next_key, queue_stack_map_mem_usage",
          "description": "封装队列/栈的标准接口方法，提供Peek/Pop操作实现，包含元素推入逻辑及内存使用统计，明确声明不支持Update/Delete操作以保持数据结构语义一致性",
          "similarity": 0.4970926344394684
        }
      ]
    }
  ]
}