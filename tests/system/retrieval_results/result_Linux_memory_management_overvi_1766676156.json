{
  "query": "Linux memory management overview",
  "timestamp": "2025-12-25 23:22:36",
  "retrieved_files": [
    {
      "source_file": "mm/memory.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:42:16\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memory.c`\n\n---\n\n# memory.c 技术文档\n\n## 1. 文件概述\n\n`memory.c` 是 Linux 内核内存管理子系统（MM）的核心实现文件之一，位于 `mm/` 目录下。该文件主要负责虚拟内存到物理内存的映射管理、缺页异常（page fault）处理、页表结构的分配与释放、以及与用户空间内存操作相关的底层机制。它实现了按需加载（demand-loading）、共享页面、交换（swapping）等关键虚拟内存功能，并为多级页表架构（如 x86-64 的四级页表）提供通用支持。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `high_memory`：指向直接映射区域（ZONE_NORMAL）的上界，用于区分低端内存和高端内存。\n- `randomize_va_space`：控制地址空间布局随机化（ASLR）策略的级别（0=关闭，1=部分启用，2=完全启用）。\n- `zero_pfn`：指向全零物理页的页帧号（PFN），用于高效实现只读零页映射。\n- `max_mapnr` 和 `mem_map`（非 NUMA 配置下）：分别表示最大页帧号和全局页描述符数组。\n- `highest_memmap_pfn`：记录系统中最高的已注册页帧号。\n\n### 关键函数\n- `free_pgd_range()`：释放指定虚拟地址范围内的用户级页表结构（从 PGD 到 PTE）。\n- `free_p4d_range()`, `free_pud_range()`, `free_pmd_range()`, `free_pte_range()`：递归释放各级页表项及其对应的页表页。\n- `do_fault()`：处理基于文件映射的缺页异常。\n- `do_anonymous_page()`：处理匿名映射（如堆、栈）的缺页异常。\n- `vmf_orig_pte_uffd_wp()`：判断原始 PTE 是否为 userfaultfd 写保护标记。\n- `init_zero_pfn()`：早期初始化 `zero_pfn`。\n- `mm_trace_rss_stat()`：触发 RSS（Resident Set Size）统计的跟踪事件。\n\n### 内联辅助函数\n- `arch_wants_old_prefaulted_pte()`：允许架构层决定预取页表项是否应标记为“old”以优化访问位更新开销。\n\n## 3. 关键实现\n\n### 页表释放机制\n- 采用自顶向下（PGD → P4D → PUD → PMD → PTE）的递归方式释放页表。\n- 每级释放函数（如 `free_pmd_range`）遍历地址范围内的页表项：\n  - 跳过空或无效项（`pmd_none_or_clear_bad`）。\n  - 递归释放下一级页表。\n  - 在满足对齐和边界条件（`floor`/`ceiling`）时，释放当前级页表页并更新 MMU gather 结构中的计数器（如 `mm_dec_nr_ptes`）。\n- 使用 `mmu_gather` 机制批量延迟 TLB 刷新和页表页释放，提升性能。\n\n### 缺页处理框架\n- 提供 `do_fault` 和 `do_anonymous_page` 作为缺页处理的核心入口，分别处理文件映射和匿名映射。\n- 支持 `userfaultfd` 写保护机制，通过 `vmf_orig_pte_uffd_wp` 检测特殊 PTE 标记。\n\n### 地址空间随机化（ASLR）\n- 通过 `randomize_va_space` 控制栈、mmap 区域、brk 等的随机化行为。\n- 支持内核启动参数 `norandmaps` 完全禁用 ASLR。\n- 兼容旧版 libc5 二进制（`CONFIG_COMPAT_BRK`），此时 brk 区域不参与随机化。\n\n### 零页优化\n- `zero_pfn` 指向一个全局只读的全零物理页，用于高效实现对未初始化数据段（如 `.bss`）或显式映射 `/dev/zero` 的只读访问，避免每次分配新页。\n\n### 架构适配\n- 通过 `arch_wants_old_prefaulted_pte` 允许特定架构优化页表项的“young/old”状态设置。\n- 依赖 `asm/mmu_context.h`、`asm/pgalloc.h`、`asm/tlb.h` 等架构相关头文件实现底层操作。\n\n## 4. 依赖关系\n\n### 内核头文件依赖\n- **内存管理核心**：`<linux/mm.h>`, `<linux/mman.h>`, `<linux/swap.h>`, `<linux/pagemap.h>`, `<linux/memcontrol.h>`\n- **进程与调度**：`<linux/sched/mm.h>`, `<linux/sched/task.h>`, `<linux/delayacct.h>`\n- **NUMA 与迁移**：`<linux/numa.h>`, `<linux/migrate.h>`, `<linux/sched/numa_balancing.h>`\n- **特殊内存类型**：`<linux/hugetlb.h>`, `<linux/highmem.h>`, `<linux/dax.h>`, `<linux/zswap.h>`\n- **调试与跟踪**：`<trace/events/kmem.h>`, `<linux/debugfs.h>`, `<linux/oom.h>`\n- **架构相关**：`<asm/io.h>`, `<asm/mmu_context.h>`, `<asm/pgalloc.h>`, `<asm/tlbflush.h>`\n\n### 内部模块依赖\n- `internal.h`：包含 MM 子系统内部通用定义。\n- `swap.h`：交换子系统接口。\n- `pgalloc-track.h`：页表分配跟踪（用于调试）。\n\n## 5. 使用场景\n\n- **进程创建与退出**：在 `fork()` 和进程终止时，通过 `free_pgd_range` 释放整个地址空间的页表。\n- **内存映射操作**：`mmap()`、`munmap()`、`mremap()` 等系统调用触发页表的建立或释放。\n- **缺页异常处理**：当 CPU 访问未映射或换出的虚拟地址时，由体系结构相关的缺页处理程序调用 `do_fault` 或 `do_anonymous_page`。\n- **内存回收**：在内存压力下，kswapd 或直接回收路径可能触发页表清理。\n- **用户态内存监控**：`userfaultfd` 机制利用 `vmf_orig_pte_uffd_wp` 实现用户空间对缺页事件的精细控制。\n- **内核初始化**：早期调用 `init_zero_pfn` 设置零页，`paging_init()`（架构相关）初始化 `high_memory` 和 `ZERO_PAGE`。",
      "similarity": 0.6674044132232666,
      "chunks": [
        {
          "chunk_id": 25,
          "file_path": "mm/memory.c",
          "start_line": 4037,
          "end_line": 4496,
          "content": [
            "static inline unsigned long thp_swap_suitable_orders(pgoff_t swp_offset,",
            "\t\t\t\t\t\t     unsigned long addr,",
            "\t\t\t\t\t\t     unsigned long orders)",
            "{",
            "\tint order, nr;",
            "",
            "\torder = highest_order(orders);",
            "",
            "\t/*",
            "\t * To swap in a THP with nr pages, we require that its first swap_offset",
            "\t * is aligned with that number, as it was when the THP was swapped out.",
            "\t * This helps filter out most invalid entries.",
            "\t */",
            "\twhile (orders) {",
            "\t\tnr = 1 << order;",
            "\t\tif ((addr >> PAGE_SHIFT) % nr == swp_offset % nr)",
            "\t\t\tbreak;",
            "\t\torder = next_order(&orders, order);",
            "\t}",
            "",
            "\treturn orders;",
            "}",
            "vm_fault_t do_swap_page(struct vm_fault *vmf)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tstruct folio *swapcache, *folio = NULL;",
            "\tDECLARE_WAITQUEUE(wait, current);",
            "\tstruct page *page;",
            "\tstruct swap_info_struct *si = NULL;",
            "\trmap_t rmap_flags = RMAP_NONE;",
            "\tbool need_clear_cache = false;",
            "\tbool exclusive = false;",
            "\tswp_entry_t entry;",
            "\tpte_t pte;",
            "\tvm_fault_t ret = 0;",
            "\tvoid *shadow = NULL;",
            "\tint nr_pages;",
            "\tunsigned long page_idx;",
            "\tunsigned long address;",
            "\tpte_t *ptep;",
            "",
            "\tif (!pte_unmap_same(vmf))",
            "\t\tgoto out;",
            "",
            "\tentry = pte_to_swp_entry(vmf->orig_pte);",
            "\tif (unlikely(non_swap_entry(entry))) {",
            "\t\tif (is_migration_entry(entry)) {",
            "\t\t\tmigration_entry_wait(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\t     vmf->address);",
            "\t\t} else if (is_device_exclusive_entry(entry)) {",
            "\t\t\tvmf->page = pfn_swap_entry_to_page(entry);",
            "\t\t\tret = remove_device_exclusive_entry(vmf);",
            "\t\t} else if (is_device_private_entry(entry)) {",
            "\t\t\tif (vmf->flags & FAULT_FLAG_VMA_LOCK) {",
            "\t\t\t\t/*",
            "\t\t\t\t * migrate_to_ram is not yet ready to operate",
            "\t\t\t\t * under VMA lock.",
            "\t\t\t\t */",
            "\t\t\t\tvma_end_read(vma);",
            "\t\t\t\tret = VM_FAULT_RETRY;",
            "\t\t\t\tgoto out;",
            "\t\t\t}",
            "",
            "\t\t\tvmf->page = pfn_swap_entry_to_page(entry);",
            "\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\tvmf->address, &vmf->ptl);",
            "\t\t\tif (unlikely(!vmf->pte ||",
            "\t\t\t\t     !pte_same(ptep_get(vmf->pte),",
            "\t\t\t\t\t\t\tvmf->orig_pte)))",
            "\t\t\t\tgoto unlock;",
            "",
            "\t\t\t/*",
            "\t\t\t * Get a page reference while we know the page can't be",
            "\t\t\t * freed.",
            "\t\t\t */",
            "\t\t\tget_page(vmf->page);",
            "\t\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\t\t\tret = vmf->page->pgmap->ops->migrate_to_ram(vmf);",
            "\t\t\tput_page(vmf->page);",
            "\t\t} else if (is_hwpoison_entry(entry)) {",
            "\t\t\tret = VM_FAULT_HWPOISON;",
            "\t\t} else if (is_pte_marker_entry(entry)) {",
            "\t\t\tret = handle_pte_marker(vmf);",
            "\t\t} else {",
            "\t\t\tprint_bad_pte(vma, vmf->address, vmf->orig_pte, NULL);",
            "\t\t\tret = VM_FAULT_SIGBUS;",
            "\t\t}",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* Prevent swapoff from happening to us. */",
            "\tsi = get_swap_device(entry);",
            "\tif (unlikely(!si))",
            "\t\tgoto out;",
            "",
            "\tfolio = swap_cache_get_folio(entry, vma, vmf->address);",
            "\tif (folio)",
            "\t\tpage = folio_file_page(folio, swp_offset(entry));",
            "\tswapcache = folio;",
            "",
            "\tif (!folio) {",
            "\t\tif (data_race(si->flags & SWP_SYNCHRONOUS_IO) &&",
            "\t\t    __swap_count(entry) == 1) {",
            "\t\t\t/* skip swapcache */",
            "\t\t\tfolio = alloc_swap_folio(vmf);",
            "\t\t\tpage = &folio->page;",
            "\t\t\tif (folio) {",
            "\t\t\t\t__folio_set_locked(folio);",
            "\t\t\t\t__folio_set_swapbacked(folio);",
            "",
            "\t\t\t\tnr_pages = folio_nr_pages(folio);",
            "\t\t\t\tif (folio_test_large(folio))",
            "\t\t\t\t\tentry.val = ALIGN_DOWN(entry.val, nr_pages);",
            "\t\t\t\t/*",
            "\t\t\t\t * Prevent parallel swapin from proceeding with",
            "\t\t\t\t * the cache flag. Otherwise, another thread",
            "\t\t\t\t * may finish swapin first, free the entry, and",
            "\t\t\t\t * swapout reusing the same entry. It's",
            "\t\t\t\t * undetectable as pte_same() returns true due",
            "\t\t\t\t * to entry reuse.",
            "\t\t\t\t */",
            "\t\t\t\tif (swapcache_prepare(entry, nr_pages)) {",
            "\t\t\t\t\t/*",
            "\t\t\t\t\t * Relax a bit to prevent rapid",
            "\t\t\t\t\t * repeated page faults.",
            "\t\t\t\t\t */",
            "\t\t\t\t\tadd_wait_queue(&swapcache_wq, &wait);",
            "\t\t\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\t\t\tremove_wait_queue(&swapcache_wq, &wait);",
            "\t\t\t\t\tgoto out_page;",
            "\t\t\t\t}",
            "\t\t\t\tneed_clear_cache = true;",
            "",
            "\t\t\t\tmem_cgroup_swapin_uncharge_swap(entry, nr_pages);",
            "",
            "\t\t\t\tshadow = get_shadow_from_swap_cache(entry);",
            "\t\t\t\tif (shadow)",
            "\t\t\t\t\tworkingset_refault(folio, shadow);",
            "",
            "\t\t\t\tfolio_add_lru(folio);",
            "",
            "\t\t\t\t/* To provide entry to swap_read_folio() */",
            "\t\t\t\tfolio->swap = entry;",
            "\t\t\t\tswap_read_folio(folio, NULL);",
            "\t\t\t\tfolio->private = NULL;",
            "\t\t\t}",
            "\t\t} else {",
            "\t\t\tpage = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,",
            "\t\t\t\t\t\tvmf);",
            "\t\t\tif (page)",
            "\t\t\t\tfolio = page_folio(page);",
            "\t\t\tswapcache = folio;",
            "\t\t}",
            "",
            "\t\tif (!folio) {",
            "\t\t\t/*",
            "\t\t\t * Back out if somebody else faulted in this pte",
            "\t\t\t * while we released the pte lock.",
            "\t\t\t */",
            "\t\t\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,",
            "\t\t\t\t\tvmf->address, &vmf->ptl);",
            "\t\t\tif (likely(vmf->pte &&",
            "\t\t\t\t   pte_same(ptep_get(vmf->pte), vmf->orig_pte)))",
            "\t\t\t\tret = VM_FAULT_OOM;",
            "\t\t\tgoto unlock;",
            "\t\t}",
            "",
            "\t\t/* Had to read the page from swap area: Major fault */",
            "\t\tret = VM_FAULT_MAJOR;",
            "\t\tcount_vm_event(PGMAJFAULT);",
            "\t\tcount_memcg_event_mm(vma->vm_mm, PGMAJFAULT);",
            "\t} else if (PageHWPoison(page)) {",
            "\t\t/*",
            "\t\t * hwpoisoned dirty swapcache pages are kept for killing",
            "\t\t * owner processes (which may be unknown at hwpoison time)",
            "\t\t */",
            "\t\tret = VM_FAULT_HWPOISON;",
            "\t\tgoto out_release;",
            "\t}",
            "",
            "\tret |= folio_lock_or_retry(folio, vmf);",
            "\tif (ret & VM_FAULT_RETRY)",
            "\t\tgoto out_release;",
            "",
            "\tif (swapcache) {",
            "\t\t/*",
            "\t\t * Make sure folio_free_swap() or swapoff did not release the",
            "\t\t * swapcache from under us.  The page pin, and pte_same test",
            "\t\t * below, are not enough to exclude that.  Even if it is still",
            "\t\t * swapcache, we need to check that the page's swap has not",
            "\t\t * changed.",
            "\t\t */",
            "\t\tif (unlikely(!folio_test_swapcache(folio) ||",
            "\t\t\t     page_swap_entry(page).val != entry.val))",
            "\t\t\tgoto out_page;",
            "",
            "\t\t/*",
            "\t\t * KSM sometimes has to copy on read faults, for example, if",
            "\t\t * page->index of !PageKSM() pages would be nonlinear inside the",
            "\t\t * anon VMA -- PageKSM() is lost on actual swapout.",
            "\t\t */",
            "\t\tfolio = ksm_might_need_to_copy(folio, vma, vmf->address);",
            "\t\tif (unlikely(!folio)) {",
            "\t\t\tret = VM_FAULT_OOM;",
            "\t\t\tfolio = swapcache;",
            "\t\t\tgoto out_page;",
            "\t\t} else if (unlikely(folio == ERR_PTR(-EHWPOISON))) {",
            "\t\t\tret = VM_FAULT_HWPOISON;",
            "\t\t\tfolio = swapcache;",
            "\t\t\tgoto out_page;",
            "\t\t}",
            "\t\tif (folio != swapcache)",
            "\t\t\tpage = folio_page(folio, 0);",
            "",
            "\t\t/*",
            "\t\t * If we want to map a page that's in the swapcache writable, we",
            "\t\t * have to detect via the refcount if we're really the exclusive",
            "\t\t * owner. Try removing the extra reference from the local LRU",
            "\t\t * caches if required.",
            "\t\t */",
            "\t\tif ((vmf->flags & FAULT_FLAG_WRITE) && folio == swapcache &&",
            "\t\t    !folio_test_ksm(folio) && !folio_test_lru(folio))",
            "\t\t\tlru_add_drain();",
            "\t}",
            "",
            "\tfolio_throttle_swaprate(folio, GFP_KERNEL);",
            "",
            "\t/*",
            "\t * Back out if somebody else already faulted in this pte.",
            "\t */",
            "\tvmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,",
            "\t\t\t&vmf->ptl);",
            "\tif (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte)))",
            "\t\tgoto out_nomap;",
            "",
            "\tif (unlikely(!folio_test_uptodate(folio))) {",
            "\t\tret = VM_FAULT_SIGBUS;",
            "\t\tgoto out_nomap;",
            "\t}",
            "",
            "\t/* allocated large folios for SWP_SYNCHRONOUS_IO */",
            "\tif (folio_test_large(folio) && !folio_test_swapcache(folio)) {",
            "\t\tunsigned long nr = folio_nr_pages(folio);",
            "\t\tunsigned long folio_start = ALIGN_DOWN(vmf->address, nr * PAGE_SIZE);",
            "\t\tunsigned long idx = (vmf->address - folio_start) / PAGE_SIZE;",
            "\t\tpte_t *folio_ptep = vmf->pte - idx;",
            "\t\tpte_t folio_pte = ptep_get(folio_ptep);",
            "",
            "\t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||",
            "\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)",
            "\t\t\tgoto out_nomap;",
            "",
            "\t\tpage_idx = idx;",
            "\t\taddress = folio_start;",
            "\t\tptep = folio_ptep;",
            "\t\tgoto check_folio;",
            "\t}",
            "",
            "\tnr_pages = 1;",
            "\tpage_idx = 0;",
            "\taddress = vmf->address;",
            "\tptep = vmf->pte;",
            "\tif (folio_test_large(folio) && folio_test_swapcache(folio)) {",
            "\t\tint nr = folio_nr_pages(folio);",
            "\t\tunsigned long idx = folio_page_idx(folio, page);",
            "\t\tunsigned long folio_start = address - idx * PAGE_SIZE;",
            "\t\tunsigned long folio_end = folio_start + nr * PAGE_SIZE;",
            "\t\tpte_t *folio_ptep;",
            "\t\tpte_t folio_pte;",
            "",
            "\t\tif (unlikely(folio_start < max(address & PMD_MASK, vma->vm_start)))",
            "\t\t\tgoto check_folio;",
            "\t\tif (unlikely(folio_end > pmd_addr_end(address, vma->vm_end)))",
            "\t\t\tgoto check_folio;",
            "",
            "\t\tfolio_ptep = vmf->pte - idx;",
            "\t\tfolio_pte = ptep_get(folio_ptep);",
            "\t\tif (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||",
            "\t\t    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)",
            "\t\t\tgoto check_folio;",
            "",
            "\t\tpage_idx = idx;",
            "\t\taddress = folio_start;",
            "\t\tptep = folio_ptep;",
            "\t\tnr_pages = nr;",
            "\t\tentry = folio->swap;",
            "\t\tpage = &folio->page;",
            "\t}",
            "",
            "check_folio:",
            "\t/*",
            "\t * PG_anon_exclusive reuses PG_mappedtodisk for anon pages. A swap pte",
            "\t * must never point at an anonymous page in the swapcache that is",
            "\t * PG_anon_exclusive. Sanity check that this holds and especially, that",
            "\t * no filesystem set PG_mappedtodisk on a page in the swapcache. Sanity",
            "\t * check after taking the PT lock and making sure that nobody",
            "\t * concurrently faulted in this page and set PG_anon_exclusive.",
            "\t */",
            "\tBUG_ON(!folio_test_anon(folio) && folio_test_mappedtodisk(folio));",
            "\tBUG_ON(folio_test_anon(folio) && PageAnonExclusive(page));",
            "",
            "\t/*",
            "\t * Check under PT lock (to protect against concurrent fork() sharing",
            "\t * the swap entry concurrently) for certainly exclusive pages.",
            "\t */",
            "\tif (!folio_test_ksm(folio)) {",
            "\t\texclusive = pte_swp_exclusive(vmf->orig_pte);",
            "\t\tif (folio != swapcache) {",
            "\t\t\t/*",
            "\t\t\t * We have a fresh page that is not exposed to the",
            "\t\t\t * swapcache -> certainly exclusive.",
            "\t\t\t */",
            "\t\t\texclusive = true;",
            "\t\t} else if (exclusive && folio_test_writeback(folio) &&",
            "\t\t\t  data_race(si->flags & SWP_STABLE_WRITES)) {",
            "\t\t\t/*",
            "\t\t\t * This is tricky: not all swap backends support",
            "\t\t\t * concurrent page modifications while under writeback.",
            "\t\t\t *",
            "\t\t\t * So if we stumble over such a page in the swapcache",
            "\t\t\t * we must not set the page exclusive, otherwise we can",
            "\t\t\t * map it writable without further checks and modify it",
            "\t\t\t * while still under writeback.",
            "\t\t\t *",
            "\t\t\t * For these problematic swap backends, simply drop the",
            "\t\t\t * exclusive marker: this is perfectly fine as we start",
            "\t\t\t * writeback only if we fully unmapped the page and",
            "\t\t\t * there are no unexpected references on the page after",
            "\t\t\t * unmapping succeeded. After fully unmapped, no",
            "\t\t\t * further GUP references (FOLL_GET and FOLL_PIN) can",
            "\t\t\t * appear, so dropping the exclusive marker and mapping",
            "\t\t\t * it only R/O is fine.",
            "\t\t\t */",
            "\t\t\texclusive = false;",
            "\t\t}",
            "\t}",
            "",
            "\t/*",
            "\t * Some architectures may have to restore extra metadata to the page",
            "\t * when reading from swap. This metadata may be indexed by swap entry",
            "\t * so this must be called before swap_free().",
            "\t */",
            "\tarch_swap_restore(folio_swap(entry, folio), folio);",
            "",
            "\t/*",
            "\t * Remove the swap entry and conditionally try to free up the swapcache.",
            "\t * We're already holding a reference on the page but haven't mapped it",
            "\t * yet.",
            "\t */",
            "\tswap_free_nr(entry, nr_pages);",
            "\tif (should_try_to_free_swap(folio, vma, vmf->flags))",
            "\t\tfolio_free_swap(folio);",
            "",
            "\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, nr_pages);",
            "\tadd_mm_counter(vma->vm_mm, MM_SWAPENTS, -nr_pages);",
            "\tpte = mk_pte(page, vma->vm_page_prot);",
            "",
            "\t/*",
            "\t * Same logic as in do_wp_page(); however, optimize for pages that are",
            "\t * certainly not shared either because we just allocated them without",
            "\t * exposing them to the swapcache or because the swap entry indicates",
            "\t * exclusivity.",
            "\t */",
            "\tif (!folio_test_ksm(folio) &&",
            "\t    (exclusive || folio_ref_count(folio) == 1)) {",
            "\t\tif (vmf->flags & FAULT_FLAG_WRITE) {",
            "\t\t\tpte = maybe_mkwrite(pte_mkdirty(pte), vma);",
            "\t\t\tvmf->flags &= ~FAULT_FLAG_WRITE;",
            "\t\t}",
            "\t\trmap_flags |= RMAP_EXCLUSIVE;",
            "\t}",
            "\tfolio_ref_add(folio, nr_pages - 1);",
            "\tflush_icache_pages(vma, page, nr_pages);",
            "\tif (pte_swp_soft_dirty(vmf->orig_pte))",
            "\t\tpte = pte_mksoft_dirty(pte);",
            "\tif (pte_swp_uffd_wp(vmf->orig_pte))",
            "\t\tpte = pte_mkuffd_wp(pte);",
            "\tvmf->orig_pte = pte_advance_pfn(pte, page_idx);",
            "",
            "\t/* ksm created a completely new copy */",
            "\tif (unlikely(folio != swapcache && swapcache)) {",
            "\t\tfolio_add_new_anon_rmap(folio, vma, address, RMAP_EXCLUSIVE);",
            "\t\tfolio_add_lru_vma(folio, vma);",
            "\t} else if (!folio_test_anon(folio)) {",
            "\t\t/*",
            "\t\t * We currently only expect small !anon folios which are either",
            "\t\t * fully exclusive or fully shared, or new allocated large",
            "\t\t * folios which are fully exclusive. If we ever get large",
            "\t\t * folios within swapcache here, we have to be careful.",
            "\t\t */",
            "\t\tVM_WARN_ON_ONCE(folio_test_large(folio) && folio_test_swapcache(folio));",
            "\t\tVM_WARN_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\t\tfolio_add_new_anon_rmap(folio, vma, address, rmap_flags);",
            "\t} else {",
            "\t\tfolio_add_anon_rmap_ptes(folio, page, nr_pages, vma, address,",
            "\t\t\t\t\trmap_flags);",
            "\t}",
            "",
            "\tVM_BUG_ON(!folio_test_anon(folio) ||",
            "\t\t\t(pte_write(pte) && !PageAnonExclusive(page)));",
            "\tset_ptes(vma->vm_mm, address, ptep, pte, nr_pages);",
            "\tarch_do_swap_page_nr(vma->vm_mm, vma, address,",
            "\t\t\tpte, pte, nr_pages);",
            "",
            "\tfolio_unlock(folio);",
            "\tif (folio != swapcache && swapcache) {",
            "\t\t/*",
            "\t\t * Hold the lock to avoid the swap entry to be reused",
            "\t\t * until we take the PT lock for the pte_same() check",
            "\t\t * (to avoid false positives from pte_same). For",
            "\t\t * further safety release the lock after the swap_free",
            "\t\t * so that the swap count won't change under a",
            "\t\t * parallel locked swapcache.",
            "\t\t */",
            "\t\tfolio_unlock(swapcache);",
            "\t\tfolio_put(swapcache);",
            "\t}",
            "",
            "\tif (vmf->flags & FAULT_FLAG_WRITE) {",
            "\t\tret |= do_wp_page(vmf);",
            "\t\tif (ret & VM_FAULT_ERROR)",
            "\t\t\tret &= VM_FAULT_ERROR;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/* No need to invalidate - it was non-present before */",
            "\tupdate_mmu_cache_range(vmf, vma, address, ptep, nr_pages);",
            "unlock:",
            "\tif (vmf->pte)",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "out:",
            "\t/* Clear the swap cache pin for direct swapin after PTL unlock */",
            "\tif (need_clear_cache) {",
            "\t\tswapcache_clear(si, entry, nr_pages);",
            "\t\tif (waitqueue_active(&swapcache_wq))",
            "\t\t\twake_up(&swapcache_wq);",
            "\t}",
            "\tif (si)",
            "\t\tput_swap_device(si);",
            "\treturn ret;",
            "out_nomap:",
            "\tif (vmf->pte)",
            "\t\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "out_page:",
            "\tfolio_unlock(folio);",
            "out_release:",
            "\tfolio_put(folio);",
            "\tif (folio != swapcache && swapcache) {",
            "\t\tfolio_unlock(swapcache);",
            "\t\tfolio_put(swapcache);",
            "\t}",
            "\tif (need_clear_cache) {",
            "\t\tswapcache_clear(si, entry, nr_pages);",
            "\t\tif (waitqueue_active(&swapcache_wq))",
            "\t\t\twake_up(&swapcache_wq);",
            "\t}",
            "\tif (si)",
            "\t\tput_swap_device(si);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "thp_swap_suitable_orders, do_swap_page",
          "description": "处理页面交换时的多种情况，如迁移条目、设备私有条目、硬件故障等，根据不同的条目类型执行相应逻辑，最终将交换页面映射到内存中。",
          "similarity": 0.5423457622528076
        },
        {
          "chunk_id": 34,
          "file_path": "mm/memory.c",
          "start_line": 5940,
          "end_line": 6040,
          "content": [
            "static void lru_gen_enter_fault(struct vm_area_struct *vma)",
            "{",
            "\t/* the LRU algorithm only applies to accesses with recency */",
            "\tcurrent->in_lru_fault = vma_has_recency(vma);",
            "}",
            "static void lru_gen_exit_fault(void)",
            "{",
            "\tcurrent->in_lru_fault = false;",
            "}",
            "static void lru_gen_enter_fault(struct vm_area_struct *vma)",
            "{",
            "}",
            "static void lru_gen_exit_fault(void)",
            "{",
            "}",
            "static vm_fault_t sanitize_fault_flags(struct vm_area_struct *vma,",
            "\t\t\t\t       unsigned int *flags)",
            "{",
            "\tif (unlikely(*flags & FAULT_FLAG_UNSHARE)) {",
            "\t\tif (WARN_ON_ONCE(*flags & FAULT_FLAG_WRITE))",
            "\t\t\treturn VM_FAULT_SIGSEGV;",
            "\t\t/*",
            "\t\t * FAULT_FLAG_UNSHARE only applies to COW mappings. Let's",
            "\t\t * just treat it like an ordinary read-fault otherwise.",
            "\t\t */",
            "\t\tif (!is_cow_mapping(vma->vm_flags))",
            "\t\t\t*flags &= ~FAULT_FLAG_UNSHARE;",
            "\t} else if (*flags & FAULT_FLAG_WRITE) {",
            "\t\t/* Write faults on read-only mappings are impossible ... */",
            "\t\tif (WARN_ON_ONCE(!(vma->vm_flags & VM_MAYWRITE)))",
            "\t\t\treturn VM_FAULT_SIGSEGV;",
            "\t\t/* ... and FOLL_FORCE only applies to COW mappings. */",
            "\t\tif (WARN_ON_ONCE(!(vma->vm_flags & VM_WRITE) &&",
            "\t\t\t\t !is_cow_mapping(vma->vm_flags)))",
            "\t\t\treturn VM_FAULT_SIGSEGV;",
            "\t}",
            "#ifdef CONFIG_PER_VMA_LOCK",
            "\t/*",
            "\t * Per-VMA locks can't be used with FAULT_FLAG_RETRY_NOWAIT because of",
            "\t * the assumption that lock is dropped on VM_FAULT_RETRY.",
            "\t */",
            "\tif (WARN_ON_ONCE((*flags &",
            "\t\t\t(FAULT_FLAG_VMA_LOCK | FAULT_FLAG_RETRY_NOWAIT)) ==",
            "\t\t\t(FAULT_FLAG_VMA_LOCK | FAULT_FLAG_RETRY_NOWAIT)))",
            "\t\treturn VM_FAULT_SIGSEGV;",
            "#endif",
            "",
            "\treturn 0;",
            "}",
            "vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,",
            "\t\t\t   unsigned int flags, struct pt_regs *regs)",
            "{",
            "\t/* If the fault handler drops the mmap_lock, vma may be freed */",
            "\tstruct mm_struct *mm = vma->vm_mm;",
            "\tvm_fault_t ret;",
            "",
            "\t__set_current_state(TASK_RUNNING);",
            "",
            "\tret = sanitize_fault_flags(vma, &flags);",
            "\tif (ret)",
            "\t\tgoto out;",
            "",
            "\tif (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,",
            "\t\t\t\t\t    flags & FAULT_FLAG_INSTRUCTION,",
            "\t\t\t\t\t    flags & FAULT_FLAG_REMOTE)) {",
            "\t\tret = VM_FAULT_SIGSEGV;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\t/*",
            "\t * Enable the memcg OOM handling for faults triggered in user",
            "\t * space.  Kernel faults are handled more gracefully.",
            "\t */",
            "\tif (flags & FAULT_FLAG_USER)",
            "\t\tmem_cgroup_enter_user_fault();",
            "",
            "\tlru_gen_enter_fault(vma);",
            "",
            "\tif (unlikely(is_vm_hugetlb_page(vma)))",
            "\t\tret = hugetlb_fault(vma->vm_mm, vma, address, flags);",
            "\telse",
            "\t\tret = __handle_mm_fault(vma, address, flags);",
            "",
            "\tlru_gen_exit_fault();",
            "",
            "\tif (flags & FAULT_FLAG_USER) {",
            "\t\tmem_cgroup_exit_user_fault();",
            "\t\t/*",
            "\t\t * The task may have entered a memcg OOM situation but",
            "\t\t * if the allocation error was handled gracefully (no",
            "\t\t * VM_FAULT_OOM), there is no need to kill anything.",
            "\t\t * Just clean up the OOM state peacefully.",
            "\t\t */",
            "\t\tif (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))",
            "\t\t\tmem_cgroup_oom_synchronize(false);",
            "\t}",
            "out:",
            "\tmm_account_fault(mm, regs, address, flags, ret);",
            "",
            "\treturn ret;",
            "}"
          ],
          "function_name": "lru_gen_enter_fault, lru_gen_exit_fault, lru_gen_enter_fault, lru_gen_exit_fault, sanitize_fault_flags, handle_mm_fault",
          "description": "维护LRU算法状态标记，校验页面故障标志有效性，并作为内存故障入口点协调权限检查、NUMA处理及统计记录流程",
          "similarity": 0.5382750034332275
        },
        {
          "chunk_id": 8,
          "file_path": "mm/memory.c",
          "start_line": 1353,
          "end_line": 1469,
          "content": [
            "int",
            "copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)",
            "{",
            "\tpgd_t *src_pgd, *dst_pgd;",
            "\tunsigned long addr = src_vma->vm_start;",
            "\tunsigned long end = src_vma->vm_end;",
            "\tstruct mm_struct *dst_mm = dst_vma->vm_mm;",
            "\tstruct mm_struct *src_mm = src_vma->vm_mm;",
            "\tstruct mmu_notifier_range range;",
            "\tunsigned long next, pfn;",
            "\tbool is_cow;",
            "\tint ret;",
            "",
            "\tif (!vma_needs_copy(dst_vma, src_vma))",
            "\t\treturn 0;",
            "",
            "\tif (is_vm_hugetlb_page(src_vma))",
            "\t\treturn copy_hugetlb_page_range(dst_mm, src_mm, dst_vma, src_vma);",
            "",
            "\tif (unlikely(src_vma->vm_flags & VM_PFNMAP)) {",
            "\t\tret = track_pfn_copy(dst_vma, src_vma, &pfn);",
            "\t\tif (ret)",
            "\t\t\treturn ret;",
            "\t}",
            "",
            "\t/*",
            "\t * We need to invalidate the secondary MMU mappings only when",
            "\t * there could be a permission downgrade on the ptes of the",
            "\t * parent mm. And a permission downgrade will only happen if",
            "\t * is_cow_mapping() returns true.",
            "\t */",
            "\tis_cow = is_cow_mapping(src_vma->vm_flags);",
            "",
            "\tif (is_cow) {",
            "\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_PAGE,",
            "\t\t\t\t\t0, src_mm, addr, end);",
            "\t\tmmu_notifier_invalidate_range_start(&range);",
            "\t\t/*",
            "\t\t * Disabling preemption is not needed for the write side, as",
            "\t\t * the read side doesn't spin, but goes to the mmap_lock.",
            "\t\t *",
            "\t\t * Use the raw variant of the seqcount_t write API to avoid",
            "\t\t * lockdep complaining about preemptibility.",
            "\t\t */",
            "\t\tvma_assert_write_locked(src_vma);",
            "\t\traw_write_seqcount_begin(&src_mm->write_protect_seq);",
            "\t}",
            "",
            "\tret = 0;",
            "\tdst_pgd = pgd_offset(dst_mm, addr);",
            "\tsrc_pgd = pgd_offset(src_mm, addr);",
            "\tdo {",
            "\t\tnext = pgd_addr_end(addr, end);",
            "\t\tif (pgd_none_or_clear_bad(src_pgd))",
            "\t\t\tcontinue;",
            "\t\tif (unlikely(copy_p4d_range(dst_vma, src_vma, dst_pgd, src_pgd,",
            "\t\t\t\t\t    addr, next))) {",
            "\t\t\tret = -ENOMEM;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t} while (dst_pgd++, src_pgd++, addr = next, addr != end);",
            "",
            "\tif (is_cow) {",
            "\t\traw_write_seqcount_end(&src_mm->write_protect_seq);",
            "\t\tmmu_notifier_invalidate_range_end(&range);",
            "\t}",
            "\tif (ret && unlikely(src_vma->vm_flags & VM_PFNMAP))",
            "\t\tuntrack_pfn_copy(dst_vma, pfn);",
            "\treturn ret;",
            "}",
            "static inline bool should_zap_cows(struct zap_details *details)",
            "{",
            "\t/* By default, zap all pages */",
            "\tif (!details)",
            "\t\treturn true;",
            "",
            "\t/* Or, we zap COWed pages only if the caller wants to */",
            "\treturn details->even_cows;",
            "}",
            "static inline bool should_zap_folio(struct zap_details *details,",
            "\t\t\t\t    struct folio *folio)",
            "{",
            "\t/* If we can make a decision without *folio.. */",
            "\tif (should_zap_cows(details))",
            "\t\treturn true;",
            "",
            "\t/* Otherwise we should only zap non-anon folios */",
            "\treturn !folio_test_anon(folio);",
            "}",
            "static inline bool zap_drop_file_uffd_wp(struct zap_details *details)",
            "{",
            "\tif (!details)",
            "\t\treturn false;",
            "",
            "\treturn details->zap_flags & ZAP_FLAG_DROP_MARKER;",
            "}",
            "static inline void",
            "zap_install_uffd_wp_if_needed(struct vm_area_struct *vma,",
            "\t\t\t      unsigned long addr, pte_t *pte, int nr,",
            "\t\t\t      struct zap_details *details, pte_t pteval)",
            "{",
            "\t/* Zap on anonymous always means dropping everything */",
            "\tif (vma_is_anonymous(vma))",
            "\t\treturn;",
            "",
            "\tif (zap_drop_file_uffd_wp(details))",
            "\t\treturn;",
            "",
            "\tfor (;;) {",
            "\t\t/* the PFN in the PTE is irrelevant. */",
            "\t\tpte_install_uffd_wp_if_needed(vma, addr, pte, pteval);",
            "\t\tif (--nr == 0)",
            "\t\t\tbreak;",
            "\t\tpte++;",
            "\t\taddr += PAGE_SIZE;",
            "\t}",
            "}"
          ],
          "function_name": "copy_page_range, should_zap_cows, should_zap_folio, zap_drop_file_uffd_wp, zap_install_uffd_wp_if_needed",
          "description": "执行完整页表复制流程，初始化内存保护通知范围，调用各层级页表复制函数，并处理特殊映射（如PFNMAP）的复制逻辑，包含清除COW页面的相关控制逻辑。",
          "similarity": 0.5179973840713501
        },
        {
          "chunk_id": 19,
          "file_path": "mm/memory.c",
          "start_line": 3088,
          "end_line": 3198,
          "content": [
            "static gfp_t __get_fault_gfp_mask(struct vm_area_struct *vma)",
            "{",
            "\tstruct file *vm_file = vma->vm_file;",
            "",
            "\tif (vm_file)",
            "\t\treturn mapping_gfp_mask(vm_file->f_mapping) | __GFP_FS | __GFP_IO;",
            "",
            "\t/*",
            "\t * Special mappings (e.g. VDSO) do not have any file so fake",
            "\t * a default GFP_KERNEL for them.",
            "\t */",
            "\treturn GFP_KERNEL;",
            "}",
            "static vm_fault_t do_page_mkwrite(struct vm_fault *vmf, struct folio *folio)",
            "{",
            "\tvm_fault_t ret;",
            "\tunsigned int old_flags = vmf->flags;",
            "",
            "\tvmf->flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;",
            "",
            "\tif (vmf->vma->vm_file &&",
            "\t    IS_SWAPFILE(vmf->vma->vm_file->f_mapping->host))",
            "\t\treturn VM_FAULT_SIGBUS;",
            "",
            "\tret = vmf->vma->vm_ops->page_mkwrite(vmf);",
            "\t/* Restore original flags so that caller is not surprised */",
            "\tvmf->flags = old_flags;",
            "\tif (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))",
            "\t\treturn ret;",
            "\tif (unlikely(!(ret & VM_FAULT_LOCKED))) {",
            "\t\tfolio_lock(folio);",
            "\t\tif (!folio->mapping) {",
            "\t\t\tfolio_unlock(folio);",
            "\t\t\treturn 0; /* retry */",
            "\t\t}",
            "\t\tret |= VM_FAULT_LOCKED;",
            "\t} else",
            "\t\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);",
            "\treturn ret;",
            "}",
            "static vm_fault_t fault_dirty_shared_page(struct vm_fault *vmf)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tstruct address_space *mapping;",
            "\tstruct folio *folio = page_folio(vmf->page);",
            "\tbool dirtied;",
            "\tbool page_mkwrite = vma->vm_ops && vma->vm_ops->page_mkwrite;",
            "",
            "\tdirtied = folio_mark_dirty(folio);",
            "\tVM_BUG_ON_FOLIO(folio_test_anon(folio), folio);",
            "\t/*",
            "\t * Take a local copy of the address_space - folio.mapping may be zeroed",
            "\t * by truncate after folio_unlock().   The address_space itself remains",
            "\t * pinned by vma->vm_file's reference.  We rely on folio_unlock()'s",
            "\t * release semantics to prevent the compiler from undoing this copying.",
            "\t */",
            "\tmapping = folio_raw_mapping(folio);",
            "\tfolio_unlock(folio);",
            "",
            "\tif (!page_mkwrite)",
            "\t\tfile_update_time(vma->vm_file);",
            "",
            "\t/*",
            "\t * Throttle page dirtying rate down to writeback speed.",
            "\t *",
            "\t * mapping may be NULL here because some device drivers do not",
            "\t * set page.mapping but still dirty their pages",
            "\t *",
            "\t * Drop the mmap_lock before waiting on IO, if we can. The file",
            "\t * is pinning the mapping, as per above.",
            "\t */",
            "\tif ((dirtied || page_mkwrite) && mapping) {",
            "\t\tstruct file *fpin;",
            "",
            "\t\tfpin = maybe_unlock_mmap_for_io(vmf, NULL);",
            "\t\tbalance_dirty_pages_ratelimited(mapping);",
            "\t\tif (fpin) {",
            "\t\t\tfput(fpin);",
            "\t\t\treturn VM_FAULT_COMPLETED;",
            "\t\t}",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static inline void wp_page_reuse(struct vm_fault *vmf, struct folio *folio)",
            "\t__releases(vmf->ptl)",
            "{",
            "\tstruct vm_area_struct *vma = vmf->vma;",
            "\tpte_t entry;",
            "",
            "\tVM_BUG_ON(!(vmf->flags & FAULT_FLAG_WRITE));",
            "",
            "\tif (folio) {",
            "\t\tVM_BUG_ON(folio_test_anon(folio) &&",
            "\t\t\t  !PageAnonExclusive(vmf->page));",
            "\t\t/*",
            "\t\t * Clear the folio's cpupid information as the existing",
            "\t\t * information potentially belongs to a now completely",
            "\t\t * unrelated process.",
            "\t\t */",
            "\t\tfolio_xchg_last_cpupid(folio, (1 << LAST_CPUPID_SHIFT) - 1);",
            "\t}",
            "",
            "\tflush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));",
            "\tentry = pte_mkyoung(vmf->orig_pte);",
            "\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);",
            "\tif (ptep_set_access_flags(vma, vmf->address, vmf->pte, entry, 1))",
            "\t\tupdate_mmu_cache_range(vmf, vma, vmf->address, vmf->pte, 1);",
            "\tpte_unmap_unlock(vmf->pte, vmf->ptl);",
            "\tcount_vm_event(PGREUSE);",
            "}"
          ],
          "function_name": "__get_fault_gfp_mask, do_page_mkwrite, fault_dirty_shared_page, wp_page_reuse",
          "description": "处理页面写入相关逻辑，__get_fault_gfp_mask计算内存分配掩码，do_page_mkwrite处理写入时的页面锁定和更新，fault_dirty_shared_page处理共享页面脏页标记，wp_page_reuse实现页面复用和访问标志更新。",
          "similarity": 0.5095808506011963
        },
        {
          "chunk_id": 16,
          "file_path": "mm/memory.c",
          "start_line": 2650,
          "end_line": 2753,
          "content": [
            "int remap_pfn_range_notrack(struct vm_area_struct *vma, unsigned long addr,",
            "\t\tunsigned long pfn, unsigned long size, pgprot_t prot)",
            "{",
            "\tint error = remap_pfn_range_internal(vma, addr, pfn, size, prot);",
            "",
            "\tif (!error)",
            "\t\treturn 0;",
            "",
            "\t/*",
            "\t * A partial pfn range mapping is dangerous: it does not",
            "\t * maintain page reference counts, and callers may free",
            "\t * pages due to the error. So zap it early.",
            "\t */",
            "\tzap_page_range_single(vma, addr, size, NULL);",
            "\treturn error;",
            "}",
            "int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,",
            "\t\t    unsigned long pfn, unsigned long size, pgprot_t prot)",
            "{",
            "\tint err;",
            "",
            "\terr = track_pfn_remap(vma, &prot, pfn, addr, PAGE_ALIGN(size));",
            "\tif (err)",
            "\t\treturn -EINVAL;",
            "",
            "\terr = remap_pfn_range_notrack(vma, addr, pfn, size, prot);",
            "\tif (err)",
            "\t\tuntrack_pfn(vma, pfn, PAGE_ALIGN(size), true);",
            "\treturn err;",
            "}",
            "int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)",
            "{",
            "\tunsigned long vm_len, pfn, pages;",
            "",
            "\t/* Check that the physical memory area passed in looks valid */",
            "\tif (start + len < start)",
            "\t\treturn -EINVAL;",
            "\t/*",
            "\t * You *really* shouldn't map things that aren't page-aligned,",
            "\t * but we've historically allowed it because IO memory might",
            "\t * just have smaller alignment.",
            "\t */",
            "\tlen += start & ~PAGE_MASK;",
            "\tpfn = start >> PAGE_SHIFT;",
            "\tpages = (len + ~PAGE_MASK) >> PAGE_SHIFT;",
            "\tif (pfn + pages < pfn)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* We start the mapping 'vm_pgoff' pages into the area */",
            "\tif (vma->vm_pgoff > pages)",
            "\t\treturn -EINVAL;",
            "\tpfn += vma->vm_pgoff;",
            "\tpages -= vma->vm_pgoff;",
            "",
            "\t/* Can we fit all of the mapping? */",
            "\tvm_len = vma->vm_end - vma->vm_start;",
            "\tif (vm_len >> PAGE_SHIFT > pages)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Ok, let it rip */",
            "\treturn io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);",
            "}",
            "static int apply_to_pte_range(struct mm_struct *mm, pmd_t *pmd,",
            "\t\t\t\t     unsigned long addr, unsigned long end,",
            "\t\t\t\t     pte_fn_t fn, void *data, bool create,",
            "\t\t\t\t     pgtbl_mod_mask *mask)",
            "{",
            "\tpte_t *pte, *mapped_pte;",
            "\tint err = 0;",
            "\tspinlock_t *ptl;",
            "",
            "\tif (create) {",
            "\t\tmapped_pte = pte = (mm == &init_mm) ?",
            "\t\t\tpte_alloc_kernel_track(pmd, addr, mask) :",
            "\t\t\tpte_alloc_map_lock(mm, pmd, addr, &ptl);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t} else {",
            "\t\tmapped_pte = pte = (mm == &init_mm) ?",
            "\t\t\tpte_offset_kernel(pmd, addr) :",
            "\t\t\tpte_offset_map_lock(mm, pmd, addr, &ptl);",
            "\t\tif (!pte)",
            "\t\t\treturn -EINVAL;",
            "\t}",
            "",
            "\tarch_enter_lazy_mmu_mode();",
            "",
            "\tif (fn) {",
            "\t\tdo {",
            "\t\t\tif (create || !pte_none(ptep_get(pte))) {",
            "\t\t\t\terr = fn(pte, addr, data);",
            "\t\t\t\tif (err)",
            "\t\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t} while (pte++, addr += PAGE_SIZE, addr != end);",
            "\t}",
            "\t*mask |= PGTBL_PTE_MODIFIED;",
            "",
            "\tarch_leave_lazy_mmu_mode();",
            "",
            "\tif (mm != &init_mm)",
            "\t\tpte_unmap_unlock(mapped_pte, ptl);",
            "\treturn err;",
            "}"
          ],
          "function_name": "remap_pfn_range_notrack, remap_pfn_range, vm_iomap_memory, apply_to_pte_range",
          "description": "提供PFN映射接口及I/O内存映射支持，remap_pfn_range_notrack封装错误处理逻辑，vm_iomap_memory实现设备内存映射，apply_to_pte_range处理页表项的创建和修改操作。",
          "similarity": 0.5052727460861206
        }
      ]
    },
    {
      "source_file": "mm/memcontrol.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:39:41\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memcontrol.c`\n\n---\n\n# memcontrol.c 技术文档\n\n## 1. 文件概述\n\n`memcontrol.c` 是 Linux 内核中内存控制组（Memory Cgroup, memcg）的核心实现文件，负责对进程组的内存资源进行隔离、限制、统计和回收。该文件实现了基于 cgroup v1 和 v2 的统一内存控制器，支持用户内存、内核内存、套接字内存以及 BPF 内存等多种内存类型的细粒度管理，并集成了内存压力通知（vmpressure）、OOM 控制、页面回收等关键机制。\n\n## 2. 核心功能\n\n### 主要全局变量\n- `memory_cgrp_subsys`：cgroup 子系统实例，注册为内存控制器。\n- `root_mem_cgroup`：根内存控制组，所有未显式归属的内存默认计入此组。\n- `int_active_memcg`（per-CPU）：中断上下文中使用的活跃 memcg 指针。\n- `cgroup_memory_nosocket` / `cgroup_memory_nokmem` / `cgroup_memory_nobpf`：控制是否启用套接字、内核内存、BPF 内存的会计功能。\n- `memcg_kmem_online_key` / `memcg_bpf_enabled_key`：静态分支键，用于优化内存会计路径。\n\n### 关键数据结构\n- `struct mem_cgroup`：内存控制组的核心结构，包含内存计数器、LRU 链表、压力监控、OOM 状态等。\n- `struct obj_cgroup`：用于内核对象（如 slab）内存会计的辅助结构，通过引用计数管理生命周期。\n\n### 主要函数\n- `mem_cgroup_css_from_folio()`：根据 folio 获取其所属 memcg 的 cgroup_subsys_state（css）。\n- `page_cgroup_ino()`：获取页面所归属 memcg 的 cgroup inode 编号（用于 procfs 等接口）。\n- `obj_cgroup_release()`：当 obj_cgroup 引用计数归零时释放剩余未清账的内存页。\n- `obj_cgroup_alloc()`：分配并初始化一个新的 obj_cgroup 实例。\n- `memcg_reparent_objcgs()`：在 memcg 被销毁时，将其关联的 obj_cgroup 重新归属到父 memcg。\n- `mem_cgroup_kmem_disabled()`：查询内核内存会计是否被禁用。\n- `memcg_to_vmpressure()` / `vmpressure_to_memcg()`：在 memcg 与 vmpressure 结构之间相互转换。\n\n## 3. 关键实现\n\n### 内存会计模型\n- **统一层级模型**：支持 cgroup v2 的统一层级（unified hierarchy），同时兼容 v1 的多层级模式。\n- **锁无关页面跟踪**：通过 per-CPU stock 机制减少高频内存分配/释放路径上的锁竞争。\n- **对象级内存会计**：使用 `obj_cgroup` 对 slab 等内核对象进行精确计费，支持延迟清账（deferred uncharge）。\n\n### 生命周期管理\n- `obj_cgroup` 使用 `percpu_ref` 引用计数机制，确保在所有 CPU 上的操作完成后再释放资源。\n- 在 memcg 销毁时，通过 `memcg_reparent_objcgs()` 将未释放的 obj_cgroup 安全迁移至父 memcg，避免内存泄漏。\n\n### 中断上下文支持\n- 通过 `int_active_memcg` per-CPU 变量，在中断或软中断上下文中临时绑定当前 memcg，使得网络、块设备等子系统可在中断中正确进行内存会计。\n\n### 内存压力监控\n- 每个 memcg 内嵌 `struct vmpressure`，用于检测内存压力等级（low/medium/critical），触发用户态通知或后台回收。\n\n### 安全与健壮性\n- `page_cgroup_ino()` 明确标注为“racy”，仅适用于不要求强一致性的只读接口（如 `/proc/pid/smaps`）。\n- 在 `obj_cgroup_release()` 中校验 `nr_charged_bytes` 是否对齐 PAGE_SIZE，防止非整页残留导致会计错误。\n\n## 4. 依赖关系\n\n### 头文件依赖\n- **核心内存管理**：`<linux/mm.h>`, `<linux/page_counter.h>`, `<linux/vmpressure.h>`, `<linux/swap.h>`\n- **cgroup 基础设施**：`<linux/cgroup.h>`, `\"internal.h\"`\n- **slab 分配器**：`\"slab.h\"`, `\"memcontrol-v1.h\"`\n- **网络子系统**：`<net/sock.h>`（用于 socket 内存会计）\n- **追踪与调试**：`<trace/events/vmscan.h>`, `<linux/kmemleak.h>`\n\n### 功能依赖\n- **页面回收**：与 vmscan 子系统紧密集成，参与 LRU 链表管理和直接/后台回收。\n- **OOM Killer**：提供 memcg 级别的 OOM 判定和 victim 选择支持。\n- **PSI（Pressure Stall Information）**：与 psi 子系统协作提供资源压力指标。\n- **Writeback 控制**：通过 `CONFIG_CGROUP_WRITEBACK` 支持脏页回写带宽限制。\n- **Zswap 与交换**：与 zswap、swap_cgroup 协同管理压缩交换内存。\n\n## 5. 使用场景\n\n- **容器资源隔离**：Docker、Kubernetes 等容器运行时通过 memcg 限制单个容器的内存使用上限。\n- **多租户系统**：云平台利用 memcg 防止单个租户耗尽系统内存。\n- **内核内存防护**：通过 `memory.kmem.limit_in_bytes` 限制 slab 等内核内存，防止内核内存耗尽（KMEM accounting）。\n- **内存压力响应**：应用程序监听 memory.pressure 接口，在内存紧张时主动释放缓存。\n- **性能分析**：通过 `/sys/fs/cgroup/memory/.../memory.stat` 等接口监控各 memcg 的内存分布和回收行为。\n- **OOM 管理**：当 memcg 超限时触发局部 OOM，仅杀死该组内进程，不影响系统其他部分。",
      "similarity": 0.6453484892845154,
      "chunks": [
        {
          "chunk_id": 15,
          "file_path": "mm/memcontrol.c",
          "start_line": 3052,
          "end_line": 3157,
          "content": [
            "unsigned long mem_cgroup_usage(struct mem_cgroup *memcg, bool swap)",
            "{",
            "\tunsigned long val;",
            "",
            "\tif (mem_cgroup_is_root(memcg)) {",
            "\t\t/*",
            "\t\t * Approximate root's usage from global state. This isn't",
            "\t\t * perfect, but the root usage was always an approximation.",
            "\t\t */",
            "\t\tval = global_node_page_state(NR_FILE_PAGES) +",
            "\t\t\tglobal_node_page_state(NR_ANON_MAPPED);",
            "\t\tif (swap)",
            "\t\t\tval += total_swap_pages - get_nr_swap_pages();",
            "\t} else {",
            "\t\tif (!swap)",
            "\t\t\tval = page_counter_read(&memcg->memory);",
            "\t\telse",
            "\t\t\tval = page_counter_read(&memcg->memsw);",
            "\t}",
            "\treturn val;",
            "}",
            "static int memcg_online_kmem(struct mem_cgroup *memcg)",
            "{",
            "\tstruct obj_cgroup *objcg;",
            "",
            "\tif (mem_cgroup_kmem_disabled())",
            "\t\treturn 0;",
            "",
            "\tif (unlikely(mem_cgroup_is_root(memcg)))",
            "\t\treturn 0;",
            "",
            "\tobjcg = obj_cgroup_alloc();",
            "\tif (!objcg)",
            "\t\treturn -ENOMEM;",
            "",
            "\tobjcg->memcg = memcg;",
            "\trcu_assign_pointer(memcg->objcg, objcg);",
            "\tobj_cgroup_get(objcg);",
            "\tmemcg->orig_objcg = objcg;",
            "",
            "\tstatic_branch_enable(&memcg_kmem_online_key);",
            "",
            "\tmemcg->kmemcg_id = memcg->id.id;",
            "",
            "\treturn 0;",
            "}",
            "static void memcg_offline_kmem(struct mem_cgroup *memcg)",
            "{",
            "\tstruct mem_cgroup *parent;",
            "",
            "\tif (mem_cgroup_kmem_disabled())",
            "\t\treturn;",
            "",
            "\tif (unlikely(mem_cgroup_is_root(memcg)))",
            "\t\treturn;",
            "",
            "\tparent = parent_mem_cgroup(memcg);",
            "\tif (!parent)",
            "\t\tparent = root_mem_cgroup;",
            "",
            "\tmemcg_reparent_objcgs(memcg, parent);",
            "",
            "\t/*",
            "\t * After we have finished memcg_reparent_objcgs(), all list_lrus",
            "\t * corresponding to this cgroup are guaranteed to remain empty.",
            "\t * The ordering is imposed by list_lru_node->lock taken by",
            "\t * memcg_reparent_list_lrus().",
            "\t */",
            "\tmemcg_reparent_list_lrus(memcg, parent);",
            "}",
            "static int memcg_wb_domain_init(struct mem_cgroup *memcg, gfp_t gfp)",
            "{",
            "\treturn wb_domain_init(&memcg->cgwb_domain, gfp);",
            "}",
            "static void memcg_wb_domain_exit(struct mem_cgroup *memcg)",
            "{",
            "\twb_domain_exit(&memcg->cgwb_domain);",
            "}",
            "static void memcg_wb_domain_size_changed(struct mem_cgroup *memcg)",
            "{",
            "\twb_domain_size_changed(&memcg->cgwb_domain);",
            "}",
            "void mem_cgroup_wb_stats(struct bdi_writeback *wb, unsigned long *pfilepages,",
            "\t\t\t unsigned long *pheadroom, unsigned long *pdirty,",
            "\t\t\t unsigned long *pwriteback)",
            "{",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_css(wb->memcg_css);",
            "\tstruct mem_cgroup *parent;",
            "",
            "\tmem_cgroup_flush_stats_ratelimited(memcg);",
            "",
            "\t*pdirty = memcg_page_state(memcg, NR_FILE_DIRTY);",
            "\t*pwriteback = memcg_page_state(memcg, NR_WRITEBACK);",
            "\t*pfilepages = memcg_page_state(memcg, NR_INACTIVE_FILE) +",
            "\t\t\tmemcg_page_state(memcg, NR_ACTIVE_FILE);",
            "",
            "\t*pheadroom = PAGE_COUNTER_MAX;",
            "\twhile ((parent = parent_mem_cgroup(memcg))) {",
            "\t\tunsigned long ceiling = min(READ_ONCE(memcg->memory.max),",
            "\t\t\t\t\t    READ_ONCE(memcg->memory.high));",
            "\t\tunsigned long used = page_counter_read(&memcg->memory);",
            "",
            "\t\t*pheadroom = min(*pheadroom, ceiling - min(ceiling, used));",
            "\t\tmemcg = parent;",
            "\t}",
            "}"
          ],
          "function_name": "mem_cgroup_usage, memcg_online_kmem, memcg_offline_kmem, memcg_wb_domain_init, memcg_wb_domain_exit, memcg_wb_domain_size_changed, mem_cgroup_wb_stats",
          "description": "该代码段实现内存控制组（memcg）的核心资源管理功能，涵盖内存使用统计、KMEM对象管理及写回域控制。  \n`mem_cgroup_usage` 计算内存控制组的当前使用量（含/不含交换），`memcg_online_kmem`/`memcg_offline_kmem` 管理KMEM对象的动态注册与迁移，`wb_domain_*` 函数序列化写回域生命周期并维护统计信息。  \n`mem_cgroup_wb_stats` 收集并递归汇总内存控制组的脏页、写回页等统计指标，用于内存压力评估与回收决策。",
          "similarity": 0.5788680911064148
        },
        {
          "chunk_id": 22,
          "file_path": "mm/memcontrol.c",
          "start_line": 4225,
          "end_line": 4326,
          "content": [
            "static ssize_t memory_reclaim(struct kernfs_open_file *of, char *buf,",
            "\t\t\t      size_t nbytes, loff_t off)",
            "{",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));",
            "\tunsigned int nr_retries = MAX_RECLAIM_RETRIES;",
            "\tunsigned long nr_to_reclaim, nr_reclaimed = 0;",
            "\tunsigned int reclaim_options;",
            "\tint err;",
            "",
            "\tbuf = strstrip(buf);",
            "\terr = page_counter_memparse(buf, \"\", &nr_to_reclaim);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\treclaim_options\t= MEMCG_RECLAIM_MAY_SWAP | MEMCG_RECLAIM_PROACTIVE;",
            "\twhile (nr_reclaimed < nr_to_reclaim) {",
            "\t\tunsigned long reclaimed;",
            "",
            "\t\tif (signal_pending(current))",
            "\t\t\treturn -EINTR;",
            "",
            "\t\t/*",
            "\t\t * This is the final attempt, drain percpu lru caches in the",
            "\t\t * hope of introducing more evictable pages for",
            "\t\t * try_to_free_mem_cgroup_pages().",
            "\t\t */",
            "\t\tif (!nr_retries)",
            "\t\t\tlru_add_drain_all();",
            "",
            "\t\treclaimed = try_to_free_mem_cgroup_pages(memcg,",
            "\t\t\t\t\tmin(nr_to_reclaim - nr_reclaimed, SWAP_CLUSTER_MAX),",
            "\t\t\t\t\tGFP_KERNEL, reclaim_options);",
            "",
            "\t\tif (!reclaimed && !nr_retries--)",
            "\t\t\treturn -EAGAIN;",
            "",
            "\t\tnr_reclaimed += reclaimed;",
            "\t}",
            "",
            "\treturn nbytes;",
            "}",
            "void mem_cgroup_calculate_protection(struct mem_cgroup *root,",
            "\t\t\t\t     struct mem_cgroup *memcg)",
            "{",
            "\tbool recursive_protection =",
            "\t\tcgrp_dfl_root.flags & CGRP_ROOT_MEMORY_RECURSIVE_PROT;",
            "",
            "\tif (mem_cgroup_disabled())",
            "\t\treturn;",
            "",
            "\tif (!root)",
            "\t\troot = root_mem_cgroup;",
            "",
            "\tpage_counter_calculate_protection(&root->memory, &memcg->memory, recursive_protection);",
            "}",
            "static int charge_memcg(struct folio *folio, struct mem_cgroup *memcg,",
            "\t\t\tgfp_t gfp)",
            "{",
            "\tint ret;",
            "",
            "\tret = try_charge(memcg, gfp, folio_nr_pages(folio));",
            "\tif (ret)",
            "\t\tgoto out;",
            "",
            "\tcss_get(&memcg->css);",
            "\tcommit_charge(folio, memcg);",
            "\tmemcg1_commit_charge(folio, memcg);",
            "out:",
            "\treturn ret;",
            "}",
            "int __mem_cgroup_charge(struct folio *folio, struct mm_struct *mm, gfp_t gfp)",
            "{",
            "\tstruct mem_cgroup *memcg;",
            "\tint ret;",
            "",
            "\tmemcg = get_mem_cgroup_from_mm(mm);",
            "\tret = charge_memcg(folio, memcg, gfp);",
            "\tcss_put(&memcg->css);",
            "",
            "\treturn ret;",
            "}",
            "int mem_cgroup_charge_hugetlb(struct folio *folio, gfp_t gfp)",
            "{",
            "\tstruct mem_cgroup *memcg = get_mem_cgroup_from_current();",
            "\tint ret = 0;",
            "",
            "\t/*",
            "\t * Even memcg does not account for hugetlb, we still want to update",
            "\t * system-level stats via lruvec_stat_mod_folio. Return 0, and skip",
            "\t * charging the memcg.",
            "\t */",
            "\tif (mem_cgroup_disabled() || !memcg_accounts_hugetlb() ||",
            "\t\t!memcg || !cgroup_subsys_on_dfl(memory_cgrp_subsys))",
            "\t\tgoto out;",
            "",
            "\tif (charge_memcg(folio, memcg, gfp))",
            "\t\tret = -ENOMEM;",
            "",
            "out:",
            "\tmem_cgroup_put(memcg);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "memory_reclaim, mem_cgroup_calculate_protection, charge_memcg, __mem_cgroup_charge, mem_cgroup_charge_hugetlb",
          "description": "实现内存回收逻辑、保护比例计算及内存页充电机制，支持普通页和HugeTLB页的内存计费管理。",
          "similarity": 0.5759139657020569
        },
        {
          "chunk_id": 21,
          "file_path": "mm/memcontrol.c",
          "start_line": 4113,
          "end_line": 4214,
          "content": [
            "static void __memory_events_show(struct seq_file *m, atomic_long_t *events)",
            "{",
            "\tseq_printf(m, \"low %lu\\n\", atomic_long_read(&events[MEMCG_LOW]));",
            "\tseq_printf(m, \"high %lu\\n\", atomic_long_read(&events[MEMCG_HIGH]));",
            "\tseq_printf(m, \"max %lu\\n\", atomic_long_read(&events[MEMCG_MAX]));",
            "\tseq_printf(m, \"oom %lu\\n\", atomic_long_read(&events[MEMCG_OOM]));",
            "\tseq_printf(m, \"oom_kill %lu\\n\",",
            "\t\t   atomic_long_read(&events[MEMCG_OOM_KILL]));",
            "\tseq_printf(m, \"oom_group_kill %lu\\n\",",
            "\t\t   atomic_long_read(&events[MEMCG_OOM_GROUP_KILL]));",
            "}",
            "static int memory_events_show(struct seq_file *m, void *v)",
            "{",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);",
            "",
            "\t__memory_events_show(m, memcg->memory_events);",
            "\treturn 0;",
            "}",
            "static int memory_events_local_show(struct seq_file *m, void *v)",
            "{",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);",
            "",
            "\t__memory_events_show(m, memcg->memory_events_local);",
            "\treturn 0;",
            "}",
            "int memory_stat_show(struct seq_file *m, void *v)",
            "{",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);",
            "\tchar *buf = kmalloc(PAGE_SIZE, GFP_KERNEL);",
            "\tstruct seq_buf s;",
            "",
            "\tif (!buf)",
            "\t\treturn -ENOMEM;",
            "\tseq_buf_init(&s, buf, PAGE_SIZE);",
            "\tmemory_stat_format(memcg, &s);",
            "\tseq_puts(m, buf);",
            "\tkfree(buf);",
            "\treturn 0;",
            "}",
            "static inline unsigned long lruvec_page_state_output(struct lruvec *lruvec,",
            "\t\t\t\t\t\t     int item)",
            "{",
            "\treturn lruvec_page_state(lruvec, item) *",
            "\t\tmemcg_page_state_output_unit(item);",
            "}",
            "static int memory_numa_stat_show(struct seq_file *m, void *v)",
            "{",
            "\tint i;",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);",
            "",
            "\tmem_cgroup_flush_stats(memcg);",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(memory_stats); i++) {",
            "\t\tint nid;",
            "",
            "\t\tif (memory_stats[i].idx >= NR_VM_NODE_STAT_ITEMS)",
            "\t\t\tcontinue;",
            "",
            "\t\tseq_printf(m, \"%s\", memory_stats[i].name);",
            "\t\tfor_each_node_state(nid, N_MEMORY) {",
            "\t\t\tu64 size;",
            "\t\t\tstruct lruvec *lruvec;",
            "",
            "\t\t\tlruvec = mem_cgroup_lruvec(memcg, NODE_DATA(nid));",
            "\t\t\tsize = lruvec_page_state_output(lruvec,",
            "\t\t\t\t\t\t\tmemory_stats[i].idx);",
            "\t\t\tseq_printf(m, \" N%d=%llu\", nid, size);",
            "\t\t}",
            "\t\tseq_putc(m, '\\n');",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "static int memory_oom_group_show(struct seq_file *m, void *v)",
            "{",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_seq(m);",
            "",
            "\tseq_printf(m, \"%d\\n\", READ_ONCE(memcg->oom_group));",
            "",
            "\treturn 0;",
            "}",
            "static ssize_t memory_oom_group_write(struct kernfs_open_file *of,",
            "\t\t\t\t      char *buf, size_t nbytes, loff_t off)",
            "{",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));",
            "\tint ret, oom_group;",
            "",
            "\tbuf = strstrip(buf);",
            "\tif (!buf)",
            "\t\treturn -EINVAL;",
            "",
            "\tret = kstrtoint(buf, 0, &oom_group);",
            "\tif (ret)",
            "\t\treturn ret;",
            "",
            "\tif (oom_group != 0 && oom_group != 1)",
            "\t\treturn -EINVAL;",
            "",
            "\tWRITE_ONCE(memcg->oom_group, oom_group);",
            "",
            "\treturn nbytes;",
            "}"
          ],
          "function_name": "__memory_events_show, memory_events_show, memory_events_local_show, memory_stat_show, lruvec_page_state_output, memory_numa_stat_show, memory_oom_group_show, memory_oom_group_write",
          "description": "提供内存事件统计、NUMA节点统计、OOM组配置等功能的读取接口，展示内存控制组的各类统计信息及事件计数。",
          "similarity": 0.5699012875556946
        },
        {
          "chunk_id": 7,
          "file_path": "mm/memcontrol.c",
          "start_line": 1507,
          "end_line": 1612,
          "content": [
            "static void memory_stat_format(struct mem_cgroup *memcg, struct seq_buf *s)",
            "{",
            "\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys))",
            "\t\tmemcg_stat_format(memcg, s);",
            "\telse",
            "\t\tmemcg1_stat_format(memcg, s);",
            "\tWARN_ON_ONCE(seq_buf_has_overflowed(s));",
            "}",
            "void mem_cgroup_print_oom_context(struct mem_cgroup *memcg, struct task_struct *p)",
            "{",
            "\trcu_read_lock();",
            "",
            "\tif (memcg) {",
            "\t\tpr_cont(\",oom_memcg=\");",
            "\t\tpr_cont_cgroup_path(memcg->css.cgroup);",
            "\t} else",
            "\t\tpr_cont(\",global_oom\");",
            "\tif (p) {",
            "\t\tpr_cont(\",task_memcg=\");",
            "\t\tpr_cont_cgroup_path(task_cgroup(p, memory_cgrp_id));",
            "\t}",
            "\trcu_read_unlock();",
            "}",
            "void mem_cgroup_print_oom_meminfo(struct mem_cgroup *memcg)",
            "{",
            "\t/* Use static buffer, for the caller is holding oom_lock. */",
            "\tstatic char buf[PAGE_SIZE];",
            "\tstruct seq_buf s;",
            "",
            "\tlockdep_assert_held(&oom_lock);",
            "",
            "\tpr_info(\"memory: usage %llukB, limit %llukB, failcnt %lu\\n\",",
            "\t\tK((u64)page_counter_read(&memcg->memory)),",
            "\t\tK((u64)READ_ONCE(memcg->memory.max)), memcg->memory.failcnt);",
            "\tif (cgroup_subsys_on_dfl(memory_cgrp_subsys))",
            "\t\tpr_info(\"swap: usage %llukB, limit %llukB, failcnt %lu\\n\",",
            "\t\t\tK((u64)page_counter_read(&memcg->swap)),",
            "\t\t\tK((u64)READ_ONCE(memcg->swap.max)), memcg->swap.failcnt);",
            "#ifdef CONFIG_MEMCG_V1",
            "\telse {",
            "\t\tpr_info(\"memory+swap: usage %llukB, limit %llukB, failcnt %lu\\n\",",
            "\t\t\tK((u64)page_counter_read(&memcg->memsw)),",
            "\t\t\tK((u64)memcg->memsw.max), memcg->memsw.failcnt);",
            "\t\tpr_info(\"kmem: usage %llukB, limit %llukB, failcnt %lu\\n\",",
            "\t\t\tK((u64)page_counter_read(&memcg->kmem)),",
            "\t\t\tK((u64)memcg->kmem.max), memcg->kmem.failcnt);",
            "\t}",
            "#endif",
            "",
            "\tpr_info(\"Memory cgroup stats for \");",
            "\tpr_cont_cgroup_path(memcg->css.cgroup);",
            "\tpr_cont(\":\");",
            "\tseq_buf_init(&s, buf, sizeof(buf));",
            "\tmemory_stat_format(memcg, &s);",
            "\tseq_buf_do_printk(&s, KERN_INFO);",
            "}",
            "unsigned long mem_cgroup_get_max(struct mem_cgroup *memcg)",
            "{",
            "\tunsigned long max = READ_ONCE(memcg->memory.max);",
            "",
            "\tif (do_memsw_account()) {",
            "\t\tif (mem_cgroup_swappiness(memcg)) {",
            "\t\t\t/* Calculate swap excess capacity from memsw limit */",
            "\t\t\tunsigned long swap = READ_ONCE(memcg->memsw.max) - max;",
            "",
            "\t\t\tmax += min(swap, (unsigned long)total_swap_pages);",
            "\t\t}",
            "\t} else {",
            "\t\tif (mem_cgroup_swappiness(memcg))",
            "\t\t\tmax += min(READ_ONCE(memcg->swap.max),",
            "\t\t\t\t   (unsigned long)total_swap_pages);",
            "\t}",
            "\treturn max;",
            "}",
            "unsigned long mem_cgroup_size(struct mem_cgroup *memcg)",
            "{",
            "\treturn page_counter_read(&memcg->memory);",
            "}",
            "static bool mem_cgroup_out_of_memory(struct mem_cgroup *memcg, gfp_t gfp_mask,",
            "\t\t\t\t     int order)",
            "{",
            "\tstruct oom_control oc = {",
            "\t\t.zonelist = NULL,",
            "\t\t.nodemask = NULL,",
            "\t\t.memcg = memcg,",
            "\t\t.gfp_mask = gfp_mask,",
            "\t\t.order = order,",
            "\t};",
            "\tbool ret = true;",
            "",
            "\tif (mutex_lock_killable(&oom_lock))",
            "\t\treturn true;",
            "",
            "\tif (mem_cgroup_margin(memcg) >= (1 << order))",
            "\t\tgoto unlock;",
            "",
            "\t/*",
            "\t * A few threads which were not waiting at mutex_lock_killable() can",
            "\t * fail to bail out. Therefore, check again after holding oom_lock.",
            "\t */",
            "\tret = out_of_memory(&oc);",
            "",
            "unlock:",
            "\tmutex_unlock(&oom_lock);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "memory_stat_format, mem_cgroup_print_oom_context, mem_cgroup_print_oom_meminfo, mem_cgroup_get_max, mem_cgroup_size, mem_cgroup_out_of_memory",
          "description": "memory_stat_format 根据内存子系统类型选择统计格式化方式；mem_cgroup_print_oom_context 打印OOM发生时的内存控制组路径；mem_cgroup_print_oom_meminfo 输出内存控制组详细内存信息；mem_cgroup_get_max 计算考虑交换空间后的内存上限；mem_cgroup_out_of_memory 检查内存控制组是否触发OOM条件",
          "similarity": 0.569076657295227
        },
        {
          "chunk_id": 19,
          "file_path": "mm/memcontrol.c",
          "start_line": 3865,
          "end_line": 3967,
          "content": [
            "static void mem_cgroup_fork(struct task_struct *task)",
            "{",
            "\t/*",
            "\t * Set the update flag to cause task->objcg to be initialized lazily",
            "\t * on the first allocation. It can be done without any synchronization",
            "\t * because it's always performed on the current task, so does",
            "\t * current_objcg_update().",
            "\t */",
            "\ttask->objcg = (struct obj_cgroup *)CURRENT_OBJCG_UPDATE_FLAG;",
            "}",
            "static void mem_cgroup_exit(struct task_struct *task)",
            "{",
            "\tstruct obj_cgroup *objcg = task->objcg;",
            "",
            "\tobjcg = (struct obj_cgroup *)",
            "\t\t((unsigned long)objcg & ~CURRENT_OBJCG_UPDATE_FLAG);",
            "\tobj_cgroup_put(objcg);",
            "",
            "\t/*",
            "\t * Some kernel allocations can happen after this point,",
            "\t * but let's ignore them. It can be done without any synchronization",
            "\t * because it's always performed on the current task, so does",
            "\t * current_objcg_update().",
            "\t */",
            "\ttask->objcg = NULL;",
            "}",
            "static void mem_cgroup_lru_gen_attach(struct cgroup_taskset *tset)",
            "{",
            "\tstruct task_struct *task;",
            "\tstruct cgroup_subsys_state *css;",
            "",
            "\t/* find the first leader if there is any */",
            "\tcgroup_taskset_for_each_leader(task, css, tset)",
            "\t\tbreak;",
            "",
            "\tif (!task)",
            "\t\treturn;",
            "",
            "\ttask_lock(task);",
            "\tif (task->mm && READ_ONCE(task->mm->owner) == task)",
            "\t\tlru_gen_migrate_mm(task->mm);",
            "\ttask_unlock(task);",
            "}",
            "static void mem_cgroup_lru_gen_attach(struct cgroup_taskset *tset) {}",
            "static void mem_cgroup_kmem_attach(struct cgroup_taskset *tset)",
            "{",
            "\tstruct task_struct *task;",
            "\tstruct cgroup_subsys_state *css;",
            "",
            "\tcgroup_taskset_for_each(task, css, tset) {",
            "\t\t/* atomically set the update bit */",
            "\t\tset_bit(CURRENT_OBJCG_UPDATE_BIT, (unsigned long *)&task->objcg);",
            "\t}",
            "}",
            "static void mem_cgroup_attach(struct cgroup_taskset *tset)",
            "{",
            "\tmem_cgroup_lru_gen_attach(tset);",
            "\tmem_cgroup_kmem_attach(tset);",
            "}",
            "static int seq_puts_memcg_tunable(struct seq_file *m, unsigned long value)",
            "{",
            "\tif (value == PAGE_COUNTER_MAX)",
            "\t\tseq_puts(m, \"max\\n\");",
            "\telse",
            "\t\tseq_printf(m, \"%llu\\n\", (u64)value * PAGE_SIZE);",
            "",
            "\treturn 0;",
            "}",
            "static u64 memory_current_read(struct cgroup_subsys_state *css,",
            "\t\t\t       struct cftype *cft)",
            "{",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);",
            "",
            "\treturn (u64)page_counter_read(&memcg->memory) * PAGE_SIZE;",
            "}",
            "static u64 memory_peak_read(struct cgroup_subsys_state *css,",
            "\t\t\t    struct cftype *cft)",
            "{",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_css(css);",
            "",
            "\treturn (u64)memcg->memory.watermark * PAGE_SIZE;",
            "}",
            "static int memory_min_show(struct seq_file *m, void *v)",
            "{",
            "\treturn seq_puts_memcg_tunable(m,",
            "\t\tREAD_ONCE(mem_cgroup_from_seq(m)->memory.min));",
            "}",
            "static ssize_t memory_min_write(struct kernfs_open_file *of,",
            "\t\t\t\tchar *buf, size_t nbytes, loff_t off)",
            "{",
            "\tstruct mem_cgroup *memcg = mem_cgroup_from_css(of_css(of));",
            "\tunsigned long min;",
            "\tint err;",
            "",
            "\tbuf = strstrip(buf);",
            "\terr = page_counter_memparse(buf, \"max\", &min);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tpage_counter_set_min(&memcg->memory, min);",
            "",
            "\treturn nbytes;",
            "}"
          ],
          "function_name": "mem_cgroup_fork, mem_cgroup_exit, mem_cgroup_lru_gen_attach, mem_cgroup_lru_gen_attach, mem_cgroup_kmem_attach, mem_cgroup_attach, seq_puts_memcg_tunable, memory_current_read, memory_peak_read, memory_min_show, memory_min_write",
          "description": "该代码段实现内存控制组（memcg）的核心管理逻辑，涵盖任务关联、资源统计及限流控制。  \n`mem_cgroup_fork/exit`负责延迟初始化任务的objcg字段并释放引用，`mem_cgroup_lru_gen_attach`和`mem_cgroup_kmem_attach`分别处理LRU迁移与kmem跟踪标记，而其余函数提供内存使用量（current/peak/min）的读写接口。  \n上下文不完整：存在重复定义的`mem_cgroup_lru_gen_attach`函数，且部分逻辑依赖未展示的条件分支或全局状态。",
          "similarity": 0.562165379524231
        }
      ]
    },
    {
      "source_file": "mm/mempolicy.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:44:01\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `mempolicy.c`\n\n---\n\n# mempolicy.c 技术文档\n\n## 1. 文件概述\n\n`mempolicy.c` 实现了 Linux 内核中的 NUMA（Non-Uniform Memory Access）内存策略机制，允许用户通过系统调用为进程或虚拟内存区域（VMA）指定内存分配偏好。该机制支持多种内存分配策略，包括本地优先、绑定节点、轮询交错和基于权重的交错分配等，以优化多节点 NUMA 系统上的内存访问性能。\n\n## 2. 核心功能\n\n### 主要数据结构\n- `struct mempolicy`：表示内存策略的核心结构，包含策略模式（如 MPOL_INTERLEAVE、MPOL_BIND、MPOL_PREFERRED 等）、节点掩码（nodemask）和引用计数。\n- `struct weighted_interleave_state`：用于实现加权交错分配策略，包含每个节点的权重表（iw_table）和自动模式标志。\n- `default_policy`：全局默认内存策略，初始为 MPOL_LOCAL（本地节点优先）。\n- `preferred_node_policy[MAX_NUMNODES]`：为每个节点预定义的首选策略数组。\n\n### 主要函数与接口\n- `get_il_weight(int node)`：获取指定节点在加权交错策略中的权重。\n- `reduce_interleave_weights(unsigned int *bw, u8 *new_iw)`：将带宽值转换为归一化的交错权重。\n- `mempolicy_set_node_perf(unsigned int node, struct access_coordinate *coords)`：根据节点性能坐标（读/写带宽）动态更新加权交错策略。\n- 多个辅助函数用于策略创建、复制、合并、验证及与 VMA 和进程上下文的集成。\n\n### 全局变量\n- `policy_cache` / `sn_cache`：用于高效分配 mempolicy 和相关子结构的 slab 缓存。\n- `policy_zone`：标识受策略控制的最高内存区域类型（zone_type），低区域（如 GFP_DMA）不应用策略。\n- `wi_state`：RCU 保护的加权交错状态指针。\n- `node_bw_table`：存储各节点带宽信息，用于动态权重计算。\n- `weightiness`：权重归一化常量（值为 32），平衡权重精度与分配公平性。\n\n## 3. 关键实现\n\n### 策略优先级与作用域\n- **VMA 策略优先于进程策略**：页错误处理时，若 VMA 有策略则使用 VMA 策略，否则回退到当前进程的策略。\n- **中断上下文忽略策略**：所有中断相关的内存分配始终尝试在本地 CPU 节点分配。\n- **策略不跨 swap 保留**：进程策略在页面换出/换入时不被保留。\n\n### 加权交错分配（Weighted Interleave）\n- 基于各 NUMA 节点的读/写带宽动态计算分配权重。\n- 使用 `weightiness=32` 对带宽进行缩放，并通过 GCD（最大公约数）约简权重以减少分配周期长度。\n- 权重状态通过 RCU 机制安全更新，读路径无锁，写路径由 `wi_state_lock` 互斥锁保护。\n\n### 策略类型详解\n- **interleave**：按偏移量（VMA）或进程计数器（进程）在节点集上轮询分配。\n- **weighted interleave**：按节点权重比例分配（如权重 [2,1] 表示节点0:节点1 = 2:1）。\n- **bind**：严格限制在指定节点集分配，无回退（当前实现按节点顺序分配，非最优）。\n- **preferred / preferred many**：优先在指定单个/多个节点分配，失败后回退到默认策略。\n- **default / local**：优先本地节点分配，VMA 中则继承进程策略。\n\n### 内存区域限制\n- 仅对 **最高 zone 层级**（如 NORMAL 或 MOVABLE）应用策略，GFP_DMA、HIGHMEM 等低层级分配忽略策略。\n\n### 特殊共享内存处理\n- **shmem/tmpfs**：策略在所有映射进程间共享，即使无活跃映射也持久保存。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `<linux/mm.h>`、`<linux/vm_area_struct.h>`、`<linux/page-flags.h>` 等进行页分配、VMA 操作和页表遍历。\n- **NUMA 感知调度**：与 `<linux/sched/numa_balancing.h>` 协同，支持自动 NUMA 迁移。\n- **CPUSET 子系统**：通过 `<linux/cpuset.h>` 集成节点可用性约束。\n- **Slab 分配器**：使用 kmem_cache 管理 mempolicy 对象生命周期。\n- **RCU 机制**：用于加权交错状态的无锁读取。\n- **系统调用接口**：通过 `sys_mbind()`、`sys_set_mempolicy()` 等提供用户空间配置入口。\n- **安全模块**：调用 LSM hooks（`security_task_movememory()`）进行权限检查。\n\n## 5. 使用场景\n\n- **高性能计算（HPC）应用**：通过 `mbind()` 将关键数据结构绑定到特定 NUMA 节点，减少远程内存访问延迟。\n- **数据库系统**：使用交错策略均衡多节点内存带宽，提升吞吐量。\n- **虚拟化环境**：VMM 可为不同虚拟机设置独立内存策略，隔离资源并优化性能。\n- **自动 NUMA 优化**：内核 NUMA balancing 机制结合默认策略，自动迁移热点页面至访问 CPU 所在节点。\n- **实时系统**：通过 `MPOL_BIND` 严格限制内存位置，确保确定性访问延迟。\n- **大页（HugeTLB）分配**：策略同样适用于透明大页和显式 HugeTLB 页面分配。",
      "similarity": 0.6391740441322327,
      "chunks": [
        {
          "chunk_id": 11,
          "file_path": "mm/mempolicy.c",
          "start_line": 1855,
          "end_line": 1971,
          "content": [
            "static int kernel_get_mempolicy(int __user *policy,",
            "\t\t\t\tunsigned long __user *nmask,",
            "\t\t\t\tunsigned long maxnode,",
            "\t\t\t\tunsigned long addr,",
            "\t\t\t\tunsigned long flags)",
            "{",
            "\tint err;",
            "\tint pval;",
            "\tnodemask_t nodes;",
            "",
            "\tif (nmask != NULL && maxnode < nr_node_ids)",
            "\t\treturn -EINVAL;",
            "",
            "\taddr = untagged_addr(addr);",
            "",
            "\terr = do_get_mempolicy(&pval, &nodes, addr, flags);",
            "",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\tif (policy && put_user(pval, policy))",
            "\t\treturn -EFAULT;",
            "",
            "\tif (nmask)",
            "\t\terr = copy_nodes_to_user(nmask, maxnode, &nodes);",
            "",
            "\treturn err;",
            "}",
            "bool vma_migratable(struct vm_area_struct *vma)",
            "{",
            "\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * DAX device mappings require predictable access latency, so avoid",
            "\t * incurring periodic faults.",
            "\t */",
            "\tif (vma_is_dax(vma))",
            "\t\treturn false;",
            "",
            "\tif (is_vm_hugetlb_page(vma) &&",
            "\t\t!hugepage_migration_supported(hstate_vma(vma)))",
            "\t\treturn false;",
            "",
            "\t/*",
            "\t * Migration allocates pages in the highest zone. If we cannot",
            "\t * do so then migration (at least from node to node) is not",
            "\t * possible.",
            "\t */",
            "\tif (vma->vm_file &&",
            "\t\tgfp_zone(mapping_gfp_mask(vma->vm_file->f_mapping))",
            "\t\t\t< policy_zone)",
            "\t\treturn false;",
            "\treturn true;",
            "}",
            "bool vma_policy_mof(struct vm_area_struct *vma)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->get_policy) {",
            "\t\tbool ret = false;",
            "\t\tpgoff_t ilx;\t\t/* ignored here */",
            "",
            "\t\tpol = vma->vm_ops->get_policy(vma, vma->vm_start, &ilx);",
            "\t\tif (pol && (pol->flags & MPOL_F_MOF))",
            "\t\t\tret = true;",
            "\t\tmpol_cond_put(pol);",
            "",
            "\t\treturn ret;",
            "\t}",
            "",
            "\tpol = vma->vm_policy;",
            "\tif (!pol)",
            "\t\tpol = get_task_policy(current);",
            "",
            "\treturn pol->flags & MPOL_F_MOF;",
            "}",
            "bool apply_policy_zone(struct mempolicy *policy, enum zone_type zone)",
            "{",
            "\tenum zone_type dynamic_policy_zone = policy_zone;",
            "",
            "\tBUG_ON(dynamic_policy_zone == ZONE_MOVABLE);",
            "",
            "\t/*",
            "\t * if policy->nodes has movable memory only,",
            "\t * we apply policy when gfp_zone(gfp) = ZONE_MOVABLE only.",
            "\t *",
            "\t * policy->nodes is intersect with node_states[N_MEMORY].",
            "\t * so if the following test fails, it implies",
            "\t * policy->nodes has movable memory only.",
            "\t */",
            "\tif (!nodes_intersects(policy->nodes, node_states[N_HIGH_MEMORY]))",
            "\t\tdynamic_policy_zone = ZONE_MOVABLE;",
            "",
            "\treturn zone >= dynamic_policy_zone;",
            "}",
            "static unsigned int weighted_interleave_nodes(struct mempolicy *policy)",
            "{",
            "\tunsigned int node;",
            "\tunsigned int cpuset_mems_cookie;",
            "",
            "retry:",
            "\t/* to prevent miscount use tsk->mems_allowed_seq to detect rebind */",
            "\tcpuset_mems_cookie = read_mems_allowed_begin();",
            "\tnode = current->il_prev;",
            "\tif (!current->il_weight || !node_isset(node, policy->nodes)) {",
            "\t\tnode = next_node_in(node, policy->nodes);",
            "\t\tif (read_mems_allowed_retry(cpuset_mems_cookie))",
            "\t\t\tgoto retry;",
            "\t\tif (node == MAX_NUMNODES)",
            "\t\t\treturn node;",
            "\t\tcurrent->il_prev = node;",
            "\t\tcurrent->il_weight = get_il_weight(node);",
            "\t}",
            "\tcurrent->il_weight--;",
            "\treturn node;",
            "}"
          ],
          "function_name": "kernel_get_mempolicy, vma_migratable, vma_policy_mof, apply_policy_zone, weighted_interleave_nodes",
          "description": "kernel_get_mempolicy 获取当前内存策略参数并复制到用户空间；vma_migratable 判断虚拟内存区域是否支持迁移；vma_policy_mof 检查VMA是否启用了MOF（Migration On Fault）策略；apply_policy_zone 确定当前zone是否满足策略要求；weighted_interleave_nodes 计算加权交错分配的目标节点。",
          "similarity": 0.5882989764213562
        },
        {
          "chunk_id": 10,
          "file_path": "mm/mempolicy.c",
          "start_line": 1735,
          "end_line": 1838,
          "content": [
            "static long kernel_set_mempolicy(int mode, const unsigned long __user *nmask,",
            "\t\t\t\t unsigned long maxnode)",
            "{",
            "\tunsigned short mode_flags;",
            "\tnodemask_t nodes;",
            "\tint lmode = mode;",
            "\tint err;",
            "",
            "\terr = sanitize_mpol_flags(&lmode, &mode_flags);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\terr = get_nodes(&nodes, nmask, maxnode);",
            "\tif (err)",
            "\t\treturn err;",
            "",
            "\treturn do_set_mempolicy(lmode, mode_flags, &nodes);",
            "}",
            "static int kernel_migrate_pages(pid_t pid, unsigned long maxnode,",
            "\t\t\t\tconst unsigned long __user *old_nodes,",
            "\t\t\t\tconst unsigned long __user *new_nodes)",
            "{",
            "\tstruct mm_struct *mm = NULL;",
            "\tstruct task_struct *task;",
            "\tnodemask_t task_nodes;",
            "\tint err;",
            "\tnodemask_t *old;",
            "\tnodemask_t *new;",
            "\tNODEMASK_SCRATCH(scratch);",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\told = &scratch->mask1;",
            "\tnew = &scratch->mask2;",
            "",
            "\terr = get_nodes(old, old_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\terr = get_nodes(new, new_nodes, maxnode);",
            "\tif (err)",
            "\t\tgoto out;",
            "",
            "\t/* Find the mm_struct */",
            "\trcu_read_lock();",
            "\ttask = pid ? find_task_by_vpid(pid) : current;",
            "\tif (!task) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -ESRCH;",
            "\t\tgoto out;",
            "\t}",
            "\tget_task_struct(task);",
            "",
            "\terr = -EINVAL;",
            "",
            "\t/*",
            "\t * Check if this process has the right to modify the specified process.",
            "\t * Use the regular \"ptrace_may_access()\" checks.",
            "\t */",
            "\tif (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {",
            "\t\trcu_read_unlock();",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "\trcu_read_unlock();",
            "",
            "\ttask_nodes = cpuset_mems_allowed(task);",
            "\t/* Is the user allowed to access the target nodes? */",
            "\tif (!nodes_subset(*new, task_nodes) && !capable(CAP_SYS_NICE)) {",
            "\t\terr = -EPERM;",
            "\t\tgoto out_put;",
            "\t}",
            "",
            "\ttask_nodes = cpuset_mems_allowed(current);",
            "\tnodes_and(*new, *new, task_nodes);",
            "\tif (nodes_empty(*new))",
            "\t\tgoto out_put;",
            "",
            "\terr = security_task_movememory(task);",
            "\tif (err)",
            "\t\tgoto out_put;",
            "",
            "\tmm = get_task_mm(task);",
            "\tput_task_struct(task);",
            "",
            "\tif (!mm) {",
            "\t\terr = -EINVAL;",
            "\t\tgoto out;",
            "\t}",
            "",
            "\terr = do_migrate_pages(mm, old, new,",
            "\t\tcapable(CAP_SYS_NICE) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE);",
            "",
            "\tmmput(mm);",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "",
            "\treturn err;",
            "",
            "out_put:",
            "\tput_task_struct(task);",
            "\tgoto out;",
            "}"
          ],
          "function_name": "kernel_set_mempolicy, kernel_migrate_pages",
          "description": "kernel_set_mempolicy 设置进程的内存放置策略，通过sanitize_mpol_flags验证模式标志并调用do_set_mempolicy应用策略；kernel_migrate_pages 实现页面迁移，检查目标进程权限，限制迁移节点范围，并调用do_migrate_pages进行实际迁移操作。",
          "similarity": 0.5743672847747803
        },
        {
          "chunk_id": 5,
          "file_path": "mm/mempolicy.c",
          "start_line": 880,
          "end_line": 996,
          "content": [
            "static long",
            "queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,",
            "\t\tnodemask_t *nodes, unsigned long flags,",
            "\t\tstruct list_head *pagelist)",
            "{",
            "\tint err;",
            "\tstruct queue_pages qp = {",
            "\t\t.pagelist = pagelist,",
            "\t\t.flags = flags,",
            "\t\t.nmask = nodes,",
            "\t\t.start = start,",
            "\t\t.end = end,",
            "\t\t.first = NULL,",
            "\t};",
            "\tconst struct mm_walk_ops *ops = (flags & MPOL_MF_WRLOCK) ?",
            "\t\t\t&queue_pages_lock_vma_walk_ops : &queue_pages_walk_ops;",
            "",
            "\terr = walk_page_range(mm, start, end, ops, &qp);",
            "",
            "\tif (!qp.first)",
            "\t\t/* whole range in hole */",
            "\t\terr = -EFAULT;",
            "",
            "\treturn err ? : qp.nr_failed;",
            "}",
            "static int vma_replace_policy(struct vm_area_struct *vma,",
            "\t\t\t\tstruct mempolicy *pol)",
            "{",
            "\tint err;",
            "\tstruct mempolicy *old;",
            "\tstruct mempolicy *new;",
            "",
            "\tvma_assert_write_locked(vma);",
            "",
            "\tnew = mpol_dup(pol);",
            "\tif (IS_ERR(new))",
            "\t\treturn PTR_ERR(new);",
            "",
            "\tif (vma->vm_ops && vma->vm_ops->set_policy) {",
            "\t\terr = vma->vm_ops->set_policy(vma, new);",
            "\t\tif (err)",
            "\t\t\tgoto err_out;",
            "\t}",
            "",
            "\told = vma->vm_policy;",
            "\tvma->vm_policy = new; /* protected by mmap_lock */",
            "\tmpol_put(old);",
            "",
            "\treturn 0;",
            " err_out:",
            "\tmpol_put(new);",
            "\treturn err;",
            "}",
            "static int mbind_range(struct vma_iterator *vmi, struct vm_area_struct *vma,",
            "\t\tstruct vm_area_struct **prev, unsigned long start,",
            "\t\tunsigned long end, struct mempolicy *new_pol)",
            "{",
            "\tunsigned long vmstart, vmend;",
            "",
            "\tvmend = min(end, vma->vm_end);",
            "\tif (start > vma->vm_start) {",
            "\t\t*prev = vma;",
            "\t\tvmstart = start;",
            "\t} else {",
            "\t\tvmstart = vma->vm_start;",
            "\t}",
            "",
            "\tif (mpol_equal(vma->vm_policy, new_pol)) {",
            "\t\t*prev = vma;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tvma =  vma_modify_policy(vmi, *prev, vma, vmstart, vmend, new_pol);",
            "\tif (IS_ERR(vma))",
            "\t\treturn PTR_ERR(vma);",
            "",
            "\t*prev = vma;",
            "\treturn vma_replace_policy(vma, new_pol);",
            "}",
            "static long do_set_mempolicy(unsigned short mode, unsigned short flags,",
            "\t\t\t     nodemask_t *nodes)",
            "{",
            "\tstruct mempolicy *new, *old;",
            "\tNODEMASK_SCRATCH(scratch);",
            "\tint ret;",
            "",
            "\tif (!scratch)",
            "\t\treturn -ENOMEM;",
            "",
            "\tnew = mpol_new(mode, flags, nodes);",
            "\tif (IS_ERR(new)) {",
            "\t\tret = PTR_ERR(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\ttask_lock(current);",
            "\tret = mpol_set_nodemask(new, nodes, scratch);",
            "\tif (ret) {",
            "\t\ttask_unlock(current);",
            "\t\tmpol_put(new);",
            "\t\tgoto out;",
            "\t}",
            "",
            "\told = current->mempolicy;",
            "\tcurrent->mempolicy = new;",
            "\tif (new && (new->mode == MPOL_INTERLEAVE ||",
            "\t\t    new->mode == MPOL_WEIGHTED_INTERLEAVE)) {",
            "\t\tcurrent->il_prev = MAX_NUMNODES-1;",
            "\t\tcurrent->il_weight = 0;",
            "\t}",
            "\ttask_unlock(current);",
            "\tmpol_put(old);",
            "\tret = 0;",
            "out:",
            "\tNODEMASK_SCRATCH_FREE(scratch);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "queue_pages_range, vma_replace_policy, mbind_range, do_set_mempolicy",
          "description": "实现内存策略设置，通过queue_pages_range队列页面，vma_replace_policy替换VMA策略，mbind_range绑定指定范围策略，do_set_mempolicy设置当前进程全局内存策略",
          "similarity": 0.5671769380569458
        },
        {
          "chunk_id": 13,
          "file_path": "mm/mempolicy.c",
          "start_line": 2149,
          "end_line": 2255,
          "content": [
            "static unsigned int interleave_nid(struct mempolicy *pol, pgoff_t ilx)",
            "{",
            "\tnodemask_t nodemask;",
            "\tunsigned int target, nnodes;",
            "\tint i;",
            "\tint nid;",
            "",
            "\tnnodes = read_once_policy_nodemask(pol, &nodemask);",
            "\tif (!nnodes)",
            "\t\treturn numa_node_id();",
            "\ttarget = ilx % nnodes;",
            "\tnid = first_node(nodemask);",
            "\tfor (i = 0; i < target; i++)",
            "\t\tnid = next_node(nid, nodemask);",
            "\treturn nid;",
            "}",
            "int huge_node(struct vm_area_struct *vma, unsigned long addr, gfp_t gfp_flags,",
            "\t\tstruct mempolicy **mpol, nodemask_t **nodemask)",
            "{",
            "\tpgoff_t ilx;",
            "\tint nid;",
            "",
            "\tnid = numa_node_id();",
            "\t*mpol = get_vma_policy(vma, addr, hstate_vma(vma)->order, &ilx);",
            "\t*nodemask = policy_nodemask(gfp_flags, *mpol, ilx, &nid);",
            "\treturn nid;",
            "}",
            "bool init_nodemask_of_mempolicy(nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "",
            "\tif (!(mask && current->mempolicy))",
            "\t\treturn false;",
            "",
            "\ttask_lock(current);",
            "\tmempolicy = current->mempolicy;",
            "\tswitch (mempolicy->mode) {",
            "\tcase MPOL_PREFERRED:",
            "\tcase MPOL_PREFERRED_MANY:",
            "\tcase MPOL_BIND:",
            "\tcase MPOL_INTERLEAVE:",
            "\tcase MPOL_WEIGHTED_INTERLEAVE:",
            "\t\t*mask = mempolicy->nodes;",
            "\t\tbreak;",
            "",
            "\tcase MPOL_LOCAL:",
            "\t\tinit_nodemask_of_node(mask, numa_node_id());",
            "\t\tbreak;",
            "",
            "\tdefault:",
            "\t\tBUG();",
            "\t}",
            "\ttask_unlock(current);",
            "",
            "\treturn true;",
            "}",
            "bool mempolicy_in_oom_domain(struct task_struct *tsk,",
            "\t\t\t\t\tconst nodemask_t *mask)",
            "{",
            "\tstruct mempolicy *mempolicy;",
            "\tbool ret = true;",
            "",
            "\tif (!mask)",
            "\t\treturn ret;",
            "",
            "\ttask_lock(tsk);",
            "\tmempolicy = tsk->mempolicy;",
            "\tif (mempolicy && mempolicy->mode == MPOL_BIND)",
            "\t\tret = nodes_intersects(mempolicy->nodes, *mask);",
            "\ttask_unlock(tsk);",
            "",
            "\treturn ret;",
            "}",
            "static unsigned long alloc_pages_bulk_array_interleave(gfp_t gfp,",
            "\t\tstruct mempolicy *pol, unsigned long nr_pages,",
            "\t\tstruct page **page_array)",
            "{",
            "\tint nodes;",
            "\tunsigned long nr_pages_per_node;",
            "\tint delta;",
            "\tint i;",
            "\tunsigned long nr_allocated;",
            "\tunsigned long total_allocated = 0;",
            "",
            "\tnodes = nodes_weight(pol->nodes);",
            "\tnr_pages_per_node = nr_pages / nodes;",
            "\tdelta = nr_pages - nodes * nr_pages_per_node;",
            "",
            "\tfor (i = 0; i < nodes; i++) {",
            "\t\tif (delta) {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node + 1, NULL,",
            "\t\t\t\t\tpage_array);",
            "\t\t\tdelta--;",
            "\t\t} else {",
            "\t\t\tnr_allocated = alloc_pages_bulk_noprof(gfp,",
            "\t\t\t\t\tinterleave_nodes(pol), NULL,",
            "\t\t\t\t\tnr_pages_per_node, NULL, page_array);",
            "\t\t}",
            "",
            "\t\tpage_array += nr_allocated;",
            "\t\ttotal_allocated += nr_allocated;",
            "\t}",
            "",
            "\treturn total_allocated;",
            "}"
          ],
          "function_name": "interleave_nid, huge_node, init_nodemask_of_mempolicy, mempolicy_in_oom_domain, alloc_pages_bulk_array_interleave",
          "description": "interleave_nid 计算简单交错分配的目标节点；huge_node 结合HugeTLB策略确定大页分配节点；init_nodemask_of_mempolicy 初始化当前进程的内存策略节点掩码；mempolicy_in_oom_domain 检查策略节点是否与OOM域重叠；alloc_pages_bulk_array_interleave 执行批量交错分配。",
          "similarity": 0.5571466684341431
        },
        {
          "chunk_id": 17,
          "file_path": "mm/mempolicy.c",
          "start_line": 2969,
          "end_line": 3093,
          "content": [
            "void mpol_put_task_policy(struct task_struct *task)",
            "{",
            "\tstruct mempolicy *pol;",
            "",
            "\ttask_lock(task);",
            "\tpol = task->mempolicy;",
            "\ttask->mempolicy = NULL;",
            "\ttask_unlock(task);",
            "\tmpol_put(pol);",
            "}",
            "static void sp_delete(struct shared_policy *sp, struct sp_node *n)",
            "{",
            "\trb_erase(&n->nd, &sp->root);",
            "\tsp_free(n);",
            "}",
            "static void sp_node_init(struct sp_node *node, unsigned long start,",
            "\t\t\tunsigned long end, struct mempolicy *pol)",
            "{",
            "\tnode->start = start;",
            "\tnode->end = end;",
            "\tnode->policy = pol;",
            "}",
            "static int shared_policy_replace(struct shared_policy *sp, pgoff_t start,",
            "\t\t\t\t pgoff_t end, struct sp_node *new)",
            "{",
            "\tstruct sp_node *n;",
            "\tstruct sp_node *n_new = NULL;",
            "\tstruct mempolicy *mpol_new = NULL;",
            "\tint ret = 0;",
            "",
            "restart:",
            "\twrite_lock(&sp->lock);",
            "\tn = sp_lookup(sp, start, end);",
            "\t/* Take care of old policies in the same range. */",
            "\twhile (n && n->start < end) {",
            "\t\tstruct rb_node *next = rb_next(&n->nd);",
            "\t\tif (n->start >= start) {",
            "\t\t\tif (n->end <= end)",
            "\t\t\t\tsp_delete(sp, n);",
            "\t\t\telse",
            "\t\t\t\tn->start = end;",
            "\t\t} else {",
            "\t\t\t/* Old policy spanning whole new range. */",
            "\t\t\tif (n->end > end) {",
            "\t\t\t\tif (!n_new)",
            "\t\t\t\t\tgoto alloc_new;",
            "",
            "\t\t\t\t*mpol_new = *n->policy;",
            "\t\t\t\tatomic_set(&mpol_new->refcnt, 1);",
            "\t\t\t\tsp_node_init(n_new, end, n->end, mpol_new);",
            "\t\t\t\tn->end = start;",
            "\t\t\t\tsp_insert(sp, n_new);",
            "\t\t\t\tn_new = NULL;",
            "\t\t\t\tmpol_new = NULL;",
            "\t\t\t\tbreak;",
            "\t\t\t} else",
            "\t\t\t\tn->end = start;",
            "\t\t}",
            "\t\tif (!next)",
            "\t\t\tbreak;",
            "\t\tn = rb_entry(next, struct sp_node, nd);",
            "\t}",
            "\tif (new)",
            "\t\tsp_insert(sp, new);",
            "\twrite_unlock(&sp->lock);",
            "\tret = 0;",
            "",
            "err_out:",
            "\tif (mpol_new)",
            "\t\tmpol_put(mpol_new);",
            "\tif (n_new)",
            "\t\tkmem_cache_free(sn_cache, n_new);",
            "",
            "\treturn ret;",
            "",
            "alloc_new:",
            "\twrite_unlock(&sp->lock);",
            "\tret = -ENOMEM;",
            "\tn_new = kmem_cache_alloc(sn_cache, GFP_KERNEL);",
            "\tif (!n_new)",
            "\t\tgoto err_out;",
            "\tmpol_new = kmem_cache_alloc(policy_cache, GFP_KERNEL);",
            "\tif (!mpol_new)",
            "\t\tgoto err_out;",
            "\tatomic_set(&mpol_new->refcnt, 1);",
            "\tgoto restart;",
            "}",
            "void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol)",
            "{",
            "\tint ret;",
            "",
            "\tsp->root = RB_ROOT;\t\t/* empty tree == default mempolicy */",
            "\trwlock_init(&sp->lock);",
            "",
            "\tif (mpol) {",
            "\t\tstruct sp_node *sn;",
            "\t\tstruct mempolicy *npol;",
            "\t\tNODEMASK_SCRATCH(scratch);",
            "",
            "\t\tif (!scratch)",
            "\t\t\tgoto put_mpol;",
            "",
            "\t\t/* contextualize the tmpfs mount point mempolicy to this file */",
            "\t\tnpol = mpol_new(mpol->mode, mpol->flags, &mpol->w.user_nodemask);",
            "\t\tif (IS_ERR(npol))",
            "\t\t\tgoto free_scratch; /* no valid nodemask intersection */",
            "",
            "\t\ttask_lock(current);",
            "\t\tret = mpol_set_nodemask(npol, &mpol->w.user_nodemask, scratch);",
            "\t\ttask_unlock(current);",
            "\t\tif (ret)",
            "\t\t\tgoto put_npol;",
            "",
            "\t\t/* alloc node covering entire file; adds ref to file's npol */",
            "\t\tsn = sp_alloc(0, MAX_LFS_FILESIZE >> PAGE_SHIFT, npol);",
            "\t\tif (sn)",
            "\t\t\tsp_insert(sp, sn);",
            "put_npol:",
            "\t\tmpol_put(npol);\t/* drop initial ref on file's npol */",
            "free_scratch:",
            "\t\tNODEMASK_SCRATCH_FREE(scratch);",
            "put_mpol:",
            "\t\tmpol_put(mpol);\t/* drop our incoming ref on sb mpol */",
            "\t}",
            "}"
          ],
          "function_name": "mpol_put_task_policy, sp_delete, sp_node_init, shared_policy_replace, mpol_shared_policy_init",
          "description": "mpol_put_task_policy释放任务级内存策略引用，sp_delete从RB树删除节点并回收资源，sp_node_init初始化共享策略节点，shared_policy_replace替换共享策略区间并处理节点分裂，mpol_shared_policy_init初始化共享策略结构体并设置初始策略。",
          "similarity": 0.5547679662704468
        }
      ]
    }
  ]
}