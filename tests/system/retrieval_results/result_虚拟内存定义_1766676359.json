{
  "query": "虚拟内存定义",
  "timestamp": "2025-12-25 23:25:59",
  "retrieved_files": [
    {
      "source_file": "mm/memory_hotplug.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:43:14\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `memory_hotplug.c`\n\n---\n\n# memory_hotplug.c 技术文档\n\n## 1. 文件概述\n\n`memory_hotplug.c` 是 Linux 内核中实现内存热插拔（Memory Hotplug）功能的核心源文件，位于 `mm/` 子系统目录下。该文件提供了在系统运行时动态添加或移除物理内存区域的能力，包括内存资源注册、页表映射管理、内存上线策略控制、以及与 NUMA 架构的协同支持。它主要处理热添加内存时的初始化、内存块（memory block）管理、vmemmap 映射优化、以及在线策略配置等关键逻辑。\n\n## 2. 核心功能\n\n### 主要全局变量与参数\n- `memmap_mode`：控制是否启用“内存上的 memmap”（memmap on memory）特性，支持 `disable`、`enable` 和 `force` 三种模式。\n- `online_policy`：定义内存上线时的默认区域分配策略，可选 `contig-zones`（保持区域连续）或 `auto-movable`（自动分配到 ZONE_MOVABLE）。\n- `auto_movable_ratio`：在 `auto-movable` 策略下，系统允许的 MOVABLE 与 KERNEL 内存的最大百分比比例（默认 301%，即约 3:1）。\n- `auto_movable_numa_aware`（仅 CONFIG_NUMA）：是否在 `auto-movable` 策略中考虑 NUMA 节点级别的内存统计。\n- `mhp_default_online_type`：内存热插拔时的默认上线类型（如 `MMOP_ONLINE_KERNEL`、`MMOP_ONLINE_MOVABLE` 等）。\n- `movable_node_enabled`：标志是否启用了可移动节点（movable node）功能。\n- `max_mem_size`：系统允许的最大内存大小上限（默认为 `U64_MAX`）。\n\n### 主要函数与接口\n- `get_online_mems()` / `put_online_mems()`：获取/释放内存热插拔读锁，用于保护内存上线/下线操作。\n- `mem_hotplug_begin()` / `mem_hotplug_done()`：执行内存热插拔写操作前后的同步原语，同时持有 CPU 热插拔读锁和内存热插拔写锁。\n- `mhp_get_default_online_type()` / `mhp_set_default_online_type()`：获取或设置内存热插拔的默认上线类型。\n- `register_memory_resource()`：将新添加的内存区域注册为 I/O 资源（`System RAM` 类型），并检查是否超出 `max_mem_size` 限制。\n- `mhp_memmap_on_memory()`：判断当前是否启用了 memmap on memory 特性。\n- `memory_block_memmap_on_memory_pages()`：计算在 memmap on memory 模式下，每个内存块所需的额外页数（可能因对齐而浪费内存）。\n\n### 回调机制\n- `online_page_callback`：指向当前用于上线单个页面的回调函数，默认为 `generic_online_page`。\n- `set_online_page_callback()` / `restore_online_page_callback()`（声明未在片段中，但有注释说明）：用于动态替换或恢复页面上线回调。\n\n### 内核参数（module_param）\n- `memmap_on_memory`：启用 memmap on memory 功能（Y/N/force）。\n- `online_policy`：设置默认上线策略。\n- `auto_movable_ratio`：设置 MOVABLE/KERNEL 内存比例上限。\n- `auto_movable_numa_aware`：是否在 NUMA 感知下应用 auto-movable 策略。\n- 启动参数 `memhp_default_state=`：通过内核命令行设置默认上线状态。\n\n## 3. 关键实现\n\n### Memmap on Memory 机制\n当启用 `CONFIG_MHP_MEMMAP_ON_MEMORY` 时，内核尝试将描述物理页的 `struct page` 数组（即 vmemmap）直接放置在待热插拔的内存区域内，而非依赖预先保留的虚拟地址空间。这减少了对固定 vmemmap 区域的依赖，提升灵活性：\n- **ENABLE 模式**：仅当 vmemmap 大小能被页块（pageblock）整除时才启用。\n- **FORCE 模式**：强制对齐到页块边界，即使造成内存浪费（通过 `pageblock_align()` 实现），确保总能使用该内存区域存放 memmap。\n\n### 内存上线策略\n- **contig-zones（默认）**：将新内存添加到现有内存区域末尾，保持 ZONE_NORMAL 等区域的物理连续性。\n- **auto-movable**：根据全局（及 NUMA 节点）的 KERNEL 与 MOVABLE 内存比例，智能决定是否将新内存加入 ZONE_MOVABLE，以提高内存可迁移性和碎片整理效率。比例由 `auto_movable_ratio` 控制。\n\n### 并发控制\n使用 `percpu_rwsem mem_hotplug_lock` 作为内存热插拔操作的主同步机制：\n- 读操作（如内存访问路径）调用 `get/put_online_mems()` 获取读锁。\n- 写操作（如 add_memory）调用 `mem_hotplug_begin/done()` 获取写锁，并同时持有 `cpus_read_lock()` 防止 CPU 热插拔干扰。\n\n### 资源与大小限制\n- 通过 `mhp_range_allowed()` 检查待添加内存是否超出 `max_mem_size`。\n- 使用 `register_memory_resource()` 将内存注册为 `IORESOURCE_SYSTEM_RAM` 资源，若资源名非 \"System RAM\" 则标记为驱动管理（`IORESOURCE_SYSRAM_DRIVER_MANAGED`）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：依赖 `mm.h`、`page-isolation.h`、`migrate.h`、`compaction.h` 等，用于页面分配、隔离、迁移和压缩。\n- **体系结构相关**：包含 `asm/tlbflush.h` 用于 TLB 刷新；依赖 `pfn.h`、`memblock.h` 处理物理页帧和启动内存布局。\n- **设备模型与 sysfs**：通过 `memory.h` 与用户空间交互（如 `/sys/devices/system/memory/`）。\n- **NUMA 支持**：在 `CONFIG_NUMA` 下使用节点感知策略。\n- **虚拟内存**：依赖 `vmalloc.h` 和 `memremap.h` 管理 vmemmap 映射。\n- **电源管理**：包含 `suspend.h`，可能与休眠/唤醒流程协调。\n- **固件接口**：使用 `firmware-map.h` 与平台固件交互内存布局信息。\n\n## 5. 使用场景\n\n- **物理内存热添加**：在支持内存热插拔的服务器（如 IBM Power、x86 ACPI 系统）上，动态增加 DIMM 或内存模块后，内核通过此文件完成内存初始化和上线。\n- **虚拟化环境**：KVM、Xen 等 hypervisor 向客户机热添加内存时，客户机内核调用此模块处理新增内存。\n- **内存故障恢复**：在某些 RAS（Reliability, Availability, Serviceability）场景中，隔离坏页后重新上线备用内存。\n- **测试与开发**：通过 sysfs 接口（如 `echo online > /sys/devices/system/memory/memoryX/state`）手动上线内存块，配合 `online_policy` 和 `memmap_on_memory` 参数进行功能验证。\n- **容器与云平台**：支持弹性内存扩展，按需分配物理内存资源。",
      "similarity": 0.5703972578048706,
      "chunks": [
        {
          "chunk_id": 13,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 2233,
          "end_line": 2352,
          "content": [
            "void __remove_memory(u64 start, u64 size)",
            "{",
            "",
            "\t/*",
            "\t * trigger BUG() if some memory is not offlined prior to calling this",
            "\t * function",
            "\t */",
            "\tif (try_remove_memory(start, size))",
            "\t\tBUG();",
            "}",
            "int remove_memory(u64 start, u64 size)",
            "{",
            "\tint rc;",
            "",
            "\tlock_device_hotplug();",
            "\trc = try_remove_memory(start, size);",
            "\tunlock_device_hotplug();",
            "",
            "\treturn rc;",
            "}",
            "static int try_offline_memory_block(struct memory_block *mem, void *arg)",
            "{",
            "\tuint8_t online_type = MMOP_ONLINE_KERNEL;",
            "\tuint8_t **online_types = arg;",
            "\tstruct page *page;",
            "\tint rc;",
            "",
            "\t/*",
            "\t * Sense the online_type via the zone of the memory block. Offlining",
            "\t * with multiple zones within one memory block will be rejected",
            "\t * by offlining code ... so we don't care about that.",
            "\t */",
            "\tpage = pfn_to_online_page(section_nr_to_pfn(mem->start_section_nr));",
            "\tif (page && zone_idx(page_zone(page)) == ZONE_MOVABLE)",
            "\t\tonline_type = MMOP_ONLINE_MOVABLE;",
            "",
            "\trc = device_offline(&mem->dev);",
            "\t/*",
            "\t * Default is MMOP_OFFLINE - change it only if offlining succeeded,",
            "\t * so try_reonline_memory_block() can do the right thing.",
            "\t */",
            "\tif (!rc)",
            "\t\t**online_types = online_type;",
            "",
            "\t(*online_types)++;",
            "\t/* Ignore if already offline. */",
            "\treturn rc < 0 ? rc : 0;",
            "}",
            "static int try_reonline_memory_block(struct memory_block *mem, void *arg)",
            "{",
            "\tuint8_t **online_types = arg;",
            "\tint rc;",
            "",
            "\tif (**online_types != MMOP_OFFLINE) {",
            "\t\tmem->online_type = **online_types;",
            "\t\trc = device_online(&mem->dev);",
            "\t\tif (rc < 0)",
            "\t\t\tpr_warn(\"%s: Failed to re-online memory: %d\",",
            "\t\t\t\t__func__, rc);",
            "\t}",
            "",
            "\t/* Continue processing all remaining memory blocks. */",
            "\t(*online_types)++;",
            "\treturn 0;",
            "}",
            "int offline_and_remove_memory(u64 start, u64 size)",
            "{",
            "\tconst unsigned long mb_count = size / memory_block_size_bytes();",
            "\tuint8_t *online_types, *tmp;",
            "\tint rc;",
            "",
            "\tif (!IS_ALIGNED(start, memory_block_size_bytes()) ||",
            "\t    !IS_ALIGNED(size, memory_block_size_bytes()) || !size)",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * We'll remember the old online type of each memory block, so we can",
            "\t * try to revert whatever we did when offlining one memory block fails",
            "\t * after offlining some others succeeded.",
            "\t */",
            "\tonline_types = kmalloc_array(mb_count, sizeof(*online_types),",
            "\t\t\t\t     GFP_KERNEL);",
            "\tif (!online_types)",
            "\t\treturn -ENOMEM;",
            "\t/*",
            "\t * Initialize all states to MMOP_OFFLINE, so when we abort processing in",
            "\t * try_offline_memory_block(), we'll skip all unprocessed blocks in",
            "\t * try_reonline_memory_block().",
            "\t */",
            "\tmemset(online_types, MMOP_OFFLINE, mb_count);",
            "",
            "\tlock_device_hotplug();",
            "",
            "\ttmp = online_types;",
            "\trc = walk_memory_blocks(start, size, &tmp, try_offline_memory_block);",
            "",
            "\t/*",
            "\t * In case we succeeded to offline all memory, remove it.",
            "\t * This cannot fail as it cannot get onlined in the meantime.",
            "\t */",
            "\tif (!rc) {",
            "\t\trc = try_remove_memory(start, size);",
            "\t\tif (rc)",
            "\t\t\tpr_err(\"%s: Failed to remove memory: %d\", __func__, rc);",
            "\t}",
            "",
            "\t/*",
            "\t * Rollback what we did. While memory onlining might theoretically fail",
            "\t * (nacked by a notifier), it barely ever happens.",
            "\t */",
            "\tif (rc) {",
            "\t\ttmp = online_types;",
            "\t\twalk_memory_blocks(start, size, &tmp,",
            "\t\t\t\t   try_reonline_memory_block);",
            "\t}",
            "\tunlock_device_hotplug();",
            "",
            "\tkfree(online_types);",
            "\treturn rc;",
            "}"
          ],
          "function_name": "__remove_memory, remove_memory, try_offline_memory_block, try_reonline_memory_block, offline_and_remove_memory",
          "description": "offline_and_remove_memory 管理内存块的离线与移除流程，记录各内存块原始在线类型，在部分失败时回滚操作；try_offline_memory_block 和 try_reonline_memory_block 分别用于设置内存块离线状态及恢复在线状态",
          "similarity": 0.4965111017227173
        },
        {
          "chunk_id": 9,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 1529,
          "end_line": 1660,
          "content": [
            "int __ref __add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags)",
            "{",
            "\tstruct resource *res;",
            "\tint ret;",
            "",
            "\tres = register_memory_resource(start, size, \"System RAM\");",
            "\tif (IS_ERR(res))",
            "\t\treturn PTR_ERR(res);",
            "",
            "\tret = add_memory_resource(nid, res, mhp_flags);",
            "\tif (ret < 0)",
            "\t\trelease_memory_resource(res);",
            "\treturn ret;",
            "}",
            "int add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags)",
            "{",
            "\tint rc;",
            "",
            "\tlock_device_hotplug();",
            "\trc = __add_memory(nid, start, size, mhp_flags);",
            "\tunlock_device_hotplug();",
            "",
            "\treturn rc;",
            "}",
            "int add_memory_driver_managed(int nid, u64 start, u64 size,",
            "\t\t\t      const char *resource_name, mhp_t mhp_flags)",
            "{",
            "\tstruct resource *res;",
            "\tint rc;",
            "",
            "\tif (!resource_name ||",
            "\t    strstr(resource_name, \"System RAM (\") != resource_name ||",
            "\t    resource_name[strlen(resource_name) - 1] != ')')",
            "\t\treturn -EINVAL;",
            "",
            "\tlock_device_hotplug();",
            "",
            "\tres = register_memory_resource(start, size, resource_name);",
            "\tif (IS_ERR(res)) {",
            "\t\trc = PTR_ERR(res);",
            "\t\tgoto out_unlock;",
            "\t}",
            "",
            "\trc = add_memory_resource(nid, res, mhp_flags);",
            "\tif (rc < 0)",
            "\t\trelease_memory_resource(res);",
            "",
            "out_unlock:",
            "\tunlock_device_hotplug();",
            "\treturn rc;",
            "}",
            "struct range __weak arch_get_mappable_range(void)",
            "{",
            "\tstruct range mhp_range = {",
            "\t\t.start = 0UL,",
            "\t\t.end = -1ULL,",
            "\t};",
            "\treturn mhp_range;",
            "}",
            "struct range mhp_get_pluggable_range(bool need_mapping)",
            "{",
            "\tconst u64 max_phys = PHYSMEM_END;",
            "\tstruct range mhp_range;",
            "",
            "\tif (need_mapping) {",
            "\t\tmhp_range = arch_get_mappable_range();",
            "\t\tif (mhp_range.start > max_phys) {",
            "\t\t\tmhp_range.start = 0;",
            "\t\t\tmhp_range.end = 0;",
            "\t\t}",
            "\t\tmhp_range.end = min_t(u64, mhp_range.end, max_phys);",
            "\t} else {",
            "\t\tmhp_range.start = 0;",
            "\t\tmhp_range.end = max_phys;",
            "\t}",
            "\treturn mhp_range;",
            "}",
            "bool mhp_range_allowed(u64 start, u64 size, bool need_mapping)",
            "{",
            "\tstruct range mhp_range = mhp_get_pluggable_range(need_mapping);",
            "\tu64 end = start + size;",
            "",
            "\tif (start < end && start >= mhp_range.start && (end - 1) <= mhp_range.end)",
            "\t\treturn true;",
            "",
            "\tpr_warn(\"Hotplug memory [%#llx-%#llx] exceeds maximum addressable range [%#llx-%#llx]\\n\",",
            "\t\tstart, end, mhp_range.start, mhp_range.end);",
            "\treturn false;",
            "}",
            "static int scan_movable_pages(unsigned long start, unsigned long end,",
            "\t\t\t      unsigned long *movable_pfn)",
            "{",
            "\tunsigned long pfn;",
            "",
            "\tfor_each_valid_pfn(pfn, start, end) {",
            "\t\tstruct page *page;",
            "\t\tstruct folio *folio;",
            "",
            "\t\tpage = pfn_to_page(pfn);",
            "\t\tif (PageLRU(page))",
            "\t\t\tgoto found;",
            "\t\tif (__PageMovable(page))",
            "\t\t\tgoto found;",
            "",
            "\t\t/*",
            "\t\t * PageOffline() pages that are not marked __PageMovable() and",
            "\t\t * have a reference count > 0 (after MEM_GOING_OFFLINE) are",
            "\t\t * definitely unmovable. If their reference count would be 0,",
            "\t\t * they could at least be skipped when offlining memory.",
            "\t\t */",
            "\t\tif (PageOffline(page) && page_count(page))",
            "\t\t\treturn -EBUSY;",
            "",
            "\t\tif (!PageHuge(page))",
            "\t\t\tcontinue;",
            "\t\tfolio = page_folio(page);",
            "\t\t/*",
            "\t\t * This test is racy as we hold no reference or lock.  The",
            "\t\t * hugetlb page could have been free'ed and head is no longer",
            "\t\t * a hugetlb page before the following check.  In such unlikely",
            "\t\t * cases false positives and negatives are possible.  Calling",
            "\t\t * code must deal with these scenarios.",
            "\t\t */",
            "\t\tif (folio_test_hugetlb_migratable(folio))",
            "\t\t\tgoto found;",
            "\t\tpfn |= folio_nr_pages(folio) - 1;",
            "\t}",
            "\treturn -ENOENT;",
            "found:",
            "\t*movable_pfn = pfn;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "__add_memory, add_memory, add_memory_driver_managed, arch_get_mappable_range, mhp_get_pluggable_range, mhp_range_allowed, scan_movable_pages",
          "description": "__add_memory 和 add_memory 添加内存资源；add_memory_driver_managed 处理驱动管理内存资源；arch_get_mappable_range 获取可映射范围；mhp_get_pluggable_range 确定可插入内存范围；mhp_range_allowed 检查地址有效性；scan_movable_pages 扫描可移动页面",
          "similarity": 0.4733540415763855
        },
        {
          "chunk_id": 4,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 592,
          "end_line": 697,
          "content": [
            "void __remove_pages(unsigned long pfn, unsigned long nr_pages,",
            "\t\t    struct vmem_altmap *altmap)",
            "{",
            "\tconst unsigned long end_pfn = pfn + nr_pages;",
            "\tunsigned long cur_nr_pages;",
            "",
            "\tif (check_pfn_span(pfn, nr_pages)) {",
            "\t\tWARN(1, \"Misaligned %s start: %#lx end: %#lx\\n\", __func__, pfn, pfn + nr_pages - 1);",
            "\t\treturn;",
            "\t}",
            "",
            "\tfor (; pfn < end_pfn; pfn += cur_nr_pages) {",
            "\t\tcond_resched();",
            "\t\t/* Select all remaining pages up to the next section boundary */",
            "\t\tcur_nr_pages = min(end_pfn - pfn,",
            "\t\t\t\t   SECTION_ALIGN_UP(pfn + 1) - pfn);",
            "\t\tsparse_remove_section(pfn, cur_nr_pages, altmap);",
            "\t}",
            "}",
            "int set_online_page_callback(online_page_callback_t callback)",
            "{",
            "\tint rc = -EINVAL;",
            "",
            "\tget_online_mems();",
            "\tmutex_lock(&online_page_callback_lock);",
            "",
            "\tif (online_page_callback == generic_online_page) {",
            "\t\tonline_page_callback = callback;",
            "\t\trc = 0;",
            "\t}",
            "",
            "\tmutex_unlock(&online_page_callback_lock);",
            "\tput_online_mems();",
            "",
            "\treturn rc;",
            "}",
            "int restore_online_page_callback(online_page_callback_t callback)",
            "{",
            "\tint rc = -EINVAL;",
            "",
            "\tget_online_mems();",
            "\tmutex_lock(&online_page_callback_lock);",
            "",
            "\tif (online_page_callback == callback) {",
            "\t\tonline_page_callback = generic_online_page;",
            "\t\trc = 0;",
            "\t}",
            "",
            "\tmutex_unlock(&online_page_callback_lock);",
            "\tput_online_mems();",
            "",
            "\treturn rc;",
            "}",
            "void generic_online_page(struct page *page, unsigned int order)",
            "{",
            "\t__free_pages_core(page, order, MEMINIT_HOTPLUG);",
            "}",
            "static void online_pages_range(unsigned long start_pfn, unsigned long nr_pages)",
            "{",
            "\tconst unsigned long end_pfn = start_pfn + nr_pages;",
            "\tunsigned long pfn;",
            "",
            "\t/*",
            "\t * Online the pages in MAX_PAGE_ORDER aligned chunks. The callback might",
            "\t * decide to not expose all pages to the buddy (e.g., expose them",
            "\t * later). We account all pages as being online and belonging to this",
            "\t * zone (\"present\").",
            "\t * When using memmap_on_memory, the range might not be aligned to",
            "\t * MAX_ORDER_NR_PAGES - 1, but pageblock aligned. __ffs() will detect",
            "\t * this and the first chunk to online will be pageblock_nr_pages.",
            "\t */",
            "\tfor (pfn = start_pfn; pfn < end_pfn;) {",
            "\t\tint order;",
            "",
            "\t\t/*",
            "\t\t * Free to online pages in the largest chunks alignment allows.",
            "\t\t *",
            "\t\t * __ffs() behaviour is undefined for 0. start == 0 is",
            "\t\t * MAX_PAGE_ORDER-aligned, Set order to MAX_PAGE_ORDER for",
            "\t\t * the case.",
            "\t\t */",
            "\t\tif (pfn)",
            "\t\t\torder = min_t(int, MAX_PAGE_ORDER, __ffs(pfn));",
            "\t\telse",
            "\t\t\torder = MAX_PAGE_ORDER;",
            "",
            "\t\t(*online_page_callback)(pfn_to_page(pfn), order);",
            "\t\tpfn += (1UL << order);",
            "\t}",
            "",
            "\t/* mark all involved sections as online */",
            "\tonline_mem_sections(start_pfn, end_pfn);",
            "}",
            "static void node_states_check_changes_online(unsigned long nr_pages,",
            "\tstruct zone *zone, struct memory_notify *arg)",
            "{",
            "\tint nid = zone_to_nid(zone);",
            "",
            "\targ->status_change_nid = NUMA_NO_NODE;",
            "\targ->status_change_nid_normal = NUMA_NO_NODE;",
            "",
            "\tif (!node_state(nid, N_MEMORY))",
            "\t\targ->status_change_nid = nid;",
            "\tif (zone_idx(zone) <= ZONE_NORMAL && !node_state(nid, N_NORMAL_MEMORY))",
            "\t\targ->status_change_nid_normal = nid;",
            "}"
          ],
          "function_name": "__remove_pages, set_online_page_callback, restore_online_page_callback, generic_online_page, online_pages_range, node_states_check_changes_online",
          "description": "实现内存页面移除操作，提供在线回调函数注册/恢复接口，分块执行页面上线处理，同步更新内存section在线状态并通知节点状态变化。",
          "similarity": 0.4729114770889282
        },
        {
          "chunk_id": 11,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 1800,
          "end_line": 2035,
          "content": [
            "static int __init cmdline_parse_movable_node(char *p)",
            "{",
            "\tmovable_node_enabled = true;",
            "\treturn 0;",
            "}",
            "static void node_states_check_changes_offline(unsigned long nr_pages,",
            "\t\tstruct zone *zone, struct memory_notify *arg)",
            "{",
            "\tstruct pglist_data *pgdat = zone->zone_pgdat;",
            "\tunsigned long present_pages = 0;",
            "\tenum zone_type zt;",
            "",
            "\targ->status_change_nid = NUMA_NO_NODE;",
            "\targ->status_change_nid_normal = NUMA_NO_NODE;",
            "",
            "\t/*",
            "\t * Check whether node_states[N_NORMAL_MEMORY] will be changed.",
            "\t * If the memory to be offline is within the range",
            "\t * [0..ZONE_NORMAL], and it is the last present memory there,",
            "\t * the zones in that range will become empty after the offlining,",
            "\t * thus we can determine that we need to clear the node from",
            "\t * node_states[N_NORMAL_MEMORY].",
            "\t */",
            "\tfor (zt = 0; zt <= ZONE_NORMAL; zt++)",
            "\t\tpresent_pages += pgdat->node_zones[zt].present_pages;",
            "\tif (zone_idx(zone) <= ZONE_NORMAL && nr_pages >= present_pages)",
            "\t\targ->status_change_nid_normal = zone_to_nid(zone);",
            "",
            "\t/*",
            "\t * We have accounted the pages from [0..ZONE_NORMAL); ZONE_HIGHMEM",
            "\t * does not apply as we don't support 32bit.",
            "\t * Here we count the possible pages from ZONE_MOVABLE.",
            "\t * If after having accounted all the pages, we see that the nr_pages",
            "\t * to be offlined is over or equal to the accounted pages,",
            "\t * we know that the node will become empty, and so, we can clear",
            "\t * it for N_MEMORY as well.",
            "\t */",
            "\tpresent_pages += pgdat->node_zones[ZONE_MOVABLE].present_pages;",
            "",
            "\tif (nr_pages >= present_pages)",
            "\t\targ->status_change_nid = zone_to_nid(zone);",
            "}",
            "static void node_states_clear_node(int node, struct memory_notify *arg)",
            "{",
            "\tif (arg->status_change_nid_normal >= 0)",
            "\t\tnode_clear_state(node, N_NORMAL_MEMORY);",
            "",
            "\tif (arg->status_change_nid >= 0)",
            "\t\tnode_clear_state(node, N_MEMORY);",
            "}",
            "static int count_system_ram_pages_cb(unsigned long start_pfn,",
            "\t\t\t\t     unsigned long nr_pages, void *data)",
            "{",
            "\tunsigned long *nr_system_ram_pages = data;",
            "",
            "\t*nr_system_ram_pages += nr_pages;",
            "\treturn 0;",
            "}",
            "int __ref offline_pages(unsigned long start_pfn, unsigned long nr_pages,",
            "\t\t\tstruct zone *zone, struct memory_group *group)",
            "{",
            "\tconst unsigned long end_pfn = start_pfn + nr_pages;",
            "\tunsigned long pfn, managed_pages, system_ram_pages = 0;",
            "\tconst int node = zone_to_nid(zone);",
            "\tunsigned long flags;",
            "\tstruct memory_notify arg;",
            "\tchar *reason;",
            "\tint ret;",
            "",
            "\t/*",
            "\t * {on,off}lining is constrained to full memory sections (or more",
            "\t * precisely to memory blocks from the user space POV).",
            "\t * memmap_on_memory is an exception because it reserves initial part",
            "\t * of the physical memory space for vmemmaps. That space is pageblock",
            "\t * aligned.",
            "\t */",
            "\tif (WARN_ON_ONCE(!nr_pages || !pageblock_aligned(start_pfn) ||",
            "\t\t\t !IS_ALIGNED(start_pfn + nr_pages, PAGES_PER_SECTION)))",
            "\t\treturn -EINVAL;",
            "",
            "\t/*",
            "\t * Don't allow to offline memory blocks that contain holes.",
            "\t * Consequently, memory blocks with holes can never get onlined",
            "\t * via the hotplug path - online_pages() - as hotplugged memory has",
            "\t * no holes. This way, we don't have to worry about memory holes,",
            "\t * don't need pfn_valid() checks, and can avoid using",
            "\t * walk_system_ram_range() later.",
            "\t */",
            "\twalk_system_ram_range(start_pfn, nr_pages, &system_ram_pages,",
            "\t\t\t      count_system_ram_pages_cb);",
            "\tif (system_ram_pages != nr_pages) {",
            "\t\tret = -EINVAL;",
            "\t\treason = \"memory holes\";",
            "\t\tgoto failed_removal;",
            "\t}",
            "",
            "\t/*",
            "\t * We only support offlining of memory blocks managed by a single zone,",
            "\t * checked by calling code. This is just a sanity check that we might",
            "\t * want to remove in the future.",
            "\t */",
            "\tif (WARN_ON_ONCE(page_zone(pfn_to_page(start_pfn)) != zone ||",
            "\t\t\t page_zone(pfn_to_page(end_pfn - 1)) != zone)) {",
            "\t\tret = -EINVAL;",
            "\t\treason = \"multizone range\";",
            "\t\tgoto failed_removal;",
            "\t}",
            "",
            "\t/*",
            "\t * Disable pcplists so that page isolation cannot race with freeing",
            "\t * in a way that pages from isolated pageblock are left on pcplists.",
            "\t */",
            "\tzone_pcp_disable(zone);",
            "\tlru_cache_disable();",
            "",
            "\t/* set above range as isolated */",
            "\tret = start_isolate_page_range(start_pfn, end_pfn,",
            "\t\t\t\t       MIGRATE_MOVABLE,",
            "\t\t\t\t       MEMORY_OFFLINE | REPORT_FAILURE,",
            "\t\t\t\t       GFP_USER | __GFP_MOVABLE | __GFP_RETRY_MAYFAIL);",
            "\tif (ret) {",
            "\t\treason = \"failure to isolate range\";",
            "\t\tgoto failed_removal_pcplists_disabled;",
            "\t}",
            "",
            "\targ.start_pfn = start_pfn;",
            "\targ.nr_pages = nr_pages;",
            "\tnode_states_check_changes_offline(nr_pages, zone, &arg);",
            "",
            "\tret = memory_notify(MEM_GOING_OFFLINE, &arg);",
            "\tret = notifier_to_errno(ret);",
            "\tif (ret) {",
            "\t\treason = \"notifier failure\";",
            "\t\tgoto failed_removal_isolated;",
            "\t}",
            "",
            "\tdo {",
            "\t\tpfn = start_pfn;",
            "\t\tdo {",
            "\t\t\t/*",
            "\t\t\t * Historically we always checked for any signal and",
            "\t\t\t * can't limit it to fatal signals without eventually",
            "\t\t\t * breaking user space.",
            "\t\t\t */",
            "\t\t\tif (signal_pending(current)) {",
            "\t\t\t\tret = -EINTR;",
            "\t\t\t\treason = \"signal backoff\";",
            "\t\t\t\tgoto failed_removal_isolated;",
            "\t\t\t}",
            "",
            "\t\t\tcond_resched();",
            "",
            "\t\t\tret = scan_movable_pages(pfn, end_pfn, &pfn);",
            "\t\t\tif (!ret) {",
            "\t\t\t\t/*",
            "\t\t\t\t * TODO: fatal migration failures should bail",
            "\t\t\t\t * out",
            "\t\t\t\t */",
            "\t\t\t\tdo_migrate_range(pfn, end_pfn);",
            "\t\t\t}",
            "\t\t} while (!ret);",
            "",
            "\t\tif (ret != -ENOENT) {",
            "\t\t\treason = \"unmovable page\";",
            "\t\t\tgoto failed_removal_isolated;",
            "\t\t}",
            "",
            "\t\t/*",
            "\t\t * Dissolve free hugepages in the memory block before doing",
            "\t\t * offlining actually in order to make hugetlbfs's object",
            "\t\t * counting consistent.",
            "\t\t */",
            "\t\tret = dissolve_free_huge_pages(start_pfn, end_pfn);",
            "\t\tif (ret) {",
            "\t\t\treason = \"failure to dissolve huge pages\";",
            "\t\t\tgoto failed_removal_isolated;",
            "\t\t}",
            "",
            "\t\tret = test_pages_isolated(start_pfn, end_pfn, MEMORY_OFFLINE);",
            "",
            "\t} while (ret);",
            "",
            "\t/* Mark all sections offline and remove free pages from the buddy. */",
            "\tmanaged_pages = __offline_isolated_pages(start_pfn, end_pfn);",
            "\tpr_debug(\"Offlined Pages %ld\\n\", nr_pages);",
            "",
            "\t/*",
            "\t * The memory sections are marked offline, and the pageblock flags",
            "\t * effectively stale; nobody should be touching them. Fixup the number",
            "\t * of isolated pageblocks, memory onlining will properly revert this.",
            "\t */",
            "\tspin_lock_irqsave(&zone->lock, flags);",
            "\tzone->nr_isolate_pageblock -= nr_pages / pageblock_nr_pages;",
            "\tspin_unlock_irqrestore(&zone->lock, flags);",
            "",
            "\tlru_cache_enable();",
            "\tzone_pcp_enable(zone);",
            "",
            "\t/* removal success */",
            "\tadjust_managed_page_count(pfn_to_page(start_pfn), -managed_pages);",
            "\tadjust_present_page_count(pfn_to_page(start_pfn), group, -nr_pages);",
            "",
            "\t/* reinitialise watermarks and update pcp limits */",
            "\tinit_per_zone_wmark_min();",
            "",
            "\tif (!populated_zone(zone)) {",
            "\t\tzone_pcp_reset(zone);",
            "\t\tbuild_all_zonelists(NULL);",
            "\t}",
            "",
            "\tnode_states_clear_node(node, &arg);",
            "\tif (arg.status_change_nid >= 0) {",
            "\t\tkcompactd_stop(node);",
            "\t\tkswapd_stop(node);",
            "\t}",
            "",
            "\twriteback_set_ratelimit();",
            "",
            "\tmemory_notify(MEM_OFFLINE, &arg);",
            "\tremove_pfn_range_from_zone(zone, start_pfn, nr_pages);",
            "\treturn 0;",
            "",
            "failed_removal_isolated:",
            "\t/* pushback to free area */",
            "\tundo_isolate_page_range(start_pfn, end_pfn, MIGRATE_MOVABLE);",
            "\tmemory_notify(MEM_CANCEL_OFFLINE, &arg);",
            "failed_removal_pcplists_disabled:",
            "\tlru_cache_enable();",
            "\tzone_pcp_enable(zone);",
            "failed_removal:",
            "\tpr_debug(\"memory offlining [mem %#010llx-%#010llx] failed due to %s\\n\",",
            "\t\t (unsigned long long) start_pfn << PAGE_SHIFT,",
            "\t\t ((unsigned long long) end_pfn << PAGE_SHIFT) - 1,",
            "\t\t reason);",
            "\treturn ret;",
            "}"
          ],
          "function_name": "cmdline_parse_movable_node, node_states_check_changes_offline, node_states_clear_node, count_system_ram_pages_cb, offline_pages",
          "description": "offline_pages 函数负责安全地将指定内存区域标记为离线，处理页面隔离、迁移、状态变更通知及内存块移除，确保内存离线过程中无冲突并维护系统一致性",
          "similarity": 0.472507506608963
        },
        {
          "chunk_id": 1,
          "file_path": "mm/memory_hotplug.c",
          "start_line": 52,
          "end_line": 153,
          "content": [
            "static inline unsigned long memory_block_memmap_size(void)",
            "{",
            "\treturn PHYS_PFN(memory_block_size_bytes()) * sizeof(struct page);",
            "}",
            "static inline unsigned long memory_block_memmap_on_memory_pages(void)",
            "{",
            "\tunsigned long nr_pages = PFN_UP(memory_block_memmap_size());",
            "",
            "\t/*",
            "\t * In \"forced\" memmap_on_memory mode, we add extra pages to align the",
            "\t * vmemmap size to cover full pageblocks. That way, we can add memory",
            "\t * even if the vmemmap size is not properly aligned, however, we might waste",
            "\t * memory.",
            "\t */",
            "\tif (memmap_mode == MEMMAP_ON_MEMORY_FORCE)",
            "\t\treturn pageblock_align(nr_pages);",
            "\treturn nr_pages;",
            "}",
            "static int set_memmap_mode(const char *val, const struct kernel_param *kp)",
            "{",
            "\tint ret, mode;",
            "\tbool enabled;",
            "",
            "\tif (sysfs_streq(val, \"force\") ||  sysfs_streq(val, \"FORCE\")) {",
            "\t\tmode = MEMMAP_ON_MEMORY_FORCE;",
            "\t} else {",
            "\t\tret = kstrtobool(val, &enabled);",
            "\t\tif (ret < 0)",
            "\t\t\treturn ret;",
            "\t\tif (enabled)",
            "\t\t\tmode = MEMMAP_ON_MEMORY_ENABLE;",
            "\t\telse",
            "\t\t\tmode = MEMMAP_ON_MEMORY_DISABLE;",
            "\t}",
            "\t*((int *)kp->arg) = mode;",
            "\tif (mode == MEMMAP_ON_MEMORY_FORCE) {",
            "\t\tunsigned long memmap_pages = memory_block_memmap_on_memory_pages();",
            "",
            "\t\tpr_info_once(\"Memory hotplug will waste %ld pages in each memory block\\n\",",
            "\t\t\t     memmap_pages - PFN_UP(memory_block_memmap_size()));",
            "\t}",
            "\treturn 0;",
            "}",
            "static int get_memmap_mode(char *buffer, const struct kernel_param *kp)",
            "{",
            "\tint mode = *((int *)kp->arg);",
            "",
            "\tif (mode == MEMMAP_ON_MEMORY_FORCE)",
            "\t\treturn sprintf(buffer, \"force\\n\");",
            "\treturn sprintf(buffer, \"%c\\n\", mode ? 'Y' : 'N');",
            "}",
            "static inline bool mhp_memmap_on_memory(void)",
            "{",
            "\treturn memmap_mode != MEMMAP_ON_MEMORY_DISABLE;",
            "}",
            "static inline bool mhp_memmap_on_memory(void)",
            "{",
            "\treturn false;",
            "}",
            "static int set_online_policy(const char *val, const struct kernel_param *kp)",
            "{",
            "\tint ret = sysfs_match_string(online_policy_to_str, val);",
            "",
            "\tif (ret < 0)",
            "\t\treturn ret;",
            "\t*((int *)kp->arg) = ret;",
            "\treturn 0;",
            "}",
            "static int get_online_policy(char *buffer, const struct kernel_param *kp)",
            "{",
            "\treturn sprintf(buffer, \"%s\\n\", online_policy_to_str[*((int *)kp->arg)]);",
            "}",
            "void get_online_mems(void)",
            "{",
            "\tpercpu_down_read(&mem_hotplug_lock);",
            "}",
            "void put_online_mems(void)",
            "{",
            "\tpercpu_up_read(&mem_hotplug_lock);",
            "}",
            "int mhp_get_default_online_type(void)",
            "{",
            "\tif (mhp_default_online_type >= 0)",
            "\t\treturn mhp_default_online_type;",
            "",
            "\tif (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_OFFLINE))",
            "\t\tmhp_default_online_type = MMOP_OFFLINE;",
            "\telse if (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_ONLINE_AUTO))",
            "\t\tmhp_default_online_type = MMOP_ONLINE;",
            "\telse if (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_ONLINE_KERNEL))",
            "\t\tmhp_default_online_type = MMOP_ONLINE_KERNEL;",
            "\telse if (IS_ENABLED(CONFIG_MHP_DEFAULT_ONLINE_TYPE_ONLINE_MOVABLE))",
            "\t\tmhp_default_online_type = MMOP_ONLINE_MOVABLE;",
            "\telse",
            "\t\tmhp_default_online_type = MMOP_OFFLINE;",
            "",
            "\treturn mhp_default_online_type;",
            "}",
            "void mhp_set_default_online_type(int online_type)",
            "{",
            "\tmhp_default_online_type = online_type;",
            "}"
          ],
          "function_name": "memory_block_memmap_size, memory_block_memmap_on_memory_pages, set_memmap_mode, get_memmap_mode, mhp_memmap_on_memory, mhp_memmap_on_memory, set_online_policy, get_online_policy, get_online_mems, put_online_mems, mhp_get_default_online_type, mhp_set_default_online_type",
          "description": "提供内存映射模式配置接口(set/get)，实现memmap_on_memory策略判断逻辑，包含在线策略设置与获取函数及默认类型处理逻辑。",
          "similarity": 0.4690176248550415
        }
      ]
    },
    {
      "source_file": "mm/sparse-vmemmap.c",
      "md_summary": "> 自动生成时间: 2025-12-07 17:24:15\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `sparse-vmemmap.c`\n\n---\n\n# sparse-vmemmap.c 技术文档\n\n## 1. 文件概述\n\n`sparse-vmemmap.c` 是 Linux 内核中用于实现 **虚拟内存映射（Virtual Memory Map, vmemmap）** 的核心文件之一。该机制为稀疏内存模型（sparse memory model）提供支持，使得 `pfn_to_page()`、`page_to_pfn()`、`virt_to_page()` 和 `page_address()` 等页管理原语可以通过简单的地址偏移计算实现，而无需访问内存中的间接结构。\n\n在支持 1:1 物理地址映射的架构上，vmemmap 利用已有的页表和 TLB 映射，仅需额外分配少量页面来构建一个连续的虚拟地址空间，用于存放所有物理页对应的 `struct page` 结构体。此文件主要负责在系统初始化阶段动态填充 vmemmap 所需的页表项，并支持使用替代内存分配器（如 ZONE_DEVICE 提供的 altmap）进行底层内存分配。\n\n## 2. 核心功能\n\n### 主要函数\n\n| 函数名 | 功能说明 |\n|--------|--------|\n| `vmemmap_alloc_block()` | 分配用于 vmemmap 或其页表的内存块，优先使用 slab 分配器，早期启动阶段回退到 memblock |\n| `vmemmap_alloc_block_buf()` | 封装分配接口，支持通过 `vmem_altmap` 指定替代内存源 |\n| `altmap_alloc_block_buf()` | 使用 `vmem_altmap` 提供的预留内存区域分配 vmemmap 缓冲区 |\n| `vmemmap_populate_address()` | 为指定虚拟地址填充完整的四级（或五级）页表路径（PGD → P4D → PUD → PMD → PTE） |\n| `vmemmap_populate_range()` | 批量填充一段虚拟地址范围的页表 |\n| `vmemmap_populate_basepages()` | 公开接口，用于以基本页（4KB）粒度填充 vmemmap 区域 |\n| `vmemmap_pte_populate()` / `vmemmap_pmd_populate()` / ... | 各级页表项的按需填充函数 |\n| `vmemmap_verify()` | 验证分配的 `struct page` 是否位于预期 NUMA 节点，避免跨节点性能问题 |\n\n### 关键数据结构\n\n- **`struct vmem_altmap`**  \n  由外部（如 device-dax 或 pmem 驱动）提供，描述一块预留的物理内存区域，可用于替代常规内存分配 vmemmap 所需的 `struct page` 存储空间。包含字段：\n  - `base_pfn`：起始物理页帧号\n  - `reserve`：保留页数（通常用于元数据）\n  - `alloc`：已分配页数\n  - `align`：对齐填充页数\n  - `free`：总可用页数\n\n## 3. 关键实现\n\n### 内存分配策略\n- **运行时分配**：当 slab 分配器可用时（`slab_is_available()` 返回 true），使用 `alloc_pages_node()` 分配高阶页面。\n- **早期启动分配**：在 slab 不可用时，调用 `memblock_alloc_try_nid_raw()` 从 bootmem 分配器获取内存。\n- **替代内存支持**：通过 `vmem_altmap` 参数，允许将 `struct page` 存储在设备内存（如持久内存）中，减少对系统 DRAM 的占用。\n\n### 页表填充机制\n- 采用 **按需填充（on-demand population）** 策略，仅在访问 vmemmap 虚拟地址时构建对应页表。\n- 支持完整的 x86_64 / ARM64 等架构的多级页表（PGD → P4D → PUD → PMD → PTE）。\n- 每级页表项若为空（`*_none()`），则分配一个 4KB 页面作为下一级页表，并通过 `*_populate()` 填充。\n- 叶子 PTE 指向实际存储 `struct page` 的物理页面，权限设为 `PAGE_KERNEL`。\n\n### 对齐与验证\n- `altmap_alloc_block_buf()` 中实现 **动态对齐**：根据请求大小计算所需对齐边界（2 的幂），确保分配地址满足页表项对齐要求。\n- `vmemmap_verify()` 在调试/警告模式下检查分配的 `struct page` 所在 NUMA 节点是否与目标节点“本地”，避免远程访问开销。\n\n### 架构钩子函数\n- 提供弱符号（`__weak`）钩子如 `kernel_pte_init()`、`pmd_init()` 等，允许特定架构在分配页表页面后执行初始化操作（如设置特殊属性位）。\n\n## 4. 依赖关系\n\n- **内存管理子系统**：\n  - `<linux/mm.h>`、`<linux/mmzone.h>`：页帧、内存域、NUMA 节点管理\n  - `<linux/memblock.h>`：早期内存分配\n  - `<linux/vmalloc.h>`：虚拟内存管理（间接）\n- **页表操作**：\n  - `<asm/pgalloc.h>`：架构相关的页表分配/释放\n  - 依赖 `pgd_offset_k()`、`pud_populate()` 等架构宏/函数\n- **稀疏内存模型**：\n  - 与 `sparse.c` 协同工作，`sparse_buffer_alloc()` 用于复用预分配的缓冲区\n- **设备内存支持**：\n  - `<linux/memremap.h>`：`vmem_altmap` 定义，用于 ZONE_DEVICE 场景\n\n## 5. 使用场景\n\n1. **稀疏内存模型初始化**  \n   在 `sparse_init()` 过程中，为每个内存 section 调用 `vmemmap_populate_basepages()` 填充对应的 `struct page` 数组。\n\n2. **热插拔内存（Memory Hotplug）**  \n   新增内存区域时，动态填充其 vmemmap 映射，使新页可被内核页管理器识别。\n\n3. **持久内存（Persistent Memory）/ DAX 设备**  \n   通过 `vmem_altmap` 将 `struct page` 存储在设备自身内存中，避免消耗系统 RAM，典型用于 `fsdax` 或 `device-dax`。\n\n4. **大页优化（未完成功能）**  \n   文件末尾存在 `vmemmap_populate_hugepages()` 声明，表明未来可能支持使用透明大页（如 2MB PMD）映射 vmemmap，减少 TLB 压力（当前实现可能不完整或依赖架构支持）。\n\n5. **NUMA 感知分配**  \n   所有分配均指定目标 NUMA 节点（`node` 参数），确保 `struct page` 尽可能靠近其所描述的物理内存，优化访问延迟。",
      "similarity": 0.5683633685112,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 1,
          "end_line": 90,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * Virtual Memory Map support",
            " *",
            " * (C) 2007 sgi. Christoph Lameter.",
            " *",
            " * Virtual memory maps allow VM primitives pfn_to_page, page_to_pfn,",
            " * virt_to_page, page_address() to be implemented as a base offset",
            " * calculation without memory access.",
            " *",
            " * However, virtual mappings need a page table and TLBs. Many Linux",
            " * architectures already map their physical space using 1-1 mappings",
            " * via TLBs. For those arches the virtual memory map is essentially",
            " * for free if we use the same page size as the 1-1 mappings. In that",
            " * case the overhead consists of a few additional pages that are",
            " * allocated to create a view of memory for vmemmap.",
            " *",
            " * The architecture is expected to provide a vmemmap_populate() function",
            " * to instantiate the mapping.",
            " */",
            "#include <linux/mm.h>",
            "#include <linux/mmzone.h>",
            "#include <linux/memblock.h>",
            "#include <linux/memremap.h>",
            "#include <linux/highmem.h>",
            "#include <linux/slab.h>",
            "#include <linux/spinlock.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/sched.h>",
            "",
            "#include <asm/dma.h>",
            "#include <asm/pgalloc.h>",
            "",
            "/*",
            " * Allocate a block of memory to be used to back the virtual memory map",
            " * or to back the page tables that are used to create the mapping.",
            " * Uses the main allocators if they are available, else bootmem.",
            " */",
            "",
            "static void * __ref __earlyonly_bootmem_alloc(int node,",
            "\t\t\t\tunsigned long size,",
            "\t\t\t\tunsigned long align,",
            "\t\t\t\tunsigned long goal)",
            "{",
            "\treturn memblock_alloc_try_nid_raw(size, align, goal,",
            "\t\t\t\t\t       MEMBLOCK_ALLOC_ACCESSIBLE, node);",
            "}",
            "",
            "void * __meminit vmemmap_alloc_block(unsigned long size, int node)",
            "{",
            "\t/* If the main allocator is up use that, fallback to bootmem. */",
            "\tif (slab_is_available()) {",
            "\t\tgfp_t gfp_mask = GFP_KERNEL|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;",
            "\t\tint order = get_order(size);",
            "\t\tstatic bool warned;",
            "\t\tstruct page *page;",
            "",
            "\t\tpage = alloc_pages_node(node, gfp_mask, order);",
            "\t\tif (page)",
            "\t\t\treturn page_address(page);",
            "",
            "\t\tif (!warned) {",
            "\t\t\twarn_alloc(gfp_mask & ~__GFP_NOWARN, NULL,",
            "\t\t\t\t   \"vmemmap alloc failure: order:%u\", order);",
            "\t\t\twarned = true;",
            "\t\t}",
            "\t\treturn NULL;",
            "\t} else",
            "\t\treturn __earlyonly_bootmem_alloc(node, size, size,",
            "\t\t\t\t__pa(MAX_DMA_ADDRESS));",
            "}",
            "",
            "static void * __meminit altmap_alloc_block_buf(unsigned long size,",
            "\t\t\t\t\t       struct vmem_altmap *altmap);",
            "",
            "/* need to make sure size is all the same during early stage */",
            "void * __meminit vmemmap_alloc_block_buf(unsigned long size, int node,",
            "\t\t\t\t\t struct vmem_altmap *altmap)",
            "{",
            "\tvoid *ptr;",
            "",
            "\tif (altmap)",
            "\t\treturn altmap_alloc_block_buf(size, altmap);",
            "",
            "\tptr = sparse_buffer_alloc(size);",
            "\tif (!ptr)",
            "\t\tptr = vmemmap_alloc_block(size, node);",
            "\treturn ptr;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了用于分配虚拟内存映射所需内存块的函数，包括对slab分配器和bootmem分配器的选择逻辑，用于在系统初始化期间为vmentry结构体分配物理存储",
          "similarity": 0.6478335857391357
        },
        {
          "chunk_id": 1,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 91,
          "end_line": 203,
          "content": [
            "static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)",
            "{",
            "\treturn altmap->base_pfn + altmap->reserve + altmap->alloc",
            "\t\t+ altmap->align;",
            "}",
            "static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long allocated = altmap->alloc + altmap->align;",
            "",
            "\tif (altmap->free > allocated)",
            "\t\treturn altmap->free - allocated;",
            "\treturn 0;",
            "}",
            "void __meminit vmemmap_verify(pte_t *pte, int node,",
            "\t\t\t\tunsigned long start, unsigned long end)",
            "{",
            "\tunsigned long pfn = pte_pfn(ptep_get(pte));",
            "\tint actual_node = early_pfn_to_nid(pfn);",
            "",
            "\tif (node_distance(actual_node, node) > LOCAL_DISTANCE)",
            "\t\tpr_warn_once(\"[%lx-%lx] potential offnode page_structs\\n\",",
            "\t\t\tstart, end - 1);",
            "}",
            "void __weak __meminit kernel_pte_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pmd_init(void *addr)",
            "{",
            "}",
            "void __weak __meminit pud_init(void *addr)",
            "{",
            "}",
            "static int __meminit vmemmap_populate_range(unsigned long start,",
            "\t\t\t\t\t    unsigned long end, int node,",
            "\t\t\t\t\t    struct vmem_altmap *altmap,",
            "\t\t\t\t\t    struct page *reuse)",
            "{",
            "\tunsigned long addr = start;",
            "\tpte_t *pte;",
            "",
            "\tfor (; addr < end; addr += PAGE_SIZE) {",
            "\t\tpte = vmemmap_populate_address(addr, node, altmap, reuse);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_basepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\treturn vmemmap_populate_range(start, end, node, altmap, NULL);",
            "}",
            "void __weak __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,",
            "\t\t\t\t      unsigned long addr, unsigned long next)",
            "{",
            "}",
            "int __weak __meminit vmemmap_check_pmd(pmd_t *pmd, int node,",
            "\t\t\t\t       unsigned long addr, unsigned long next)",
            "{",
            "\treturn 0;",
            "}",
            "int __meminit vmemmap_populate_hugepages(unsigned long start, unsigned long end,",
            "\t\t\t\t\t int node, struct vmem_altmap *altmap)",
            "{",
            "\tunsigned long addr;",
            "\tunsigned long next;",
            "\tpgd_t *pgd;",
            "\tp4d_t *p4d;",
            "\tpud_t *pud;",
            "\tpmd_t *pmd;",
            "",
            "\tfor (addr = start; addr < end; addr = next) {",
            "\t\tnext = pmd_addr_end(addr, end);",
            "",
            "\t\tpgd = vmemmap_pgd_populate(addr, node);",
            "\t\tif (!pgd)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tp4d = vmemmap_p4d_populate(pgd, addr, node);",
            "\t\tif (!p4d)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpud = vmemmap_pud_populate(p4d, addr, node);",
            "\t\tif (!pud)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tpmd = pmd_offset(pud, addr);",
            "\t\tif (pmd_none(READ_ONCE(*pmd))) {",
            "\t\t\tvoid *p;",
            "",
            "\t\t\tp = vmemmap_alloc_block_buf(PMD_SIZE, node, altmap);",
            "\t\t\tif (p) {",
            "\t\t\t\tvmemmap_set_pmd(pmd, p, node, addr, next);",
            "\t\t\t\tcontinue;",
            "\t\t\t} else if (altmap) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No fallback: In any case we care about, the",
            "\t\t\t\t * altmap should be reasonably sized and aligned",
            "\t\t\t\t * such that vmemmap_alloc_block_buf() will always",
            "\t\t\t\t * succeed. For consistency with the PTE case,",
            "\t\t\t\t * return an error here as failure could indicate",
            "\t\t\t\t * a configuration issue with the size of the altmap.",
            "\t\t\t\t */",
            "\t\t\t\treturn -ENOMEM;",
            "\t\t\t}",
            "\t\t} else if (vmemmap_check_pmd(pmd, node, addr, next))",
            "\t\t\tcontinue;",
            "\t\tif (vmemmap_populate_basepages(addr, next, node, altmap))",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "\treturn 0;",
            "}"
          ],
          "function_name": "vmem_altmap_next_pfn, vmem_altmap_nr_free, vmemmap_verify, kernel_pte_init, pmd_init, pud_init, vmemmap_populate_range, vmemmap_populate_basepages, vmemmap_set_pmd, vmemmap_check_pmd, vmemmap_populate_hugepages",
          "description": "实现了虚拟内存映射验证、页表初始化及大页填充逻辑，包含检查页表项节点一致性、弱函数声明以及递归填充连续地址范围的辅助函数",
          "similarity": 0.5624125003814697
        },
        {
          "chunk_id": 2,
          "file_path": "mm/sparse-vmemmap.c",
          "start_line": 377,
          "end_line": 435,
          "content": [
            "static bool __meminit reuse_compound_section(unsigned long start_pfn,",
            "\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long nr_pages = pgmap_vmemmap_nr(pgmap);",
            "\tunsigned long offset = start_pfn -",
            "\t\tPHYS_PFN(pgmap->ranges[pgmap->nr_range].start);",
            "",
            "\treturn !IS_ALIGNED(offset, nr_pages) && nr_pages > PAGES_PER_SUBSECTION;",
            "}",
            "static int __meminit vmemmap_populate_compound_pages(unsigned long start_pfn,",
            "\t\t\t\t\t\t     unsigned long start,",
            "\t\t\t\t\t\t     unsigned long end, int node,",
            "\t\t\t\t\t\t     struct dev_pagemap *pgmap)",
            "{",
            "\tunsigned long size, addr;",
            "\tpte_t *pte;",
            "\tint rc;",
            "",
            "\tif (reuse_compound_section(start_pfn, pgmap)) {",
            "\t\tpte = compound_section_tail_page(start);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the page that was populated in the prior iteration",
            "\t\t * with just tail struct pages.",
            "\t\t */",
            "\t\treturn vmemmap_populate_range(start, end, node, NULL,",
            "\t\t\t\t\t      pte_page(ptep_get(pte)));",
            "\t}",
            "",
            "\tsize = min(end - start, pgmap_vmemmap_nr(pgmap) * sizeof(struct page));",
            "\tfor (addr = start; addr < end; addr += size) {",
            "\t\tunsigned long next, last = addr + size;",
            "",
            "\t\t/* Populate the head page vmemmap page */",
            "\t\tpte = vmemmap_populate_address(addr, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/* Populate the tail pages vmemmap page */",
            "\t\tnext = addr + PAGE_SIZE;",
            "\t\tpte = vmemmap_populate_address(next, node, NULL, NULL);",
            "\t\tif (!pte)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\t/*",
            "\t\t * Reuse the previous page for the rest of tail pages",
            "\t\t * See layout diagram in Documentation/mm/vmemmap_dedup.rst",
            "\t\t */",
            "\t\tnext += PAGE_SIZE;",
            "\t\trc = vmemmap_populate_range(next, last, node, NULL,",
            "\t\t\t\t\t    pte_page(ptep_get(pte)));",
            "\t\tif (rc)",
            "\t\t\treturn -ENOMEM;",
            "\t}",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "reuse_compound_section, vmemmap_populate_compound_pages",
          "description": "提供复合页面内存复用机制，通过判断偏移对齐情况决定是否复用上一次迭代产生的尾部页面，从而优化vmentry结构体的内存分配效率",
          "similarity": 0.47714048624038696
        }
      ]
    },
    {
      "source_file": "mm/highmem.c",
      "md_summary": "> 自动生成时间: 2025-12-07 16:03:56\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `highmem.c`\n\n---\n\n# highmem.c 技术文档\n\n## 1. 文件概述\n\n`highmem.c` 是 Linux 内核中用于管理高内存（High Memory）的核心实现文件。在 32 位系统中，由于虚拟地址空间有限（通常为 4GB），内核无法将全部物理内存直接映射到内核虚拟地址空间。高内存指那些不能被永久映射到内核地址空间的物理内存页。该文件提供了对高内存页进行临时映射（kmap）和本地映射（kmap_local）的通用机制，使得内核代码能够安全地访问高内存中的数据。\n\n## 2. 核心功能\n\n### 主要函数\n- `kmap_high(struct page *page)`：将高内存页映射到内核虚拟地址空间，返回其虚拟地址。\n- `__kmap_flush_unused(void)`：释放所有未使用的持久性 kmap 映射。\n- `__kmap_to_page(void *vaddr)`：根据虚拟地址反向查找对应的 `struct page`。\n- `map_new_virtual(struct page *page)`：为指定页面分配一个新的持久性 kmap 虚拟地址。\n- `flush_all_zero_pkmaps(void)`：清除所有引用计数为 1（即空闲但尚未解除映射）的 PKMAP 条目。\n\n### 主要数据结构与变量\n- `pkmap_count[LAST_PKMAP]`：记录每个持久性 kmap 槽的引用状态。\n- `pkmap_page_table`：指向持久性 kmap 区域的页表项数组。\n- `_totalhigh_pages`：系统中高内存页的总数（全局可导出变量）。\n- `kmap_lock`：保护持久性 kmap 操作的自旋锁。\n- `__nr_free_highpages(void)`：计算当前系统中空闲的高内存页数量。\n\n## 3. 关键实现\n\n### 持久性 kmap 机制（PKMAP）\n- 使用固定大小的虚拟地址窗口（`PKMAP_ADDR(0)` 到 `PKMAP_ADDR(LAST_PKMAP)`）作为高内存页的映射区域。\n- `pkmap_count[]` 数组不仅记录引用计数，还编码映射状态：\n  - **0**：槽位空闲且 TLB 已刷新，可安全复用。\n  - **1**：槽位空闲但自上次 TLB 刷新后曾被使用，需先刷新 TLB 才能复用。\n  - **n (>1)**：当前有 (n-1) 个活跃用户。\n- 当无可用槽位时，调用者会进入不可中断睡眠，等待其他线程调用 `kunmap` 释放槽位。\n\n### 缓存着色支持（Cache Coloring）\n- 在具有别名数据缓存（Aliasing Data Cache）的架构上，通过 `get_pkmap_color()` 等钩子函数控制映射的“颜色”（即缓存索引），避免缓存冲突。\n- 默认实现（无缓存别名架构）返回颜色 0，所有映射共享同一等待队列。\n\n### 本地 kmap 支持（kmap_local）\n- 通过 `CONFIG_KMAP_LOCAL` 配置选项启用。\n- 每个 CPU 拥有独立的固定映射区域（FIX_KMAP），避免全局锁竞争。\n- 使用 `arch_kmap_local_map_idx()` 计算 per-CPU 的映射索引。\n\n### 锁与中断处理\n- 根据 `ARCH_NEEDS_KMAP_HIGH_GET` 宏决定是否在获取 kmap 锁时禁用中断，以优化性能。\n\n### 地址反查\n- `__kmap_to_page()` 支持从持久性 kmap 和本地 kmap 的虚拟地址反向解析出原始 `struct page`。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - `<linux/mm.h>`、`<linux/highmem.h>`：内存管理和高内存接口定义。\n  - `<asm/tlbflush.h>`：TLB 刷新操作。\n  - `<linux/vmalloc.h>`：虚拟内存分配相关。\n  - 架构特定头文件（如 `asm/highmem.h`）：提供架构相关的 kmap 实现。\n  \n- **内核子系统依赖**：\n  - 内存管理子系统（MM）：页分配、zone 管理。\n  - 虚拟内存管理：页表操作、TLB 控制。\n  - SMP 支持：per-CPU 数据、自旋锁。\n\n- **导出符号**：\n  - `_totalhigh_pages`：供其他模块查询高内存总量。\n  - `__kmap_to_page`：供调试或特殊用途反查页结构。\n\n## 5. 使用场景\n\n- **块设备 I/O**：当 bio 请求涉及高内存页时，驱动程序使用 `kmap()` 获取线性地址进行 DMA 或拷贝。\n- **文件系统缓存**：页缓存可能位于高内存，读写时需临时映射。\n- **网络子系统**：SKB 数据页若在高内存，协议栈需映射后处理。\n- **内核调试**：KGDB 等调试器可能需要访问高内存内容。\n- **内存压缩/迁移**：在内存管理高级功能中临时访问高内存页。\n\n> 注意：`kmap_high()` 可能阻塞，因此**不能在中断上下文或持有自旋锁时调用**。对于不能睡眠的场景，应使用 `kmap_atomic()`（已废弃）或 `kmap_local_page()`。",
      "similarity": 0.5628746151924133,
      "chunks": [
        {
          "chunk_id": 4,
          "file_path": "mm/highmem.c",
          "start_line": 700,
          "end_line": 743,
          "content": [
            "void kmap_local_fork(struct task_struct *tsk)",
            "{",
            "\tif (WARN_ON_ONCE(tsk->kmap_ctrl.idx))",
            "\t\tmemset(&tsk->kmap_ctrl, 0, sizeof(tsk->kmap_ctrl));",
            "}",
            "void set_page_address(struct page *page, void *virtual)",
            "{",
            "\tunsigned long flags;",
            "\tstruct page_address_slot *pas;",
            "\tstruct page_address_map *pam;",
            "",
            "\tBUG_ON(!PageHighMem(page));",
            "",
            "\tpas = page_slot(page);",
            "\tif (virtual) {\t\t/* Add */",
            "\t\tpam = &page_address_maps[PKMAP_NR((unsigned long)virtual)];",
            "\t\tpam->page = page;",
            "\t\tpam->virtual = virtual;",
            "",
            "\t\tspin_lock_irqsave(&pas->lock, flags);",
            "\t\tlist_add_tail(&pam->list, &pas->lh);",
            "\t\tspin_unlock_irqrestore(&pas->lock, flags);",
            "\t} else {\t\t/* Remove */",
            "\t\tspin_lock_irqsave(&pas->lock, flags);",
            "\t\tlist_for_each_entry(pam, &pas->lh, list) {",
            "\t\t\tif (pam->page == page) {",
            "\t\t\t\tlist_del(&pam->list);",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t}",
            "\t\tspin_unlock_irqrestore(&pas->lock, flags);",
            "\t}",
            "",
            "\treturn;",
            "}",
            "void __init page_address_init(void)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(page_address_htable); i++) {",
            "\t\tINIT_LIST_HEAD(&page_address_htable[i].lh);",
            "\t\tspin_lock_init(&page_address_htable[i].lock);",
            "\t}",
            "}"
          ],
          "function_name": "kmap_local_fork, set_page_address, page_address_init",
          "description": "初始化高内存页地址关联数据结构，提供任务fork时的映射状态重置及页面虚拟地址的增删操作接口。",
          "similarity": 0.5344085097312927
        },
        {
          "chunk_id": 3,
          "file_path": "mm/highmem.c",
          "start_line": 472,
          "end_line": 591,
          "content": [
            "static inline int kmap_local_idx(void)",
            "{",
            "\treturn current->kmap_ctrl.idx - 1;",
            "}",
            "static inline void kmap_local_idx_pop(void)",
            "{",
            "\tcurrent->kmap_ctrl.idx -= KM_INCR;",
            "\tBUG_ON(current->kmap_ctrl.idx < 0);",
            "}",
            "static inline bool kmap_high_unmap_local(unsigned long vaddr)",
            "{",
            "#ifdef ARCH_NEEDS_KMAP_HIGH_GET",
            "\tif (vaddr >= PKMAP_ADDR(0) && vaddr < PKMAP_ADDR(LAST_PKMAP)) {",
            "\t\tkunmap_high(pte_page(ptep_get(&pkmap_page_table[PKMAP_NR(vaddr)])));",
            "\t\treturn true;",
            "\t}",
            "#endif",
            "\treturn false;",
            "}",
            "void kunmap_local_indexed(const void *vaddr)",
            "{",
            "\tunsigned long addr = (unsigned long) vaddr & PAGE_MASK;",
            "\tpte_t *kmap_pte;",
            "\tint idx;",
            "",
            "\tif (addr < __fix_to_virt(FIX_KMAP_END) ||",
            "\t    addr > __fix_to_virt(FIX_KMAP_BEGIN)) {",
            "\t\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP)) {",
            "\t\t\t/* This _should_ never happen! See above. */",
            "\t\t\tWARN_ON_ONCE(1);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\t/*",
            "\t\t * Handle mappings which were obtained by kmap_high_get()",
            "\t\t * first as the virtual address of such mappings is below",
            "\t\t * PAGE_OFFSET. Warn for all other addresses which are in",
            "\t\t * the user space part of the virtual address space.",
            "\t\t */",
            "\t\tif (!kmap_high_unmap_local(addr))",
            "\t\t\tWARN_ON_ONCE(addr < PAGE_OFFSET);",
            "\t\treturn;",
            "\t}",
            "",
            "\tpreempt_disable();",
            "\tidx = arch_kmap_local_unmap_idx(kmap_local_idx(), addr);",
            "\tWARN_ON_ONCE(addr != __fix_to_virt(FIX_KMAP_BEGIN + idx));",
            "",
            "\tkmap_pte = kmap_get_pte(addr, idx);",
            "\tarch_kmap_local_pre_unmap(addr);",
            "\tpte_clear(&init_mm, addr, kmap_pte);",
            "\tarch_kmap_local_post_unmap(addr);",
            "\tcurrent->kmap_ctrl.pteval[kmap_local_idx()] = __pte(0);",
            "\tkmap_local_idx_pop();",
            "\tpreempt_enable();",
            "\tmigrate_enable();",
            "}",
            "void __kmap_local_sched_out(void)",
            "{",
            "\tstruct task_struct *tsk = current;",
            "\tpte_t *kmap_pte;",
            "\tint i;",
            "",
            "\t/* Clear kmaps */",
            "\tfor (i = 0; i < tsk->kmap_ctrl.idx; i++) {",
            "\t\tpte_t pteval = tsk->kmap_ctrl.pteval[i];",
            "\t\tunsigned long addr;",
            "\t\tint idx;",
            "",
            "\t\t/* With debug all even slots are unmapped and act as guard */",
            "\t\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL) && !(i & 0x01)) {",
            "\t\t\tWARN_ON_ONCE(pte_val(pteval) != 0);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (WARN_ON_ONCE(pte_none(pteval)))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * This is a horrible hack for XTENSA to calculate the",
            "\t\t * coloured PTE index. Uses the PFN encoded into the pteval",
            "\t\t * and the map index calculation because the actual mapped",
            "\t\t * virtual address is not stored in task::kmap_ctrl.",
            "\t\t * For any sane architecture this is optimized out.",
            "\t\t */",
            "\t\tidx = arch_kmap_local_map_idx(i, pte_pfn(pteval));",
            "",
            "\t\taddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);",
            "\t\tkmap_pte = kmap_get_pte(addr, idx);",
            "\t\tarch_kmap_local_pre_unmap(addr);",
            "\t\tpte_clear(&init_mm, addr, kmap_pte);",
            "\t\tarch_kmap_local_post_unmap(addr);",
            "\t}",
            "}",
            "void __kmap_local_sched_in(void)",
            "{",
            "\tstruct task_struct *tsk = current;",
            "\tpte_t *kmap_pte;",
            "\tint i;",
            "",
            "\t/* Restore kmaps */",
            "\tfor (i = 0; i < tsk->kmap_ctrl.idx; i++) {",
            "\t\tpte_t pteval = tsk->kmap_ctrl.pteval[i];",
            "\t\tunsigned long addr;",
            "\t\tint idx;",
            "",
            "\t\t/* With debug all even slots are unmapped and act as guard */",
            "\t\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL) && !(i & 0x01)) {",
            "\t\t\tWARN_ON_ONCE(pte_val(pteval) != 0);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (WARN_ON_ONCE(pte_none(pteval)))",
            "\t\t\tcontinue;",
            "",
            "\t\t/* See comment in __kmap_local_sched_out() */",
            "\t\tidx = arch_kmap_local_map_idx(i, pte_pfn(pteval));",
            "\t\taddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);",
            "\t\tkmap_pte = kmap_get_pte(addr, idx);",
            "\t\tset_pte_at(&init_mm, addr, kmap_pte, pteval);",
            "\t\tarch_kmap_local_post_map(addr, pteval);",
            "\t}",
            "}"
          ],
          "function_name": "kmap_local_idx, kmap_local_idx_pop, kmap_high_unmap_local, kunmap_local_indexed, __kmap_local_sched_out, __kmap_local_sched_in",
          "description": "实现任务调度上下文中的高内存映射管理，包含保存/恢复页表项、处理虚实地址转换及架构特定的预后处理操作。",
          "similarity": 0.5059453845024109
        },
        {
          "chunk_id": 0,
          "file_path": "mm/highmem.c",
          "start_line": 1,
          "end_line": 33,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0",
            "/*",
            " * High memory handling common code and variables.",
            " *",
            " * (C) 1999 Andrea Arcangeli, SuSE GmbH, andrea@suse.de",
            " *          Gerhard Wichert, Siemens AG, Gerhard.Wichert@pdb.siemens.de",
            " *",
            " *",
            " * Redesigned the x86 32-bit VM architecture to deal with",
            " * 64-bit physical space. With current x86 CPUs this",
            " * means up to 64 Gigabytes physical RAM.",
            " *",
            " * Rewrote high memory support to move the page cache into",
            " * high memory. Implemented permanent (schedulable) kmaps",
            " * based on Linus' idea.",
            " *",
            " * Copyright (C) 1999 Ingo Molnar <mingo@redhat.com>",
            " */",
            "",
            "#include <linux/mm.h>",
            "#include <linux/export.h>",
            "#include <linux/swap.h>",
            "#include <linux/bio.h>",
            "#include <linux/pagemap.h>",
            "#include <linux/mempool.h>",
            "#include <linux/init.h>",
            "#include <linux/hash.h>",
            "#include <linux/highmem.h>",
            "#include <linux/kgdb.h>",
            "#include <asm/tlbflush.h>",
            "#include <linux/vmalloc.h>",
            "",
            "#ifdef CONFIG_KMAP_LOCAL"
          ],
          "function_name": null,
          "description": "声明高内存支持的公共代码和变量，包含页表管理、永久kmaps等功能的实现及相关头文件导入。",
          "similarity": 0.502091646194458
        },
        {
          "chunk_id": 2,
          "file_path": "mm/highmem.c",
          "start_line": 348,
          "end_line": 451,
          "content": [
            "void kunmap_high(struct page *page)",
            "{",
            "\tunsigned long vaddr;",
            "\tunsigned long nr;",
            "\tunsigned long flags;",
            "\tint need_wakeup;",
            "\tunsigned int color = get_pkmap_color(page);",
            "\twait_queue_head_t *pkmap_map_wait;",
            "",
            "\tlock_kmap_any(flags);",
            "\tvaddr = (unsigned long)page_address(page);",
            "\tBUG_ON(!vaddr);",
            "\tnr = PKMAP_NR(vaddr);",
            "",
            "\t/*",
            "\t * A count must never go down to zero",
            "\t * without a TLB flush!",
            "\t */",
            "\tneed_wakeup = 0;",
            "\tswitch (--pkmap_count[nr]) {",
            "\tcase 0:",
            "\t\tBUG();",
            "\tcase 1:",
            "\t\t/*",
            "\t\t * Avoid an unnecessary wake_up() function call.",
            "\t\t * The common case is pkmap_count[] == 1, but",
            "\t\t * no waiters.",
            "\t\t * The tasks queued in the wait-queue are guarded",
            "\t\t * by both the lock in the wait-queue-head and by",
            "\t\t * the kmap_lock.  As the kmap_lock is held here,",
            "\t\t * no need for the wait-queue-head's lock.  Simply",
            "\t\t * test if the queue is empty.",
            "\t\t */",
            "\t\tpkmap_map_wait = get_pkmap_wait_queue_head(color);",
            "\t\tneed_wakeup = waitqueue_active(pkmap_map_wait);",
            "\t}",
            "\tunlock_kmap_any(flags);",
            "",
            "\t/* do wake-up, if needed, race-free outside of the spin lock */",
            "\tif (need_wakeup)",
            "\t\twake_up(pkmap_map_wait);",
            "}",
            "void zero_user_segments(struct page *page, unsigned start1, unsigned end1,",
            "\t\tunsigned start2, unsigned end2)",
            "{",
            "\tunsigned int i;",
            "",
            "\tBUG_ON(end1 > page_size(page) || end2 > page_size(page));",
            "",
            "\tif (start1 >= end1)",
            "\t\tstart1 = end1 = 0;",
            "\tif (start2 >= end2)",
            "\t\tstart2 = end2 = 0;",
            "",
            "\tfor (i = 0; i < compound_nr(page); i++) {",
            "\t\tvoid *kaddr = NULL;",
            "",
            "\t\tif (start1 >= PAGE_SIZE) {",
            "\t\t\tstart1 -= PAGE_SIZE;",
            "\t\t\tend1 -= PAGE_SIZE;",
            "\t\t} else {",
            "\t\t\tunsigned this_end = min_t(unsigned, end1, PAGE_SIZE);",
            "",
            "\t\t\tif (end1 > start1) {",
            "\t\t\t\tkaddr = kmap_local_page(page + i);",
            "\t\t\t\tmemset(kaddr + start1, 0, this_end - start1);",
            "\t\t\t}",
            "\t\t\tend1 -= this_end;",
            "\t\t\tstart1 = 0;",
            "\t\t}",
            "",
            "\t\tif (start2 >= PAGE_SIZE) {",
            "\t\t\tstart2 -= PAGE_SIZE;",
            "\t\t\tend2 -= PAGE_SIZE;",
            "\t\t} else {",
            "\t\t\tunsigned this_end = min_t(unsigned, end2, PAGE_SIZE);",
            "",
            "\t\t\tif (end2 > start2) {",
            "\t\t\t\tif (!kaddr)",
            "\t\t\t\t\tkaddr = kmap_local_page(page + i);",
            "\t\t\t\tmemset(kaddr + start2, 0, this_end - start2);",
            "\t\t\t}",
            "\t\t\tend2 -= this_end;",
            "\t\t\tstart2 = 0;",
            "\t\t}",
            "",
            "\t\tif (kaddr) {",
            "\t\t\tkunmap_local(kaddr);",
            "\t\t\tflush_dcache_page(page + i);",
            "\t\t}",
            "",
            "\t\tif (!end1 && !end2)",
            "\t\t\tbreak;",
            "\t}",
            "",
            "\tBUG_ON((start1 | start2 | end1 | end2) != 0);",
            "}",
            "static inline int kmap_local_idx_push(void)",
            "{",
            "\tWARN_ON_ONCE(in_hardirq() && !irqs_disabled());",
            "\tcurrent->kmap_ctrl.idx += KM_INCR;",
            "\tBUG_ON(current->kmap_ctrl.idx >= KM_MAX_IDX);",
            "\treturn current->kmap_ctrl.idx - 1;",
            "}"
          ],
          "function_name": "kunmap_high, zero_user_segments, kmap_local_idx_push",
          "description": "提供高内存页解除映射和零填充功能，通过原子操作更新计数器并唤醒等待者，处理页面内容清零的特殊场景。",
          "similarity": 0.46427837014198303
        },
        {
          "chunk_id": 1,
          "file_path": "mm/highmem.c",
          "start_line": 34,
          "end_line": 168,
          "content": [
            "static inline int kmap_local_calc_idx(int idx)",
            "{",
            "\treturn idx + KM_MAX_IDX * smp_processor_id();",
            "}",
            "static inline unsigned int get_pkmap_color(struct page *page)",
            "{",
            "\treturn 0;",
            "}",
            "static inline unsigned int get_next_pkmap_nr(unsigned int color)",
            "{",
            "\tstatic unsigned int last_pkmap_nr;",
            "",
            "\tlast_pkmap_nr = (last_pkmap_nr + 1) & LAST_PKMAP_MASK;",
            "\treturn last_pkmap_nr;",
            "}",
            "static inline int no_more_pkmaps(unsigned int pkmap_nr, unsigned int color)",
            "{",
            "\treturn pkmap_nr == 0;",
            "}",
            "static inline int get_pkmap_entries_count(unsigned int color)",
            "{",
            "\treturn LAST_PKMAP;",
            "}",
            "unsigned int __nr_free_highpages(void)",
            "{",
            "\tstruct zone *zone;",
            "\tunsigned int pages = 0;",
            "",
            "\tfor_each_populated_zone(zone) {",
            "\t\tif (is_highmem(zone))",
            "\t\t\tpages += zone_page_state(zone, NR_FREE_PAGES);",
            "\t}",
            "",
            "\treturn pages;",
            "}",
            "static void flush_all_zero_pkmaps(void)",
            "{",
            "\tint i;",
            "\tint need_flush = 0;",
            "",
            "\tflush_cache_kmaps();",
            "",
            "\tfor (i = 0; i < LAST_PKMAP; i++) {",
            "\t\tstruct page *page;",
            "\t\tpte_t ptent;",
            "",
            "\t\t/*",
            "\t\t * zero means we don't have anything to do,",
            "\t\t * >1 means that it is still in use. Only",
            "\t\t * a count of 1 means that it is free but",
            "\t\t * needs to be unmapped",
            "\t\t */",
            "\t\tif (pkmap_count[i] != 1)",
            "\t\t\tcontinue;",
            "\t\tpkmap_count[i] = 0;",
            "",
            "\t\t/* sanity check */",
            "\t\tptent = ptep_get(&pkmap_page_table[i]);",
            "\t\tBUG_ON(pte_none(ptent));",
            "",
            "\t\t/*",
            "\t\t * Don't need an atomic fetch-and-clear op here;",
            "\t\t * no-one has the page mapped, and cannot get at",
            "\t\t * its virtual address (and hence PTE) without first",
            "\t\t * getting the kmap_lock (which is held here).",
            "\t\t * So no dangers, even with speculative execution.",
            "\t\t */",
            "\t\tpage = pte_page(ptent);",
            "\t\tpte_clear(&init_mm, PKMAP_ADDR(i), &pkmap_page_table[i]);",
            "",
            "\t\tset_page_address(page, NULL);",
            "\t\tneed_flush = 1;",
            "\t}",
            "\tif (need_flush)",
            "\t\tflush_tlb_kernel_range(PKMAP_ADDR(0), PKMAP_ADDR(LAST_PKMAP));",
            "}",
            "void __kmap_flush_unused(void)",
            "{",
            "\tlock_kmap();",
            "\tflush_all_zero_pkmaps();",
            "\tunlock_kmap();",
            "}",
            "static inline unsigned long map_new_virtual(struct page *page)",
            "{",
            "\tunsigned long vaddr;",
            "\tint count;",
            "\tunsigned int last_pkmap_nr;",
            "\tunsigned int color = get_pkmap_color(page);",
            "",
            "start:",
            "\tcount = get_pkmap_entries_count(color);",
            "\t/* Find an empty entry */",
            "\tfor (;;) {",
            "\t\tlast_pkmap_nr = get_next_pkmap_nr(color);",
            "\t\tif (no_more_pkmaps(last_pkmap_nr, color)) {",
            "\t\t\tflush_all_zero_pkmaps();",
            "\t\t\tcount = get_pkmap_entries_count(color);",
            "\t\t}",
            "\t\tif (!pkmap_count[last_pkmap_nr])",
            "\t\t\tbreak;\t/* Found a usable entry */",
            "\t\tif (--count)",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * Sleep for somebody else to unmap their entries",
            "\t\t */",
            "\t\t{",
            "\t\t\tDECLARE_WAITQUEUE(wait, current);",
            "\t\t\twait_queue_head_t *pkmap_map_wait =",
            "\t\t\t\tget_pkmap_wait_queue_head(color);",
            "",
            "\t\t\t__set_current_state(TASK_UNINTERRUPTIBLE);",
            "\t\t\tadd_wait_queue(pkmap_map_wait, &wait);",
            "\t\t\tunlock_kmap();",
            "\t\t\tschedule();",
            "\t\t\tremove_wait_queue(pkmap_map_wait, &wait);",
            "\t\t\tlock_kmap();",
            "",
            "\t\t\t/* Somebody else might have mapped it while we slept */",
            "\t\t\tif (page_address(page))",
            "\t\t\t\treturn (unsigned long)page_address(page);",
            "",
            "\t\t\t/* Re-start */",
            "\t\t\tgoto start;",
            "\t\t}",
            "\t}",
            "\tvaddr = PKMAP_ADDR(last_pkmap_nr);",
            "\tset_pte_at(&init_mm, vaddr,",
            "\t\t   &(pkmap_page_table[last_pkmap_nr]), mk_pte(page, kmap_prot));",
            "",
            "\tpkmap_count[last_pkmap_nr] = 1;",
            "\tset_page_address(page, (void *)vaddr);",
            "",
            "\treturn vaddr;",
            "}"
          ],
          "function_name": "kmap_local_calc_idx, get_pkmap_color, get_next_pkmap_nr, no_more_pkmaps, get_pkmap_entries_count, __nr_free_highpages, flush_all_zero_pkmaps, __kmap_flush_unused, map_new_virtual",
          "description": "实现高内存页表项管理逻辑，包含计算索引、获取颜色值、遍历页表项、统计空闲页及刷新无效页表项的操作。",
          "similarity": 0.45917898416519165
        }
      ]
    }
  ]
}