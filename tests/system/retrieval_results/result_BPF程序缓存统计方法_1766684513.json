{
  "query": "BPF程序缓存统计方法",
  "timestamp": "2025-12-26 01:41:53",
  "retrieved_files": [
    {
      "source_file": "kernel/bpf/cpumask.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:07:30\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\cpumask.c`\n\n---\n\n# `bpf/cpumask.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/cpumask.c` 实现了 BPF（Berkeley Packet Filter）子系统中用于 CPU 掩码（cpumask）操作的一组内核函数（kfuncs）。该文件定义了一个引用计数的 `struct bpf_cpumask` 结构体，并提供了一系列安全、非阻塞的 BPF 可调用接口，用于创建、操作、查询和释放 cpumask 对象。这些接口专为 BPF 程序设计，确保与 BPF 验证器兼容，并通过 BPF 内存分配器实现 RCU 安全的内存管理。\n\n## 2. 核心功能\n\n### 数据结构\n\n- **`struct bpf_cpumask`**  \n  引用计数的 BPF cpumask 包装结构体：\n  - `cpumask_t cpumask`：嵌入的实际 cpumask 位图。\n  - `refcount_t usage`：引用计数器，归零时通过 RCU 回调释放内存。\n\n### 主要函数（BPF kfuncs）\n\n| 函数 | 功能 |\n|------|------|\n| `bpf_cpumask_create()` | 创建一个新的可变 BPF cpumask 对象 |\n| `bpf_cpumask_acquire()` | 增加 cpumask 引用计数 |\n| `bpf_cpumask_release()` | 减少引用计数，归零时异步释放内存 |\n| `bpf_cpumask_first()` | 返回 cpumask 中第一个设置的 CPU 编号 |\n| `bpf_cpumask_first_zero()` | 返回 cpumask 中第一个未设置的 CPU 编号 |\n| `bpf_cpumask_first_and()` | 返回两个 cpumask 按位与后第一个设置的 CPU 编号 |\n| `bpf_cpumask_set_cpu()` | 设置指定 CPU 位 |\n| `bpf_cpumask_clear_cpu()` | 清除指定 CPU 位 |\n| `bpf_cpumask_test_cpu()` | 测试指定 CPU 是否被设置 |\n| `bpf_cpumask_test_and_set_cpu()` | 原子地测试并设置指定 CPU 位 |\n| `bpf_cpumask_test_and_clear_cpu()` | 原子地测试并清除指定 CPU 位 |\n| `bpf_cpumask_setall()` | 设置所有有效 CPU 位 |\n| `bpf_cpumask_clear()` | 清除所有 CPU 位 |\n| `bpf_cpumask_and()` | 对两个 cpumask 执行按位与操作并存入目标 |\n\n> 所有函数均使用 `__bpf_kfunc` 标记，表示其为 BPF 程序可安全调用的内核函数。\n\n## 3. 关键实现\n\n### 内存管理与引用计数\n- 使用 `bpf_mem_alloc` 子系统（`bpf_cpumask_ma`）进行非阻塞内存分配。\n- `bpf_cpumask_release()` 在引用计数归零时调用 `bpf_mem_cache_free_rcu()`，确保在 RCU 宽限期后释放内存，避免并发访问问题。\n- 释放前调用 `migrate_disable()`/`migrate_enable()` 禁用 CPU 迁移，保证 RCU 回调上下文安全。\n\n### 与 cpumask 类型兼容性\n- 显式嵌入 `cpumask_t`（而非 `cpumask_var_t`），避免因 `CONFIG_CPUMASK_OFFSTACK` 配置差异导致 BPF 验证器类型混淆。\n- 通过 `BUILD_BUG_ON(offsetof(...) != 0)` 确保 `cpumask` 成员位于结构体起始位置，允许安全地将 `struct bpf_cpumask *` 强制转换为 `struct cpumask *`。\n\n### CPU 有效性检查\n- 所有涉及 CPU 编号的操作（如 `set_cpu`、`test_cpu` 等）均先调用 `cpu_valid(u32 cpu)` 验证 `cpu < nr_cpu_ids`，防止越界访问。\n\n### BPF 验证器兼容性\n- 所有导出函数使用 `__bpf_kfunc` 宏声明，确保被 BPF 验证器识别为合法调用目标。\n- 参数类型设计（如接受 `const struct cpumask *`）允许 BPF 程序传入 `struct bpf_cpumask *` 指针，利用结构体布局兼容性。\n\n## 4. 依赖关系\n\n- **`<linux/bpf.h>`**：BPF 核心头文件，提供 kfunc 声明宏。\n- **`<linux/bpf_mem_alloc.h>`**：BPF 内存分配器接口，用于 RCU 安全的对象分配与释放。\n- **`<linux/cpumask.h>`**：标准 cpumask 操作函数（如 `cpumask_set_cpu`、`cpumask_first` 等）。\n- **`<linux/refcount.h>`**：引用计数原语（通过 `refcount_t` 和相关操作）。\n- **`<linux/btf.h>` / `<linux/btf_ids.h>`**：支持 BTF（BPF Type Format）类型信息生成，用于 kfunc 元数据。\n\n## 5. 使用场景\n\n- **BPF 程序中的 CPU 亲和性控制**：例如在调度器 BPF 程序中动态构造或修改任务的 CPU 亲和掩码。\n- **资源隔离与负载均衡**：网络或跟踪 BPF 程序可根据系统状态动态生成 CPU 掩码，用于指导工作线程绑定。\n- **安全策略实施**：限制某些 BPF 程序仅在特定 CPU 集合上执行。\n- **与 BPF map 集成**：通过 `kptr`（内核指针）将 `struct bpf_cpumask` 存入 BPF map，实现跨 BPF 程序实例共享或持久化 cpumask 状态。\n- **原子操作支持**：`test_and_set`/`test_and_clear` 等接口适用于需要无锁并发修改 cpumask 的场景。",
      "similarity": 0.6776154637336731,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/cpumask.c",
          "start_line": 374,
          "end_line": 467,
          "content": [
            "__bpf_kfunc bool bpf_cpumask_full(const struct cpumask *cpumask)",
            "{",
            "\treturn cpumask_full(cpumask);",
            "}",
            "__bpf_kfunc void bpf_cpumask_copy(struct bpf_cpumask *dst, const struct cpumask *src)",
            "{",
            "\tcpumask_copy((struct cpumask *)dst, src);",
            "}",
            "__bpf_kfunc u32 bpf_cpumask_any_distribute(const struct cpumask *cpumask)",
            "{",
            "\treturn cpumask_any_distribute(cpumask);",
            "}",
            "__bpf_kfunc u32 bpf_cpumask_any_and_distribute(const struct cpumask *src1,",
            "\t\t\t\t\t       const struct cpumask *src2)",
            "{",
            "\treturn cpumask_any_and_distribute(src1, src2);",
            "}",
            "__bpf_kfunc u32 bpf_cpumask_weight(const struct cpumask *cpumask)",
            "{",
            "\treturn cpumask_weight(cpumask);",
            "}",
            "__bpf_kfunc int bpf_cpumask_populate(struct cpumask *cpumask, void *src, size_t src__sz)",
            "{",
            "\tunsigned long source = (unsigned long)src;",
            "",
            "\t/* The memory region must be large enough to populate the entire CPU mask. */",
            "\tif (src__sz < bitmap_size(nr_cpu_ids))",
            "\t\treturn -EACCES;",
            "",
            "\t/* If avoiding unaligned accesses, the input region must be aligned to the nearest long. */",
            "\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) &&",
            "\t\t!IS_ALIGNED(source, sizeof(long)))",
            "\t\treturn -EINVAL;",
            "",
            "\tbitmap_copy(cpumask_bits(cpumask), src, nr_cpu_ids);",
            "",
            "\treturn 0;",
            "}",
            "BTF_KFUNCS_START(cpumask_kfunc_btf_ids)",
            "BTF_ID_FLAGS(func, bpf_cpumask_create, KF_ACQUIRE | KF_RET_NULL)",
            "BTF_ID_FLAGS(func, bpf_cpumask_release, KF_RELEASE)",
            "BTF_ID_FLAGS(func, bpf_cpumask_acquire, KF_ACQUIRE | KF_TRUSTED_ARGS)",
            "BTF_ID_FLAGS(func, bpf_cpumask_first, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_first_zero, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_first_and, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_set_cpu, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_clear_cpu, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_test_cpu, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_test_and_set_cpu, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_test_and_clear_cpu, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_setall, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_clear, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_and, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_or, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_xor, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_equal, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_intersects, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_subset, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_empty, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_full, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_copy, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_any_distribute, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_any_and_distribute, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_weight, KF_RCU)",
            "BTF_ID_FLAGS(func, bpf_cpumask_populate, KF_RCU)",
            "BTF_KFUNCS_END(cpumask_kfunc_btf_ids)",
            "",
            "static const struct btf_kfunc_id_set cpumask_kfunc_set = {",
            "\t.owner = THIS_MODULE,",
            "\t.set   = &cpumask_kfunc_btf_ids,",
            "};",
            "",
            "BTF_ID_LIST(cpumask_dtor_ids)",
            "BTF_ID(struct, bpf_cpumask)",
            "BTF_ID(func, bpf_cpumask_release)",
            "",
            "static int __init cpumask_kfunc_init(void)",
            "{",
            "\tint ret;",
            "\tconst struct btf_id_dtor_kfunc cpumask_dtors[] = {",
            "\t\t{",
            "\t\t\t.btf_id\t      = cpumask_dtor_ids[0],",
            "\t\t\t.kfunc_btf_id = cpumask_dtor_ids[1]",
            "\t\t},",
            "\t};",
            "",
            "\tret = bpf_mem_alloc_init(&bpf_cpumask_ma, sizeof(struct bpf_cpumask), false);",
            "\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_TRACING, &cpumask_kfunc_set);",
            "\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS, &cpumask_kfunc_set);",
            "\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_SYSCALL, &cpumask_kfunc_set);",
            "\treturn  ret ?: register_btf_id_dtor_kfuncs(cpumask_dtors,",
            "\t\t\t\t\t\t   ARRAY_SIZE(cpumask_dtors),",
            "\t\t\t\t\t\t   THIS_MODULE);",
            "}"
          ],
          "function_name": "bpf_cpumask_full, bpf_cpumask_copy, bpf_cpumask_any_distribute, bpf_cpumask_any_and_distribute, bpf_cpumask_weight, bpf_cpumask_populate, BTF_ID_FLAGS",
          "description": "提供了CPU掩码数据填充、权重计算等扩展功能，并通过BTF ID标注系统注册所有kfunc到指定ID集合，包含内存分配器初始化、KFUNC集注册及析构函数注册逻辑，实现BPF程序对CPU掩码对象的全生命周期管理。",
          "similarity": 0.6551828384399414
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/cpumask.c",
          "start_line": 32,
          "end_line": 134,
          "content": [
            "static bool cpu_valid(u32 cpu)",
            "{",
            "\treturn cpu < nr_cpu_ids;",
            "}",
            "__bpf_kfunc void bpf_cpumask_release(struct bpf_cpumask *cpumask)",
            "{",
            "\tif (!refcount_dec_and_test(&cpumask->usage))",
            "\t\treturn;",
            "",
            "\tmigrate_disable();",
            "\tbpf_mem_cache_free_rcu(&bpf_cpumask_ma, cpumask);",
            "\tmigrate_enable();",
            "}",
            "__bpf_kfunc u32 bpf_cpumask_first(const struct cpumask *cpumask)",
            "{",
            "\treturn cpumask_first(cpumask);",
            "}",
            "__bpf_kfunc u32 bpf_cpumask_first_zero(const struct cpumask *cpumask)",
            "{",
            "\treturn cpumask_first_zero(cpumask);",
            "}",
            "__bpf_kfunc u32 bpf_cpumask_first_and(const struct cpumask *src1,",
            "\t\t\t\t      const struct cpumask *src2)",
            "{",
            "\treturn cpumask_first_and(src1, src2);",
            "}",
            "__bpf_kfunc void bpf_cpumask_set_cpu(u32 cpu, struct bpf_cpumask *cpumask)",
            "{",
            "\tif (!cpu_valid(cpu))",
            "\t\treturn;",
            "",
            "\tcpumask_set_cpu(cpu, (struct cpumask *)cpumask);",
            "}",
            "__bpf_kfunc void bpf_cpumask_clear_cpu(u32 cpu, struct bpf_cpumask *cpumask)",
            "{",
            "\tif (!cpu_valid(cpu))",
            "\t\treturn;",
            "",
            "\tcpumask_clear_cpu(cpu, (struct cpumask *)cpumask);",
            "}",
            "__bpf_kfunc bool bpf_cpumask_test_cpu(u32 cpu, const struct cpumask *cpumask)",
            "{",
            "\tif (!cpu_valid(cpu))",
            "\t\treturn false;",
            "",
            "\treturn cpumask_test_cpu(cpu, (struct cpumask *)cpumask);",
            "}",
            "__bpf_kfunc bool bpf_cpumask_test_and_set_cpu(u32 cpu, struct bpf_cpumask *cpumask)",
            "{",
            "\tif (!cpu_valid(cpu))",
            "\t\treturn false;",
            "",
            "\treturn cpumask_test_and_set_cpu(cpu, (struct cpumask *)cpumask);",
            "}",
            "__bpf_kfunc bool bpf_cpumask_test_and_clear_cpu(u32 cpu, struct bpf_cpumask *cpumask)",
            "{",
            "\tif (!cpu_valid(cpu))",
            "\t\treturn false;",
            "",
            "\treturn cpumask_test_and_clear_cpu(cpu, (struct cpumask *)cpumask);",
            "}",
            "__bpf_kfunc void bpf_cpumask_setall(struct bpf_cpumask *cpumask)",
            "{",
            "\tcpumask_setall((struct cpumask *)cpumask);",
            "}",
            "__bpf_kfunc void bpf_cpumask_clear(struct bpf_cpumask *cpumask)",
            "{",
            "\tcpumask_clear((struct cpumask *)cpumask);",
            "}",
            "__bpf_kfunc bool bpf_cpumask_and(struct bpf_cpumask *dst,",
            "\t\t\t\t const struct cpumask *src1,",
            "\t\t\t\t const struct cpumask *src2)",
            "{",
            "\treturn cpumask_and((struct cpumask *)dst, src1, src2);",
            "}",
            "__bpf_kfunc void bpf_cpumask_or(struct bpf_cpumask *dst,",
            "\t\t\t\tconst struct cpumask *src1,",
            "\t\t\t\tconst struct cpumask *src2)",
            "{",
            "\tcpumask_or((struct cpumask *)dst, src1, src2);",
            "}",
            "__bpf_kfunc void bpf_cpumask_xor(struct bpf_cpumask *dst,",
            "\t\t\t\t const struct cpumask *src1,",
            "\t\t\t\t const struct cpumask *src2)",
            "{",
            "\tcpumask_xor((struct cpumask *)dst, src1, src2);",
            "}",
            "__bpf_kfunc bool bpf_cpumask_equal(const struct cpumask *src1, const struct cpumask *src2)",
            "{",
            "\treturn cpumask_equal(src1, src2);",
            "}",
            "__bpf_kfunc bool bpf_cpumask_intersects(const struct cpumask *src1, const struct cpumask *src2)",
            "{",
            "\treturn cpumask_intersects(src1, src2);",
            "}",
            "__bpf_kfunc bool bpf_cpumask_subset(const struct cpumask *src1, const struct cpumask *src2)",
            "{",
            "\treturn cpumask_subset(src1, src2);",
            "}",
            "__bpf_kfunc bool bpf_cpumask_empty(const struct cpumask *cpumask)",
            "{",
            "\treturn cpumask_empty(cpumask);",
            "}"
          ],
          "function_name": "cpu_valid, bpf_cpumask_release, bpf_cpumask_first, bpf_cpumask_first_zero, bpf_cpumask_first_and, bpf_cpumask_set_cpu, bpf_cpumask_clear_cpu, bpf_cpumask_test_cpu, bpf_cpumask_test_and_set_cpu, bpf_cpumask_test_and_clear_cpu, bpf_cpumask_setall, bpf_cpumask_clear, bpf_cpumask_and, bpf_cpumask_or, bpf_cpumask_xor, bpf_cpumask_equal, bpf_cpumask_intersects, bpf_cpumask_subset, bpf_cpumask_empty",
          "description": "实现了针对bpf_cpumask的各类CPU掩码操作函数，包括引用计数释放、位操作、集合运算及状态查询等功能，所有函数均通过__bpf_kfunc声明为可被BPF程序调用的内核接口，其中release函数负责RCU安全的内存回收。",
          "similarity": 0.6469370126724243
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/cpumask.c",
          "start_line": 1,
          "end_line": 31,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2023 Meta, Inc */",
            "#include <linux/bpf.h>",
            "#include <linux/bpf_mem_alloc.h>",
            "#include <linux/btf.h>",
            "#include <linux/btf_ids.h>",
            "#include <linux/cpumask.h>",
            "",
            "/**",
            " * struct bpf_cpumask - refcounted BPF cpumask wrapper structure",
            " * @cpumask:\tThe actual cpumask embedded in the struct.",
            " * @usage:\tObject reference counter. When the refcount goes to 0, the",
            " *\t\tmemory is released back to the BPF allocator, which provides",
            " *\t\tRCU safety.",
            " *",
            " * Note that we explicitly embed a cpumask_t rather than a cpumask_var_t.  This",
            " * is done to avoid confusing the verifier due to the typedef of cpumask_var_t",
            " * changing depending on whether CONFIG_CPUMASK_OFFSTACK is defined or not. See",
            " * the details in <linux/cpumask.h>. The consequence is that this structure is",
            " * likely a bit larger than it needs to be when CONFIG_CPUMASK_OFFSTACK is",
            " * defined due to embedding the whole NR_CPUS-size bitmap, but the extra memory",
            " * overhead is minimal. For the more typical case of CONFIG_CPUMASK_OFFSTACK",
            " * not being defined, the structure is the same size regardless.",
            " */",
            "struct bpf_cpumask {",
            "\tcpumask_t cpumask;",
            "\trefcount_t usage;",
            "};",
            "",
            "static struct bpf_mem_alloc bpf_cpumask_ma;",
            ""
          ],
          "function_name": null,
          "description": "定义了bpf_cpumask结构体，包含cpumask_t类型的cpumask字段和引用计数器usage，用于包装并管理BPF程序中的CPU掩码对象，通过引用计数控制内存生命周期，避免直接使用typedef可能引起的验证器混淆问题。",
          "similarity": 0.6000809669494629
        }
      ]
    },
    {
      "source_file": "kernel/bpf/memalloc.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:19:22\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\memalloc.c`\n\n---\n\n# `bpf/memalloc.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/memalloc.c` 实现了一个专用于 BPF（Berkeley Packet Filter）程序的内存分配器，支持在任意上下文（包括 NMI、中断、不可抢占上下文等）中安全地分配和释放小块内存。该分配器通过每 CPU 的多级缓存桶（per-CPU per-bucket free list）机制，避免在 BPF 程序执行路径中直接调用可能不安全的 `kmalloc()`。缓存桶的填充和回收由 `irq_work` 异步完成，确保主执行路径的低延迟和高可靠性。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_mem_cache`**  \n  每个缓存桶的核心结构，包含：\n  - `free_llist` / `free_llist_extra`：无锁链表（llist），用于存储空闲对象。\n  - `active`：本地原子计数器，用于保护对 `free_llist` 的并发访问。\n  - `refill_work`：`irq_work` 结构，用于触发异步填充。\n  - `objcg`：对象 cgroup 指针，用于内存记账。\n  - `unit_size`：该缓存桶中对象的固定大小。\n  - `free_cnt`、`low_watermark`、`high_watermark`、`batch`：缓存管理参数。\n  - `percpu_size`：标识是否为 per-CPU 分配。\n  - RCU 相关字段（`free_by_rcu`、`rcu` 等）：用于延迟释放内存，避免在不可睡眠上下文中调用 `kfree`。\n\n- **`struct bpf_mem_caches`**  \n  包含 `NUM_CACHES`（11 个）不同大小的 `bpf_mem_cache` 实例，对应预定义的内存块尺寸。\n\n- **`sizes[NUM_CACHES]`**  \n  定义了 11 种支持的分配尺寸：`{96, 192, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096}` 字节。\n\n- **`size_index[24]`**  \n  查找表，将请求大小（≤192 字节）映射到对应的缓存桶索引。\n\n### 主要函数\n\n- **`bpf_mem_cache_idx(size_t size)`**  \n  根据请求大小返回对应的缓存桶索引（0~10），超出 `BPF_MEM_ALLOC_SIZE_MAX`（4096）则返回 -1。\n\n- **`__alloc()`**  \n  底层分配函数，根据是否为 per-CPU 类型调用 `kmalloc_node` 或 `__alloc_percpu_gfp`。\n\n- **`add_obj_to_free_list()`**  \n  将对象安全地加入当前 CPU 的空闲链表，使用 `active` 计数器保护。\n\n- **`alloc_bulk()`**  \n  批量分配对象并填充缓存桶，优先从延迟释放队列（如 `free_by_rcu_ttrace`）回收，再尝试从全局分配器分配。\n\n- **`free_one()` / `free_all()`**  \n  释放单个或多个对象，区分普通和 per-CPU 类型。\n\n- **`__free_rcu()` / `__free_rcu_tasks_trace()`**  \n  RCU 回调函数，用于在宽限期结束后真正释放内存。\n\n- **`enque_to_free()` / `do_call_rcu_ttrace()`**  \n  将待释放对象加入 RCU 延迟队列，并触发 RCU 宽限期。\n\n## 3. 关键实现\n\n### 内存布局与对齐\n- 每个分配的对象末尾附加 8 字节的 `struct llist_node`，用于无锁链表管理。\n- 所有分配均对齐至 8 字节边界。\n\n### 并发控制\n- 使用 `local_t active` 计数器保护对 `free_llist` 的访问。在分配/释放时，通过 `inc_active()`/`dec_active()` 禁用中断（尤其在 `CONFIG_PREEMPT_RT` 下），确保 NMI 或中断上下文不会破坏链表结构。\n- `free_llist_extra` 用于在 `active` 忙时暂存释放对象，避免失败。\n\n### 异步填充机制\n- 当缓存桶水位低于 `low_watermark` 时，通过 `irq_work` 触发 `alloc_bulk()`。\n- `alloc_bulk()` 优先从 RCU 延迟释放队列中回收对象，减少全局分配压力。\n- 使用 `set_active_memcg()` 确保内存分配计入正确的 memcg。\n\n### RCU 延迟释放\n- 在不可睡眠上下文（如 NMI）中释放内存时，对象被加入 `free_by_rcu_ttrace` 队列。\n- 通过 `call_rcu_tasks_trace()` 或 `call_rcu()` 触发宽限期，之后在软中断上下文中真正释放。\n- 支持 `rcu_trace_implies_rcu_gp()` 优化，避免双重 RCU 调用。\n\n### 尺寸映射策略\n- 对 ≤192 字节的请求，使用 `size_index` 查找表快速定位桶。\n- 对 >192 字节的请求，使用 `fls(size - 1) - 2` 计算桶索引，覆盖 256~4096 字节范围。\n\n## 4. 依赖关系\n\n- **内存管理**：依赖 `<linux/mm.h>`、`<linux/memcontrol.h>` 进行底层分配和 memcg 记账。\n- **BPF 子系统**：通过 `<linux/bpf.h>` 和 `<linux/bpf_mem_alloc.h>` 与 BPF 运行时集成。\n- **无锁数据结构**：使用 `<linux/llist.h>` 提供的无锁链表。\n- **中断与延迟执行**：依赖 `<linux/irq_work.h>` 实现异步填充。\n- **RCU 机制**：使用 RCU 和 RCU Tasks Trace 宽限期实现安全延迟释放。\n- **架构相关**：使用 `<asm/local.h>` 的 per-CPU 原子操作。\n\n## 5. 使用场景\n\n- **BPF tracing 程序**：当 BPF 程序 attach 到 `kprobe`、`fentry` 等 hook 点时，可能运行在任意内核上下文（包括 NMI、中断、不可抢占区域）。此时标准 `kmalloc` 不安全，必须使用本分配器。\n- **高可靠性内存分配**：在不允许睡眠、不能触发内存回收的上下文中，提供确定性的内存分配能力。\n- **低延迟要求**：通过 per-CPU 缓存避免锁竞争和全局分配器开销，满足 BPF 程序对性能的严苛要求。\n- **内存隔离与记账**：支持通过 `objcg` 将 BPF 内存消耗计入特定 cgroup，便于资源控制。",
      "similarity": 0.6637243032455444,
      "chunks": [
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 68,
          "end_line": 168,
          "content": [
            "static int bpf_mem_cache_idx(size_t size)",
            "{",
            "\tif (!size || size > BPF_MEM_ALLOC_SIZE_MAX)",
            "\t\treturn -1;",
            "",
            "\tif (size <= 192)",
            "\t\treturn size_index[(size - 1) / 8] - 1;",
            "",
            "\treturn fls(size - 1) - 2;",
            "}",
            "static void inc_active(struct bpf_mem_cache *c, unsigned long *flags)",
            "{",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\t/* In RT irq_work runs in per-cpu kthread, so disable",
            "\t\t * interrupts to avoid preemption and interrupts and",
            "\t\t * reduce the chance of bpf prog executing on this cpu",
            "\t\t * when active counter is busy.",
            "\t\t */",
            "\t\tlocal_irq_save(*flags);",
            "\t/* alloc_bulk runs from irq_work which will not preempt a bpf",
            "\t * program that does unit_alloc/unit_free since IRQs are",
            "\t * disabled there. There is no race to increment 'active'",
            "\t * counter. It protects free_llist from corruption in case NMI",
            "\t * bpf prog preempted this loop.",
            "\t */",
            "\tWARN_ON_ONCE(local_inc_return(&c->active) != 1);",
            "}",
            "static void dec_active(struct bpf_mem_cache *c, unsigned long *flags)",
            "{",
            "\tlocal_dec(&c->active);",
            "\tif (IS_ENABLED(CONFIG_PREEMPT_RT))",
            "\t\tlocal_irq_restore(*flags);",
            "}",
            "static void add_obj_to_free_list(struct bpf_mem_cache *c, void *obj)",
            "{",
            "\tunsigned long flags;",
            "",
            "\tinc_active(c, &flags);",
            "\t__llist_add(obj, &c->free_llist);",
            "\tc->free_cnt++;",
            "\tdec_active(c, &flags);",
            "}",
            "static void alloc_bulk(struct bpf_mem_cache *c, int cnt, int node, bool atomic)",
            "{",
            "\tstruct mem_cgroup *memcg = NULL, *old_memcg;",
            "\tgfp_t gfp;",
            "\tvoid *obj;",
            "\tint i;",
            "",
            "\tgfp = __GFP_NOWARN | __GFP_ACCOUNT;",
            "\tgfp |= atomic ? GFP_NOWAIT : GFP_KERNEL;",
            "",
            "\tfor (i = 0; i < cnt; i++) {",
            "\t\t/*",
            "\t\t * For every 'c' llist_del_first(&c->free_by_rcu_ttrace); is",
            "\t\t * done only by one CPU == current CPU. Other CPUs might",
            "\t\t * llist_add() and llist_del_all() in parallel.",
            "\t\t */",
            "\t\tobj = llist_del_first(&c->free_by_rcu_ttrace);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tif (i >= cnt)",
            "\t\treturn;",
            "",
            "\tfor (; i < cnt; i++) {",
            "\t\tobj = llist_del_first(&c->waiting_for_gp_ttrace);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tif (i >= cnt)",
            "\t\treturn;",
            "",
            "\tmemcg = get_memcg(c);",
            "\told_memcg = set_active_memcg(memcg);",
            "\tfor (; i < cnt; i++) {",
            "\t\t/* Allocate, but don't deplete atomic reserves that typical",
            "\t\t * GFP_ATOMIC would do. irq_work runs on this cpu and kmalloc",
            "\t\t * will allocate from the current numa node which is what we",
            "\t\t * want here.",
            "\t\t */",
            "\t\tobj = __alloc(c, node, gfp);",
            "\t\tif (!obj)",
            "\t\t\tbreak;",
            "\t\tadd_obj_to_free_list(c, obj);",
            "\t}",
            "\tset_active_memcg(old_memcg);",
            "\tmem_cgroup_put(memcg);",
            "}",
            "static void free_one(void *obj, bool percpu)",
            "{",
            "\tif (percpu) {",
            "\t\tfree_percpu(((void __percpu **)obj)[1]);",
            "\t\tkfree(obj);",
            "\t\treturn;",
            "\t}",
            "",
            "\tkfree(obj);",
            "}"
          ],
          "function_name": "bpf_mem_cache_idx, inc_active, dec_active, add_obj_to_free_list, alloc_bulk, free_one",
          "description": "实现BPF内存缓存核心控制逻辑，包含大小索引计算、活跃计数器管理、对象回收到自由链表、批量分配与释放流程。通过irq_work异步补充缓存，处理多CPU间的内存对象迁移与回收。",
          "similarity": 0.6793594360351562
        },
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 1,
          "end_line": 67,
          "content": [
            "// SPDX-License-Identifier: GPL-2.0-only",
            "/* Copyright (c) 2022 Meta Platforms, Inc. and affiliates. */",
            "#include <linux/mm.h>",
            "#include <linux/llist.h>",
            "#include <linux/bpf.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/bpf_mem_alloc.h>",
            "#include <linux/memcontrol.h>",
            "#include <asm/local.h>",
            "",
            "/* Any context (including NMI) BPF specific memory allocator.",
            " *",
            " * Tracing BPF programs can attach to kprobe and fentry. Hence they",
            " * run in unknown context where calling plain kmalloc() might not be safe.",
            " *",
            " * Front-end kmalloc() with per-cpu per-bucket cache of free elements.",
            " * Refill this cache asynchronously from irq_work.",
            " *",
            " * CPU_0 buckets",
            " * 16 32 64 96 128 196 256 512 1024 2048 4096",
            " * ...",
            " * CPU_N buckets",
            " * 16 32 64 96 128 196 256 512 1024 2048 4096",
            " *",
            " * The buckets are prefilled at the start.",
            " * BPF programs always run with migration disabled.",
            " * It's safe to allocate from cache of the current cpu with irqs disabled.",
            " * Free-ing is always done into bucket of the current cpu as well.",
            " * irq_work trims extra free elements from buckets with kfree",
            " * and refills them with kmalloc, so global kmalloc logic takes care",
            " * of freeing objects allocated by one cpu and freed on another.",
            " *",
            " * Every allocated objected is padded with extra 8 bytes that contains",
            " * struct llist_node.",
            " */",
            "#define LLIST_NODE_SZ sizeof(struct llist_node)",
            "",
            "#define BPF_MEM_ALLOC_SIZE_MAX 4096",
            "",
            "/* similar to kmalloc, but sizeof == 8 bucket is gone */",
            "static u8 size_index[24] __ro_after_init = {",
            "\t3,\t/* 8 */",
            "\t3,\t/* 16 */",
            "\t4,\t/* 24 */",
            "\t4,\t/* 32 */",
            "\t5,\t/* 40 */",
            "\t5,\t/* 48 */",
            "\t5,\t/* 56 */",
            "\t5,\t/* 64 */",
            "\t1,\t/* 72 */",
            "\t1,\t/* 80 */",
            "\t1,\t/* 88 */",
            "\t1,\t/* 96 */",
            "\t6,\t/* 104 */",
            "\t6,\t/* 112 */",
            "\t6,\t/* 120 */",
            "\t6,\t/* 128 */",
            "\t2,\t/* 136 */",
            "\t2,\t/* 144 */",
            "\t2,\t/* 152 */",
            "\t2,\t/* 160 */",
            "\t2,\t/* 168 */",
            "\t2,\t/* 176 */",
            "\t2,\t/* 184 */",
            "\t2\t/* 192 */",
            "};",
            ""
          ],
          "function_name": null,
          "description": "定义BPF专用内存分配器基础结构，通过per-CPU缓存和异步填充机制管理小对象分配。建立大小到缓存索引的映射表，预分配不同大小的桶并注册到各CPU，支持NMI和中断上下文的安全内存分配。",
          "similarity": 0.6688777208328247
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 689,
          "end_line": 807,
          "content": [
            "static void free_mem_alloc(struct bpf_mem_alloc *ma)",
            "{",
            "\t/* waiting_for_gp[_ttrace] lists were drained, but RCU callbacks",
            "\t * might still execute. Wait for them.",
            "\t *",
            "\t * rcu_barrier_tasks_trace() doesn't imply synchronize_rcu_tasks_trace(),",
            "\t * but rcu_barrier_tasks_trace() and rcu_barrier() below are only used",
            "\t * to wait for the pending __free_rcu_tasks_trace() and __free_rcu(),",
            "\t * so if call_rcu(head, __free_rcu) is skipped due to",
            "\t * rcu_trace_implies_rcu_gp(), it will be OK to skip rcu_barrier() by",
            "\t * using rcu_trace_implies_rcu_gp() as well.",
            "\t */",
            "\trcu_barrier(); /* wait for __free_by_rcu */",
            "\trcu_barrier_tasks_trace(); /* wait for __free_rcu */",
            "\tif (!rcu_trace_implies_rcu_gp())",
            "\t\trcu_barrier();",
            "\tfree_mem_alloc_no_barrier(ma);",
            "}",
            "static void free_mem_alloc_deferred(struct work_struct *work)",
            "{",
            "\tstruct bpf_mem_alloc *ma = container_of(work, struct bpf_mem_alloc, work);",
            "",
            "\tfree_mem_alloc(ma);",
            "\tkfree(ma);",
            "}",
            "static void destroy_mem_alloc(struct bpf_mem_alloc *ma, int rcu_in_progress)",
            "{",
            "\tstruct bpf_mem_alloc *copy;",
            "",
            "\tif (!rcu_in_progress) {",
            "\t\t/* Fast path. No callbacks are pending, hence no need to do",
            "\t\t * rcu_barrier-s.",
            "\t\t */",
            "\t\tfree_mem_alloc_no_barrier(ma);",
            "\t\treturn;",
            "\t}",
            "",
            "\tcopy = kmemdup(ma, sizeof(*ma), GFP_KERNEL);",
            "\tif (!copy) {",
            "\t\t/* Slow path with inline barrier-s */",
            "\t\tfree_mem_alloc(ma);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Defer barriers into worker to let the rest of map memory to be freed */",
            "\tmemset(ma, 0, sizeof(*ma));",
            "\tINIT_WORK(&copy->work, free_mem_alloc_deferred);",
            "\tqueue_work(system_unbound_wq, &copy->work);",
            "}",
            "void bpf_mem_alloc_destroy(struct bpf_mem_alloc *ma)",
            "{",
            "\tstruct bpf_mem_caches *cc;",
            "\tstruct bpf_mem_cache *c;",
            "\tint cpu, i, rcu_in_progress;",
            "",
            "\tif (ma->cache) {",
            "\t\trcu_in_progress = 0;",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(ma->cache, cpu);",
            "\t\t\tWRITE_ONCE(c->draining, true);",
            "\t\t\tirq_work_sync(&c->refill_work);",
            "\t\t\tdrain_mem_cache(c);",
            "\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
            "\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
            "\t\t}",
            "\t\tobj_cgroup_put(ma->objcg);",
            "\t\tdestroy_mem_alloc(ma, rcu_in_progress);",
            "\t}",
            "\tif (ma->caches) {",
            "\t\trcu_in_progress = 0;",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tcc = per_cpu_ptr(ma->caches, cpu);",
            "\t\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\t\tc = &cc->cache[i];",
            "\t\t\t\tWRITE_ONCE(c->draining, true);",
            "\t\t\t\tirq_work_sync(&c->refill_work);",
            "\t\t\t\tdrain_mem_cache(c);",
            "\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);",
            "\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);",
            "\t\t\t}",
            "\t\t}",
            "\t\tobj_cgroup_put(ma->objcg);",
            "\t\tdestroy_mem_alloc(ma, rcu_in_progress);",
            "\t}",
            "}",
            "static void notrace unit_free(struct bpf_mem_cache *c, void *ptr)",
            "{",
            "\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;",
            "\tunsigned long flags;",
            "\tint cnt = 0;",
            "",
            "\tBUILD_BUG_ON(LLIST_NODE_SZ > 8);",
            "",
            "\t/*",
            "\t * Remember bpf_mem_cache that allocated this object.",
            "\t * The hint is not accurate.",
            "\t */",
            "\tc->tgt = *(struct bpf_mem_cache **)llnode;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (local_inc_return(&c->active) == 1) {",
            "\t\t__llist_add(llnode, &c->free_llist);",
            "\t\tcnt = ++c->free_cnt;",
            "\t} else {",
            "\t\t/* unit_free() cannot fail. Therefore add an object to atomic",
            "\t\t * llist. free_bulk() will drain it. Though free_llist_extra is",
            "\t\t * a per-cpu list we have to use atomic llist_add here, since",
            "\t\t * it also can be interrupted by bpf nmi prog that does another",
            "\t\t * unit_free() into the same free_llist_extra.",
            "\t\t */",
            "\t\tllist_add(llnode, &c->free_llist_extra);",
            "\t}",
            "\tlocal_dec(&c->active);",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (cnt > c->high_watermark)",
            "\t\t/* free few objects from current cpu into global kmalloc pool */",
            "\t\tirq_work_raise(c);",
            "}"
          ],
          "function_name": "free_mem_alloc, free_mem_alloc_deferred, destroy_mem_alloc, bpf_mem_alloc_destroy, unit_free",
          "description": "实现BPF内存分配销毁逻辑，通过RCU屏障等待回调完成并安全释放资源，deferred路径利用workqueue异步释放，destroy_mem_alloc处理缓存清理和RCU状态同步。",
          "similarity": 0.6239123940467834
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 859,
          "end_line": 938,
          "content": [
            "static void notrace unit_free_rcu(struct bpf_mem_cache *c, void *ptr)",
            "{",
            "\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;",
            "\tunsigned long flags;",
            "",
            "\tc->tgt = *(struct bpf_mem_cache **)llnode;",
            "",
            "\tlocal_irq_save(flags);",
            "\tif (local_inc_return(&c->active) == 1) {",
            "\t\tif (__llist_add(llnode, &c->free_by_rcu))",
            "\t\t\tc->free_by_rcu_tail = llnode;",
            "\t} else {",
            "\t\tllist_add(llnode, &c->free_llist_extra_rcu);",
            "\t}",
            "\tlocal_dec(&c->active);",
            "\tlocal_irq_restore(flags);",
            "",
            "\tif (!atomic_read(&c->call_rcu_in_progress))",
            "\t\tirq_work_raise(c);",
            "}",
            "void notrace bpf_mem_free(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tstruct bpf_mem_cache *c;",
            "\tint idx;",
            "",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tc = *(void **)(ptr - LLIST_NODE_SZ);",
            "\tidx = bpf_mem_cache_idx(c->unit_size);",
            "\tif (WARN_ON_ONCE(idx < 0))",
            "\t\treturn;",
            "",
            "\tunit_free(this_cpu_ptr(ma->caches)->cache + idx, ptr);",
            "}",
            "void notrace bpf_mem_free_rcu(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tstruct bpf_mem_cache *c;",
            "\tint idx;",
            "",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tc = *(void **)(ptr - LLIST_NODE_SZ);",
            "\tidx = bpf_mem_cache_idx(c->unit_size);",
            "\tif (WARN_ON_ONCE(idx < 0))",
            "\t\treturn;",
            "",
            "\tunit_free_rcu(this_cpu_ptr(ma->caches)->cache + idx, ptr);",
            "}",
            "void notrace bpf_mem_cache_free(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tunit_free(this_cpu_ptr(ma->cache), ptr);",
            "}",
            "void notrace bpf_mem_cache_free_rcu(struct bpf_mem_alloc *ma, void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tunit_free_rcu(this_cpu_ptr(ma->cache), ptr);",
            "}",
            "void bpf_mem_cache_raw_free(void *ptr)",
            "{",
            "\tif (!ptr)",
            "\t\treturn;",
            "",
            "\tkfree(ptr - LLIST_NODE_SZ);",
            "}",
            "int bpf_mem_alloc_check_size(bool percpu, size_t size)",
            "{",
            "\t/* The size of percpu allocation doesn't have LLIST_NODE_SZ overhead */",
            "\tif ((percpu && size > BPF_MEM_ALLOC_SIZE_MAX) ||",
            "\t    (!percpu && size > BPF_MEM_ALLOC_SIZE_MAX - LLIST_NODE_SZ))",
            "\t\treturn -E2BIG;",
            "",
            "\treturn 0;",
            "}"
          ],
          "function_name": "unit_free_rcu, bpf_mem_free, bpf_mem_free_rcu, bpf_mem_cache_free, bpf_mem_cache_free_rcu, bpf_mem_cache_raw_free, bpf_mem_alloc_check_size",
          "description": "提供基于RCU的内存释放接口，unit_free系列函数将对象加入链表实现批量回收，bpf_mem_free系列根据上下文选择普通或RCU释放路径，check_size验证分配尺寸合法性",
          "similarity": 0.5558765530586243
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/bpf/memalloc.c",
          "start_line": 381,
          "end_line": 545,
          "content": [
            "static void check_free_by_rcu(struct bpf_mem_cache *c)",
            "{",
            "\tstruct llist_node *llnode, *t;",
            "\tunsigned long flags;",
            "",
            "\t/* drain free_llist_extra_rcu */",
            "\tif (unlikely(!llist_empty(&c->free_llist_extra_rcu))) {",
            "\t\tinc_active(c, &flags);",
            "\t\tllist_for_each_safe(llnode, t, llist_del_all(&c->free_llist_extra_rcu))",
            "\t\t\tif (__llist_add(llnode, &c->free_by_rcu))",
            "\t\t\t\tc->free_by_rcu_tail = llnode;",
            "\t\tdec_active(c, &flags);",
            "\t}",
            "",
            "\tif (llist_empty(&c->free_by_rcu))",
            "\t\treturn;",
            "",
            "\tif (atomic_xchg(&c->call_rcu_in_progress, 1)) {",
            "\t\t/*",
            "\t\t * Instead of kmalloc-ing new rcu_head and triggering 10k",
            "\t\t * call_rcu() to hit rcutree.qhimark and force RCU to notice",
            "\t\t * the overload just ask RCU to hurry up. There could be many",
            "\t\t * objects in free_by_rcu list.",
            "\t\t * This hint reduces memory consumption for an artificial",
            "\t\t * benchmark from 2 Gbyte to 150 Mbyte.",
            "\t\t */",
            "\t\trcu_request_urgent_qs_task(current);",
            "\t\treturn;",
            "\t}",
            "",
            "\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));",
            "",
            "\tinc_active(c, &flags);",
            "\tWRITE_ONCE(c->waiting_for_gp.first, __llist_del_all(&c->free_by_rcu));",
            "\tc->waiting_for_gp_tail = c->free_by_rcu_tail;",
            "\tdec_active(c, &flags);",
            "",
            "\tif (unlikely(READ_ONCE(c->draining))) {",
            "\t\tfree_all(llist_del_all(&c->waiting_for_gp), !!c->percpu_size);",
            "\t\tatomic_set(&c->call_rcu_in_progress, 0);",
            "\t} else {",
            "\t\tcall_rcu_hurry(&c->rcu, __free_by_rcu);",
            "\t}",
            "}",
            "static void bpf_mem_refill(struct irq_work *work)",
            "{",
            "\tstruct bpf_mem_cache *c = container_of(work, struct bpf_mem_cache, refill_work);",
            "\tint cnt;",
            "",
            "\t/* Racy access to free_cnt. It doesn't need to be 100% accurate */",
            "\tcnt = c->free_cnt;",
            "\tif (cnt < c->low_watermark)",
            "\t\t/* irq_work runs on this cpu and kmalloc will allocate",
            "\t\t * from the current numa node which is what we want here.",
            "\t\t */",
            "\t\talloc_bulk(c, c->batch, NUMA_NO_NODE, true);",
            "\telse if (cnt > c->high_watermark)",
            "\t\tfree_bulk(c);",
            "",
            "\tcheck_free_by_rcu(c);",
            "}",
            "static void notrace irq_work_raise(struct bpf_mem_cache *c)",
            "{",
            "\tirq_work_queue(&c->refill_work);",
            "}",
            "static void init_refill_work(struct bpf_mem_cache *c)",
            "{",
            "\tinit_irq_work(&c->refill_work, bpf_mem_refill);",
            "\tif (c->percpu_size) {",
            "\t\tc->low_watermark = 1;",
            "\t\tc->high_watermark = 3;",
            "\t} else if (c->unit_size <= 256) {",
            "\t\tc->low_watermark = 32;",
            "\t\tc->high_watermark = 96;",
            "\t} else {",
            "\t\t/* When page_size == 4k, order-0 cache will have low_mark == 2",
            "\t\t * and high_mark == 6 with batch alloc of 3 individual pages at",
            "\t\t * a time.",
            "\t\t * 8k allocs and above low == 1, high == 3, batch == 1.",
            "\t\t */",
            "\t\tc->low_watermark = max(32 * 256 / c->unit_size, 1);",
            "\t\tc->high_watermark = max(96 * 256 / c->unit_size, 3);",
            "\t}",
            "\tc->batch = max((c->high_watermark - c->low_watermark) / 4 * 3, 1);",
            "}",
            "static void prefill_mem_cache(struct bpf_mem_cache *c, int cpu)",
            "{",
            "\tint cnt = 1;",
            "",
            "\t/* To avoid consuming memory, for non-percpu allocation, assume that",
            "\t * 1st run of bpf prog won't be doing more than 4 map_update_elem from",
            "\t * irq disabled region if unit size is less than or equal to 256.",
            "\t * For all other cases, let us just do one allocation.",
            "\t */",
            "\tif (!c->percpu_size && c->unit_size <= 256)",
            "\t\tcnt = 4;",
            "\talloc_bulk(c, cnt, cpu_to_node(cpu), false);",
            "}",
            "int bpf_mem_alloc_init(struct bpf_mem_alloc *ma, int size, bool percpu)",
            "{",
            "\tstruct bpf_mem_caches *cc; struct bpf_mem_caches __percpu *pcc;",
            "\tstruct bpf_mem_cache *c; struct bpf_mem_cache __percpu *pc;",
            "\tstruct obj_cgroup *objcg = NULL;",
            "\tint cpu, i, unit_size, percpu_size = 0;",
            "",
            "\tif (percpu && size == 0)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* room for llist_node and per-cpu pointer */",
            "\tif (percpu)",
            "\t\tpercpu_size = LLIST_NODE_SZ + sizeof(void *);",
            "\tma->percpu = percpu;",
            "",
            "\tif (size) {",
            "\t\tpc = __alloc_percpu_gfp(sizeof(*pc), 8, GFP_KERNEL);",
            "\t\tif (!pc)",
            "\t\t\treturn -ENOMEM;",
            "",
            "\t\tif (!percpu)",
            "\t\t\tsize += LLIST_NODE_SZ; /* room for llist_node */",
            "\t\tunit_size = size;",
            "",
            "#ifdef CONFIG_MEMCG",
            "\t\tif (memcg_bpf_enabled())",
            "\t\t\tobjcg = get_obj_cgroup_from_current();",
            "#endif",
            "\t\tma->objcg = objcg;",
            "",
            "\t\tfor_each_possible_cpu(cpu) {",
            "\t\t\tc = per_cpu_ptr(pc, cpu);",
            "\t\t\tc->unit_size = unit_size;",
            "\t\t\tc->objcg = objcg;",
            "\t\t\tc->percpu_size = percpu_size;",
            "\t\t\tc->tgt = c;",
            "\t\t\tinit_refill_work(c);",
            "\t\t\tprefill_mem_cache(c, cpu);",
            "\t\t}",
            "\t\tma->cache = pc;",
            "\t\treturn 0;",
            "\t}",
            "",
            "\tpcc = __alloc_percpu_gfp(sizeof(*cc), 8, GFP_KERNEL);",
            "\tif (!pcc)",
            "\t\treturn -ENOMEM;",
            "#ifdef CONFIG_MEMCG",
            "\tobjcg = get_obj_cgroup_from_current();",
            "#endif",
            "\tma->objcg = objcg;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tcc = per_cpu_ptr(pcc, cpu);",
            "\t\tfor (i = 0; i < NUM_CACHES; i++) {",
            "\t\t\tc = &cc->cache[i];",
            "\t\t\tc->unit_size = sizes[i];",
            "\t\t\tc->objcg = objcg;",
            "\t\t\tc->percpu_size = percpu_size;",
            "\t\t\tc->tgt = c;",
            "",
            "\t\t\tinit_refill_work(c);",
            "\t\t\tprefill_mem_cache(c, cpu);",
            "\t\t}",
            "\t}",
            "",
            "\tma->caches = pcc;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "check_free_by_rcu, bpf_mem_refill, irq_work_raise, init_refill_work, prefill_mem_cache, bpf_mem_alloc_init",
          "description": "初始化内存缓存的刷新工作队列，设置水位标记控制缓存规模，实现动态扩容/缩容。包含预填充缓存的初始化函数，根据对象大小配置不同的低/高水位阈值，通过中断工作线程维护缓存状态。",
          "similarity": 0.5520960092544556
        }
      ]
    },
    {
      "source_file": "kernel/bpf/ringbuf.c",
      "md_summary": "> 自动生成时间: 2025-10-25 12:29:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `bpf\\ringbuf.c`\n\n---\n\n# `bpf/ringbuf.c` 技术文档\n\n## 1. 文件概述\n\n`bpf/ringbuf.c` 实现了 BPF（Berkeley Packet Filter）子系统中的**环形缓冲区（Ring Buffer）**机制，用于在内核与用户空间之间高效、安全地传递数据。该机制支持两种生产者模式：**内核生产者**（如 BPF 程序）和**用户空间生产者**，并提供内存映射（`mmap`）、等待队列通知、并发控制等核心功能，是 BPF 数据输出（如 perf event 替代方案）的关键基础设施。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct bpf_ringbuf`**  \n  环形缓冲区的核心结构体，包含：\n  - `waitq`：等待队列，用于通知用户空间有新数据\n  - `work`：IRQ 工作项，用于异步唤醒等待队列\n  - `mask`：环形缓冲区大小掩码（`data_sz - 1`），用于快速取模\n  - `pages` / `nr_pages`：物理页数组，支持双映射\n  - `spinlock`：用于内核生产者的自旋锁（SMP 对齐）\n  - `busy`：原子变量，用于用户空间生产者的互斥访问（避免持有自旋锁过久）\n  - `consumer_pos` / `producer_pos` / `pending_pos`：消费者、生产者和待提交位置（各自独占一页，支持不同 mmap 权限）\n  - `data[]`：实际数据存储区域（页对齐）\n\n- **`struct bpf_ringbuf_map`**  \n  封装标准 `bpf_map`，关联一个 `bpf_ringbuf` 实例。\n\n- **`struct bpf_ringbuf_hdr`**  \n  8 字节记录头，包含：\n  - `len`：记录有效载荷长度\n  - `pg_off`：记录在页内的偏移（用于跨页处理）\n\n### 主要函数\n\n- **`bpf_ringbuf_area_alloc()`**  \n  分配并初始化环形缓冲区的虚拟内存区域，采用**双映射数据页**技术简化环绕处理。\n\n- **`bpf_ringbuf_alloc()`**  \n  初始化 `bpf_ringbuf` 结构体，设置锁、等待队列、IRQ 工作项及初始位置。\n\n- **`bpf_ringbuf_free()`**  \n  释放环形缓冲区占用的虚拟内存和物理页。\n\n- **`ringbuf_map_alloc()`**  \n  BPF map 分配器回调，验证参数并创建 `bpf_ringbuf_map`。\n\n- **`ringbuf_map_free()`**  \n  BPF map 释放器回调，清理资源。\n\n- **`ringbuf_map_*_elem()` / `ringbuf_map_get_next_key()`**  \n  禁用标准 map 操作（返回 `-ENOTSUPP`），因为 ringbuf 不支持键值操作。\n\n- **`bpf_ringbuf_notify()`**  \n  IRQ 工作回调，唤醒所有等待数据的用户进程。\n\n## 3. 关键实现\n\n### 双映射数据页（Double-Mapped Data Pages）\n\n为简化环形缓冲区**环绕（wrap-around）**时的数据读取逻辑，数据页被**连续映射两次**：\n```\n[meta pages][data pages][data pages (same as first copy)]\n```\n当读取跨越缓冲区末尾时，可直接线性读取第二份映射，无需特殊处理。此设计同时适用于内核和用户空间 `mmap`。\n\n### 权限隔离与安全\n\n- **`consumer_pos` 和 `producer_pos` 各占独立页**，允许通过 `mmap` 设置不同权限：\n  - **内核生产者模式**：`producer_pos` 和数据页对用户空间为**只读**，防止篡改。\n  - **用户空间生产者模式**：仅 `consumer_pos` 对用户空间为**只读**，内核需严格验证用户提交的记录。\n\n### 并发控制策略\n\n- **内核生产者**：使用 `raw_spinlock_t` 保证多生产者安全。\n- **用户空间生产者**：使用 `atomic_t busy` 原子变量，避免在 BPF 程序回调期间长期持有 IRQ 自旋锁（可能导致死锁或延迟）。若 `busy` 被占用，`__bpf_user_ringbuf_peek()` 返回 `-EBUSY`。\n\n### 内存布局与对齐\n\n- 非 `mmap` 部分（`waitq` 到 `pending_pos`）大小由 `RINGBUF_PGOFF` 定义。\n- `consumer_pos`、`producer_pos` 和 `data` 均按 `PAGE_SIZE` 对齐，确保可独立映射。\n- 总元数据页数：`RINGBUF_NR_META_PAGES = RINGBUF_PGOFF + 2`（含 consumer/producer 页）。\n\n### 大小限制\n\n- 最大记录大小：`RINGBUF_MAX_RECORD_SZ = UINT_MAX / 4`（约 1GB）。\n- 最大缓冲区大小受 `bpf_ringbuf_hdr.pg_off`（32 位页偏移）限制，理论最大约 **64GB**。\n\n## 4. 依赖关系\n\n- **BPF 子系统**：依赖 `bpf_map` 基础设施（`bpf_map_area_alloc/free`、`bpf_map_init_from_attr`）。\n- **内存管理**：使用 `alloc_pages_node`、`vmap`/`vunmap`、`__free_page` 管理物理页和虚拟映射。\n- **同步机制**：依赖 `wait_queue`、`irq_work`、`raw_spinlock` 和 `atomic_t`。\n- **BTF（BPF Type Format）**：包含 BTF 相关头文件，可能用于未来类型验证（当前未直接使用）。\n- **用户 API**：与 `uapi/linux/bpf.h` 中的 `BPF_F_NUMA_NODE` 等标志交互。\n\n## 5. 使用场景\n\n- **BPF 程序输出数据**：替代 `bpf_perf_event_output()`，提供更低开销、更高吞吐的内核到用户空间数据通道。\n- **用户空间主动提交数据**：允许用户程序通过 ringbuf 向内核提交样本（需内核验证）。\n- **实时监控与追踪**：用于 eBPF 监控工具（如 `bpftrace`、`libbpf` 应用）高效收集内核事件。\n- **NUMA 感知分配**：支持通过 `BPF_F_NUMA_NODE` 标志在指定 NUMA 节点分配内存，优化性能。",
      "similarity": 0.6609419584274292,
      "chunks": [
        {
          "chunk_id": 0,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 1,
          "end_line": 149,
          "content": [
            "#include <linux/bpf.h>",
            "#include <linux/btf.h>",
            "#include <linux/err.h>",
            "#include <linux/irq_work.h>",
            "#include <linux/slab.h>",
            "#include <linux/filter.h>",
            "#include <linux/mm.h>",
            "#include <linux/vmalloc.h>",
            "#include <linux/wait.h>",
            "#include <linux/poll.h>",
            "#include <linux/kmemleak.h>",
            "#include <uapi/linux/btf.h>",
            "#include <linux/btf_ids.h>",
            "",
            "#define RINGBUF_CREATE_FLAG_MASK (BPF_F_NUMA_NODE)",
            "",
            "/* non-mmap()'able part of bpf_ringbuf (everything up to consumer page) */",
            "#define RINGBUF_PGOFF \\",
            "\t(offsetof(struct bpf_ringbuf, consumer_pos) >> PAGE_SHIFT)",
            "/* consumer page and producer page */",
            "#define RINGBUF_POS_PAGES 2",
            "#define RINGBUF_NR_META_PAGES (RINGBUF_PGOFF + RINGBUF_POS_PAGES)",
            "",
            "#define RINGBUF_MAX_RECORD_SZ (UINT_MAX/4)",
            "",
            "struct bpf_ringbuf {",
            "\twait_queue_head_t waitq;",
            "\tstruct irq_work work;",
            "\tu64 mask;",
            "\tstruct page **pages;",
            "\tint nr_pages;",
            "\traw_spinlock_t spinlock ____cacheline_aligned_in_smp;",
            "\t/* For user-space producer ring buffers, an atomic_t busy bit is used",
            "\t * to synchronize access to the ring buffers in the kernel, rather than",
            "\t * the spinlock that is used for kernel-producer ring buffers. This is",
            "\t * done because the ring buffer must hold a lock across a BPF program's",
            "\t * callback:",
            "\t *",
            "\t *    __bpf_user_ringbuf_peek() // lock acquired",
            "\t * -> program callback_fn()",
            "\t * -> __bpf_user_ringbuf_sample_release() // lock released",
            "\t *",
            "\t * It is unsafe and incorrect to hold an IRQ spinlock across what could",
            "\t * be a long execution window, so we instead simply disallow concurrent",
            "\t * access to the ring buffer by kernel consumers, and return -EBUSY from",
            "\t * __bpf_user_ringbuf_peek() if the busy bit is held by another task.",
            "\t */",
            "\tatomic_t busy ____cacheline_aligned_in_smp;",
            "\t/* Consumer and producer counters are put into separate pages to",
            "\t * allow each position to be mapped with different permissions.",
            "\t * This prevents a user-space application from modifying the",
            "\t * position and ruining in-kernel tracking. The permissions of the",
            "\t * pages depend on who is producing samples: user-space or the",
            "\t * kernel. Note that the pending counter is placed in the same",
            "\t * page as the producer, so that it shares the same cache line.",
            "\t *",
            "\t * Kernel-producer",
            "\t * ---------------",
            "\t * The producer position and data pages are mapped as r/o in",
            "\t * userspace. For this approach, bits in the header of samples are",
            "\t * used to signal to user-space, and to other producers, whether a",
            "\t * sample is currently being written.",
            "\t *",
            "\t * User-space producer",
            "\t * -------------------",
            "\t * Only the page containing the consumer position is mapped r/o in",
            "\t * user-space. User-space producers also use bits of the header to",
            "\t * communicate to the kernel, but the kernel must carefully check and",
            "\t * validate each sample to ensure that they're correctly formatted, and",
            "\t * fully contained within the ring buffer.",
            "\t */",
            "\tunsigned long consumer_pos __aligned(PAGE_SIZE);",
            "\tunsigned long producer_pos __aligned(PAGE_SIZE);",
            "\tunsigned long pending_pos;",
            "\tchar data[] __aligned(PAGE_SIZE);",
            "};",
            "",
            "struct bpf_ringbuf_map {",
            "\tstruct bpf_map map;",
            "\tstruct bpf_ringbuf *rb;",
            "};",
            "",
            "/* 8-byte ring buffer record header structure */",
            "struct bpf_ringbuf_hdr {",
            "\tu32 len;",
            "\tu32 pg_off;",
            "};",
            "",
            "static struct bpf_ringbuf *bpf_ringbuf_area_alloc(size_t data_sz, int numa_node)",
            "{",
            "\tconst gfp_t flags = GFP_KERNEL_ACCOUNT | __GFP_RETRY_MAYFAIL |",
            "\t\t\t    __GFP_NOWARN | __GFP_ZERO;",
            "\tint nr_meta_pages = RINGBUF_NR_META_PAGES;",
            "\tint nr_data_pages = data_sz >> PAGE_SHIFT;",
            "\tint nr_pages = nr_meta_pages + nr_data_pages;",
            "\tstruct page **pages, *page;",
            "\tstruct bpf_ringbuf *rb;",
            "\tsize_t array_size;",
            "\tint i;",
            "",
            "\t/* Each data page is mapped twice to allow \"virtual\"",
            "\t * continuous read of samples wrapping around the end of ring",
            "\t * buffer area:",
            "\t * ------------------------------------------------------",
            "\t * | meta pages |  real data pages  |  same data pages  |",
            "\t * ------------------------------------------------------",
            "\t * |            | 1 2 3 4 5 6 7 8 9 | 1 2 3 4 5 6 7 8 9 |",
            "\t * ------------------------------------------------------",
            "\t * |            | TA             DA | TA             DA |",
            "\t * ------------------------------------------------------",
            "\t *                               ^^^^^^^",
            "\t *                                  |",
            "\t * Here, no need to worry about special handling of wrapped-around",
            "\t * data due to double-mapped data pages. This works both in kernel and",
            "\t * when mmap()'ed in user-space, simplifying both kernel and",
            "\t * user-space implementations significantly.",
            "\t */",
            "\tarray_size = (nr_meta_pages + 2 * nr_data_pages) * sizeof(*pages);",
            "\tpages = bpf_map_area_alloc(array_size, numa_node);",
            "\tif (!pages)",
            "\t\treturn NULL;",
            "",
            "\tfor (i = 0; i < nr_pages; i++) {",
            "\t\tpage = alloc_pages_node(numa_node, flags, 0);",
            "\t\tif (!page) {",
            "\t\t\tnr_pages = i;",
            "\t\t\tgoto err_free_pages;",
            "\t\t}",
            "\t\tpages[i] = page;",
            "\t\tif (i >= nr_meta_pages)",
            "\t\t\tpages[nr_data_pages + i] = page;",
            "\t}",
            "",
            "\trb = vmap(pages, nr_meta_pages + 2 * nr_data_pages,",
            "\t\t  VM_MAP | VM_USERMAP, PAGE_KERNEL);",
            "\tif (rb) {",
            "\t\tkmemleak_not_leak(pages);",
            "\t\trb->pages = pages;",
            "\t\trb->nr_pages = nr_pages;",
            "\t\treturn rb;",
            "\t}",
            "",
            "err_free_pages:",
            "\tfor (i = 0; i < nr_pages; i++)",
            "\t\t__free_page(pages[i]);",
            "\tbpf_map_area_free(pages);",
            "\treturn NULL;",
            "}",
            ""
          ],
          "function_name": null,
          "description": "定义了bpf_ringbuf结构体及其相关宏，用于管理BPF环形缓冲区的元数据和数据区域。通过页面数组实现环形缓冲区的虚拟连续读取，支持用户态和内核态生产者的差异化权限控制，其中包含消费者/生产者位置指针、忙位原子变量及锁保护的元数据。",
          "similarity": 0.5912353992462158
        },
        {
          "chunk_id": 2,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 335,
          "end_line": 447,
          "content": [
            "static u64 ringbuf_map_mem_usage(const struct bpf_map *map)",
            "{",
            "\tstruct bpf_ringbuf *rb;",
            "\tint nr_data_pages;",
            "\tint nr_meta_pages;",
            "\tu64 usage = sizeof(struct bpf_ringbuf_map);",
            "",
            "\trb = container_of(map, struct bpf_ringbuf_map, map)->rb;",
            "\tusage += (u64)rb->nr_pages << PAGE_SHIFT;",
            "\tnr_meta_pages = RINGBUF_NR_META_PAGES;",
            "\tnr_data_pages = map->max_entries >> PAGE_SHIFT;",
            "\tusage += (nr_meta_pages + 2 * nr_data_pages) * sizeof(struct page *);",
            "\treturn usage;",
            "}",
            "static size_t bpf_ringbuf_rec_pg_off(struct bpf_ringbuf *rb,",
            "\t\t\t\t     struct bpf_ringbuf_hdr *hdr)",
            "{",
            "\treturn ((void *)hdr - (void *)rb) >> PAGE_SHIFT;",
            "}",
            "static void bpf_ringbuf_commit(void *sample, u64 flags, bool discard)",
            "{",
            "\tunsigned long rec_pos, cons_pos;",
            "\tstruct bpf_ringbuf_hdr *hdr;",
            "\tstruct bpf_ringbuf *rb;",
            "\tu32 new_len;",
            "",
            "\thdr = sample - BPF_RINGBUF_HDR_SZ;",
            "\trb = bpf_ringbuf_restore_from_rec(hdr);",
            "\tnew_len = hdr->len ^ BPF_RINGBUF_BUSY_BIT;",
            "\tif (discard)",
            "\t\tnew_len |= BPF_RINGBUF_DISCARD_BIT;",
            "",
            "\t/* update record header with correct final size prefix */",
            "\txchg(&hdr->len, new_len);",
            "",
            "\t/* if consumer caught up and is waiting for our record, notify about",
            "\t * new data availability",
            "\t */",
            "\trec_pos = (void *)hdr - (void *)rb->data;",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos) & rb->mask;",
            "",
            "\tif (flags & BPF_RB_FORCE_WAKEUP)",
            "\t\tirq_work_queue(&rb->work);",
            "\telse if (cons_pos == rec_pos && !(flags & BPF_RB_NO_WAKEUP))",
            "\t\tirq_work_queue(&rb->work);",
            "}",
            "static int __bpf_user_ringbuf_peek(struct bpf_ringbuf *rb, void **sample, u32 *size)",
            "{",
            "\tint err;",
            "\tu32 hdr_len, sample_len, total_len, flags, *hdr;",
            "\tu64 cons_pos, prod_pos;",
            "",
            "\t/* Synchronizes with smp_store_release() in user-space producer. */",
            "\tprod_pos = smp_load_acquire(&rb->producer_pos);",
            "\tif (prod_pos % 8)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* Synchronizes with smp_store_release() in __bpf_user_ringbuf_sample_release() */",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos);",
            "\tif (cons_pos >= prod_pos)",
            "\t\treturn -ENODATA;",
            "",
            "\thdr = (u32 *)((uintptr_t)rb->data + (uintptr_t)(cons_pos & rb->mask));",
            "\t/* Synchronizes with smp_store_release() in user-space producer. */",
            "\thdr_len = smp_load_acquire(hdr);",
            "\tflags = hdr_len & (BPF_RINGBUF_BUSY_BIT | BPF_RINGBUF_DISCARD_BIT);",
            "\tsample_len = hdr_len & ~flags;",
            "\ttotal_len = round_up(sample_len + BPF_RINGBUF_HDR_SZ, 8);",
            "",
            "\t/* The sample must fit within the region advertised by the producer position. */",
            "\tif (total_len > prod_pos - cons_pos)",
            "\t\treturn -EINVAL;",
            "",
            "\t/* The sample must fit within the data region of the ring buffer. */",
            "\tif (total_len > ringbuf_total_data_sz(rb))",
            "\t\treturn -E2BIG;",
            "",
            "\t/* The sample must fit into a struct bpf_dynptr. */",
            "\terr = bpf_dynptr_check_size(sample_len);",
            "\tif (err)",
            "\t\treturn -E2BIG;",
            "",
            "\tif (flags & BPF_RINGBUF_DISCARD_BIT) {",
            "\t\t/* If the discard bit is set, the sample should be skipped.",
            "\t\t *",
            "\t\t * Update the consumer pos, and return -EAGAIN so the caller",
            "\t\t * knows to skip this sample and try to read the next one.",
            "\t\t */",
            "\t\tsmp_store_release(&rb->consumer_pos, cons_pos + total_len);",
            "\t\treturn -EAGAIN;",
            "\t}",
            "",
            "\tif (flags & BPF_RINGBUF_BUSY_BIT)",
            "\t\treturn -ENODATA;",
            "",
            "\t*sample = (void *)((uintptr_t)rb->data +",
            "\t\t\t   (uintptr_t)((cons_pos + BPF_RINGBUF_HDR_SZ) & rb->mask));",
            "\t*size = sample_len;",
            "\treturn 0;",
            "}",
            "static void __bpf_user_ringbuf_sample_release(struct bpf_ringbuf *rb, size_t size, u64 flags)",
            "{",
            "\tu64 consumer_pos;",
            "\tu32 rounded_size = round_up(size + BPF_RINGBUF_HDR_SZ, 8);",
            "",
            "\t/* Using smp_load_acquire() is unnecessary here, as the busy-bit",
            "\t * prevents another task from writing to consumer_pos after it was read",
            "\t * by this task with smp_load_acquire() in __bpf_user_ringbuf_peek().",
            "\t */",
            "\tconsumer_pos = rb->consumer_pos;",
            "\t /* Synchronizes with smp_load_acquire() in user-space producer. */",
            "\tsmp_store_release(&rb->consumer_pos, consumer_pos + rounded_size);",
            "}"
          ],
          "function_name": "ringbuf_map_mem_usage, bpf_ringbuf_rec_pg_off, bpf_ringbuf_commit, __bpf_user_ringbuf_peek, __bpf_user_ringbuf_sample_release",
          "description": "提供了环形缓冲区的内存占用统计、记录位置转换、样本提交及消费操作。包含用户态生产者与消费者的同步机制，通过忙位防止竞态条件，确保样本数据完整性校验和消费进度更新的有序性。",
          "similarity": 0.5373861789703369
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/bpf/ringbuf.c",
          "start_line": 150,
          "end_line": 258,
          "content": [
            "static void bpf_ringbuf_notify(struct irq_work *work)",
            "{",
            "\tstruct bpf_ringbuf *rb = container_of(work, struct bpf_ringbuf, work);",
            "",
            "\twake_up_all(&rb->waitq);",
            "}",
            "static void bpf_ringbuf_free(struct bpf_ringbuf *rb)",
            "{",
            "\t/* copy pages pointer and nr_pages to local variable, as we are going",
            "\t * to unmap rb itself with vunmap() below",
            "\t */",
            "\tstruct page **pages = rb->pages;",
            "\tint i, nr_pages = rb->nr_pages;",
            "",
            "\tvunmap(rb);",
            "\tfor (i = 0; i < nr_pages; i++)",
            "\t\t__free_page(pages[i]);",
            "\tbpf_map_area_free(pages);",
            "}",
            "static void ringbuf_map_free(struct bpf_map *map)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tbpf_ringbuf_free(rb_map->rb);",
            "\tbpf_map_area_free(rb_map);",
            "}",
            "static long ringbuf_map_update_elem(struct bpf_map *map, void *key, void *value,",
            "\t\t\t\t    u64 flags)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static long ringbuf_map_delete_elem(struct bpf_map *map, void *key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int ringbuf_map_get_next_key(struct bpf_map *map, void *key,",
            "\t\t\t\t    void *next_key)",
            "{",
            "\treturn -ENOTSUPP;",
            "}",
            "static int ringbuf_map_mmap_kern(struct bpf_map *map, struct vm_area_struct *vma)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "",
            "\tif (vma->vm_flags & VM_WRITE) {",
            "\t\t/* allow writable mapping for the consumer_pos only */",
            "\t\tif (vma->vm_pgoff != 0 || vma->vm_end - vma->vm_start != PAGE_SIZE)",
            "\t\t\treturn -EPERM;",
            "\t}",
            "\t/* remap_vmalloc_range() checks size and offset constraints */",
            "\treturn remap_vmalloc_range(vma, rb_map->rb,",
            "\t\t\t\t   vma->vm_pgoff + RINGBUF_PGOFF);",
            "}",
            "static int ringbuf_map_mmap_user(struct bpf_map *map, struct vm_area_struct *vma)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "",
            "\tif (vma->vm_flags & VM_WRITE) {",
            "\t\tif (vma->vm_pgoff == 0)",
            "\t\t\t/* Disallow writable mappings to the consumer pointer,",
            "\t\t\t * and allow writable mappings to both the producer",
            "\t\t\t * position, and the ring buffer data itself.",
            "\t\t\t */",
            "\t\t\treturn -EPERM;",
            "\t}",
            "\t/* remap_vmalloc_range() checks size and offset constraints */",
            "\treturn remap_vmalloc_range(vma, rb_map->rb, vma->vm_pgoff + RINGBUF_PGOFF);",
            "}",
            "static unsigned long ringbuf_avail_data_sz(struct bpf_ringbuf *rb)",
            "{",
            "\tunsigned long cons_pos, prod_pos;",
            "",
            "\tcons_pos = smp_load_acquire(&rb->consumer_pos);",
            "\tprod_pos = smp_load_acquire(&rb->producer_pos);",
            "\treturn prod_pos - cons_pos;",
            "}",
            "static u32 ringbuf_total_data_sz(const struct bpf_ringbuf *rb)",
            "{",
            "\treturn rb->mask + 1;",
            "}",
            "static __poll_t ringbuf_map_poll_kern(struct bpf_map *map, struct file *filp,",
            "\t\t\t\t      struct poll_table_struct *pts)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tpoll_wait(filp, &rb_map->rb->waitq, pts);",
            "",
            "\tif (ringbuf_avail_data_sz(rb_map->rb))",
            "\t\treturn EPOLLIN | EPOLLRDNORM;",
            "\treturn 0;",
            "}",
            "static __poll_t ringbuf_map_poll_user(struct bpf_map *map, struct file *filp,",
            "\t\t\t\t      struct poll_table_struct *pts)",
            "{",
            "\tstruct bpf_ringbuf_map *rb_map;",
            "",
            "\trb_map = container_of(map, struct bpf_ringbuf_map, map);",
            "\tpoll_wait(filp, &rb_map->rb->waitq, pts);",
            "",
            "\tif (ringbuf_avail_data_sz(rb_map->rb) < ringbuf_total_data_sz(rb_map->rb))",
            "\t\treturn EPOLLOUT | EPOLLWRNORM;",
            "\treturn 0;",
            "}"
          ],
          "function_name": "bpf_ringbuf_notify, bpf_ringbuf_free, ringbuf_map_free, ringbuf_map_update_elem, ringbuf_map_delete_elem, ringbuf_map_get_next_key, ringbuf_map_mmap_kern, ringbuf_map_mmap_user, ringbuf_avail_data_sz, ringbuf_total_data_sz, ringbuf_map_poll_kern, ringbuf_map_poll_user",
          "description": "实现了环形缓冲区的事件通知、资源释放、内存映射控制及I/O监控功能。包含针对用户态和内核态的差异化mmap处理逻辑，通过spinlock和atomic_t实现并发控制，提供poll接口检测缓冲区可用数据状态。",
          "similarity": 0.5100575685501099
        }
      ]
    }
  ]
}