{
  "query": "内核级线程调度器实现方式",
  "timestamp": "2025-12-26 00:49:54",
  "retrieved_files": [
    {
      "source_file": "kernel/workqueue.c",
      "md_summary": "> 自动生成时间: 2025-10-25 17:53:20\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `workqueue.c`\n\n---\n\n# workqueue.c 技术文档\n\n## 1. 文件概述\n\n`workqueue.c` 是 Linux 内核中实现通用异步执行机制的核心文件，提供基于共享工作线程池（worker pool）的延迟任务调度功能。工作项（work items）在进程上下文中执行，支持 CPU 绑定和非绑定两种模式。每个 CPU 默认拥有两个标准工作池（普通优先级和高优先级），同时支持动态创建非绑定工作池以满足不同工作队列的需求。该机制替代了早期的 taskqueue/keventd 实现，具有更高的可扩展性和资源利用率。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct worker_pool`**  \n  工作线程池结构体，管理一组工作线程（workers），包含：\n  - `lock`：保护池状态的自旋锁\n  - `cpu` / `node`：关联的 CPU 和 NUMA 节点（绑定池）\n  - `worklist`：待处理工作项队列\n  - `idle_list` / `busy_hash`：空闲和忙碌工作线程的管理结构\n  - `nr_workers` / `nr_idle`：工作线程数量统计\n  - `attrs`：工作线程属性（如优先级、CPU 亲和性）\n  - `mayday_timer`：紧急情况下的救援请求定时器\n\n- **`struct pool_workqueue`**  \n  工作队列与工作池之间的关联结构，每个工作队列在每个池中都有一个对应的 `pool_workqueue` 实例，用于：\n  - 管理工作项的入队和执行\n  - 实现 `max_active` 限制（控制并发执行数）\n  - 支持 flush 操作（等待所有工作完成）\n  - 统计性能指标（如启动/完成次数、CPU 时间等）\n\n- **`struct worker`**（定义在 `workqueue_internal.h`）  \n  工作线程的运行时上下文，包含状态标志（如 `WORKER_IDLE`, `WORKER_UNBOUND`）、当前执行的工作项等。\n\n### 关键枚举与常量\n\n- **池/工作线程标志**：\n  - `POOL_DISASSOCIATED`：CPU 离线时池进入非绑定状态\n  - `WORKER_UNBOUND`：工作线程可在任意 CPU 上运行\n  - `WORKER_CPU_INTENSIVE`：标记 CPU 密集型任务，影响并发控制\n\n- **配置参数**：\n  - `NR_STD_WORKER_POOLS = 2`：每 CPU 标准池数量（普通 + 高优先级）\n  - `IDLE_WORKER_TIMEOUT = 300 * HZ`：空闲线程保留时间（5 分钟）\n  - `MAYDAY_INITIAL_TIMEOUT`：工作积压时触发救援的延迟（10ms）\n\n- **统计指标**（`pool_workqueue_stats`）：\n  - `PWQ_STAT_STARTED` / `PWQ_STAT_COMPLETED`：工作项执行统计\n  - `PWQ_STAT_MAYDAY` / `PWQ_STAT_RESCUED`：紧急救援事件计数\n\n## 3. 关键实现\n\n### 工作池管理\n- **绑定池（Bound Pool）**：与特定 CPU 关联，工作线程默认绑定到该 CPU。当 CPU 离线时，池进入 `DISASSOCIATED` 状态，工作线程转为非绑定模式。\n- **非绑定池（Unbound Pool）**：动态创建，通过哈希表（`unbound_pool_hash`）按属性（`workqueue_attrs`）去重，支持跨 CPU 调度。\n- **并发控制**：通过 `nr_running` 计数器和 `max_active` 限制，防止工作项过度并发执行。\n\n### 工作线程生命周期\n- **空闲管理**：空闲线程加入 `idle_list`，超时（`IDLE_WORKER_TIMEOUT`）后被回收。\n- **动态伸缩**：当工作积压时，通过 `mayday_timer` 触发新线程创建；若创建失败，向全局救援线程（rescuer）求助。\n- **状态标志**：使用位标志（如 `WORKER_IDLE`, `WORKER_PREP`）高效管理线程状态，避免锁竞争。\n\n### 内存与同步\n- **RCU 保护**：工作池销毁通过 RCU 延迟释放，确保 `get_work_pool()` 等读取路径无锁安全。\n- **锁分层**：\n  - `pool->lock`（自旋锁）：保护池内部状态\n  - `wq_pool_mutex`：全局池管理互斥锁\n  - `wq_pool_attach_mutex`：防止 CPU 绑定状态变更冲突\n\n### 工作项调度\n- **数据指针复用**：`work_struct->data` 的高有效位存储 `pool_workqueue` 指针，低有效位用于标志位（如 `WORK_STRUCT_INACTIVE`）。\n- **优先级支持**：高优先级工作池使用 `HIGHPRI_NICE_LEVEL = MIN_NICE` 提升调度优先级。\n\n## 4. 依赖关系\n\n- **内核子系统**：\n  - **调度器**（`<linux/sched.h>`）：创建工作线程（kworker），管理 CPU 亲和性\n  - **内存管理**（`<linux/slab.h>`）：分配工作池、工作队列等结构\n  - **CPU 热插拔**（`<linux/cpu.h>`）：处理 CPU 上下线时的池绑定状态切换\n  - **RCU**（`<linux/rculist.h>`）：实现无锁读取路径\n  - **定时器**（`<linux/timer.h>`）：实现空闲超时和救援机制\n\n- **内部依赖**：\n  - `workqueue_internal.h`：定义 `struct worker` 等内部结构\n  - `Documentation/core-api/workqueue.rst`：详细设计文档\n\n## 5. 使用场景\n\n- **驱动程序延迟操作**：硬件中断后调度下半部处理（如网络包处理、磁盘 I/O 完成回调）。\n- **内核子系统异步任务**：文件系统元数据更新、内存回收、电源管理状态切换。\n- **高优先级任务**：使用 `WQ_HIGHPRI` 标志创建工作队列，确保关键任务及时执行（如死锁恢复）。\n- **CPU 密集型任务**：标记 `WQ_CPU_INTENSIVE` 避免占用过多并发槽位，提升系统响应性。\n- **NUMA 感知调度**：非绑定工作队列可指定 NUMA 节点，优化内存访问延迟。",
      "similarity": 0.6252987384796143,
      "chunks": [
        {
          "chunk_id": 33,
          "file_path": "kernel/workqueue.c",
          "start_line": 6492,
          "end_line": 6592,
          "content": [
            "static void panic_on_wq_watchdog(void)",
            "{",
            "\tstatic unsigned int wq_stall;",
            "",
            "\tif (wq_panic_on_stall) {",
            "\t\twq_stall++;",
            "\t\tBUG_ON(wq_stall >= wq_panic_on_stall);",
            "\t}",
            "}",
            "static void wq_watchdog_reset_touched(void)",
            "{",
            "\tint cpu;",
            "",
            "\twq_watchdog_touched = jiffies;",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(wq_watchdog_touched_cpu, cpu) = jiffies;",
            "}",
            "static void wq_watchdog_timer_fn(struct timer_list *unused)",
            "{",
            "\tunsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;",
            "\tbool lockup_detected = false;",
            "\tbool cpu_pool_stall = false;",
            "\tunsigned long now = jiffies;",
            "\tstruct worker_pool *pool;",
            "\tint pi;",
            "",
            "\tif (!thresh)",
            "\t\treturn;",
            "",
            "\trcu_read_lock();",
            "",
            "\tfor_each_pool(pool, pi) {",
            "\t\tunsigned long pool_ts, touched, ts;",
            "",
            "\t\tpool->cpu_stall = false;",
            "\t\tif (list_empty(&pool->worklist))",
            "\t\t\tcontinue;",
            "",
            "\t\t/*",
            "\t\t * If a virtual machine is stopped by the host it can look to",
            "\t\t * the watchdog like a stall.",
            "\t\t */",
            "\t\tkvm_check_and_clear_guest_paused();",
            "",
            "\t\t/* get the latest of pool and touched timestamps */",
            "\t\tif (pool->cpu >= 0)",
            "\t\t\ttouched = READ_ONCE(per_cpu(wq_watchdog_touched_cpu, pool->cpu));",
            "\t\telse",
            "\t\t\ttouched = READ_ONCE(wq_watchdog_touched);",
            "\t\tpool_ts = READ_ONCE(pool->watchdog_ts);",
            "",
            "\t\tif (time_after(pool_ts, touched))",
            "\t\t\tts = pool_ts;",
            "\t\telse",
            "\t\t\tts = touched;",
            "",
            "\t\t/* did we stall? */",
            "\t\tif (time_after(now, ts + thresh)) {",
            "\t\t\tlockup_detected = true;",
            "\t\t\tif (pool->cpu >= 0) {",
            "\t\t\t\tpool->cpu_stall = true;",
            "\t\t\t\tcpu_pool_stall = true;",
            "\t\t\t}",
            "\t\t\tpr_emerg(\"BUG: workqueue lockup - pool\");",
            "\t\t\tpr_cont_pool_info(pool);",
            "\t\t\tpr_cont(\" stuck for %us!\\n\",",
            "\t\t\t\tjiffies_to_msecs(now - pool_ts) / 1000);",
            "\t\t}",
            "",
            "",
            "\t}",
            "",
            "\trcu_read_unlock();",
            "",
            "\tif (lockup_detected)",
            "\t\tshow_all_workqueues();",
            "",
            "\tif (cpu_pool_stall)",
            "\t\tshow_cpu_pools_hogs();",
            "",
            "\tif (lockup_detected)",
            "\t\tpanic_on_wq_watchdog();",
            "",
            "\twq_watchdog_reset_touched();",
            "\tmod_timer(&wq_watchdog_timer, jiffies + thresh);",
            "}",
            "notrace void wq_watchdog_touch(int cpu)",
            "{",
            "\tunsigned long thresh = READ_ONCE(wq_watchdog_thresh) * HZ;",
            "\tunsigned long touch_ts = READ_ONCE(wq_watchdog_touched);",
            "\tunsigned long now = jiffies;",
            "",
            "\tif (cpu >= 0)",
            "\t\tper_cpu(wq_watchdog_touched_cpu, cpu) = now;",
            "\telse",
            "\t\tWARN_ONCE(1, \"%s should be called with valid CPU\", __func__);",
            "",
            "\t/* Don't unnecessarily store to global cacheline */",
            "\tif (time_after(now, touch_ts + thresh / 4))",
            "\t\tWRITE_ONCE(wq_watchdog_touched, jiffies);",
            "}"
          ],
          "function_name": "panic_on_wq_watchdog, wq_watchdog_reset_touched, wq_watchdog_timer_fn, wq_watchdog_touch",
          "description": "实现工作队列看门狗机制，通过定时器周期性检测任务阻塞状态，当检测到CPU池超时时触发警告日志和panic，包含超时阈值管理、时间戳更新及阻塞状态标识逻辑。",
          "similarity": 0.5815500617027283
        },
        {
          "chunk_id": 8,
          "file_path": "kernel/workqueue.c",
          "start_line": 1832,
          "end_line": 1933,
          "content": [
            "bool queue_work_on(int cpu, struct workqueue_struct *wq,",
            "\t\t   struct work_struct *work)",
            "{",
            "\tbool ret = false;",
            "\tunsigned long flags;",
            "",
            "\tlocal_irq_save(flags);",
            "",
            "\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {",
            "\t\t__queue_work(cpu, wq, work);",
            "\t\tret = true;",
            "\t}",
            "",
            "\tlocal_irq_restore(flags);",
            "\treturn ret;",
            "}",
            "static int select_numa_node_cpu(int node)",
            "{",
            "\tint cpu;",
            "",
            "\t/* Delay binding to CPU if node is not valid or online */",
            "\tif (node < 0 || node >= MAX_NUMNODES || !node_online(node))",
            "\t\treturn WORK_CPU_UNBOUND;",
            "",
            "\t/* Use local node/cpu if we are already there */",
            "\tcpu = raw_smp_processor_id();",
            "\tif (node == cpu_to_node(cpu))",
            "\t\treturn cpu;",
            "",
            "\t/* Use \"random\" otherwise know as \"first\" online CPU of node */",
            "\tcpu = cpumask_any_and(cpumask_of_node(node), cpu_online_mask);",
            "",
            "\t/* If CPU is valid return that, otherwise just defer */",
            "\treturn cpu < nr_cpu_ids ? cpu : WORK_CPU_UNBOUND;",
            "}",
            "bool queue_work_node(int node, struct workqueue_struct *wq,",
            "\t\t     struct work_struct *work)",
            "{",
            "\tunsigned long flags;",
            "\tbool ret = false;",
            "",
            "\t/*",
            "\t * This current implementation is specific to unbound workqueues.",
            "\t * Specifically we only return the first available CPU for a given",
            "\t * node instead of cycling through individual CPUs within the node.",
            "\t *",
            "\t * If this is used with a per-cpu workqueue then the logic in",
            "\t * workqueue_select_cpu_near would need to be updated to allow for",
            "\t * some round robin type logic.",
            "\t */",
            "\tWARN_ON_ONCE(!(wq->flags & WQ_UNBOUND));",
            "",
            "\tlocal_irq_save(flags);",
            "",
            "\tif (!test_and_set_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))) {",
            "\t\tint cpu = select_numa_node_cpu(node);",
            "",
            "\t\t__queue_work(cpu, wq, work);",
            "\t\tret = true;",
            "\t}",
            "",
            "\tlocal_irq_restore(flags);",
            "\treturn ret;",
            "}",
            "void delayed_work_timer_fn(struct timer_list *t)",
            "{",
            "\tstruct delayed_work *dwork = from_timer(dwork, t, timer);",
            "",
            "\t/* should have been called from irqsafe timer with irq already off */",
            "\t__queue_work(dwork->cpu, dwork->wq, &dwork->work);",
            "}",
            "static void __queue_delayed_work(int cpu, struct workqueue_struct *wq,",
            "\t\t\t\tstruct delayed_work *dwork, unsigned long delay)",
            "{",
            "\tstruct timer_list *timer = &dwork->timer;",
            "\tstruct work_struct *work = &dwork->work;",
            "",
            "\tWARN_ON_ONCE(!wq);",
            "\tWARN_ON_ONCE(timer->function != delayed_work_timer_fn);",
            "\tWARN_ON_ONCE(timer_pending(timer));",
            "\tWARN_ON_ONCE(!list_empty(&work->entry));",
            "",
            "\t/*",
            "\t * If @delay is 0, queue @dwork->work immediately.  This is for",
            "\t * both optimization and correctness.  The earliest @timer can",
            "\t * expire is on the closest next tick and delayed_work users depend",
            "\t * on that there's no such delay when @delay is 0.",
            "\t */",
            "\tif (!delay) {",
            "\t\t__queue_work(cpu, wq, &dwork->work);",
            "\t\treturn;",
            "\t}",
            "",
            "\tdwork->wq = wq;",
            "\tdwork->cpu = cpu;",
            "\ttimer->expires = jiffies + delay;",
            "",
            "\tif (unlikely(cpu != WORK_CPU_UNBOUND))",
            "\t\tadd_timer_on(timer, cpu);",
            "\telse",
            "\t\tadd_timer(timer);",
            "}"
          ],
          "function_name": "queue_work_on, select_numa_node_cpu, queue_work_node, delayed_work_timer_fn, __queue_delayed_work",
          "description": "该代码块实现基于NUMA节点的延迟工作调度。queue_work_on指定CPU提交工作；select_numa_node_cpu选择节点对应的CPU；delayed_work_timer_fn作为延迟工作超时时的回调；__queue_delayed_work设置定时器并安排工作项执行。",
          "similarity": 0.5687105655670166
        },
        {
          "chunk_id": 30,
          "file_path": "kernel/workqueue.c",
          "start_line": 6019,
          "end_line": 6134,
          "content": [
            "static void apply_wqattrs_unlock(void)",
            "{",
            "\tmutex_unlock(&wq_pool_mutex);",
            "\tcpus_read_unlock();",
            "}",
            "static ssize_t wq_nice_show(struct device *dev, struct device_attribute *attr,",
            "\t\t\t    char *buf)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "\tint written;",
            "",
            "\tmutex_lock(&wq->mutex);",
            "\twritten = scnprintf(buf, PAGE_SIZE, \"%d\\n\", wq->unbound_attrs->nice);",
            "\tmutex_unlock(&wq->mutex);",
            "",
            "\treturn written;",
            "}",
            "static ssize_t wq_nice_store(struct device *dev, struct device_attribute *attr,",
            "\t\t\t     const char *buf, size_t count)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "\tstruct workqueue_attrs *attrs;",
            "\tint ret = -ENOMEM;",
            "",
            "\tapply_wqattrs_lock();",
            "",
            "\tattrs = wq_sysfs_prep_attrs(wq);",
            "\tif (!attrs)",
            "\t\tgoto out_unlock;",
            "",
            "\tif (sscanf(buf, \"%d\", &attrs->nice) == 1 &&",
            "\t    attrs->nice >= MIN_NICE && attrs->nice <= MAX_NICE)",
            "\t\tret = apply_workqueue_attrs_locked(wq, attrs);",
            "\telse",
            "\t\tret = -EINVAL;",
            "",
            "out_unlock:",
            "\tapply_wqattrs_unlock();",
            "\tfree_workqueue_attrs(attrs);",
            "\treturn ret ?: count;",
            "}",
            "static ssize_t wq_cpumask_show(struct device *dev,",
            "\t\t\t       struct device_attribute *attr, char *buf)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "\tint written;",
            "",
            "\tmutex_lock(&wq->mutex);",
            "\twritten = scnprintf(buf, PAGE_SIZE, \"%*pb\\n\",",
            "\t\t\t    cpumask_pr_args(wq->unbound_attrs->cpumask));",
            "\tmutex_unlock(&wq->mutex);",
            "\treturn written;",
            "}",
            "static ssize_t wq_cpumask_store(struct device *dev,",
            "\t\t\t\tstruct device_attribute *attr,",
            "\t\t\t\tconst char *buf, size_t count)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "\tstruct workqueue_attrs *attrs;",
            "\tint ret = -ENOMEM;",
            "",
            "\tapply_wqattrs_lock();",
            "",
            "\tattrs = wq_sysfs_prep_attrs(wq);",
            "\tif (!attrs)",
            "\t\tgoto out_unlock;",
            "",
            "\tret = cpumask_parse(buf, attrs->cpumask);",
            "\tif (!ret)",
            "\t\tret = apply_workqueue_attrs_locked(wq, attrs);",
            "",
            "out_unlock:",
            "\tapply_wqattrs_unlock();",
            "\tfree_workqueue_attrs(attrs);",
            "\treturn ret ?: count;",
            "}",
            "static ssize_t wq_affn_scope_show(struct device *dev,",
            "\t\t\t\t  struct device_attribute *attr, char *buf)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "\tint written;",
            "",
            "\tmutex_lock(&wq->mutex);",
            "\tif (wq->unbound_attrs->affn_scope == WQ_AFFN_DFL)",
            "\t\twritten = scnprintf(buf, PAGE_SIZE, \"%s (%s)\\n\",",
            "\t\t\t\t    wq_affn_names[WQ_AFFN_DFL],",
            "\t\t\t\t    wq_affn_names[wq_affn_dfl]);",
            "\telse",
            "\t\twritten = scnprintf(buf, PAGE_SIZE, \"%s\\n\",",
            "\t\t\t\t    wq_affn_names[wq->unbound_attrs->affn_scope]);",
            "\tmutex_unlock(&wq->mutex);",
            "",
            "\treturn written;",
            "}",
            "static ssize_t wq_affn_scope_store(struct device *dev,",
            "\t\t\t\t   struct device_attribute *attr,",
            "\t\t\t\t   const char *buf, size_t count)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "\tstruct workqueue_attrs *attrs;",
            "\tint affn, ret = -ENOMEM;",
            "",
            "\taffn = parse_affn_scope(buf);",
            "\tif (affn < 0)",
            "\t\treturn affn;",
            "",
            "\tapply_wqattrs_lock();",
            "\tattrs = wq_sysfs_prep_attrs(wq);",
            "\tif (attrs) {",
            "\t\tattrs->affn_scope = affn;",
            "\t\tret = apply_workqueue_attrs_locked(wq, attrs);",
            "\t}",
            "\tapply_wqattrs_unlock();",
            "\tfree_workqueue_attrs(attrs);",
            "\treturn ret ?: count;",
            "}"
          ],
          "function_name": "apply_wqattrs_unlock, wq_nice_show, wq_nice_store, wq_cpumask_show, wq_cpumask_store, wq_affn_scope_show, wq_affn_scope_store",
          "description": "实现工作队列属性的读写接口，通过device attribute接口暴露nice值、CPU掩码及亲和范围配置，支持动态调整工作队列调度策略，包含互斥锁保护和属性应用逻辑。",
          "similarity": 0.5680526494979858
        },
        {
          "chunk_id": 29,
          "file_path": "kernel/workqueue.c",
          "start_line": 5864,
          "end_line": 5966,
          "content": [
            "int workqueue_unbound_exclude_cpumask(cpumask_var_t exclude_cpumask)",
            "{",
            "\tcpumask_var_t cpumask;",
            "\tint ret = 0;",
            "",
            "\tif (!zalloc_cpumask_var(&cpumask, GFP_KERNEL))",
            "\t\treturn -ENOMEM;",
            "",
            "\tlockdep_assert_cpus_held();",
            "\tmutex_lock(&wq_pool_mutex);",
            "",
            "\t/* Save the current isolated cpumask & export it via sysfs */",
            "\tcpumask_copy(wq_isolated_cpumask, exclude_cpumask);",
            "",
            "\t/*",
            "\t * If the operation fails, it will fall back to",
            "\t * wq_requested_unbound_cpumask which is initially set to",
            "\t * (HK_TYPE_WQ ∩ HK_TYPE_DOMAIN) house keeping mask and rewritten",
            "\t * by any subsequent write to workqueue/cpumask sysfs file.",
            "\t */",
            "\tif (!cpumask_andnot(cpumask, wq_requested_unbound_cpumask, exclude_cpumask))",
            "\t\tcpumask_copy(cpumask, wq_requested_unbound_cpumask);",
            "\tif (!cpumask_equal(cpumask, wq_unbound_cpumask))",
            "\t\tret = workqueue_apply_unbound_cpumask(cpumask);",
            "",
            "\tmutex_unlock(&wq_pool_mutex);",
            "\tfree_cpumask_var(cpumask);",
            "\treturn ret;",
            "}",
            "static int parse_affn_scope(const char *val)",
            "{",
            "\tint i;",
            "",
            "\tfor (i = 0; i < ARRAY_SIZE(wq_affn_names); i++) {",
            "\t\tif (!strncasecmp(val, wq_affn_names[i], strlen(wq_affn_names[i])))",
            "\t\t\treturn i;",
            "\t}",
            "\treturn -EINVAL;",
            "}",
            "static int wq_affn_dfl_set(const char *val, const struct kernel_param *kp)",
            "{",
            "\tstruct workqueue_struct *wq;",
            "\tint affn, cpu;",
            "",
            "\taffn = parse_affn_scope(val);",
            "\tif (affn < 0)",
            "\t\treturn affn;",
            "\tif (affn == WQ_AFFN_DFL)",
            "\t\treturn -EINVAL;",
            "",
            "\tcpus_read_lock();",
            "\tmutex_lock(&wq_pool_mutex);",
            "",
            "\twq_affn_dfl = affn;",
            "",
            "\tlist_for_each_entry(wq, &workqueues, list) {",
            "\t\tfor_each_online_cpu(cpu) {",
            "\t\t\twq_update_pod(wq, cpu, cpu, true);",
            "\t\t}",
            "\t}",
            "",
            "\tmutex_unlock(&wq_pool_mutex);",
            "\tcpus_read_unlock();",
            "",
            "\treturn 0;",
            "}",
            "static int wq_affn_dfl_get(char *buffer, const struct kernel_param *kp)",
            "{",
            "\treturn scnprintf(buffer, PAGE_SIZE, \"%s\\n\", wq_affn_names[wq_affn_dfl]);",
            "}",
            "static ssize_t per_cpu_show(struct device *dev, struct device_attribute *attr,",
            "\t\t\t    char *buf)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "",
            "\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", (bool)!(wq->flags & WQ_UNBOUND));",
            "}",
            "static ssize_t max_active_show(struct device *dev,",
            "\t\t\t       struct device_attribute *attr, char *buf)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "",
            "\treturn scnprintf(buf, PAGE_SIZE, \"%d\\n\", wq->saved_max_active);",
            "}",
            "static ssize_t max_active_store(struct device *dev,",
            "\t\t\t\tstruct device_attribute *attr, const char *buf,",
            "\t\t\t\tsize_t count)",
            "{",
            "\tstruct workqueue_struct *wq = dev_to_wq(dev);",
            "\tint val;",
            "",
            "\tif (sscanf(buf, \"%d\", &val) != 1 || val <= 0)",
            "\t\treturn -EINVAL;",
            "",
            "\tworkqueue_set_max_active(wq, val);",
            "\treturn count;",
            "}",
            "static void apply_wqattrs_lock(void)",
            "{",
            "\t/* CPUs should stay stable across pwq creations and installations */",
            "\tcpus_read_lock();",
            "\tmutex_lock(&wq_pool_mutex);",
            "}"
          ],
          "function_name": "workqueue_unbound_exclude_cpumask, parse_affn_scope, wq_affn_dfl_set, wq_affn_dfl_get, per_cpu_show, max_active_show, max_active_store, apply_wqattrs_lock",
          "description": "配置非绑定工作者的CPU排除掩码和默认亲和性策略，暴露工作队列属性供sysfs访问并管理最大并发数参数",
          "similarity": 0.5661500692367554
        },
        {
          "chunk_id": 7,
          "file_path": "kernel/workqueue.c",
          "start_line": 1686,
          "end_line": 1815,
          "content": [
            "static int wq_select_unbound_cpu(int cpu)",
            "{",
            "\tint new_cpu;",
            "",
            "\tif (likely(!wq_debug_force_rr_cpu)) {",
            "\t\tif (cpumask_test_cpu(cpu, wq_unbound_cpumask))",
            "\t\t\treturn cpu;",
            "\t} else {",
            "\t\tpr_warn_once(\"workqueue: round-robin CPU selection forced, expect performance impact\\n\");",
            "\t}",
            "",
            "\tnew_cpu = __this_cpu_read(wq_rr_cpu_last);",
            "\tnew_cpu = cpumask_next_and(new_cpu, wq_unbound_cpumask, cpu_online_mask);",
            "\tif (unlikely(new_cpu >= nr_cpu_ids)) {",
            "\t\tnew_cpu = cpumask_first_and(wq_unbound_cpumask, cpu_online_mask);",
            "\t\tif (unlikely(new_cpu >= nr_cpu_ids))",
            "\t\t\treturn cpu;",
            "\t}",
            "\t__this_cpu_write(wq_rr_cpu_last, new_cpu);",
            "",
            "\treturn new_cpu;",
            "}",
            "static void __queue_work(int cpu, struct workqueue_struct *wq,",
            "\t\t\t struct work_struct *work)",
            "{",
            "\tstruct pool_workqueue *pwq;",
            "\tstruct worker_pool *last_pool, *pool;",
            "\tunsigned int work_flags;",
            "\tunsigned int req_cpu = cpu;",
            "",
            "\t/*",
            "\t * While a work item is PENDING && off queue, a task trying to",
            "\t * steal the PENDING will busy-loop waiting for it to either get",
            "\t * queued or lose PENDING.  Grabbing PENDING and queueing should",
            "\t * happen with IRQ disabled.",
            "\t */",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "",
            "\t/*",
            "\t * For a draining wq, only works from the same workqueue are",
            "\t * allowed. The __WQ_DESTROYING helps to spot the issue that",
            "\t * queues a new work item to a wq after destroy_workqueue(wq).",
            "\t */",
            "\tif (unlikely(wq->flags & (__WQ_DESTROYING | __WQ_DRAINING) &&",
            "\t\t     WARN_ON_ONCE(!is_chained_work(wq))))",
            "\t\treturn;",
            "\trcu_read_lock();",
            "retry:",
            "\t/* pwq which will be used unless @work is executing elsewhere */",
            "\tif (req_cpu == WORK_CPU_UNBOUND) {",
            "\t\tif (wq->flags & WQ_UNBOUND)",
            "\t\t\tcpu = wq_select_unbound_cpu(raw_smp_processor_id());",
            "\t\telse",
            "\t\t\tcpu = raw_smp_processor_id();",
            "\t}",
            "",
            "\tpwq = rcu_dereference(*per_cpu_ptr(wq->cpu_pwq, cpu));",
            "\tpool = pwq->pool;",
            "",
            "\t/*",
            "\t * If @work was previously on a different pool, it might still be",
            "\t * running there, in which case the work needs to be queued on that",
            "\t * pool to guarantee non-reentrancy.",
            "\t */",
            "\tlast_pool = get_work_pool(work);",
            "\tif (last_pool && last_pool != pool) {",
            "\t\tstruct worker *worker;",
            "",
            "\t\traw_spin_lock(&last_pool->lock);",
            "",
            "\t\tworker = find_worker_executing_work(last_pool, work);",
            "",
            "\t\tif (worker && worker->current_pwq->wq == wq) {",
            "\t\t\tpwq = worker->current_pwq;",
            "\t\t\tpool = pwq->pool;",
            "\t\t\tWARN_ON_ONCE(pool != last_pool);",
            "\t\t} else {",
            "\t\t\t/* meh... not running there, queue here */",
            "\t\t\traw_spin_unlock(&last_pool->lock);",
            "\t\t\traw_spin_lock(&pool->lock);",
            "\t\t}",
            "\t} else {",
            "\t\traw_spin_lock(&pool->lock);",
            "\t}",
            "",
            "\t/*",
            "\t * pwq is determined and locked. For unbound pools, we could have raced",
            "\t * with pwq release and it could already be dead. If its refcnt is zero,",
            "\t * repeat pwq selection. Note that unbound pwqs never die without",
            "\t * another pwq replacing it in cpu_pwq or while work items are executing",
            "\t * on it, so the retrying is guaranteed to make forward-progress.",
            "\t */",
            "\tif (unlikely(!pwq->refcnt)) {",
            "\t\tif (wq->flags & WQ_UNBOUND) {",
            "\t\t\traw_spin_unlock(&pool->lock);",
            "\t\t\tcpu_relax();",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\t/* oops */",
            "\t\tWARN_ONCE(true, \"workqueue: per-cpu pwq for %s on cpu%d has 0 refcnt\",",
            "\t\t\t  wq->name, cpu);",
            "\t}",
            "",
            "\t/* pwq determined, queue */",
            "\ttrace_workqueue_queue_work(req_cpu, pwq, work);",
            "",
            "\tif (WARN_ON(!list_empty(&work->entry)))",
            "\t\tgoto out;",
            "",
            "\tpwq->nr_in_flight[pwq->work_color]++;",
            "\twork_flags = work_color_to_flags(pwq->work_color);",
            "",
            "\tif (likely(pwq->nr_active < pwq->max_active)) {",
            "\t\tif (list_empty(&pool->worklist))",
            "\t\t\tpool->watchdog_ts = jiffies;",
            "",
            "\t\ttrace_workqueue_activate_work(work);",
            "\t\tpwq->nr_active++;",
            "\t\tinsert_work(pwq, work, &pool->worklist, work_flags);",
            "\t\tkick_pool(pool);",
            "\t} else {",
            "\t\twork_flags |= WORK_STRUCT_INACTIVE;",
            "\t\tinsert_work(pwq, work, &pwq->inactive_works, work_flags);",
            "\t}",
            "",
            "out:",
            "\traw_spin_unlock(&pool->lock);",
            "\trcu_read_unlock();",
            "}"
          ],
          "function_name": "wq_select_unbound_cpu, __queue_work",
          "description": "该代码块处理工作项的CPU亲和性调度。wq_select_unbound_cpu选择非绑定CPU；__queue_work将工作项分配到对应worker池，处理NUMA节点绑定、工作项激活及负载均衡逻辑。",
          "similarity": 0.5644606947898865
        }
      ]
    },
    {
      "source_file": "kernel/stop_machine.c",
      "md_summary": "> 自动生成时间: 2025-10-25 16:30:03\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `stop_machine.c`\n\n---\n\n# `stop_machine.c` 技术文档\n\n## 1. 文件概述\n\n`stop_machine.c` 实现了 Linux 内核中用于在所有（或指定）CPU 上同步执行特定函数的机制，即 **stop_machine** 机制。该机制通过为每个 CPU 创建一个高优先级的内核线程（称为 stopper），在需要时唤醒这些线程以执行指定任务，并确保在执行期间其他任务无法抢占，从而实现对整个系统或部分 CPU 的“冻结”式同步操作。此机制常用于需要全局一致状态的关键内核操作，如 CPU 热插拔、模块加载、内核热补丁（livepatch）等。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct cpu_stop_done`**  \n  用于协调多个 CPU 上 stop 任务的完成状态，包含待完成任务计数（`nr_todo`）、返回值（`ret`）和完成信号量（`completion`）。\n\n- **`struct cpu_stopper`**  \n  每个 CPU 对应一个 stopper 实例，包含：\n  - `thread`：stopper 内核线程\n  - `lock`：保护 pending works 链表的自旋锁\n  - `enabled`：该 stopper 是否启用（对应 CPU 是否在线）\n  - `works`：待执行的 `cpu_stop_work` 链表\n  - `stop_work`、`caller`、`fn`：用于 `stop_cpus` 的临时字段\n\n- **`struct multi_stop_data`**  \n  用于多 CPU 同步执行的共享控制结构，包含：\n  - `fn` 和 `data`：要执行的函数及其参数\n  - `num_threads`：参与同步的线程数\n  - `active_cpus`：指定哪些 CPU 需要实际执行函数\n  - `state`：全局状态机（`MULTI_STOP_*` 枚举）\n  - `thread_ack`：用于状态同步的原子计数器\n\n- **`enum multi_stop_state`**  \n  多 CPU 同步执行的状态机，包括：\n  - `MULTI_STOP_NONE`\n  - `MULTI_STOP_PREPARE`\n  - `MULTI_STOP_DISABLE_IRQ`\n  - `MULTI_STOP_RUN`\n  - `MULTI_STOP_EXIT`\n\n### 主要函数\n\n- **`stop_one_cpu(cpu, fn, arg)`**  \n  在指定 CPU 上执行函数 `fn(arg)`，阻塞等待执行完成。若 CPU 离线则返回 `-ENOENT`。\n\n- **`cpu_stop_queue_work(cpu, work)`**  \n  将 stop 任务加入指定 CPU 的 stopper 队列，若 CPU 在线则唤醒其 stopper 线程。\n\n- **`multi_cpu_stop(data)`**  \n  stopper 线程的主函数，实现多 CPU 同步状态机，负责禁用中断、执行函数、状态同步等。\n\n- **`print_stop_info(log_lvl, task)`**  \n  调试辅助函数，若 `task` 是 stopper 线程，则打印其当前执行函数及调用者信息。\n\n- **`set_state()` / `ack_state()`**  \n  控制多 CPU 同步状态机的推进：`set_state` 设置新状态并重置 ack 计数器，`ack_state` 用于线程确认状态，最后一个确认者推进到下一状态。\n\n## 3. 关键实现\n\n### Stopper 线程模型\n- 每个可能的 CPU 都有一个 `cpu_stopper` 实例，其中包含一个专用内核线程。\n- 该线程运行 `multi_cpu_stop` 函数，处于高优先级实时调度策略（由 `smpboot` 框架设置），可抢占普通任务。\n- 当有 stop 任务时，通过 `wake_up_process` 唤醒对应 stopper 线程。\n\n### 多 CPU 同步状态机\n- 使用共享的 `multi_stop_data` 结构协调所有参与 CPU。\n- 状态转换通过 `set_state` 触发，所有线程通过轮询 `msdata->state` 检测状态变化。\n- 每个状态变更需所有线程调用 `ack_state` 确认，最后一个确认者推进到下一状态，确保严格同步。\n- 在 `MULTI_STOP_DISABLE_IRQ` 状态下，所有参与 CPU 禁用本地中断（包括硬中断），ARM64 还会屏蔽 SDEI 事件。\n- 仅 `active_cpus` 中的 CPU 在 `MULTI_STOP_RUN` 状态执行实际函数。\n\n### 中断与 NMI 安全\n- 执行期间禁用本地中断，防止中断处理程序干扰关键操作。\n- 在等待状态循环中调用 `touch_nmi_watchdog()` 防止 NMI watchdog 误报硬锁死。\n- 使用 `rcu_momentary_dyntick_idle()` 通知 RCU 系统当前 CPU 处于空闲状态，避免 RCU stall。\n\n### CPU 热插拔处理\n- `cpu_stopper.enabled` 标志反映 CPU 在线状态。\n- 若 CPU 离线时提交 stop 任务，则立即完成（调用 `cpu_stop_signal_done`），避免阻塞。\n- 支持从非活动 CPU（如 CPU hotplug 的 bringup 路径）调用 `stop_machine`，此时中断可能已禁用，需保存/恢复中断状态。\n\n### 死锁预防\n- `cpu_stop_queue_two_works` 函数通过嵌套锁（`SINGLE_DEPTH_NESTING`）和重试机制，确保两个 stopper 的入队操作原子性，避免与 `stop_cpus` 并发导致的死锁。\n- 使用 `preempt_disable()` 保证唤醒操作在不可抢占上下文中完成，防止唤醒丢失。\n\n## 4. 依赖关系\n\n- **调度子系统**：依赖 `kthread` 创建 stopper 线程，使用 `wake_up_process` 唤醒。\n- **SMP 子系统**：依赖 `smpboot.h` 的 CPU 热插拔通知机制来启用/禁用 stopper。\n- **中断子系统**：调用 `local_irq_disable/restore`、`hard_irq_disable` 控制中断。\n- **RCU 子系统**：通过 `rcu_momentary_dyntick_idle` 与 RCU 交互。\n- **NMI 子系统**：调用 `touch_nmi_watchdog` 避免 watchdog 误报。\n- **ARM64 架构**：条件编译包含 SDEI（Software Delegated Exception Interface）屏蔽/解除屏蔽。\n- **Per-CPU 基础设施**：使用 `DEFINE_PER_CPU` 和 `per_cpu_ptr` 管理 per-CPU stopper 实例。\n\n## 5. 使用场景\n\n- **CPU 热插拔**：在 CPU 上线/下线过程中执行需要全局同步的操作。\n- **内核模块加载/卸载**：某些架构或功能（如 ftrace）需要 stop_machine 来安全修改内核文本。\n- **内核热补丁（Livepatch）**：在应用补丁时冻结所有 CPU 以确保一致性。\n- **动态 tracing（如 ftrace）**：修改函数入口指令时需 stop_machine 保证原子性。\n- **内存热插拔**：某些内存操作需要全局同步。\n- **内核调试与诊断**：通过 `print_stop_info` 辅助分析 stopper 行为。\n- **架构特定操作**：如 ARM64 的 SDEI 事件处理需要在 stop_machine 上下文中屏蔽。",
      "similarity": 0.6240414381027222,
      "chunks": [
        {
          "chunk_id": 2,
          "file_path": "kernel/stop_machine.c",
          "start_line": 269,
          "end_line": 369,
          "content": [
            "static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,",
            "\t\t\t\t    int cpu2, struct cpu_stop_work *work2)",
            "{",
            "\tstruct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);",
            "\tstruct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);",
            "\tint err;",
            "",
            "retry:",
            "\t/*",
            "\t * The waking up of stopper threads has to happen in the same",
            "\t * scheduling context as the queueing.  Otherwise, there is a",
            "\t * possibility of one of the above stoppers being woken up by another",
            "\t * CPU, and preempting us. This will cause us to not wake up the other",
            "\t * stopper forever.",
            "\t */",
            "\tpreempt_disable();",
            "\traw_spin_lock_irq(&stopper1->lock);",
            "\traw_spin_lock_nested(&stopper2->lock, SINGLE_DEPTH_NESTING);",
            "",
            "\tif (!stopper1->enabled || !stopper2->enabled) {",
            "\t\terr = -ENOENT;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\t/*",
            "\t * Ensure that if we race with __stop_cpus() the stoppers won't get",
            "\t * queued up in reverse order leading to system deadlock.",
            "\t *",
            "\t * We can't miss stop_cpus_in_progress if queue_stop_cpus_work() has",
            "\t * queued a work on cpu1 but not on cpu2, we hold both locks.",
            "\t *",
            "\t * It can be falsely true but it is safe to spin until it is cleared,",
            "\t * queue_stop_cpus_work() does everything under preempt_disable().",
            "\t */",
            "\tif (unlikely(stop_cpus_in_progress)) {",
            "\t\terr = -EDEADLK;",
            "\t\tgoto unlock;",
            "\t}",
            "",
            "\terr = 0;",
            "\t__cpu_stop_queue_work(stopper1, work1);",
            "\t__cpu_stop_queue_work(stopper2, work2);",
            "",
            "unlock:",
            "\traw_spin_unlock(&stopper2->lock);",
            "\traw_spin_unlock_irq(&stopper1->lock);",
            "",
            "\tif (unlikely(err == -EDEADLK)) {",
            "\t\tpreempt_enable();",
            "",
            "\t\twhile (stop_cpus_in_progress)",
            "\t\t\tcpu_relax();",
            "",
            "\t\tgoto retry;",
            "\t}",
            "",
            "\tif (!err) {",
            "\t\twake_up_process(stopper1->thread);",
            "\t\twake_up_process(stopper2->thread);",
            "\t}",
            "\tpreempt_enable();",
            "",
            "\treturn err;",
            "}",
            "int stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tstruct cpu_stop_done done;",
            "\tstruct cpu_stop_work work1, work2;",
            "\tstruct multi_stop_data msdata;",
            "",
            "\tmsdata = (struct multi_stop_data){",
            "\t\t.fn = fn,",
            "\t\t.data = arg,",
            "\t\t.num_threads = 2,",
            "\t\t.active_cpus = cpumask_of(cpu1),",
            "\t};",
            "",
            "\twork1 = work2 = (struct cpu_stop_work){",
            "\t\t.fn = multi_cpu_stop,",
            "\t\t.arg = &msdata,",
            "\t\t.done = &done,",
            "\t\t.caller = _RET_IP_,",
            "\t};",
            "",
            "\tcpu_stop_init_done(&done, 2);",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "",
            "\tif (cpu1 > cpu2)",
            "\t\tswap(cpu1, cpu2);",
            "\tif (cpu_stop_queue_two_works(cpu1, &work1, cpu2, &work2))",
            "\t\treturn -ENOENT;",
            "",
            "\twait_for_completion(&done.completion);",
            "\treturn done.ret;",
            "}",
            "bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,",
            "\t\t\tstruct cpu_stop_work *work_buf)",
            "{",
            "\t*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, .caller = _RET_IP_, };",
            "\treturn cpu_stop_queue_work(cpu, work_buf);",
            "}"
          ],
          "function_name": "cpu_stop_queue_two_works, stop_two_cpus, stop_one_cpu_nowait",
          "description": "实现双CPU停止操作的协同逻辑，通过原子操作防止死锁，确保两个CPU的停止工作被正确排队和唤醒。包含针对两个CPU的停止接口和非阻塞式单CPU停止标记功能。",
          "similarity": 0.5860996246337891
        },
        {
          "chunk_id": 3,
          "file_path": "kernel/stop_machine.c",
          "start_line": 401,
          "end_line": 502,
          "content": [
            "static bool queue_stop_cpus_work(const struct cpumask *cpumask,",
            "\t\t\t\t cpu_stop_fn_t fn, void *arg,",
            "\t\t\t\t struct cpu_stop_done *done)",
            "{",
            "\tstruct cpu_stop_work *work;",
            "\tunsigned int cpu;",
            "\tbool queued = false;",
            "",
            "\t/*",
            "\t * Disable preemption while queueing to avoid getting",
            "\t * preempted by a stopper which might wait for other stoppers",
            "\t * to enter @fn which can lead to deadlock.",
            "\t */",
            "\tpreempt_disable();",
            "\tstop_cpus_in_progress = true;",
            "\tbarrier();",
            "\tfor_each_cpu(cpu, cpumask) {",
            "\t\twork = &per_cpu(cpu_stopper.stop_work, cpu);",
            "\t\twork->fn = fn;",
            "\t\twork->arg = arg;",
            "\t\twork->done = done;",
            "\t\twork->caller = _RET_IP_;",
            "\t\tif (cpu_stop_queue_work(cpu, work))",
            "\t\t\tqueued = true;",
            "\t}",
            "\tbarrier();",
            "\tstop_cpus_in_progress = false;",
            "\tpreempt_enable();",
            "",
            "\treturn queued;",
            "}",
            "static int __stop_cpus(const struct cpumask *cpumask,",
            "\t\t       cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tstruct cpu_stop_done done;",
            "",
            "\tcpu_stop_init_done(&done, cpumask_weight(cpumask));",
            "\tif (!queue_stop_cpus_work(cpumask, fn, arg, &done))",
            "\t\treturn -ENOENT;",
            "\twait_for_completion(&done.completion);",
            "\treturn done.ret;",
            "}",
            "static int stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tint ret;",
            "",
            "\t/* static works are used, process one request at a time */",
            "\tmutex_lock(&stop_cpus_mutex);",
            "\tret = __stop_cpus(cpumask, fn, arg);",
            "\tmutex_unlock(&stop_cpus_mutex);",
            "\treturn ret;",
            "}",
            "static int cpu_stop_should_run(unsigned int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\tunsigned long flags;",
            "\tint run;",
            "",
            "\traw_spin_lock_irqsave(&stopper->lock, flags);",
            "\trun = !list_empty(&stopper->works);",
            "\traw_spin_unlock_irqrestore(&stopper->lock, flags);",
            "\treturn run;",
            "}",
            "static void cpu_stopper_thread(unsigned int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\tstruct cpu_stop_work *work;",
            "",
            "repeat:",
            "\twork = NULL;",
            "\traw_spin_lock_irq(&stopper->lock);",
            "\tif (!list_empty(&stopper->works)) {",
            "\t\twork = list_first_entry(&stopper->works,",
            "\t\t\t\t\tstruct cpu_stop_work, list);",
            "\t\tlist_del_init(&work->list);",
            "\t}",
            "\traw_spin_unlock_irq(&stopper->lock);",
            "",
            "\tif (work) {",
            "\t\tcpu_stop_fn_t fn = work->fn;",
            "\t\tvoid *arg = work->arg;",
            "\t\tstruct cpu_stop_done *done = work->done;",
            "\t\tint ret;",
            "",
            "\t\t/* cpu stop callbacks must not sleep, make in_atomic() == T */",
            "\t\tstopper->caller = work->caller;",
            "\t\tstopper->fn = fn;",
            "\t\tpreempt_count_inc();",
            "\t\tret = fn(arg);",
            "\t\tif (done) {",
            "\t\t\tif (ret)",
            "\t\t\t\tdone->ret = ret;",
            "\t\t\tcpu_stop_signal_done(done);",
            "\t\t}",
            "\t\tpreempt_count_dec();",
            "\t\tstopper->fn = NULL;",
            "\t\tstopper->caller = 0;",
            "\t\tWARN_ONCE(preempt_count(),",
            "\t\t\t  \"cpu_stop: %ps(%p) leaked preempt count\\n\", fn, arg);",
            "\t\tgoto repeat;",
            "\t}",
            "}"
          ],
          "function_name": "queue_stop_cpus_work, __stop_cpus, stop_cpus, cpu_stop_should_run, cpu_stopper_thread",
          "description": "实现批量CPU停止的中枢逻辑，通过互斥锁保证串行化处理。包含工作分发、停止执行、状态追踪等功能，支持通用CPU掩码的停止操作，并提供预处理检查和结果收集机制。",
          "similarity": 0.5726058483123779
        },
        {
          "chunk_id": 1,
          "file_path": "kernel/stop_machine.c",
          "start_line": 56,
          "end_line": 201,
          "content": [
            "void print_stop_info(const char *log_lvl, struct task_struct *task)",
            "{",
            "\t/*",
            "\t * If @task is a stopper task, it cannot migrate and task_cpu() is",
            "\t * stable.",
            "\t */",
            "\tstruct cpu_stopper *stopper = per_cpu_ptr(&cpu_stopper, task_cpu(task));",
            "",
            "\tif (task != stopper->thread)",
            "\t\treturn;",
            "",
            "\tprintk(\"%sStopper: %pS <- %pS\\n\", log_lvl, stopper->fn, (void *)stopper->caller);",
            "}",
            "static void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)",
            "{",
            "\tmemset(done, 0, sizeof(*done));",
            "\tatomic_set(&done->nr_todo, nr_todo);",
            "\tinit_completion(&done->completion);",
            "}",
            "static void cpu_stop_signal_done(struct cpu_stop_done *done)",
            "{",
            "\tif (atomic_dec_and_test(&done->nr_todo))",
            "\t\tcomplete(&done->completion);",
            "}",
            "static void __cpu_stop_queue_work(struct cpu_stopper *stopper,",
            "\t\t\t\t  struct cpu_stop_work *work)",
            "{",
            "\tlist_add_tail(&work->list, &stopper->works);",
            "}",
            "static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\tunsigned long flags;",
            "\tbool enabled;",
            "",
            "\tpreempt_disable();",
            "\traw_spin_lock_irqsave(&stopper->lock, flags);",
            "\tenabled = stopper->enabled;",
            "\tif (enabled)",
            "\t\t__cpu_stop_queue_work(stopper, work);",
            "\telse if (work->done)",
            "\t\tcpu_stop_signal_done(work->done);",
            "\traw_spin_unlock_irqrestore(&stopper->lock, flags);",
            "",
            "\tif (enabled)",
            "\t\twake_up_process(stopper->thread);",
            "\tpreempt_enable();",
            "",
            "\treturn enabled;",
            "}",
            "int stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)",
            "{",
            "\tstruct cpu_stop_done done;",
            "\tstruct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done, .caller = _RET_IP_ };",
            "",
            "\tcpu_stop_init_done(&done, 1);",
            "\tif (!cpu_stop_queue_work(cpu, &work))",
            "\t\treturn -ENOENT;",
            "\t/*",
            "\t * In case @cpu == smp_proccessor_id() we can avoid a sleep+wakeup",
            "\t * cycle by doing a preemption:",
            "\t */",
            "\tcond_resched();",
            "\twait_for_completion(&done.completion);",
            "\treturn done.ret;",
            "}",
            "static void set_state(struct multi_stop_data *msdata,",
            "\t\t      enum multi_stop_state newstate)",
            "{",
            "\t/* Reset ack counter. */",
            "\tatomic_set(&msdata->thread_ack, msdata->num_threads);",
            "\tsmp_wmb();",
            "\tWRITE_ONCE(msdata->state, newstate);",
            "}",
            "static void ack_state(struct multi_stop_data *msdata)",
            "{",
            "\tif (atomic_dec_and_test(&msdata->thread_ack))",
            "\t\tset_state(msdata, msdata->state + 1);",
            "}",
            "notrace void __weak stop_machine_yield(const struct cpumask *cpumask)",
            "{",
            "\tcpu_relax();",
            "}",
            "static int multi_cpu_stop(void *data)",
            "{",
            "\tstruct multi_stop_data *msdata = data;",
            "\tenum multi_stop_state newstate, curstate = MULTI_STOP_NONE;",
            "\tint cpu = smp_processor_id(), err = 0;",
            "\tconst struct cpumask *cpumask;",
            "\tunsigned long flags;",
            "\tbool is_active;",
            "",
            "\t/*",
            "\t * When called from stop_machine_from_inactive_cpu(), irq might",
            "\t * already be disabled.  Save the state and restore it on exit.",
            "\t */",
            "\tlocal_save_flags(flags);",
            "",
            "\tif (!msdata->active_cpus) {",
            "\t\tcpumask = cpu_online_mask;",
            "\t\tis_active = cpu == cpumask_first(cpumask);",
            "\t} else {",
            "\t\tcpumask = msdata->active_cpus;",
            "\t\tis_active = cpumask_test_cpu(cpu, cpumask);",
            "\t}",
            "",
            "\t/* Simple state machine */",
            "\tdo {",
            "\t\t/* Chill out and ensure we re-read multi_stop_state. */",
            "\t\tstop_machine_yield(cpumask);",
            "\t\tnewstate = READ_ONCE(msdata->state);",
            "\t\tif (newstate != curstate) {",
            "\t\t\tcurstate = newstate;",
            "\t\t\tswitch (curstate) {",
            "\t\t\tcase MULTI_STOP_DISABLE_IRQ:",
            "\t\t\t\tlocal_irq_disable();",
            "\t\t\t\thard_irq_disable();",
            "#ifdef CONFIG_ARM64",
            "\t\t\t\tsdei_mask_local_cpu();",
            "#endif",
            "\t\t\t\tbreak;",
            "\t\t\tcase MULTI_STOP_RUN:",
            "\t\t\t\tif (is_active)",
            "\t\t\t\t\terr = msdata->fn(msdata->data);",
            "\t\t\t\tbreak;",
            "\t\t\tdefault:",
            "\t\t\t\tbreak;",
            "\t\t\t}",
            "\t\t\tack_state(msdata);",
            "\t\t} else if (curstate > MULTI_STOP_PREPARE) {",
            "\t\t\t/*",
            "\t\t\t * At this stage all other CPUs we depend on must spin",
            "\t\t\t * in the same loop. Any reason for hard-lockup should",
            "\t\t\t * be detected and reported on their side.",
            "\t\t\t */",
            "\t\t\ttouch_nmi_watchdog();",
            "\t\t}",
            "\t\trcu_momentary_dyntick_idle();",
            "\t} while (curstate != MULTI_STOP_EXIT);",
            "",
            "#ifdef CONFIG_ARM64",
            "\tsdei_unmask_local_cpu();",
            "#endif",
            "\tlocal_irq_restore(flags);",
            "\treturn err;",
            "}"
          ],
          "function_name": "print_stop_info, cpu_stop_init_done, cpu_stop_signal_done, __cpu_stop_queue_work, cpu_stop_queue_work, stop_one_cpu, set_state, ack_state, stop_machine_yield, multi_cpu_stop",
          "description": "实现了停止操作的核心控制逻辑，包括工作队列管理、状态同步、单CPU停止处理及多CPU状态机。提供打印停止信息、初始化完成状态、信号完成、排队工作、单CPU停止、状态切换等辅助函数。",
          "similarity": 0.560387134552002
        },
        {
          "chunk_id": 4,
          "file_path": "kernel/stop_machine.c",
          "start_line": 536,
          "end_line": 641,
          "content": [
            "void stop_machine_park(int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "\t/*",
            "\t * Lockless. cpu_stopper_thread() will take stopper->lock and flush",
            "\t * the pending works before it parks, until then it is fine to queue",
            "\t * the new works.",
            "\t */",
            "\tstopper->enabled = false;",
            "\tkthread_park(stopper->thread);",
            "}",
            "static void cpu_stop_create(unsigned int cpu)",
            "{",
            "\tsched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));",
            "}",
            "static void cpu_stop_park(unsigned int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "",
            "\tWARN_ON(!list_empty(&stopper->works));",
            "}",
            "void stop_machine_unpark(int cpu)",
            "{",
            "\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "",
            "\tstopper->enabled = true;",
            "\tkthread_unpark(stopper->thread);",
            "}",
            "static int __init cpu_stop_init(void)",
            "{",
            "\tunsigned int cpu;",
            "",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);",
            "",
            "\t\traw_spin_lock_init(&stopper->lock);",
            "\t\tINIT_LIST_HEAD(&stopper->works);",
            "\t}",
            "",
            "\tBUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));",
            "\tstop_machine_unpark(raw_smp_processor_id());",
            "\tstop_machine_initialized = true;",
            "\treturn 0;",
            "}",
            "int stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,",
            "\t\t\t    const struct cpumask *cpus)",
            "{",
            "\tstruct multi_stop_data msdata = {",
            "\t\t.fn = fn,",
            "\t\t.data = data,",
            "\t\t.num_threads = num_online_cpus(),",
            "\t\t.active_cpus = cpus,",
            "\t};",
            "",
            "\tlockdep_assert_cpus_held();",
            "",
            "\tif (!stop_machine_initialized) {",
            "\t\t/*",
            "\t\t * Handle the case where stop_machine() is called",
            "\t\t * early in boot before stop_machine() has been",
            "\t\t * initialized.",
            "\t\t */",
            "\t\tunsigned long flags;",
            "\t\tint ret;",
            "",
            "\t\tWARN_ON_ONCE(msdata.num_threads != 1);",
            "",
            "\t\tlocal_irq_save(flags);",
            "\t\thard_irq_disable();",
            "\t\tret = (*fn)(data);",
            "\t\tlocal_irq_restore(flags);",
            "",
            "\t\treturn ret;",
            "\t}",
            "",
            "\t/* Set the initial state and stop all online cpus. */",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "\treturn stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);",
            "}",
            "int stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)",
            "{",
            "\tint ret;",
            "",
            "\t/* No CPUs can come up or down during this. */",
            "\tcpus_read_lock();",
            "\tret = stop_machine_cpuslocked(fn, data, cpus);",
            "\tcpus_read_unlock();",
            "\treturn ret;",
            "}",
            "int stop_core_cpuslocked(unsigned int cpu, cpu_stop_fn_t fn, void *data)",
            "{",
            "\tconst struct cpumask *smt_mask = cpu_smt_mask(cpu);",
            "",
            "\tstruct multi_stop_data msdata = {",
            "\t\t.fn = fn,",
            "\t\t.data = data,",
            "\t\t.num_threads = cpumask_weight(smt_mask),",
            "\t\t.active_cpus = smt_mask,",
            "\t};",
            "",
            "\tlockdep_assert_cpus_held();",
            "",
            "\t/* Set the initial state and stop all online cpus. */",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "\treturn stop_cpus(smt_mask, multi_cpu_stop, &msdata);",
            "}"
          ],
          "function_name": "stop_machine_park, cpu_stop_create, cpu_stop_park, stop_machine_unpark, cpu_stop_init, stop_machine_cpuslocked, stop_machine, stop_core_cpuslocked",
          "description": "负责停止器线程的生命周期管理和初始化。包含线程创建、挂起/恢复操作、核心初始化函数，以及带锁的CPU停止接口。提供针对特定CPU核心的停止功能和早期启动阶段的降级处理路径。",
          "similarity": 0.5417853593826294
        },
        {
          "chunk_id": 5,
          "file_path": "kernel/stop_machine.c",
          "start_line": 687,
          "end_line": 716,
          "content": [
            "int stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,",
            "\t\t\t\t  const struct cpumask *cpus)",
            "{",
            "\tstruct multi_stop_data msdata = { .fn = fn, .data = data,",
            "\t\t\t\t\t    .active_cpus = cpus };",
            "\tstruct cpu_stop_done done;",
            "\tint ret;",
            "",
            "\t/* Local CPU must be inactive and CPU hotplug in progress. */",
            "\tBUG_ON(cpu_active(raw_smp_processor_id()));",
            "\tmsdata.num_threads = num_active_cpus() + 1;\t/* +1 for local */",
            "",
            "\t/* No proper task established and can't sleep - busy wait for lock. */",
            "\twhile (!mutex_trylock(&stop_cpus_mutex))",
            "\t\tcpu_relax();",
            "",
            "\t/* Schedule work on other CPUs and execute directly for local CPU */",
            "\tset_state(&msdata, MULTI_STOP_PREPARE);",
            "\tcpu_stop_init_done(&done, num_active_cpus());",
            "\tqueue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,",
            "\t\t\t     &done);",
            "\tret = multi_cpu_stop(&msdata);",
            "",
            "\t/* Busy wait for completion. */",
            "\twhile (!completion_done(&done.completion))",
            "\t\tcpu_relax();",
            "",
            "\tmutex_unlock(&stop_cpus_mutex);",
            "\treturn ret ?: done.ret;",
            "}"
          ],
          "function_name": "stop_machine_from_inactive_cpu",
          "description": "该函数用于在非活跃CPU上协调多CPU的停止操作，确保在CPU热插拔期间正确执行停止回调函数。  \n通过获取互斥锁、异步调度任务并在本地直接执行，最终阻塞等待所有停止操作完成。  \n依赖未显示的辅助函数（如`queue_stop_cpus_work`和`multi_cpu_stop`），上下文可能存在不完整情况。",
          "similarity": 0.510989785194397
        }
      ]
    },
    {
      "source_file": "kernel/rcu/tree.c",
      "md_summary": "> 自动生成时间: 2025-10-25 15:46:46\n> \n> 生成工具: 通义千问 API (qwen3-max)\n> \n> 原始文件: `rcu\\tree.c`\n\n---\n\n# `rcu/tree.c` 技术文档\n\n## 1. 文件概述\n\n`rcu/tree.c` 是 Linux 内核中 **Read-Copy Update (RCU)** 机制的树形（tree-based）实现核心文件。RCU 是一种高性能的同步原语，用于在读多写少的场景下实现无锁读取与安全更新。该文件实现了基于分层树结构的 RCU 状态管理、宽限期（Grace Period）检测、回调处理、CPU 离线/上线处理以及与调度器、中断、kthread 等子系统的集成。树形结构的设计使得 RCU 能够高效扩展到大规模多核系统（数百甚至上千 CPU）。\n\n## 2. 核心功能\n\n### 主要数据结构\n\n- **`struct rcu_data`**（每 CPU）  \n  存储每个 CPU 的 RCU 状态，包括待处理的回调链表、宽限期序列号、QS（Quiescent State）状态等。\n  \n- **`struct rcu_state`**（全局）  \n  全局 RCU 状态机，包含宽限期状态（`gp_state`）、序列号（`gp_seq`）、树形节点层级结构（`level[]`）、互斥锁（如 `barrier_mutex`、`exp_mutex`）等。\n\n- **`struct rcu_node`**（层级节点）  \n  构成 RCU 树的内部节点，用于聚合子节点（CPU 或下层 `rcu_node`）的宽限期完成状态，实现分层检测，减少全局同步开销。\n\n- **全局变量**：\n  - `rcu_scheduler_active`：指示调度器是否已激活，影响 RCU 初始化和优化策略。\n  - `rcu_scheduler_fully_active`：指示 RCU 是否已完全初始化（包括 kthread 启动）。\n  - `rcu_num_lvls` / `num_rcu_lvl[]` / `rcu_num_nodes`：描述 RCU 树的层级结构和节点数量。\n  - 多个模块参数（如 `use_softirq`, `rcu_fanout_leaf`, `kthread_prio` 等）用于运行时调优和调试。\n\n### 主要函数（声明/定义）\n\n- **宽限期管理**：\n  - `rcu_report_qs_rnp()`：向上报告某 `rcu_node` 的 QS 状态。\n  - `invoke_rcu_core()`：触发 RCU 核心处理（如宽限期推进或回调执行）。\n\n- **回调处理**：\n  - `rcu_report_exp_rdp()`：报告扩展（expedited）宽限期完成。\n  - `check_cb_ovld_locked()`：检查回调过载情况。\n\n- **CPU 热插拔支持**：\n  - `rcu_boost_kthread_setaffinity()`：调整 RCU boost kthread 的 CPU 亲和性。\n  - `sync_sched_exp_online_cleanup()`：清理 CPU 上线时的扩展同步状态。\n  - `rcu_cleanup_dead_rnp()` / `rcu_init_new_rnp()`：处理 CPU 离线/上线时的 `rcu_node` 结构。\n\n- **辅助函数**：\n  - `rcu_rdp_is_offloaded()`：判断 RCU 回调是否被卸载到专用 kthread（NO_HZ_FULL/NO_CB 场景）。\n  - `rcu_rdp_cpu_online()`：检查对应 CPU 是否在线。\n  - `rcu_init_invoked()`：判断 RCU 初始化是否已启动。\n\n- **导出接口**：\n  - `rcu_get_gp_kthreads_prio()`：供 `rcutorture` 等测试模块获取 RCU kthread 优先级。\n\n## 3. 关键实现\n\n### 树形宽限期检测机制\nRCU 使用分层树结构（`rcu_node` 树）来高效检测宽限期完成：\n- 叶子层对应 CPU，上层节点聚合子节点状态。\n- 每个 `rcu_node` 维护一个位图（`qsmask`），记录哪些子节点尚未报告 QS。\n- 当所有子节点都报告 QS 后，该节点向上层报告，最终根节点完成整个宽限期。\n- 此设计将 O(N) 的全局同步开销降低为 O(log N)，适用于大规模系统。\n\n### 宽限期状态机\n- 全局状态 `rcu_state.gp_state` 控制宽限期生命周期（IDLE → WAITING → DONE 等）。\n- 使用 64 位序列号 `gp_seq` 标识宽限期，通过位移（`RCU_SEQ_CTR_SHIFT`）区分状态。\n- 初始序列号设为 `(0UL - 300UL) << RCU_SEQ_CTR_SHIFT`，确保启动时处于有效状态。\n\n### 调度器集成与启动阶段优化\n- `rcu_scheduler_active` 分三阶段：\n  1. `RCU_SCHEDULER_INACTIVE`：单任务阶段，`synchronize_rcu()` 退化为内存屏障。\n  2. `RCU_SCHEDULER_INIT`：调度器启动但 RCU 未完全初始化。\n  3. `RCU_SCHEDULER_RUNNING`：RCU 完全激活。\n- `rcu_scheduler_fully_active` 确保 RCU 回调和 kthread 在调度器支持多任务后才启用。\n\n### 回调处理策略\n- 支持两种回调执行模式：\n  - **软中断（`RCU_SOFTIRQ`）**：默认模式，通过 `rcu_softirq` 处理。\n  - **专用 kthread（`rcuc`/`rcub`）**：在 `PREEMPT_RT` 或配置 `NO_CB` 时使用，避免软中断延迟。\n- 通过 `use_softirq` 模块参数控制模式选择。\n\n### 调试与调优支持\n- 多个延迟参数（`gp_preinit_delay` 等）用于注入延迟以暴露竞态条件。\n- `rcu_unlock_delay` 在 `CONFIG_RCU_STRICT_GRACE_PERIOD` 下强制延迟 `rcu_read_unlock()`。\n- `dump_tree` 参数可在启动时打印 RCU 树结构用于验证。\n- `rcu_fanout_leaf` 和 `rcu_fanout_exact` 控制树的扇出（fanout）结构。\n\n### 内存与资源管理\n- `rcu_min_cached_objs` 控制每 CPU 缓存的最小对象数（以页为单位）。\n- `rcu_delay_page_cache_fill_msec` 在内存压力下延迟填充 RCU 缓存，避免与页回收冲突。\n\n## 4. 依赖关系\n\n- **头文件依赖**：\n  - 基础内核设施：`<linux/smp.h>`, `<linux/sched.h>`, `<linux/interrupt.h>`, `<linux/percpu.h>` 等。\n  - 时间子系统：`<linux/tick.h>`, `<linux/jiffies.h>`。\n  - 内存管理：`<linux/mm.h>`, `<linux/slab.h>`, `<linux/vmalloc.h>`。\n  - 调试与追踪：`<linux/lockdep.h>`, `<linux/ftrace.h>`, `<linux/kasan.h>`。\n  - RCU 内部头文件：`\"tree.h\"`, `\"rcu.h\"`。\n\n- **模块依赖**：\n  - 与调度器深度集成（`rcu_scheduler_active` 状态依赖 `sched/`）。\n  - 依赖中断子系统处理 QS 检测（如 tick 中断）。\n  - 与 CPU 热插拔机制协同（`cpuhp` 框架）。\n  - 在 `PREEMPT_RT` 下依赖实时调度特性。\n  - 与内存回收（shrinker）交互以管理缓存。\n\n## 5. 使用场景\n\n- **内核同步原语**：为 `synchronize_rcu()`, `call_rcu()` 等 API 提供底层实现。\n- **大规模多核系统**：通过树形结构支持数百至数千 CPU 的高效宽限期检测。\n- **实时系统**：通过 `rcuc` kthread 和优先级控制（`kthread_prio`）满足实时性要求。\n- **CPU 热插拔**：动态调整 RCU 树结构以适应 CPU 在线/离线。\n- **内存压力场景**：与页回收协同，避免 RCU 缓存加剧内存紧张。\n- **内核调试与测试**：\n  - `rcutorture` 模块利用此文件接口进行压力测试。\n  - 通过延迟参数和 `dump_tree` 辅助调试竞态和结构问题。\n- **低延迟场景**：在 `NO_HZ_FULL` 或 `NO_CB` 配置下，将 RCU 回调卸载到专用 CPU，减少主 CPU 干扰。",
      "similarity": 0.6128596663475037,
      "chunks": [
        {
          "chunk_id": 12,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2273,
          "end_line": 2388,
          "content": [
            "void rcu_sched_clock_irq(int user)",
            "{",
            "\tunsigned long j;",
            "",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU)) {",
            "\t\tj = jiffies;",
            "\t\tWARN_ON_ONCE(time_before(j, __this_cpu_read(rcu_data.last_sched_clock)));",
            "\t\t__this_cpu_write(rcu_data.last_sched_clock, j);",
            "\t}",
            "\ttrace_rcu_utilization(TPS(\"Start scheduler-tick\"));",
            "\tlockdep_assert_irqs_disabled();",
            "\traw_cpu_inc(rcu_data.ticks_this_gp);",
            "\t/* The load-acquire pairs with the store-release setting to true. */",
            "\tif (smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {",
            "\t\t/* Idle and userspace execution already are quiescent states. */",
            "\t\tif (!rcu_is_cpu_rrupt_from_idle() && !user) {",
            "\t\t\tset_tsk_need_resched(current);",
            "\t\t\tset_preempt_need_resched();",
            "\t\t}",
            "\t\t__this_cpu_write(rcu_data.rcu_urgent_qs, false);",
            "\t}",
            "\trcu_flavor_sched_clock_irq(user);",
            "\tif (rcu_pending(user))",
            "\t\tinvoke_rcu_core();",
            "\tif (user || rcu_is_cpu_rrupt_from_idle())",
            "\t\trcu_note_voluntary_context_switch(current);",
            "\tlockdep_assert_irqs_disabled();",
            "",
            "\ttrace_rcu_utilization(TPS(\"End scheduler-tick\"));",
            "}",
            "static void force_qs_rnp(int (*f)(struct rcu_data *rdp))",
            "{",
            "\tint cpu;",
            "\tunsigned long flags;",
            "\tstruct rcu_node *rnp;",
            "",
            "\trcu_state.cbovld = rcu_state.cbovldnext;",
            "\trcu_state.cbovldnext = false;",
            "\trcu_for_each_leaf_node(rnp) {",
            "\t\tunsigned long mask = 0;",
            "\t\tunsigned long rsmask = 0;",
            "",
            "\t\tcond_resched_tasks_rcu_qs();",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\trcu_state.cbovldnext |= !!rnp->cbovldmask;",
            "\t\tif (rnp->qsmask == 0) {",
            "\t\t\tif (rcu_preempt_blocked_readers_cgp(rnp)) {",
            "\t\t\t\t/*",
            "\t\t\t\t * No point in scanning bits because they",
            "\t\t\t\t * are all zero.  But we might need to",
            "\t\t\t\t * priority-boost blocked readers.",
            "\t\t\t\t */",
            "\t\t\t\trcu_initiate_boost(rnp, flags);",
            "\t\t\t\t/* rcu_initiate_boost() releases rnp->lock */",
            "\t\t\t\tcontinue;",
            "\t\t\t}",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rnp->qsmask) {",
            "\t\t\tstruct rcu_data *rdp;",
            "\t\t\tint ret;",
            "",
            "\t\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "\t\t\tret = f(rdp);",
            "\t\t\tif (ret > 0) {",
            "\t\t\t\tmask |= rdp->grpmask;",
            "\t\t\t\trcu_disable_urgency_upon_qs(rdp);",
            "\t\t\t}",
            "\t\t\tif (ret < 0)",
            "\t\t\t\trsmask |= rdp->grpmask;",
            "\t\t}",
            "\t\tif (mask != 0) {",
            "\t\t\t/* Idle/offline CPUs, report (releases rnp->lock). */",
            "\t\t\trcu_report_qs_rnp(mask, rnp, rnp->gp_seq, flags);",
            "\t\t} else {",
            "\t\t\t/* Nothing to do here, so just drop the lock. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t}",
            "",
            "\t\tfor_each_leaf_node_cpu_mask(rnp, cpu, rsmask)",
            "\t\t\tresched_cpu(cpu);",
            "\t}",
            "}",
            "void rcu_force_quiescent_state(void)",
            "{",
            "\tunsigned long flags;",
            "\tbool ret;",
            "\tstruct rcu_node *rnp;",
            "\tstruct rcu_node *rnp_old = NULL;",
            "",
            "\t/* Funnel through hierarchy to reduce memory contention. */",
            "\trnp = raw_cpu_read(rcu_data.mynode);",
            "\tfor (; rnp != NULL; rnp = rnp->parent) {",
            "\t\tret = (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) ||",
            "\t\t       !raw_spin_trylock(&rnp->fqslock);",
            "\t\tif (rnp_old != NULL)",
            "\t\t\traw_spin_unlock(&rnp_old->fqslock);",
            "\t\tif (ret)",
            "\t\t\treturn;",
            "\t\trnp_old = rnp;",
            "\t}",
            "\t/* rnp_old == rcu_get_root(), rnp == NULL. */",
            "",
            "\t/* Reached the root of the rcu_node tree, acquire lock. */",
            "\traw_spin_lock_irqsave_rcu_node(rnp_old, flags);",
            "\traw_spin_unlock(&rnp_old->fqslock);",
            "\tif (READ_ONCE(rcu_state.gp_flags) & RCU_GP_FLAG_FQS) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\t\treturn;  /* Someone beat us to it. */",
            "\t}",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp_old, flags);",
            "\trcu_gp_kthread_wake();",
            "}"
          ],
          "function_name": "rcu_sched_clock_irq, force_qs_rnp, rcu_force_quiescent_state",
          "description": "实现强制quiescent状态触发与调度器中断处理，包含优先级提升、回调加速及grace period推进等关键控制流，维护RCU状态一致性",
          "similarity": 0.6045853495597839
        },
        {
          "chunk_id": 14,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 2523,
          "end_line": 2628,
          "content": [
            "static void rcu_cpu_kthread_park(unsigned int cpu)",
            "{",
            "\tper_cpu(rcu_data.rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;",
            "}",
            "static int rcu_cpu_kthread_should_run(unsigned int cpu)",
            "{",
            "\treturn __this_cpu_read(rcu_data.rcu_cpu_has_work);",
            "}",
            "static void rcu_cpu_kthread(unsigned int cpu)",
            "{",
            "\tunsigned int *statusp = this_cpu_ptr(&rcu_data.rcu_cpu_kthread_status);",
            "\tchar work, *workp = this_cpu_ptr(&rcu_data.rcu_cpu_has_work);",
            "\tunsigned long *j = this_cpu_ptr(&rcu_data.rcuc_activity);",
            "\tint spincnt;",
            "",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_run\"));",
            "\tfor (spincnt = 0; spincnt < 10; spincnt++) {",
            "\t\tWRITE_ONCE(*j, jiffies);",
            "\t\tlocal_bh_disable();",
            "\t\t*statusp = RCU_KTHREAD_RUNNING;",
            "\t\tlocal_irq_disable();",
            "\t\twork = *workp;",
            "\t\tWRITE_ONCE(*workp, 0);",
            "\t\tlocal_irq_enable();",
            "\t\tif (work)",
            "\t\t\trcu_core();",
            "\t\tlocal_bh_enable();",
            "\t\tif (!READ_ONCE(*workp)) {",
            "\t\t\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_wait\"));",
            "\t\t\t*statusp = RCU_KTHREAD_WAITING;",
            "\t\t\treturn;",
            "\t\t}",
            "\t}",
            "\t*statusp = RCU_KTHREAD_YIELDING;",
            "\ttrace_rcu_utilization(TPS(\"Start CPU kthread@rcu_yield\"));",
            "\tschedule_timeout_idle(2);",
            "\ttrace_rcu_utilization(TPS(\"End CPU kthread@rcu_yield\"));",
            "\t*statusp = RCU_KTHREAD_WAITING;",
            "\tWRITE_ONCE(*j, jiffies);",
            "}",
            "static int __init rcu_spawn_core_kthreads(void)",
            "{",
            "\tint cpu;",
            "",
            "\tfor_each_possible_cpu(cpu)",
            "\t\tper_cpu(rcu_data.rcu_cpu_has_work, cpu) = 0;",
            "\tif (use_softirq)",
            "\t\treturn 0;",
            "\tWARN_ONCE(smpboot_register_percpu_thread(&rcu_cpu_thread_spec),",
            "\t\t  \"%s: Could not start rcuc kthread, OOM is now expected behavior\\n\", __func__);",
            "\treturn 0;",
            "}",
            "static void rcutree_enqueue(struct rcu_data *rdp, struct rcu_head *head, rcu_callback_t func)",
            "{",
            "\trcu_segcblist_enqueue(&rdp->cblist, head);",
            "\tif (__is_kvfree_rcu_offset((unsigned long)func))",
            "\t\ttrace_rcu_kvfree_callback(rcu_state.name, head,",
            "\t\t\t\t\t (unsigned long)func,",
            "\t\t\t\t\t rcu_segcblist_n_cbs(&rdp->cblist));",
            "\telse",
            "\t\ttrace_rcu_callback(rcu_state.name, head,",
            "\t\t\t\t   rcu_segcblist_n_cbs(&rdp->cblist));",
            "\ttrace_rcu_segcb_stats(&rdp->cblist, TPS(\"SegCBQueued\"));",
            "}",
            "static void call_rcu_core(struct rcu_data *rdp, struct rcu_head *head,",
            "\t\t\t  rcu_callback_t func, unsigned long flags)",
            "{",
            "\trcutree_enqueue(rdp, head, func);",
            "\t/*",
            "\t * If called from an extended quiescent state, invoke the RCU",
            "\t * core in order to force a re-evaluation of RCU's idleness.",
            "\t */",
            "\tif (!rcu_is_watching())",
            "\t\tinvoke_rcu_core();",
            "",
            "\t/* If interrupts were disabled or CPU offline, don't invoke RCU core. */",
            "\tif (irqs_disabled_flags(flags) || cpu_is_offline(smp_processor_id()))",
            "\t\treturn;",
            "",
            "\t/*",
            "\t * Force the grace period if too many callbacks or too long waiting.",
            "\t * Enforce hysteresis, and don't invoke rcu_force_quiescent_state()",
            "\t * if some other CPU has recently done so.  Also, don't bother",
            "\t * invoking rcu_force_quiescent_state() if the newly enqueued callback",
            "\t * is the only one waiting for a grace period to complete.",
            "\t */",
            "\tif (unlikely(rcu_segcblist_n_cbs(&rdp->cblist) >",
            "\t\t     rdp->qlen_last_fqs_check + qhimark)) {",
            "",
            "\t\t/* Are we ignoring a completed grace period? */",
            "\t\tnote_gp_changes(rdp);",
            "",
            "\t\t/* Start a new grace period if one not already started. */",
            "\t\tif (!rcu_gp_in_progress()) {",
            "\t\t\trcu_accelerate_cbs_unlocked(rdp->mynode, rdp);",
            "\t\t} else {",
            "\t\t\t/* Give the grace period a kick. */",
            "\t\t\trdp->blimit = DEFAULT_MAX_RCU_BLIMIT;",
            "\t\t\tif (READ_ONCE(rcu_state.n_force_qs) == rdp->n_force_qs_snap &&",
            "\t\t\t    rcu_segcblist_first_pend_cb(&rdp->cblist) != head)",
            "\t\t\t\trcu_force_quiescent_state();",
            "\t\t\trdp->n_force_qs_snap = READ_ONCE(rcu_state.n_force_qs);",
            "\t\t\trdp->qlen_last_fqs_check = rcu_segcblist_n_cbs(&rdp->cblist);",
            "\t\t}",
            "\t}",
            "}"
          ],
          "function_name": "rcu_cpu_kthread_park, rcu_cpu_kthread_should_run, rcu_cpu_kthread, rcu_spawn_core_kthreads, rcutree_enqueue, call_rcu_core",
          "description": "实现RCU k线程管理与回调分发基础设施，包含线程启动、回调入队及触发条件判断逻辑，提供跨CPU的异步处理能力",
          "similarity": 0.6023751497268677
        },
        {
          "chunk_id": 22,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 4072,
          "end_line": 4226,
          "content": [
            "static void rcu_barrier_trace(const char *s, int cpu, unsigned long done)",
            "{",
            "\ttrace_rcu_barrier(rcu_state.name, s, cpu,",
            "\t\t\t  atomic_read(&rcu_state.barrier_cpu_count), done);",
            "}",
            "static void rcu_barrier_callback(struct rcu_head *rhp)",
            "{",
            "\tunsigned long __maybe_unused s = rcu_state.barrier_sequence;",
            "",
            "\tif (atomic_dec_and_test(&rcu_state.barrier_cpu_count)) {",
            "\t\trcu_barrier_trace(TPS(\"LastCB\"), -1, s);",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "\t} else {",
            "\t\trcu_barrier_trace(TPS(\"CB\"), -1, s);",
            "\t}",
            "}",
            "static void rcu_barrier_entrain(struct rcu_data *rdp)",
            "{",
            "\tunsigned long gseq = READ_ONCE(rcu_state.barrier_sequence);",
            "\tunsigned long lseq = READ_ONCE(rdp->barrier_seq_snap);",
            "\tbool wake_nocb = false;",
            "\tbool was_alldone = false;",
            "",
            "\tlockdep_assert_held(&rcu_state.barrier_lock);",
            "\tif (rcu_seq_state(lseq) || !rcu_seq_state(gseq) || rcu_seq_ctr(lseq) != rcu_seq_ctr(gseq))",
            "\t\treturn;",
            "\trcu_barrier_trace(TPS(\"IRQ\"), -1, rcu_state.barrier_sequence);",
            "\trdp->barrier_head.func = rcu_barrier_callback;",
            "\tdebug_rcu_head_queue(&rdp->barrier_head);",
            "\trcu_nocb_lock(rdp);",
            "\t/*",
            "\t * Flush bypass and wakeup rcuog if we add callbacks to an empty regular",
            "\t * queue. This way we don't wait for bypass timer that can reach seconds",
            "\t * if it's fully lazy.",
            "\t */",
            "\twas_alldone = rcu_rdp_is_offloaded(rdp) && !rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));",
            "\twake_nocb = was_alldone && rcu_segcblist_pend_cbs(&rdp->cblist);",
            "\tif (rcu_segcblist_entrain(&rdp->cblist, &rdp->barrier_head)) {",
            "\t\tatomic_inc(&rcu_state.barrier_cpu_count);",
            "\t} else {",
            "\t\tdebug_rcu_head_unqueue(&rdp->barrier_head);",
            "\t\trcu_barrier_trace(TPS(\"IRQNQ\"), -1, rcu_state.barrier_sequence);",
            "\t}",
            "\trcu_nocb_unlock(rdp);",
            "\tif (wake_nocb)",
            "\t\twake_nocb_gp(rdp, false);",
            "\tsmp_store_release(&rdp->barrier_seq_snap, gseq);",
            "}",
            "static void rcu_barrier_handler(void *cpu_in)",
            "{",
            "\tuintptr_t cpu = (uintptr_t)cpu_in;",
            "\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\tlockdep_assert_irqs_disabled();",
            "\tWARN_ON_ONCE(cpu != rdp->cpu);",
            "\tWARN_ON_ONCE(cpu != smp_processor_id());",
            "\traw_spin_lock(&rcu_state.barrier_lock);",
            "\trcu_barrier_entrain(rdp);",
            "\traw_spin_unlock(&rcu_state.barrier_lock);",
            "}",
            "void rcu_barrier(void)",
            "{",
            "\tuintptr_t cpu;",
            "\tunsigned long flags;",
            "\tunsigned long gseq;",
            "\tstruct rcu_data *rdp;",
            "\tunsigned long s = rcu_seq_snap(&rcu_state.barrier_sequence);",
            "",
            "\trcu_barrier_trace(TPS(\"Begin\"), -1, s);",
            "",
            "\t/* Take mutex to serialize concurrent rcu_barrier() requests. */",
            "\tmutex_lock(&rcu_state.barrier_mutex);",
            "",
            "\t/* Did someone else do our work for us? */",
            "\tif (rcu_seq_done(&rcu_state.barrier_sequence, s)) {",
            "\t\trcu_barrier_trace(TPS(\"EarlyExit\"), -1, rcu_state.barrier_sequence);",
            "\t\tsmp_mb(); /* caller's subsequent code after above check. */",
            "\t\tmutex_unlock(&rcu_state.barrier_mutex);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Mark the start of the barrier operation. */",
            "\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\trcu_seq_start(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\trcu_barrier_trace(TPS(\"Inc1\"), -1, rcu_state.barrier_sequence);",
            "",
            "\t/*",
            "\t * Initialize the count to two rather than to zero in order",
            "\t * to avoid a too-soon return to zero in case of an immediate",
            "\t * invocation of the just-enqueued callback (or preemption of",
            "\t * this task).  Exclude CPU-hotplug operations to ensure that no",
            "\t * offline non-offloaded CPU has callbacks queued.",
            "\t */",
            "\tinit_completion(&rcu_state.barrier_completion);",
            "\tatomic_set(&rcu_state.barrier_cpu_count, 2);",
            "\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "",
            "\t/*",
            "\t * Force each CPU with callbacks to register a new callback.",
            "\t * When that callback is invoked, we will know that all of the",
            "\t * corresponding CPU's preceding callbacks have been invoked.",
            "\t */",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "retry:",
            "\t\tif (smp_load_acquire(&rdp->barrier_seq_snap) == gseq)",
            "\t\t\tcontinue;",
            "\t\traw_spin_lock_irqsave(&rcu_state.barrier_lock, flags);",
            "\t\tif (!rcu_segcblist_n_cbs(&rdp->cblist)) {",
            "\t\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"NQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\tif (!rcu_rdp_cpu_online(rdp)) {",
            "\t\t\trcu_barrier_entrain(rdp);",
            "\t\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\t\trcu_barrier_trace(TPS(\"OfflineNoCBQ\"), cpu, rcu_state.barrier_sequence);",
            "\t\t\tcontinue;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore(&rcu_state.barrier_lock, flags);",
            "\t\tif (smp_call_function_single(cpu, rcu_barrier_handler, (void *)cpu, 1)) {",
            "\t\t\tschedule_timeout_uninterruptible(1);",
            "\t\t\tgoto retry;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(READ_ONCE(rdp->barrier_seq_snap) != gseq);",
            "\t\trcu_barrier_trace(TPS(\"OnlineQ\"), cpu, rcu_state.barrier_sequence);",
            "\t}",
            "",
            "\t/*",
            "\t * Now that we have an rcu_barrier_callback() callback on each",
            "\t * CPU, and thus each counted, remove the initial count.",
            "\t */",
            "\tif (atomic_sub_and_test(2, &rcu_state.barrier_cpu_count))",
            "\t\tcomplete(&rcu_state.barrier_completion);",
            "",
            "\t/* Wait for all rcu_barrier_callback() callbacks to be invoked. */",
            "\twait_for_completion(&rcu_state.barrier_completion);",
            "",
            "\t/* Mark the end of the barrier operation. */",
            "\trcu_barrier_trace(TPS(\"Inc2\"), -1, rcu_state.barrier_sequence);",
            "\trcu_seq_end(&rcu_state.barrier_sequence);",
            "\tgseq = rcu_state.barrier_sequence;",
            "\tfor_each_possible_cpu(cpu) {",
            "\t\trdp = per_cpu_ptr(&rcu_data, cpu);",
            "",
            "\t\tWRITE_ONCE(rdp->barrier_seq_snap, gseq);",
            "\t}",
            "",
            "\t/* Other rcu_barrier() invocations can now safely proceed. */",
            "\tmutex_unlock(&rcu_state.barrier_mutex);",
            "}"
          ],
          "function_name": "rcu_barrier_trace, rcu_barrier_callback, rcu_barrier_entrain, rcu_barrier_handler, rcu_barrier",
          "description": "实现RCU屏障功能，通过分发回调函数强制所有CPU完成当前RCU操作，使用原子计数器跟踪完成状态，通过completion等待所有回调完成",
          "similarity": 0.5938913822174072
        },
        {
          "chunk_id": 10,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1840,
          "end_line": 1973,
          "content": [
            "static int __noreturn rcu_gp_kthread(void *unused)",
            "{",
            "\trcu_bind_gp_kthread();",
            "\tfor (;;) {",
            "",
            "\t\t/* Handle grace-period start. */",
            "\t\tfor (;;) {",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwait\"));",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_WAIT_GPS);",
            "\t\t\tswait_event_idle_exclusive(rcu_state.gp_wq,",
            "\t\t\t\t\t READ_ONCE(rcu_state.gp_flags) &",
            "\t\t\t\t\t RCU_GP_FLAG_INIT);",
            "\t\t\trcu_gp_torture_wait();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_DONE_GPS);",
            "\t\t\t/* Locking provides needed memory barrier. */",
            "\t\t\tif (rcu_gp_init())",
            "\t\t\t\tbreak;",
            "\t\t\tcond_resched_tasks_rcu_qs();",
            "\t\t\tWRITE_ONCE(rcu_state.gp_activity, jiffies);",
            "\t\t\tWARN_ON(signal_pending(current));",
            "\t\t\ttrace_rcu_grace_period(rcu_state.name, rcu_state.gp_seq,",
            "\t\t\t\t\t       TPS(\"reqwaitsig\"));",
            "\t\t}",
            "",
            "\t\t/* Handle quiescent-state forcing. */",
            "\t\trcu_gp_fqs_loop();",
            "",
            "\t\t/* Handle grace-period end. */",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANUP);",
            "\t\trcu_gp_cleanup();",
            "\t\tWRITE_ONCE(rcu_state.gp_state, RCU_GP_CLEANED);",
            "\t}",
            "}",
            "static void rcu_report_qs_rsp(unsigned long flags)",
            "\t__releases(rcu_get_root()->lock)",
            "{",
            "\traw_lockdep_assert_held_rcu_node(rcu_get_root());",
            "\tWARN_ON_ONCE(!rcu_gp_in_progress());",
            "\tWRITE_ONCE(rcu_state.gp_flags,",
            "\t\t   READ_ONCE(rcu_state.gp_flags) | RCU_GP_FLAG_FQS);",
            "\traw_spin_unlock_irqrestore_rcu_node(rcu_get_root(), flags);",
            "\trcu_gp_kthread_wake();",
            "}",
            "static void rcu_report_qs_rnp(unsigned long mask, struct rcu_node *rnp,",
            "\t\t\t      unsigned long gps, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long oldmask = 0;",
            "\tstruct rcu_node *rnp_c;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\t/* Walk up the rcu_node hierarchy. */",
            "\tfor (;;) {",
            "\t\tif ((!(rnp->qsmask & mask) && mask) || rnp->gp_seq != gps) {",
            "",
            "\t\t\t/*",
            "\t\t\t * Our bit has already been cleared, or the",
            "\t\t\t * relevant grace period is already over, so done.",
            "\t\t\t */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\tWARN_ON_ONCE(oldmask); /* Any child must be all zeroed! */",
            "\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp) &&",
            "\t\t\t     rcu_preempt_blocked_readers_cgp(rnp));",
            "\t\tWRITE_ONCE(rnp->qsmask, rnp->qsmask & ~mask);",
            "\t\ttrace_rcu_quiescent_state_report(rcu_state.name, rnp->gp_seq,",
            "\t\t\t\t\t\t mask, rnp->qsmask, rnp->level,",
            "\t\t\t\t\t\t rnp->grplo, rnp->grphi,",
            "\t\t\t\t\t\t !!rnp->gp_tasks);",
            "\t\tif (rnp->qsmask != 0 || rcu_preempt_blocked_readers_cgp(rnp)) {",
            "",
            "\t\t\t/* Other bits still set at this level, so done. */",
            "\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\t\treturn;",
            "\t\t}",
            "\t\trnp->completedqs = rnp->gp_seq;",
            "\t\tmask = rnp->grpmask;",
            "\t\tif (rnp->parent == NULL) {",
            "",
            "\t\t\t/* No more levels.  Exit loop holding root lock. */",
            "",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\trnp_c = rnp;",
            "\t\trnp = rnp->parent;",
            "\t\traw_spin_lock_irqsave_rcu_node(rnp, flags);",
            "\t\toldmask = READ_ONCE(rnp_c->qsmask);",
            "\t}",
            "",
            "\t/*",
            "\t * Get here if we are the last CPU to pass through a quiescent",
            "\t * state for this grace period.  Invoke rcu_report_qs_rsp()",
            "\t * to clean up and start the next grace period if one is needed.",
            "\t */",
            "\trcu_report_qs_rsp(flags); /* releases rnp->lock. */",
            "}",
            "static void __maybe_unused",
            "rcu_report_unblock_qs_rnp(struct rcu_node *rnp, unsigned long flags)",
            "\t__releases(rnp->lock)",
            "{",
            "\tunsigned long gps;",
            "\tunsigned long mask;",
            "\tstruct rcu_node *rnp_p;",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "\tif (WARN_ON_ONCE(!IS_ENABLED(CONFIG_PREEMPT_RCU)) ||",
            "\t    WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)) ||",
            "\t    rnp->qsmask != 0) {",
            "\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\t\treturn;  /* Still need more quiescent states! */",
            "\t}",
            "",
            "\trnp->completedqs = rnp->gp_seq;",
            "\trnp_p = rnp->parent;",
            "\tif (rnp_p == NULL) {",
            "\t\t/*",
            "\t\t * Only one rcu_node structure in the tree, so don't",
            "\t\t * try to report up to its nonexistent parent!",
            "\t\t */",
            "\t\trcu_report_qs_rsp(flags);",
            "\t\treturn;",
            "\t}",
            "",
            "\t/* Report up the rest of the hierarchy, tracking current ->gp_seq. */",
            "\tgps = rnp->gp_seq;",
            "\tmask = rnp->grpmask;",
            "\traw_spin_unlock_rcu_node(rnp);\t/* irqs remain disabled. */",
            "\traw_spin_lock_rcu_node(rnp_p);\t/* irqs already disabled. */",
            "\trcu_report_qs_rnp(mask, rnp_p, gps, flags);",
            "}"
          ],
          "function_name": "rcu_gp_kthread, rcu_report_qs_rsp, rcu_report_qs_rnp, rcu_report_unblock_qs_rnp",
          "description": "实现RCU grace period的主线程循环，处理grace period启动、强制quiescent状态报告及结束逻辑，通过锁和状态标志协调各子系统同步",
          "similarity": 0.5857332348823547
        },
        {
          "chunk_id": 6,
          "file_path": "kernel/rcu/tree.c",
          "start_line": 1221,
          "end_line": 1330,
          "content": [
            "static bool __note_gp_changes(struct rcu_node *rnp, struct rcu_data *rdp)",
            "{",
            "\tbool ret = false;",
            "\tbool need_qs;",
            "\tconst bool offloaded = rcu_rdp_is_offloaded(rdp);",
            "",
            "\traw_lockdep_assert_held_rcu_node(rnp);",
            "",
            "\tif (rdp->gp_seq == rnp->gp_seq)",
            "\t\treturn false; /* Nothing to do. */",
            "",
            "\t/* Handle the ends of any preceding grace periods first. */",
            "\tif (rcu_seq_completed_gp(rdp->gp_seq, rnp->gp_seq) ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) {",
            "\t\tif (!offloaded)",
            "\t\t\tret = rcu_advance_cbs(rnp, rdp); /* Advance CBs. */",
            "\t\trdp->core_needs_qs = false;",
            "\t\ttrace_rcu_grace_period(rcu_state.name, rdp->gp_seq, TPS(\"cpuend\"));",
            "\t} else {",
            "\t\tif (!offloaded)",
            "\t\t\tret = rcu_accelerate_cbs(rnp, rdp); /* Recent CBs. */",
            "\t\tif (rdp->core_needs_qs)",
            "\t\t\trdp->core_needs_qs = !!(rnp->qsmask & rdp->grpmask);",
            "\t}",
            "",
            "\t/* Now handle the beginnings of any new-to-this-CPU grace periods. */",
            "\tif (rcu_seq_new_gp(rdp->gp_seq, rnp->gp_seq) ||",
            "\t    unlikely(READ_ONCE(rdp->gpwrap))) {",
            "\t\t/*",
            "\t\t * If the current grace period is waiting for this CPU,",
            "\t\t * set up to detect a quiescent state, otherwise don't",
            "\t\t * go looking for one.",
            "\t\t */",
            "\t\ttrace_rcu_grace_period(rcu_state.name, rnp->gp_seq, TPS(\"cpustart\"));",
            "\t\tneed_qs = !!(rnp->qsmask & rdp->grpmask);",
            "\t\trdp->cpu_no_qs.b.norm = need_qs;",
            "\t\trdp->core_needs_qs = need_qs;",
            "\t\tzero_cpu_stall_ticks(rdp);",
            "\t}",
            "\trdp->gp_seq = rnp->gp_seq;  /* Remember new grace-period state. */",
            "\tif (ULONG_CMP_LT(rdp->gp_seq_needed, rnp->gp_seq_needed) || rdp->gpwrap)",
            "\t\tWRITE_ONCE(rdp->gp_seq_needed, rnp->gp_seq_needed);",
            "\tif (IS_ENABLED(CONFIG_PROVE_RCU) && READ_ONCE(rdp->gpwrap))",
            "\t\tWRITE_ONCE(rdp->last_sched_clock, jiffies);",
            "\tWRITE_ONCE(rdp->gpwrap, false);",
            "\trcu_gpnum_ovf(rnp, rdp);",
            "\treturn ret;",
            "}",
            "static void note_gp_changes(struct rcu_data *rdp)",
            "{",
            "\tunsigned long flags;",
            "\tbool needwake;",
            "\tstruct rcu_node *rnp;",
            "",
            "\tlocal_irq_save(flags);",
            "\trnp = rdp->mynode;",
            "\tif ((rdp->gp_seq == rcu_seq_current(&rnp->gp_seq) &&",
            "\t     !unlikely(READ_ONCE(rdp->gpwrap))) || /* w/out lock. */",
            "\t    !raw_spin_trylock_rcu_node(rnp)) { /* irqs already off, so later. */",
            "\t\tlocal_irq_restore(flags);",
            "\t\treturn;",
            "\t}",
            "\tneedwake = __note_gp_changes(rnp, rdp);",
            "\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);",
            "\trcu_strict_gp_check_qs();",
            "\tif (needwake)",
            "\t\trcu_gp_kthread_wake();",
            "}",
            "void rcu_gp_slow_register(atomic_t *rgssp)",
            "{",
            "\tWARN_ON_ONCE(rcu_gp_slow_suppress);",
            "",
            "\tWRITE_ONCE(rcu_gp_slow_suppress, rgssp);",
            "}",
            "void rcu_gp_slow_unregister(atomic_t *rgssp)",
            "{",
            "\tWARN_ON_ONCE(rgssp && rgssp != rcu_gp_slow_suppress && rcu_gp_slow_suppress != NULL);",
            "",
            "\tWRITE_ONCE(rcu_gp_slow_suppress, NULL);",
            "}",
            "static bool rcu_gp_slow_is_suppressed(void)",
            "{",
            "\tatomic_t *rgssp = READ_ONCE(rcu_gp_slow_suppress);",
            "",
            "\treturn rgssp && atomic_read(rgssp);",
            "}",
            "static void rcu_gp_slow(int delay)",
            "{",
            "\tif (!rcu_gp_slow_is_suppressed() && delay > 0 &&",
            "\t    !(rcu_seq_ctr(rcu_state.gp_seq) % (rcu_num_nodes * PER_RCU_NODE_PERIOD * delay)))",
            "\t\tschedule_timeout_idle(delay);",
            "}",
            "void rcu_gp_set_torture_wait(int duration)",
            "{",
            "\tif (IS_ENABLED(CONFIG_RCU_TORTURE_TEST) && duration > 0)",
            "\t\tWRITE_ONCE(sleep_duration, duration);",
            "}",
            "static void rcu_gp_torture_wait(void)",
            "{",
            "\tunsigned long duration;",
            "",
            "\tif (!IS_ENABLED(CONFIG_RCU_TORTURE_TEST))",
            "\t\treturn;",
            "\tduration = xchg(&sleep_duration, 0UL);",
            "\tif (duration > 0) {",
            "\t\tpr_alert(\"%s: Waiting %lu jiffies\\n\", __func__, duration);",
            "\t\tschedule_timeout_idle(duration);",
            "\t\tpr_alert(\"%s: Wait complete\\n\", __func__);",
            "\t}",
            "}"
          ],
          "function_name": "__note_gp_changes, note_gp_changes, rcu_gp_slow_register, rcu_gp_slow_unregister, rcu_gp_slow_is_suppressed, rcu_gp_slow, rcu_gp_set_torture_wait, rcu_gp_torture_wait",
          "description": "提供慢速模式下的grace period注册/注销接口，通过原子变量控制抑制状态，并实现基于延迟的调度控制逻辑，支持严格模式下的边界检查。",
          "similarity": 0.5738465785980225
        }
      ]
    }
  ]
}